,timestamp,content
0,2022-09-04,"With the rise of global action around data privacy and protection laws, researchers need to think about how participant privacy is maintained before, during, and after a research study. While the data collected is valuable to researchers, it is even more important to the participants in a research study. It is our responsibility to make sure we do not violate research- ethics principles and we respect participant involvement every step of the way.
Why Protecting Participant Data Is Important
As data collection and dependence on the Internet have risen, so have data breaches and cyber threats. Research participants are more vulnerable than ever as researchers conduct remote usability tests, use third party applications, or store and share data online. In 2021, according to Politico, nearly 50 million people in the U.S. faced a health-data breach. Data breaches pose huge privacy and security concerns for consumers and cost the health-care industry billions of dollars. To avoid such data losses and privacy infringements, regardless of industry, we need to develop privacy and security practices that seamlessly fit into the user-research process.
Key Terms
Data-privacy terminology isn’t always the easiest to understand. Before discussing best practices for data privacy and protection, we need to define a few terms.

Encryption is the process of scrambling data or converting it into code. Encrypted formats make the data unreadable, which is important when transferring and sharing data online. Encrypted data comes with a key provided by the sender. This often takes the form of a password or passphrase.
Confidentiality vs. anonymity: These terms are often used interchangeably but understanding the difference between them is critical for protecting research participants. Confidentiality is the state of keeping information private. When data is confidential, it means that it is known and associated with a particular participant, but that information is not shared with others. In contrast, the information is anonymous if it cannot be linked back to the identity of the participant who was the source of that information.
Informed consent is the process through which research participants (1) are informed of what data will be collected and how it will be used and (2) agree to these terms.

6 Best Practices for Protecting Participant Data in User Research
While data-protection efforts should be considered carefully for each study, below you will find a list of foundational best practices to follow before, during, and after a study.
Before Data Collection Begins
1. Establish a data-management process.
When it comes to data, you want to be proactive.  Create guidelines about how data should be collected, stored, protected, and shared with others. Then make sure you communicate these to all the members of a team. A designated editor can update the guidelines as laws or company policies change. Guidelines should include information about:

Consent forms and what they should include
Storing and sharing participant data
Deleting data when a study is complete
A plan in case there is a data breach

2. Develop a data-collection plan for preserving participants’ confidentiality.
Before collecting your data, understand the laws and regulations that require data to be confidential. Laws are dependent on where you live, but a good place to start is European Union’s General Data Protection Regulation (GDPR), which is considered the strictest privacy and security law in the world. While laws and regulations are complex, they exist to minimize the risk of data breaches and cyber threats. Researchers need to follow the law and use it as a guiding framework when developing a data-collection plan. Your data-collection plan should include what data will be collected, how it will be used, and who it will potentially be shared with. Developing a data-collection plan around confidentiality requires researchers to ask themselves the following questions:

Do I need to collect this identifiable data? Will this data affect my results?  As a researcher, you need to be able to justify the identifiable data you are collecting. A good way to do this is to make a list of identifiers you are collecting and describe why you need each and how it will be used. This information will be helpful for creating participant-consent forms, as well.
What tools should I use to collect data safely and securely? When multiple people are conducting research, you often end up with multiple tools and, as a result, multiple ways in which data is stored. Teams need to decide which tools to use before data collection, to avoid having data stored in multiple places and, thus, manage the risk associated with a data breach.

3. Informed Consent
Informed consent creates a two-way street between the researcher and the participant before the start of data collection. Researchers inform participants about what their involvement in the study entails, what data will be collected, and how it will be used. Then, participants are given the opportunity to make an informed decision about their involvement based on the information that is provided. This communication is typically presented as a consent form, which should include the following:

Information about the study and the activities involved
What data will be collected
How the data collected will be used
Steps researchers will take to secure their data

During and After Data Collection
4. Maintain participant anonymity. 
Anonymity should be preserved while taking notes, while cleaning data and preparing it for storage, or while disseminating results. Qualitative researchers need to pay close attention to how they present participants’ personal details. While they may not be using names and other key identifiers, personal information can still be deduced based on individual or group traits represented in the data. With that in mind, research teams should follow these best practices:

Be intentional about the data they collect and ask only for information they really need from participants.
Do not use participant names or any other key identifies (e.g., Social Security number, date of birth) in notes and file names.
In usability testing, pause the recording while participants type in usernames, passwords, addresses, or other identifying information. You can also consider using fake credentials if appropriate for the study.
If, in spite of precautions, a video or audio recording does include identifying information, make sure to delete that part of the recording or blur it as soon as possible.

5. Share files in a secure way with only those people who need them.
In general, researchers should control who has access to data and when they have access to it. Access should only be given to people who actually need it. Researchers should not collect and share data on cloud-storage services like Google Drive, Dropbox, and One Drive. The challenging part about cloud storage is that you don’t have complete control over data. For example, if cloud services are down or hacked, you don’t have the control to fix the issue. While using cloud services makes it easy to share information, researchers do not have complete control over data stored in the cloud and further expose research participants to data breaches and cyber threats. Instead, researchers should consider using an external drive to storing encrypted data and find more-secure ways to share data (e.g., using secure file-transfer services like Hightail).  
6. Delete data that is no longer needed.
It can be tempting to hold on to data but one of the best ways to protect data is to delete it once you are done with it. This practice reduces data-breach risks. There will also be instances where your client or participant wants data permanently removed. To make this as easy as possible, researchers should have:

A clear way to identify and retrieve a participant’s individual data. For example, researchers could follow a standard format for how they name data files and folders. This practice makes it easier to navigate to the appropriate folder with all the data from that participant. If you include participant data in a presentation or other deliverables, you could use tags in data files to indicate where that data point was used externally.
No duplications of data in multiple places
A consistent way of storing data so it can be found easily in the case it needs to be deleted immediately. Researchers shouldn’t be searching for places where the data files could be. They should already know where it is. Consistency helps researchers keep track of the data effectively and efficiently.

Applying Best Practices
Applying privacy and security best practices shouldn’t involve extra work. Rather, it should fit seamlessly into the 5 steps of the user-research process:

Develop a research plan
Recruit participants
Conduct research
Synthesize Data
Share Data



How privacy and security best practices can be integrated into the 5-step user-research process




How privacy and security best practices can be integrated into the 5-step user-research process


To heighten the value and impact of privacy and security of research participants at scale, these best practices should be implemented into existing Research Operations (ResearchOps). ResearchOps streamlines dedicated roles and efforts toward managing operational aspects associated with privacy and security. Building in these practices and finding ways to operationalize them allows researchers to spend more time conducting studies and uncovering insights at scale, in a safe and secure way.
Conclusion
Maintaining participant data privacy and security should be a priority for all researchers. As researchers continue to develop new technologies and work with more research participants to do so, following these best practices is an important place to start.
Reference
Kaiser, K., 2009. Protecting Respondent Confidentiality in Qualitative Research. [ebook] Chicago: Qualitative Health Research. (Nov. 2009), DOI: https://doi.org/10.1177/1049732309350879
 "
1,2022-08-21,"

















For further detail, see When to Use Which User-Experience Research Methods."
2,2022-07-24,"The funnel technique has been around since qualitative interviews emerged as a research method. This technique involves asking broad open-ended questions before gradually introducing more narrowly-scoped open-ended questions, as well as closed questions. 
This idea of starting broad before getting more specific is valuable in other types of studies besides user interviews. This technique can help you organize:

Interview questions
Followup questions
Usability tasks
Research in a multimethod study

This article discusses how the funnel technique can be used in user interviews and in moderated usability tests. 
Why Is It Called the Funnel Technique?
A funnel is broad at the top and narrow at the bottom, which makes it easy to pour a substance (for example, oil or rice) into a container with a narrow opening (for example, a bottle or jar). 
Similarly, the funnel technique in user research involves moving from broad to narrow — in other words, from general to specific. The funnel is a fitting metaphor because a user interview or usability-test session should start with broad, exploratory questions or tasks before introducing specific, narrowly scoped questions or tasks. 
The purpose of the funnel technique is to avoid influencing user behavior or perceptions as much as possible. When we ask specific questions or give specific tasks too early in a research session, we risk introducing bias and missing important data.


A funnel starts broad and gradually gets narrower. In qualitative interviews and usability tests, starting with broad, exploratory questions or tasks can generate lots of unbiased information about users’ natural thoughts and behaviors. As we introduce specific questions or tasks, we gather less information but more specific details.


The Funnel Technique in User Interviews
An interview guide for a user interview will usually consist of 5–8 open-ended questions that get participants to share relevant stories or experiences. These are followed by followup questions, which can be open-ended or closed.
An interview should start with broad, open-ended questions like Tell me about the last time you ordered movie tickets.
Starting with broad open-ended questions:

Gets the participant comfortable with talking
Allows the participant to begin sharing stories
Generates lots of new, unanticipated information
Avoids the researcher priming the participant

After the participant responds, the interviewer should ask open-ended followup questions (known as probing questions) to dig into areas of the participant’s response that the researcher wants to learn more about. These include questions like:

Can you expand on that?
What do you mean by that?
How do you feel about that?
Why do you think that?

The interviewer may also want to gather additional information and details that the participant omitted, by introducing closed questions, such as: 

When did you see this movie?
Did you go alone?
How long was the movie?

For each main question prepared in the interview guide, the interviewer is moving from broad open-ended questions to closed questions. With each new question in the guide, the funneling process repeats.


Each main interview question is usually broad and open-ended; it is followed by open-ended followup questions and gradually more closed questions. With each new interview question, the funnel process repeats.


When we make use of the funnel technique, we allow space for us to learn new things before gathering detail. This approach to interviewing is the best way to build rapport and learn what users really think without priming them.
The Funnel Technique in Qualitative Usability Tests
In qualitative usability testing, funneling can be used in: 

Creating and ordering tasks
Devising the sequence of followup questions after a participant completes a task

Funneling Tasks
The same process of moving from broad to specific can apply to constructing and ordering tasks for a qualitative usability test. Broad exploratory tasks — which allow us to learn how people do things on their own — should be given before specific, directed tasks that ask users to find or do something that they might not otherwise do.
Another metaphor for the funnel technique is a mouse in a maze. Imagine a researcher in a lab wants to see if a mouse can find a piece of cheese in a large maze. They might start off by giving the mouse the entire maze and seeing whether it finds the cheese. After some time, if the mouse hasn’t arrived at the cheese, they could slowly start closing off incorrect pathways, increasing the chances that the mouse will find the cheese.



Applying the funnel technique can work a bit like guiding a mouse in a maze — we're first seeing if the mouse can find its way to the cheese on its own, but then closing off some pathways as needed.






Applying the funnel technique can work a bit like guiding a mouse in a maze — we're first seeing if the mouse can find its way to the cheese on its own, but then closing off some pathways as needed.



In this metaphor, the mouse is the participant, and the cheese is the aspect of the design that we’re interested in studying. It’s better to first see if our participants get there on their own, but we might have to nudge them a bit.




Task to give the participant


Rationale




Task 1: Using the site, see if there’s a recipe you might like to cook.


Broad exploratory task: We’re allowing the user to define the end point of the task and complete it in their own way.




Task 2: Use the site to find an easy recipe you might like to cook for dinner.


Directed task: This task has a defined end point and success criterion. We’re asking participants to find something that they might not otherwise look for.




Task 3: Using the search, find a recipe for chicken noodle soup.


Directed task: This task asks users to use a specific feature to do a task. We’re asking users to find something they might not otherwise look for and use a feature they might not use otherwise.




If we started with task 3, and moved in the reverse order, we would prime participants on how to behave in task 1 and task 2. It’s likely that we would miss out on learning how they would do things on their own without prompting. Funneling tasks, by starting with broad exploratory tasks before introducing directed tasks, ensures you get valid data about behavior.


Starting with broad exploratory tasks helps us learn about natural behavior, while directed tasks give us less information about how users would do something but more about a specific UI element.


The funnel technique is the idea behind stepped tasks. Stepped tasks are multistep tasks that start broad and then provide more specific instructions as necessary. 
For example, let’s imagine that we’re interested in studying the comparison feature on an ecommerce site. 




Task to give the participant


Rationale




Task 4: Choose a pair of headphones you might like to buy and add them to your cart.


Broad task: We’re giving participants a very natural task and hoping that they happen to use the feature we’re interested in along the way.




Task 4.1:  Your friend is considering buying the Apple Airpods or Apple Airpods Pro but isn’t sure which one to get. Use the site to decide which one you’d recommend to your friend and why.


If the participant doesn’t naturally discover and use the comparison feature on their own, we might give them a followup instruction like this. The goal is to put the participant in a more specific situation that might be less natural but will encourage them to use the feature.




Task 4.2: Can you find a way to compare those two headphones without having to switch back and forth between pages?


If the participant still doesn’t engage with the feature of interest, we might get even more direct. In this example, we’re strongly hinting that the comparison feature exists and asking the participant to use it. 




If we get to Task 4.2 in the stepped task example above, we might conclude that there are some discoverability or desirability issues with the feature. However, providing this level of specificity would allow us to make those conclusions while also getting a chance to see if there are any interaction-design problems with the feature itself.
Funneling in Followup Questions
In qualitative usability tests, the facilitator will make use of the funnel technique when asking followup questions. For example, we might ask the following questions after the participant has finished a task:




Question to give the participant


Rationale




Question 1: Do you have any thoughts about doing this activity on the website?


This broad, open-ended question allows participants to share anything related to the task they have completed.




Question 2: Was there anything easy or difficult?


This more specific open-ended question is focused on the ease of use. Here we’re prompting users to recall anything they might have liked using or had difficulty doing.




Question 3: What did you think about the filters you were using?


This open-ended question is even more specific. Here, we’re drawing the user’s attention to a specific UI element that they used in the task.




In the above example, we are starting with broad, open-ended questions before narrowing the scope to specific, open-ended questions that relate to a UI element that we want more feedback on. 
We shouldn’t start with Question 3 and move in the reverse direction, because it’s always better if participants volunteer information unprompted. We’re hoping the participant will organically provide feedback without our specific direction, because unsolicited feedback is more likely to be the participant’s true opinion. Asking participants for feedback on specific elements can be risky — there’s always a chance that the participant will simply make up an opinion for the researcher. For example, if the participant said the filters were really helpful in response to question 1, we can be more confident that they really think so than if they gave us a similar response to question 3.
Summary
The funnel technique can be used when administering questions or tasks in user interviews or usability tests. Start with broad open-ended questions or tasks before introducing more specific questions or tasks. This approach helps to ensure that you don’t miss out on important information and you avoid prompting or priming participants too early.
 
To learn more about interviews and moderating usability tests, explore our 5-day course on Qualitative Research or our 1-day courses on User Interviews and Usability Testing."
3,2022-07-17,"The field of user experience has a wide range of research methods available, ranging from tried-and-true methods such as lab-based usability testing to those that have been more recently developed, such as unmoderated UX assessments.
While it's not realistic to use the full set of methods on a given project, nearly all projects would benefit from multiple research methods and from combining insights. Unfortunately, many design teams only use one or two methods that they are most familiar with. The key question is what to use when. To better understand when to use which method, it is helpful to view them along a 3-dimensional framework with the following axes:

Attitudinal vs. Behavioral
Qualitative vs. Quantitative
Context of Use

The following chart illustrates where 20 popular methods appear along these dimensions:

Each dimension provides a way to distinguish among studies in terms of the questions they answer and the purposes they are most suited for. The methods placed in the middle of the quantitative–qualitative axis can be used to gather both qualitative and quantitative data.

The Attitudinal vs. Behavioral Dimension
This distinction can be summed up by contrasting ""what people say"" versus ""what people do"" (very often the two are quite different). The purpose of attitudinal research is usually to understand or measure people's stated beliefs, but it is limited by what people are aware of and willing to report.
While most usability studies should rely on behavior, methods that use self-reported information can still be quite useful to designers. For example, card sorting provides insights about users' mental model of an information space and can help determine the best information architecture for your product, application, or website. Surveys measure and categorize attitudes or collect self-reported data that can help track or discover important issues to address. Focus groups tend to be less useful for usability purposes, for a variety of reasons, but can provide a top-of-mind view of what people think about a brand or product concept in a group setting.
On the other end of this dimension, methods that focus mostly on behavior seek to understand ""what people do"" with the product or service in question. For example A/B testing presents changes to a site's design to random samples of site visitors but attempts to hold all else constant, in order to see the effect of different site-design choices on behavior, while eyetracking seeks to understand how users visually interact with a design or visual stimulus.
Between these two extremes lie the two most popular methods we use: usability studies and field studies. They utilize a mixture of self-reported and behavioral data and can move toward either end of this dimension, though leaning toward the behavioral side is generally recommended.
The Qualitative vs. Quantitative Dimension
The distinction here is an important one and goes well beyond the narrow view of qualitative as in an open-ended survey question. Rather, studies that are qualitative in nature generate data about behaviors or attitudes based on observing or hearing them directly, whereas in quantitative studies, the data about the behavior or attitudes in question are gathered indirectly, through a measurement or an instrument such as a survey or an analytics tool. In field studies and usability testing, for example, researchers directly observe how people use (or do not use) technology to meet their needs or to complete tasks. These observations give them the ability to ask questions, probe on behavior, or possibly even adjust the study protocol to better meet study objectives. Analysis of the data is usually not mathematical.
In contrast, the kind of data collected in quantitative methods is predetermined — it could include task time, success, whether the user has clicked on a given UI element or whether they selected a certain answer to a multiple-choice question. The insights in quantitative methods are typically derived from mathematical analysis, since the instrument of data collection (e.g., survey tool or analytics tool) captures such large amounts of data that are automatically coded numerically.
Due to the nature of their differences, qualitative methods are much better suited for answering questions about why or how to fix a problem, whereas quantitative methods do a much better job answering how many and how much types of questions. Having such numbers helps prioritize resources, for example to focus on issues with the biggest impact. The following chart illustrates how the first two dimensions affect the types of questions that can be asked:

The Context of Product Use
The third distinction has to do with how and whether participants in the study are using the product or service in question. This can be described as:

Natural or near-natural use of the product
Scripted use of the product
Limited in which a limited form of the product is used to study a specific aspect of the user experience
Not using the product during the study (decontextualized)

When studying natural use of the product, the goal is to minimize interference from the study in order to understand behavior or attitudes as close to reality as possible. This provides greater external validity but less control over what topics you learn about. Many ethnographic field studies attempt to do this, though there are always some observation biases. Intercept surveys and data mining or other analytic techniques are quantitative examples of this.
A scripted study of product usage is done in order to focus the insights on specific product areas, such as a newly redesigned flow. The degree of scripting can vary quite a bit, depending on the study goals. For example, a benchmarking study is usually very tightly scripted, so that it can produce reliable usability metrics by ensuring consistency across participants.
Limited methods use a limited form of a product to study a specific or abstracted aspect of the experience. For example, participatory-design methods allow users to interact with and rearrange design elements that could be part of a product experience, in order discuss how their proposed solutions would meet their needs and why they made certain choices. Concept-testing methods employ an expression of the idea of a product or service that gets at the heart of what it would provide (and not at the details of the experience) in order to understand if users would want or need such a product or service.  Card sorting and tree testing focus on how the information architecture is or could be arranged to best make sense to participants and make navigation easier.
Studies where the product is not used are conducted to examine issues that are broader than usage and usability, such as a study of the brand or discovering the aesthetic attributes that participants associate with a specific design style.
Many of the methods in the chart can move along one or more dimensions, and some do so even in the same study, usually to satisfy multiple goals. For example, field studies can focus a little more on what people say (ethnographic interviews) or emphasize studying what they do (extended observations); concept testing, desirability studies, and card sorting have both qualitative and quantitative versions; and eyetracking can be natural or scripted.
Phases of Product Development (the Time Dimension)
Another important distinction to consider when making a choice among research methodologies is the phase of product development and its associated objectives.  For example, in the beginning of the product-development process, you are typically more interested in the strategic question of what direction to take the product, so methods at this stage are often generative in nature, because they help generate ideas and answers about which way to go.  Once a direction is selected, the design phase begins, so methods in this stage are well-described as formative, because they inform how you can improve the design.  After a product has been developed enough to measure it, it can be assessed against earlier versions of itself or competitors, and methods that do this are called summative. This following table describes where many methods map to these stages in time:




Product-Development Stage




Strategize


Design


Launch & Assess




Research goal: 
			Find new directions and opportunities


Research goal:
			Improve usability of design


Research goal:
			Measure product performance against itself or its competition




Generative research methods


Formative research methods


Summative research methods




Example methods




Field studies, diary studies, interviews, surveys, participatory design, concept testing


Card sorting, tree testing, usability testing, remote testing (moderated and unmoderated)


Usability benchmarking, unmoderated UX testing, A/B testing, clickstream / analytics, surveys




Art or Science?
While many user-experience research methods have their roots in scientific practice, their aims are not purely scientific and still need to be adjusted to meet stakeholder needs. This is why the characterizations of the methods here are meant as general guidelines, rather than rigid classifications.
In the end, the success of your work will be determined by how much of an impact it has on improving the user experience of the website or product in question. These classifications are meant to help you make the best choice at the right time.
20 UX Methods in Brief
Here’s a short description of the user research methods shown in the above chart:
Usability testing (aka usability-lab studies): Participants are brought into a lab, one-on-one with a researcher, and given a set of scenarios that lead to tasks and usage of specific interest within a product or service.
Field studies: Researchers study participants in their own environment (work or home), where they would most likely encounter the product or service being used in the most realistic or natural environment.
Contextual inquiry: Researchers and participants collaborate together in the participants own environment to inquire about and observe the nature of the tasks and work at hand. This method is very similar to a field study and was developed to study complex systems and in-depth processes.
Participatory design: Participants are given design elements or creative materials in order to construct their ideal experience in a concrete way that expresses what matters to them most and why.
Focus groups: Groups of 3–12 participants are led through a discussion about a set of topics, giving verbal and written feedback through discussion and exercises.
Interviews: a researcher meets with participants one-on-one to discuss in depth what the participant thinks about the topic in question.
Eyetracking: an eyetracking device is configured to precisely measure where participants look as they perform tasks or interact naturally with websites, applications, physical products, or environments.
Usability benchmarking: tightly scripted usability studies are performed with larger numbers of participants, using precise and predetermined measures of performance, usually with the goal of tracking usability improvements of a product over time or comparing with competitors.
Remote moderated testing: Usability studies are conducted remotely, with the use of tools such as video conferencing, screen-sharing software, and remote-control capabilities.
Unmoderated testing: An automated method that can be used in both quantitative and qualitative studies and that uses a specialized research tool to capture participant behaviors and attitudes, usually by giving participants goals or scenarios to accomplish with a site, app, or prototype. The tool can  record a video stream of each user session, and can gather usability metrics such as success rate, task time, and perceived ease of use.
Concept testing: A researcher shares an approximation of a product or service that captures the key essence (the value proposition) of a new concept or product in order to determine if it meets the needs of the target audience. It can be done one-on-one or with larger numbers of participants, and either in person or online.
Diary studies: Participants are using a mechanism (e.g., paper or digital diary, camera, smartphone app) to record and describe aspects of their lives that are relevant to a product or service or simply core to the target audience. Diary studies are typically longitudinal and can be done only for data that is easily recorded by participants.
Customer feedback: Open-ended and/or close-ended information is provided by a self-selected sample of users, often through a feedback link, button, form, or email.
Desirability studies: Participants are offered different visual-design alternatives and are expected to associate each alternative with a set of attributes selected from a closed list. These studies can be both qualitative and quantitative.
Card sorting: A quantitative or qualitative method that asks users to organize items into groups and assign categories to each group. This method helps create or refine the information architecture of a site by exposing users’ mental models.
Tree testing: A quantitative method of testing an information architecture to determine how easy it is to find items in the hierarchy. This method can be conducted on an existing information architecture to benchmark it and then again, after the information architecture is improved with card sorting, to demonstrate improvement.
Analytics: Analyzing data collected from user behavior like clicks, form filling, and other recorded interactions. It requires the site or application to be instrumented properly in advance.
Clickstream analytics:  A particular type of analytics that involves analyzing the sequence of pages that users visit as they use a site or software application.
A/B testing (aka multivariate testing, live testing, or bucket testing): A method of scientifically testing different designs on a site by randomly assigning groups of users to interact with each of the different designs and measuring the effect of these assignments on user behavior.
Surveys: A quantitative measure of attitudes through a series of questions, typically more closed-ended than open-ended.  A survey that is triggered during the use of a site or application is an intercept survey, often triggered by user behavior. More typically, participants are recruited from an email message or reached through some other channel such as social media.
In-Depth Course
More details about the methods and the dimensions of use in the full-day training course User Research Methods: From Strategy to Requirements to Design and the article A Guide to Using User-Experience Research Methods."
4,2022-07-03,"To ensure that user research provides meaningful data, UX teams must get feedback from representative customers. You can certainly reach out to a customer for a quick phone call about their experiences without calling it a research session or needing a consent form. However, as soon as you transcribe quotes from that call or share that person’s information with other people on your team, you are technically (mis)using that person’s data in a manner to which they did not agree.
In other words, by not informing your participants about how you will use their data and by misleading them into believing that they were having a private conversation, you may be violating research ethics principles and taking advantage of that person’s kindness and willingness to help.
Informed consent is an exchange of information in which both of the following must occur:

The researcher informs the participant about what involvement in the study entails and the potential consequences of participating.
The participant fully understands the terms of the study and can make an informed decision about whether they would like to voluntarily participate.

A research consent form (also known as an informed consent form) serves as written documentation of this exchange.
(A downloadable example of a research consent form can be found at the end of this article.)
Why Is Informed Consent Important?
While researchers may be well-intended in their hopes of learning about user behaviors, the intent does not matter nearly as much as the execution of the study and its impact. There are many documented cases of unethical research and its dangerous outcomes. One famous example of research abuse is the Nuremberg doctors’ trial. The doctors were placed on trial for war crimes, due to harmful and lethal experiments on inmates at Nazi concentration camps. This trial led to the creation of the Nuremberg Code, which is a set of ethics principles for human experimentation.
The Tuskegee syphilis study is also notorious for its lethal impact on more than 100 of the 400 African American participants, all of whom were not fully informed on the nature of the experiment, which studied the effects of untreated syphilis. This trial led to creation of the National Research Act and the Belmont Report, which outlines additional “ethical principles and guidelines for the protection of human subjects” for both biomedical and behavioral research.
To avoid further harm, most research institutions (like universities) have established institutional review boards (IRBs) that provide ethical oversight and approval authority on any planned research. Whether your organization has an IRB or not, informed consent is a fundamental part of an ethical research program that respects participants and does not take advantage of (or deceive) them.
Informed Consent Means Respecting and Protecting Participants
While most user research with digital products will typically be nonclinical in nature (and, thus, relatively low-risk), there is still risk to the participants' wellbeing, especially if the tasks they conduct during the study cause temporary hardship, distress, discomfort, or pain. This is particularly true for vulnerable populations who are already experiencing these things due to injury, abuse, trauma, or general misfortune. To be clear, we can still do research on sensitive topics or with vulnerable or disadvantaged populations, but we must take extra precautions and communicate the study’s potential impact in advance to protect participants’ wellbeing.
It’s worth noting that there are populations who are vulnerable or incapable of granting informed consent, due to the criteria of the second bullet point: the participant’s understanding and voluntary participation. For example, children/minors, adults with cognitive impairments, low or no literacy, or mentally incapacitated individuals (e.g., unconscious or under the influence of alcohol or drugs) may not be capable of fully understanding the terms of the study or lack the full agency needed to make voluntary decisions (at times relying upon guardians to make these decisions). Thus, these parties cannot offer informed consent on their own, and researchers need the consent of their guardian before conducting any research, no matter how low-risk.
Similarly, inmates and prisoners are already performing an involuntary task, so the second criteria, voluntary participation, is not always possible. The decisions with these populations are rarely fully voluntary, and voluntary withdrawal may not be seen as a valid option due to fear of authorities or retribution. Furthermore, there may be additional legal requirements like consent of a prisoner representative and approval from an IRB before any research is conducted.
Are Consent Forms Really Necessary?
Writing out consent forms may seem like unnecessary work when you could just tell the participant what the study will be like. However, written consent forms are critical for a number of reasons:

They consistently inform every participant with the same level of detail, whether they are the first or the 1,000th participant in the study and regardless of which researcher works with them. The written documentation serves as a paper trail to ensure that the researcher satisfactorily informed all participants and that all the key study details were covered every single time.
They hold researchers accountable to protect the welfare and wellbeing of study participants and their data. A participant may forget what they initially consented to, but a signed form empowers the participant to be in control of their participation and protect their own welfare, wellbeing, and data, should the researcher deviate from the original plan.
They prevent the research team (to some extent) from running a poor study. Granted, there can still be poor studies with consent forms, but consent-form writing helps researchers think through all the nuances of their study, including the activities the participant will do, the data that is truly needed, and how that data will be protected during and after the session.

That said, there are exceptions where a consent form is not necessary. A/B testing, for example, can be characterized as routine use of a product, and therefore, does not require additional involvement on the part of the participant. Plus, data from any single user is used only as part of a bigger statistic.
You may also not need a full-length consent form for an intercept study, which is the equivalent of interrupting someone while they are in the middle of a task to get their thoughts on that particular task. That said, you should still begin the intercept study with an agreement page containing a brief summary of how the information they provide will be used, stored, shared, and deleted. Also, if you are physically intercepting them in a public location, you should still get written permission from any physical establishment in which you are recruiting, to avoid getting in trouble for soliciting on a private property — which is illegal in many countries.
What Belongs in a Consent Form?
At the end of this article, you’ll find a consent-form template that you can repurpose for future research, but remember that your consent forms should be carefully considered and crafted based on the unique needs and format of each study.
 In general, a consent form should have the following information:

High-level purpose of the study
	Provide just enough detail to inform the participant about the nature of the study, without giving away any hypotheses or details that may skew behavior during the study. That said, this statement shouldn’t be as vague as “to learn about our customers.” Consider mentioning the objective of the research — for instance, to improve an existing product or strategize for future service development.
Format of the study (activities, tasks, and duration)
	Describe the types of activities and tasks the participant may complete. This is where you can say how the data will be collected and to what extent the participant can expect to interact with the researcher(s). With that, the duration of the study should be clearly stated. “Umbrella” consent forms in which the user consents to an indefinite timeframe of research for current and future studies can be very problematic in their lack of specificity and susceptibility to abuse (and, generally, participants probably will not want to participate in something that has no defined end point).
Voluntary-participation clause
	While research participants usually understand that they have volunteered to participate in the study, it may not always be clear that withdrawal is voluntary, or that it’s permissible for them to change their minds. Signing paperwork often can feel very “final,” so this clause helps to reassure that the participant is always in control of their participation.
Participant-data handling
	Explain how personal data, findings, and recordings (if applicable) will be handled by answering key questions like:
	
Will data be anonymized? If so, how much of it will be anonymized?
How will it be used, accessed, and stored? Who has access to this information?
Will it ever be published anywhere?
When will the raw, recorded data be deleted (if ever)?
Can the participant request that information is deleted?


Consent statement
	While many consent forms will simply have a signature block for participants to indicate consent or a single checkbox labeled, I consent to the terms of this study, this section should be carefully formatted and worded to instill confidence and give the participant agency and choices over their involvement.
Multiple checkboxes 
	Instead of requiring the participant to agree to a blanket statement, consider a modular consent structure, allowing participants to consent to certain parts or aspects of the study but not others. For example, participants may consent to participating in the study but may opt out of recordings or data publication. So, instead of I agree and consent to the terms of this study, understanding that our session may be recorded, your consent form may include a series of checkboxes:
	
I agree to the terms of this study and consent to participate.
I consent to audio being recorded during the session for analysis purposes.
I consent to video being recorded during the session for analysis purposes.



This modular consent form increases the likelihood that the participant will participate and contribute data to the study, even if they do not consent to recordings.
Other optional information to include, if applicable:

Compensation 
	If you will be providing compensation for the participants’ time, be it monetary or as free products or services, clearly state it in the consent form. Regardless which method you choose, make it clear that compensation is not based on the participants’ abilities or on whether they provide positive feedback. Especially during any form of “testing,” participants can sometimes feel like they are being tested for their abilities and performance, rather than the design.
	Tip: For in-person testing, it helps to reinforce this idea by paying participants at the beginning of the session, before tasks begin, right after they sign the consent form.
Parent or guardian involvement and approval
	If your participant is a minor or an adult with a guardian, a parent or guardian will likely accompany them into the session. Include a designated Parent or guardian signature block, to make it clear the signature belongs to a parent or guardian and that the participant was not coerced into signing. Any special considerations regarding studies with minors or other adults with guardians should be clearly communicated in the consent form.
Information (or debrief) sheet
	Some study formats may warrant an additional information sheet (separate from the signed consent form) which contains a more detailed description of the study’s purpose. Such an information sheet is usually given to the participant at the end of the study, to avoid priming them to behave differently during the study.

Lastly, regardless if you use a separate information sheet or not, you should always give the participant a copy of the signed consent form, so they can refer to it after the study. This ensures the participant knows what they signed and who to contact for further questions about the research.
Conclusion
Human-centered design begins with human-centered research, and, as such, improvements to a design should never come at the cost of another person’s wellbeing. Getting informed consent from participants is critical for researchers to carry out fair, transparent, and accurate research while doing their part to minimize harm. By writing a thorough consent form, researchers can think through important details of their study and build trust and confidence with participants.
Resources
Shuster, E. Fifty Years Later: The Significance of the Nuremberg Code. New England Journal of Medicine 337, 20 (1997), 1436-1440.
The Belmont Report. Office for Human Research Protections, 1979.
https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html.
"
5,2022-05-29,"There are many things to do and say when moderating a qualitative usability test and it is easy to forget essential items. Therefore, researchers often employ checklists or a formal facilitator’s guide. (You can find an example of a facilitator’s guide at the bottom of this article.)
Unlike a research plan or test plan, a facilitator’s guide is a document that is produced for and used solely by the researcher. It usually includes an introduction script, as well as checklists of items the researcher should not forget to do or say.
This article walks through 10 steps for moderating a usability test. If you’re in the study-planning stage, review our checklist for planning a usability test.
1. Welcome the Participant
When the participant arrives at an in-person or remote session, introduce yourself and thank them for volunteering to help with your research.
It’s important to put the participant at ease, since some participants may feel nervous and not sure what to expect. Avoid the word “test”, since it can make participants think they are being tested. (Remember: we’re not testing users; we’re testing the design!) Instead, describe the session as “research” or a “study”. Below is an example of an introduction.
Hello. It’s nice to meet you. My name is Maria and I work for Nielsen Norman Group. Thank you for agreeing to help with our research. In this session, I will be giving you a website to use and asking for your feedback.
2. Check Name Pronunciation
Sometimes you might need to stop the participant or get their attention during a session. Make sure you check with the participant how to pronounce their name, especially if the name is unfamiliar to you or could be pronounced in several ways. Make a note of the pronunciation if you need to. Using the participant’s name at the beginning of the session can build a bit of rapport and make the participant feel more at ease.
Before we get started, can I just check how to pronounce your name? Is it KEY-ARA or KEE-RA?
3. Inform the Participant About Observers and Recordings
Tell participants about observers and recordings during the recruitment stage, so they make an informed decision about whether they want to take part in your study. However, even if you have provided this information ahead of the session, remind participants of any observers or recordings you will be making, since that detail can be forgotten or missed in the recruitment materials.
I just want to let you know that a couple of my colleagues are observing the session today. They won’t be participating and will be taking notes for me. Would that be okay with you? As mentioned in the recruitment email, we would also like to record the session, but only with your permission.
4. Ask the Participant to Sign the Consent Form
In a remote session, sending a link to an online form via the chat feature is the easiest method of handling a consent form. In an in-person session, participants usually sign a paper version, but you can have the participant complete it electronically if you prefer.
Tell your participants to ask you if they have any questions before they sign it. Do not rush participants.
Before we begin, I’ll need you to sign a consent form. I’m pasting a link into the chat. Please take your time to read over it and please let me know if you have any questions. Once you’re ready, please feel free to sign it.
Once the participant has signed the consent form, check whether you have access to it (if it’s been submitted electronically) and whether it’s been completed correctly.
If the participant can opt in or out of various conditions of the research (like recording), make a note of their preferences before you proceed with the session. If the participant has agreed to a recording, start the recording.
5. Conduct a Short Interview (if Needed)
If you need to know a little about the participant’s background, perform a very short interview. This interview can help you tailor tasks during the session. For example, if you were going to test an ecommerce platform, you might ask about recent experiences the participant had shopping online. This information might help you tailor some of the tasks you give the participant during the session or choose tasks which will be appropriate.
If you do need to run an interview alongside a usability test, pilot your study to make sure you have time for both. Don’t ask questions which could prime participants into certain behaviors that could alter results of your test and compromise the study’s internal validity.  For example, if you used a specific term in one of your interview questions and this term later appeared in the interface, the participant may have been given a clue on what to look for when doing the usability test.
6. Manage Expectations & Introduce Think-Aloud Protocol (if Used)
It’s likely that your participant has not taken part in a usability test before, so set their expectations and communicate what you will and won’t be doing during the session and how you’d like them to behave. Before the test begins, inform your participant that:

You will be giving them tasks to do one at a time.
Some tasks will be short and others will be longer.
They should do whatever they would do normally.
You will not be saying much and will be mostly taking notes.
You may not be able to help right away if they have questions.
You’d like them to think out loud while they work.

(However, some of these may not apply to your study. In particular, think-aloud protocols are often not used in quantitative user tests or even in some eyetracking user tests.)
The participant should understand that you are not testing their abilities or their knowledge. You should encourage them to share their open and honest thoughts with you.
Just to let you know, we’re not testing you, we’re testing the design. If something doesn’t make sense to you or is difficult, that’s something we need to know so we can make the design easier to use. Please feel free to share anything that comes to mind — both good and bad. I won’t be offended with anything you have to share. All your feedback and comments will be helpful to us. 
(Note: when describing tasks to participants, we like to call them “activities” since that term feels less daunting and less like a chore.)
7. Give Tasks One at a Time
It’s a good idea to give the participant a written version of each task, especially if your tasks are scenarios that contain many details needed to complete the task. Depending on whether the session is remote or in person, you can deliver your tasks through a chat interface or on printed slips of paper. Allow the participant to keep hold of the task so they can refer to it again if they need.
To ensure that the participant reads the task properly, ask the participant to read the task out loud. Check that the participant understands the activity before they start attempting it. Asking them to read each task out loud can ease participants into thinking aloud.
I’m going to give you your first activity in the chat. Could you read the activity instructions out loud, just so I have it on the recording? Any questions about this activity before you start?
8. Ask Followup Questions
After the participant has attempted each task, you may want to ask them prepared followup questions, such as:

Do you have any thoughts about doing this activity on the website you just used?
Was there anything easy or difficult about doing this activity?

These kinds of questions give additional information about the task. It’s best to start with broad, open-ended questions (like 1) before you ask more-specific questions (like 2) about the interface.
Sometimes you may have questions about what the participant did or said. It’s often best to ask these questions after the participant has completed the task, rather than interrupting them during the task, to prevent participants from losing track of what they were doing or shifting into interview mode. There are some exceptions to this best practice, and that’s when tasks are very long that you fear the participant won’t remember what they did or said and gathering more insight about something is important to your research goals.
You can also save some questions for the end of the session, if you want to make sure all tasks are given during the session.
9. Check Whether Observers Have Questions
If your observers have questions for the participant, these can be asked at the end of the session. The observers can ask the questions themselves or give them to the facilitator, who can then communicate them to the participant. Either approach is fine; use the one you feel most comfortable with. Make sure your observers know in advance that you will dedicate the last 5 minutes of the session to their questions.
10. Thank the Participant and End the Session
Thank the participant for their time and tell them how they will receive the incentive (if you haven’t already given it to them at the beginning of the session). End any recordings and, if the session is remote, invite the participant to leave the meeting or end the meeting as the host. If the session is in person, allow the participant to collect their things and see them out of the room or building.
Summary
There are many steps to moderate a usability test but following a checklist of steps can ensure you don’t miss any important tasks or information. You can download an example facilitator guide below.
Example Facilitator Guide (PDF)"
6,2021-12-19,"Download a free paper prototyping kit at the bottom of this article.
Paper prototyping is a fast and cheap way to test designs early in the design process, when it’s easy to make changes to the experience. However, paper prototyping is often overlooked in real design projects even though it has such a potential for discovering fundamental design problems and quick solutions for them.
But first, what is paper prototyping? Paper prototyping is the process of sketching out potential concepts, flows or ideas on paper and testing those sketches with users.
The idea is that, before you invest in a sophisticated digital prototype, you simply sketch your ideas on a piece of paper and run a quick usability test on them. Like any user-testing session, testing with paper prototypes involves a facilitator and a participant; additionally, it can also involve a special dedicated helper who plays the role of the “computer” (or other device such as a smartphone) and reacts to user actions by moving through the different prototype screens. (See 40-minute video that demonstrates this testing process.)
Paper prototypes can be quickly sketched by hand or drawn using your preferred prototyping software and then printed out. Consider starting with a hand-drawn prototype; once you have a good idea of the elements that your design will include, you can capture it digitally and further refine it.
This article provides you with a kit for creating your own paper prototypes.


Your mockup can be drawn by hand (left) or by using your favorite software (right).


The Prototyping Kit: What’s Included
Let’s look at what is included in our paper-prototyping kit:

A cutout of a phone and of a browser: This cutout is intended to frame the different prototype screens. Print the cutout on paper slightly larger than US letter or A4 (for example, US tabloid), so that you can use letter or A4-sized paper for your screens. If you don’t have larger paper, then resize your screens to be slightly smaller than this frame. Tip: If you are testing mobile designs and want the participant to hold the “phone,” print this cutout on heavy cardstock (or glue it to cardstock after printing it).
Under construction page: This page is meant to replace those pages for which you did not create a prototype in your premade collection of screens; it lets the participant know that a particular page is out of scope for the test.
Loading indicator: This page should be used when you need extra time to find a specific screen. It helps keep the participant in the experience.
Blank paper: If you notice obvious usability problems while testing, use this blank paper for on-the-fly changes or for creating new screens between sessions.



The paper-prototyping kit cut out and ready to use


Tips for Testing with Paper-Prototypes
Before the Test
Make sure that the person playing the computer is familiar with the screens and their intended order before starting the test, to prevent excessive wait time between the participant’s action and the “computer’s” response.  
Be consistent with the fidelity of your screens. If you show a few high-fidelity cutouts among grayscale, low-fidelity pages, participants will focus on those brightly colored, full-detailed areas and you may get inadequate feedback for the low-fidelity screens.
During the Test: Simulating Various Design Aspects and Elements

Scrolling. It is important to capture scrolling especially when testing mobile prototypes, because mobile pages that are too long tend to lead to usability issues. To simulate scrolling, use a long sheet of paper (or tape several pieces together) to draw all the screens that belong to the same page. Have the participant pull the paper up or down (or left or right for horizontal scrolling).



The participant can pull the longer piece of paper through the phone cutout (or just move the phone cutout) to simulate scrolling.



Dropdowns. To simulate dropdowns lay a small piece of paper with the selections on top of the screen.



A dropdown (or an overlay) can be simulated by laying a piece of paper with the corresponding options on top of the page.



Overlays. Have the overlay content on its own sheet of paper. The computer will then lay it out on top of the screen for the participant to see.



Overlays can be sketched on a separate piece of paper and laid on top of the existing screen.


Testing the Prototype
Standard best practices for usability testing apply when testing your paper prototype. If a usability problem keeps coming up during testing, use the blank pieces of paper to create new screens between testing sessions. Quick, iterative design changes can uncover a design solution quickly.
Conclusion
Paper prototyping is a valuable technique that allows you to test and improve on your early designs easily and quickly. The attached kit can get you started.
Learn more with our 40 minute training video, Paper Prototyping: A How-To Video."
7,2021-11-28,"UX designers often profess they have strengths in facilitating UX workshops, but weaknesses in conducting research. On the flipside, many UX researchers feel unprepared for facilitating UX workshops but have command over a multitude of user-research methods.
The truth is that facilitating workshops and moderating user tests are not so different: you can transfer your facilitation skills from one to the other. This article discusses 5 facilitation techniques that can be applied in both settings.
Differences Between UX Workshops and User Tests
At their core, UX workshops and user tests are quite different; a few of the major differences are summarized in the table below.




 


Workshops 


User Tests




Goals


Collaborate to achieve a desired outcome


Study a design to learn about what does and doesn’t work and why




Participants


Internal stakeholders and product-team members


Target users




Interaction


Participants collaborate and work together.


Participant works alone, observed by facilitator and team.




Facilitation focus


Help the team to reach a shared understanding and agreement


Run a sound study, gather observations and quotes (but not a final decision or design ideas).




Method


Input from participants is grouped and prioritized during the workshop.


Input from the test participants is collected, but not analyzed during the test.




Facilitation Principles That Transfer Across Settings
Even with these differences, 5 important facilitation principles naturally carry over between workshops and usability tests. These are:

Begin with goals or research questions (not activities or user tasks).
Follow a guide.
Be open to improvisation.
Encourage action.
Don’t take center stage.

1. Begin with Goals or Research Questions (Not Activities or User Tasks)
Effective workshops and usability tests begin with goals. What is desired from the workshop? Which questions do we want answered from the usability test?
Examples of UX-workshop goals are:

Define a UX vision for the project.
Generate 2 design alternatives for the checkout flow.

Examples of goals for user tests are:

What makes searching on the website easy or difficult, and why?
Do users discover the Compare feature when performing top tasks? Why or why not?

After the goals are declared and agreed upon by stakeholders, the facilitator can begin planning the workshop agenda or the user tasks for the test. And, during the workshop, the facilitator posts and reviews the workshop’s goals in order to keep participants focused.
In user tests, facilitators tell observers which research questions drive each user task and consult them during the test to decide if they need more evidence to address those questions.
When planning and facilitating a workshop, it’s tempting to start by choosing the activities participants will do. Similarly, when planning usability tests, you may want to dive right in to writing user tasks. But don’t bypass the very important first step of defining goals: state what you are trying to achieve and remind the workshop participants and test observers that you will focus on these things.
2. Follow a Guide
Skilled facilitators make everything look easy. But if you’re observing a great test or participating in an awesome workshop, chances are the facilitator spent many hours getting ready for it and is probably following a detailed guide throughout. A guide includes an agenda, reminders, goals, research questions, and many other elements. For example, a workshop facilitator’s guide may include information like:

the list of activities
backup activities
schedule and timing of each activity

A user-test facilitator’s guide may include:

user tasks for the participants
research questions related to each task
the happy path for each task (i.e., the most efficient way to achieve that task)
how a prototype is meant to act in various places
post-task and post-test question

3. Improvise
Even with a guide, great facilitators know when and how to improvise. For example, if, during a workshop, they see that an activity is not productive, they may give more direction.
In a usability test, if the participant is very interested in a part of the interface that isn’t in the test plan and the team is learning helpful things, the facilitator may encourage the person to proceed or may even make up some related tasks in the moment.
Planning is important, but we cannot predict what a user will do or how our teams will act when they collaborate. If we could, we probably would not need testing or workshops at all. And this unpredictability is the real value of these methods. So, plan, listen, pause, think, then adapt as needed.
4. Encourage Action
In both user tests and workshops, participants are meant to do things. In a usability test, we aim to get participants to do activities, not tell us how they think they might try them. In a workshop, we don’t want participants passively observing; rather, for optimal outcomes, we need everyone to fully participate.
It can be challenging to get people to act. Workshop participants can get tired or bored. Test participants may not understand what they’re supposed to do or even that they must act, not just talk.
To get test participants to act, each test task should include a clear call to action. For example, rather than asking How would you find an insurance policy for your home? try Find an insurance policy you might want for your home. See the difference? The first is a question and beckons a verbal response. The second is a command that asks the user to do something.
If your workshops have inactive or low-energy participants, insert variations such as having people move around the room, alternating between group and pair work, or even taking a break. 
5. Don’t Take Center Stage
Humans like to connect and share. Being with other people and not talking to them can be uncomfortable. But for both workshop and user-test facilitators, silence is a necessary tool that can:

open space for interesting things
prevent them from leading the participants
keep them from giving advice or explanations

Remember that test and workshop participants need time to think before they act and talk. You just need to wait for them. Know that silence does not equal lack of engagement. Rather, it could mean that the participant is thinking.  
Communicating without words is also possible. Here are two successful methods that you can use:

Give visual cues to encourage participation, but do not judge. In a user test, to build the user’s confidence in the testing process, nod your head, take notes, look intently at the same screen at which they are looking. Doing these things indicates to users that you are paying attention, even if you’re not talking. In a workshop, you may also nod, take notes, even walk near or away from people to encourage or discourage participation.
Count to 7 before speaking. In user tests, when I am about to interrupt a user, I force myself to count to 7 in my head. By the time I finish, usually the situation has resolved itself. In workshops, this same technique works when the group is not responsive.

 
 

Other Similarities Between Workshops and User Tests
While there are many differences between UX workshops and user tests, there are also many commonalities. Ensuring success often includes:

involving the stakeholders in early planning
integrating the planning of and response to activities in the project schedule
considering everything to be relevant
collaborating with team members to cluster ideas and observations and to prioritize insights
sharing insights and tracking outcomes

Conclusion
If you look for common ground across UX specialties, you’ll likely find many related practices. Skills needed and techniques used to facilitate a user test and a stakeholder workshop are more similar than you might think. So, if you believe you are good at one but not the other, capitalize on the skills you are confident that you already have and try them out in a different context. You may be pleasantly surprised by your success. And know this: the more times you facilitate, the better at it you will become. So, practice."
8,2021-11-07,"Recruiting participants for research studies is a difficult task: you have to attract interested participants, schedule times to meet for the study, remind them to come to the study, and then hope that they do, in fact, remember to come to their scheduled session. To make matters worse, sometimes participants who do show up are not good candidates for the study because they simply do not have relevant life experiences to contribute meaningful feedback or insights, even with their best efforts. Suboptimal study participants negatively affect the quality of your research and of your design decisions.
Screening surveys (also known as “screeners”) are questionnaires that gather information about candidate participants’ experiences to:

quickly identify and prioritize optimal candidates that are representative for your target audience
exclude any candidates who may not be a “good fit” for your research study

In this article we discuss the importance of screening in the user-research recruitment process and how to incorporate it into your recruitment strategy.
Biased Sampling
First, for any user study, you want to make sure that you recruit people who are representative for your audience. In order to do so, you need to be aware of the fact that the recruiting process may be biased towards certain types of participants.
For example, whenever you are using a remote-testing platform which includes its own participant panel,  you may run into “professional testers” — people who make a significant portion of their income by participating in different kinds of user research. While these participants are not inherently “bad” for a study, their motivations may lead to behavior that skews research results. For example, some may answer questions or perform tasks very quickly (or not at all) during unmoderated tests, rather than making an honest effort to do them in a realistic way. Others may know what researchers are looking for in user tests and deliberately exacerbate feedback to respond to the perceived study needs.
While there is no foolproof method to evade professional testers, you can alleviate the problem by further vetting screening-survey answers to determine whether they are reasonable and honest  (e.g.,  participants did not type “abc” instead of a meaningful sentence). To be clear, the intent behind screening surveys is to alleviate some of the manual work of vetting candidates, but some effort is still required (whether by the researcher or a professional recruiter) to ensure that quality candidates get selected.
Similarly, if you rely upon your personal network to recruit participants (a “convenience” sample), these people will already have somewhat of a relationship with you and may feel reluctant to give honest negative feedback. Testing with coworkers can also bias results because they may be familiar with the project, the organization, and even with different types of user research and their goals.
In general, you should also screen out UX professionals (or those who are “UX-adjacent” with an interest in interface design) since they will be too sensitive to UX issues and more likely to offer an expert review than realistic user feedback.
Even if your customers are “everybody” (that is, they draw from the general audience and cover a mix of genders and ages), you need to make sure that your study has external validity — namely, that your participants match the goals and interests of your audience. For example, someone who is not at all into sports or hiking may not be too motivated to shop on an outdoor-equipment website.
How to Screen Participants
1.  Define eligibility criteria.
First, you and your team should identify participant criteria for your study. Think about both the demographics of your target audience and its goals as they use your products. (For a thorough breakdown of this process, check out our free report How to Recruit Participants for Usability Studies.) These criteria will determine your recruitment strategy and your screener.
If you use automated-recruiting platforms, be careful about overly restricting your survey by having extraneous elimination criteria.  For example, say you were concerned about professional testers and you included an exclusionary question like “When did you last participate in a research study?” You may exclude more participants than necessary, like second-time participants who just happened to participate in a completely different type of study that same month.
In a similar vein, in an attempt to recruit marketing professionals, you might choose to accept only individuals who select Advertising and Marketing as their industry. However, marketing professionals are not exclusive to advertising and marketing agencies and, more often than not, they do marketing functions within nonmarketing organizations (like grocery or apparel stores). Thus, it would be more productive to accept people from multiple industries who have the term “marketing” within job titles or descriptions.
2.  Construct your screener. 
When writing your questions, consider using open-ended or multiple-choice questions to avoid giving away the study’s intent.
For example, a yes/no question like Do you play video games? hints that the study might be related to video games and that the desirable answer is yes. However, if you phrased the question as Which of these activities have you done in the last 4 weeks? with a list of options like hiking, reading, shopping, and video games, then the intent of the study is less obvious. Having distractor answers keeps respondents honest and prevents them from gaming the questionnaire.
Place your most important exclusion criteria near the beginning of the survey so that you can quickly eliminate obvious misfits without wasting their time.
Survey logic can expedite the screening process, so consider picking a survey tool with strong branching capabilities.
3.  Pick your strategy for recruiting participants.
You could find participants for your study in a number of different ways. Each method has its strengths and weaknesses, and, depending on your study’s research questions, you may want to recruit from multiple sources.

Professional recruiting agency
	There are many professional recruiters whose sole purpose is to help UX or market researchers find qualified research candidates; most will even take over some of the work of interacting with prospective and final participants (e.g., scheduling, communication, payment). These agencies have a relatively wide reach in their ability to recruit “general audience” participants (e.g., a mix of genders, backgrounds, financial status, and ages).

	They are especially useful for addressing highly specialized recruiting needs or find specialized user groups (i.e., finding participants with a certain disability, or someone with a very specific type of context or background). They often vet participants in advance, a practice that results in a low no-show rate. Consequently, they are often more costly than automated tools or leveraging existing pools of users.
	 
Automated recruiting platform
	There are many online platforms for user research on the market, and some of them offer automated screening capabilities or recruiting services. If you target a “general audience,” then this method could be beneficial, due to its relatively low cost, ability to outsource some recruiting work, and its relatively quick turnaround (due to automation).

	However, with an automated recruiting platform, there is a greater risk of recruiting “professional testers” and sometimes screening-configuration options (like survey branching and logic) are limited within the tool.
	 
Internal panel of existing users
	Many companies will opt to recruit from their existing userbase, and establish a pool of willing volunteers who are willing to either try out new features (e.g., A/B split testing or beta testing) or participate in research opportunities in general. This approach can be great for recruiting experienced users of a specific product or for getting insights about employee-facing products.

	This method has lower immediate recruiting costs (both monetary and time-related): candidates are already semiqualified and there are no external recruiting fees. You would, however, most often need provide participants with some compensation — monetary or of some other type such as products — for their time. There is also the cost of creating a user panel and maintaining it, as well as doing the work of scheduling and coordinating participants. Some larger companies have a dedicated in-house research recruiter — staff hired to create and maintain the user panel and respond to various internal research needs as needed.

	The reach of an internal panel is limited to participants who already are familiar with your brand and offerings. Most likely, with such a recruiting pool you will not capture new-user perspectives. There might also be some sampling and confirmation bias due to participants’ existing brand loyalty — since they already have a relationship with the brand, they may already like it and provide positive feedback
	 
Online forums and groups (e.g., discussion boards, professional networking groups, or other social media groups)
	When the existing userbase and automated panels fail to provide the degree of specialization required, turning to online groups and forums can identify participants who have certain experiences, interests, or backgrounds. These groups can allow you to address specialized recruiting needs at a lower cost than with recruiting agencies or platforms.

	While these can help provide a group of highly motivated participants, the reach will also be narrow and limited to the members of those groups, which are not always representative of all individuals in the targeted audience. Particularly when the groups themselves have certain predominant viewpoints or vocal community members, there is a higher likelihood of sampling bias or groupthink. There is also going to be more time and effort required to communicate with participants and establish eligibility. If you decide to contact members of an internet group or social-media platform, make sure to request permission in advance from the group moderator.
	 
Intercept studies (or ""hallway recruiting"")
	If you ever heard or read the phrase “Would you be willing to take 5 minutes participate in a quick survey?” you know what an intercept study is. These types of studies can be done virtually (via a popup or modal dialog), on the phone (via interactive voice response (IVR) systems), or in person (in a shopping mall, office hallway, or coffee shop). They are ideal for recruiting visitors or existing customers and for finding participants with a specific goal or task in mind (like using a specific feature). Recruiting can thus be automated for unmoderated studies or surveys.

	Unfortunately, these studies can also result in some wasted researcher time (for moderated or in-person sessions, the researcher has to be available and ready to go whether a participant is found or not). Depending on the specificity of your needs, they might have greater turnaround time — for example, it may be hard to recruit 100 participants for a quantitative study of people who subscribe to a newsletter. Like internal panels, such studies (especially if online) can be subject to sampling and confirmation bias because they involve customers who have already decided to interact with the brand.






Recruiting Method


Reach


Cost


Effort


Time


Bias Risk




Professional recruiter


Wide
			General users
			Specialized users


High


Low


Med


Low




Automated recruiting platforms


Wide
			General users


Med


Low-Med


Low


Low




Internal user panels


Narrow
			Existing users
			Power users
			Employees


Low


Med


Low


Med-High




Online forums and groups


Narrow
			Specialized users


Low


High


Med


Med-High




Intercept studies


Narrow
			Visitors (new/existing)
			Task-oriented users


Low


High


Med


Med










Recruiting Method


Cost


Effort


Time


Bias
			Risk




Professional recruiter


High


Low


Med


Low




Automated platforms


Med

Low-Med

Low


Low




Internal user panels


Low


Med


Low


Med-High




Online forums and groups


Low


High


Med


Med-High




Intercept studies


Low


High


Med


Med





 
4.  Adapt your recruitment strategy and screening survey to attract the right participant.
Time-poor or high-earning professionals (like night-shift or swing-shift workers, executives, doctors, or lawyers) may need high incentives to justify time spent away from work or their personal lives. They also might be less likely to spend a lot of time filling out a lengthy screener survey.
 
5.  If you are recruiting two or more types of user segments for the study...
You can attempt to use one screening survey for all these sets of criteria, but there are tradeoffs:

On the plus side, participants don’t have to fill out the survey more than once, which increases the likelihood you’ll get a sufficient sample size.
You may also end up with a pretty complicated, branching screener or may need to manually filter candidates even after they filled out the screener

For example, let’s say you wanted to recruit two user types: power users who do not work in the technology sector and novice users who work in tech. You could have survey logic that routes tech professionals through the novice qualification criteria and routes nontech professionals through the power-user qualification criteria. Or, you could ask everybody whether they work in tech and take them through both the power-user and novice questions; then look at the answers and manually decide whether they fit your combination of requirements.
6.  Review the survey responses of both qualified and disqualified candidates before finalizing your approved list of participants. 
(If you work with a recruitment agency, ask them to provide all the filled screeners for you or at least those of the near matches.)
By reviewing the survey responses submitted, you can identify any close-fit study applicants that you may want to consider as a backup candidate if they have most, but not all the criteria for your target audience.
For example, if your target audience was parents with multiple children, a primary–caregiver aunt or uncle may have been disqualified, yet acceptable for the study. Alternatively, some participants may have been screened out because they picked one of several possible answers (e.g., they may have answered Android to the question Which type of phone do you own? even though they own both an Android and an iPhone).
7.  If you’re unsure whether eligible participants are truly a good fit for the study (because, for instance, your recruit is highly specific)...
It might be worth breaking the study into two parts: a 15-minute screening interview and a 30- or 60-minute research session. This screening interview can serve two purposes:

evaluating candidates to clarify their screening-survey responses and validate whether they are a good fit for the interview
preparing the selected participants for the main study session by reviewing key logistic details (e.g., consent forms, communication methods, expectations for participating, device requirements, applications that need to be downloaded, and other setup work like account creation).

8.  Finally, notify your fully qualified and approved participants, and begin scheduling sessions for your study.
Avoid scheduling anything with semiqualified candidates unless you know for sure you will invite them to the study. Otherwise, once you promise a spot, you are technically obliged to provide compensation if you cancel the appointment.
Conclusion
No matter who your users are, you must screen research participants to ensure you are using your research time and budget wisely. After all, your design decisions will only as good as your data is. By recruiting representative study participants, your team can reduce bias and build experiences tailored to your specific users’ needs."
9,2021-10-10,"In a remote usability testing session, the researcher and participant are not in the same physical location. This differs from traditional usability testing, in which the researcher and participant would meet in person.
Here’s a list of NN/g’s most useful introductory articles and videos about remote usability testing. Within each section, the resources are in recommended reading order.
For hands-on training, check out our full-day course on Remote User Research.
Moderated vs. Unmoderated
There are two types of remote usability testing: moderated and unmoderated.

Remote moderated usability testing is very much like an in-person study. The primary difference is that the facilitator and participant aren’t in the same physical location. Researchers often use video-conferencing software to conduct remote moderated testing.
Remote unmoderated usability testing does not use a facilitator at all. Instead, the researcher sets up instructions and tasks in a remote-testing platform). Then the participant completes those tasks on their own, while recording their screen, voice, and (sometimes) their webcam.





Number


Link


Format


Description




1


Remote Usability Tests: Moderated and Unmoderated


Article


The key differences between the two types of remote testing, and when to use each 




2


Remote Usability Testing Costs


Video


A comparison of the costs of the two types of remote usability testing




3


Remote Usability-Testing Costs: Moderated vs. Unmoderated


Article




4


The Case for Remote Moderated Usability Testing


Video


Why you might want to consider moderated testing, even though unmoderated testing is cheaper and faster




5


Remote Moderated Usability Tests: Why to Do them


Article




Remote Moderated Usability Testing




Number


Link


Format


Description




1


Running a Remote Usability Test, Part 1


Video


Step-by-step instructions for running a moderated test




2


Running a Remote Usability Test, Part 2


Video




3


Remote Moderated Usability Tests: How to Do Them


Article




4


How to Run a Usability Test with Users Who Are on Your Site Now


Article


How to use intercept surveys to recruit participants and immediately run moderated testing




Remote Unmoderated Usability Testing




Number


Link


Format


Description




1


Unmoderated User Tests: How and Why to Do Them


Article


When to use unmoderated testing, along with step-by-step instructions for running an unmoderated test 




2


Tools for Unmoderated Usability Testing


Article


How to choose the best unmoderated-testing tool for your needs, plus a comparison of the features of many testing tools




3


Catching Cheaters and Outliers in Remote Unmoderated Studies


Video


Tips for catching and removing problem participants (one of the risks of running unmoderated testing)




4


Catching Problem Participants in Remote Unmoderated Studies


Article




 "
10,2021-08-08,"In the following list, we group together some of our articles and videos on topics related to qualitative usability testing. Within each section, the resources are shown in recommended reading order. 
Qualitative Usability Testing: The Method
In a usability-testing session, a researcher (called a “facilitator” or a “moderator”) asks a participant to perform tasks, usually using one or more specific user interfaces. While the participant completes each task, the researcher observes the participant’s behavior and listens for feedback.
If you’re totally new to usability testing, we recommend you explore the following resources in order. If you have some prior experience, feel free to pick a subtopic and start there.




Number


Link


Format


Description




1


Usability Testing 101


Article


A basic introduction to the concept of qualitative usability testing




2


User Testing: Why & How 


Video


A short explanation of why you’d want to conduct a usability test (sometimes called “user testing”) and how these tests are typically performed




3


How to Setup a Desktop Usability Test


Video


The equipment typically used for in-person usability testing (on desktop/laptop or mobile devices) and how to arrange that equipment




4


How to Set Up a Mobile Usability Test 


Video




5


Usability Testing for Content


Video


Tips for using usability testing to evaluate content




6


Quantitative vs. Qualitative Usability Tests 
 


Article


How different types of study goals inform the setup of usability tests




Participants
This list of resources covers:

Why the participants (people who participate in your study) you recruit are so important
How to recruit participants
How many people to recruit for your study 
Whether or not it’s okay to use the same participant in multiple studies
How to collect informed consent from your participants





Number


Link


Format


Description




1


Usability Testing 101


Article


A basic introduction to the concept of usability testing




2


1st Pillar of Usability Testing: Typical Users


Video


Why having realistic (“typical”) users is critical to your research




3


Recruiting Test Participants for Usability Studies 


Article


Tips for finding people and convincing them to participate




4


How Many Test Users in a Usability Study? 


Article


Why we recommend conducting qualitative usability testing with about 5 participants per user group 




5


Usability Testing with 5 Users: Design Process 


Video


More detail about the logic behind the 5-participant recommendation




6


Usability Testing with 5 Users: ROI Criteria 


Video




7


Usability Testing with 5 Users: Information Foraging 


Video




8


Employees as Usability-Test Participants 


Article


When it’s okay to use coworkers as usability-testing participants




9


Using Usability-Test Participants Multiple Times 


Video


Using the same participants for multiple usability tests



10
Obtaining Consent for User Research
Article
Why informed consent is important, and how to write a consent form for your study



Tasks
This list of resources covers:

Why we need to write usability-testing tasks carefully
What makes a good task
How to write and order good tasks 





Number


Link


Format


Description




1


2nd Pillar of Usability Testing: Appropriate Tasks


Video


Why task instructions are so important




2


Turning User Goals into Task Scenarios for Usability Testing 


Article


How to decide which tasks you might want to write




3


Writing Tasks for Quantitative and Qualitative Usability Studies


Article


How writing tasks for qualitative usability testing differs from writing tasks for quantitative studies




4


Eyetracking Shows How Task Scenarios Influence Where People Look


Video


An illustration of how the exact way you write your task will influence your user’s behavior




5


How to Maximize Insights in User Testing: Stepped User Tasks


Article


The stepped task strategy — starting with open-ended tasks and then moving to directed, focused tasks




6


Write Better Qualitative Usability Tasks: Top 10 Mistakes to Avoid


Article


The most common task-writing mistakes and how avoid them




Facilitating a Usability Test
This list of resources covers:

The importance of having an experienced facilitator
How you improve your own facilitation skills
Encouraging participants to think out loud
How to handle observers





Number


Link


Format


Description




1


3rd Pillar of Usability Testing: Skilled Facilitator


Video


Why it’s important to have an experienced facilitator run the test




2


User Testing Facilitation Techniques 


Video


Some useful techniques for facilitating usability testing




3


Talking with Participants During a Usability Test 


Article


How to communicate with participants without influencing or distracting them from the study




4


Thinking Aloud: The #1 Usability Tool 


Article


Why thinking out loud (sometimes called the “think-aloud protocol”) is useful




5


Team Members Behaving Badly During Usability Tests 


Article


How to handle observers — people who watch your study



6
Observers in In-Person and Remote Usability Studies
Video
Avoid having the observers distract and bias the study participants


7
Checklist for Moderating a Usability Test
Article
Ten steps for facilitating usability testing, plus a sample facilitator guide



Remote Usability Testing
This list of resources covers remote usability testing. There are two primary types of remote usability testing:

Remote moderated usability testing is conducted synchronously. The participant and facilitator meet virtually, often using video conferencing apps like Zoom. 
Remote unmoderated usability testing is conducted asynchronously — the researcher sets up the tasks and instructions in a testing platform, and the participant performs the tasks on their own while recording a video.





Number


Link


Format


Description




1


Remote Usability Tests: Moderated and Unmoderated 
 


Article


An introduction and comparison of these two types of remote usability testing




2


Remote Moderated Usability Tests: How and Why to Do Them 
 


Article


When it’s a good idea to use moderated instead of unmoderated tested and how to conduct a remote moderated study




3


Remote Unmoderated User Tests: How and Why to Do Them 


Article


When it’s a good idea to use unmoderated instead of moderated testing and how to conduct a remote unmoderated study




4


Tools for Unmoderated Usability Testing 


Article


Tips for choosing among the many unmoderated testing tools and a spreadsheet comparing popular tools




Special Usability Testing Studies or User Groups
Sometimes we need to modify our methodology when we’re testing with special groups of users or within special contexts. These resources cover some common special situations you might encounter.




Number


Link


Format


Description




1


International Usability Testing 


Article


Language and cultural challenges related to testing with people who live in different countries 




2


Usability Testing with Minors 


Article


Modifying the testing setup to make teens and children comfortable and focused during the study




3


Conducting Usability Testing with Real Users’ Real Data 
 


Article


Extra steps needed to ensure the participants’ comfort, privacy, and security in studies  involving personal information (particularly health or financial information) 




4


How to Conduct Usability Studies for Accessibility 


Report


How to adapt usability-testing practices to test with participants who use assistive technology (This report isn’t free, unlike the other resources listed in this study guide.)




Courses
NN/g offers the following hands-on, in-depth courses:

Usability Testing
Remote User Research
Qualitative Research Series
"
11,2021-06-06,"User interviews are a research tool that can provide insights into users’ needs and beliefs, while also building empathy.  
Not all interviews qualify as research interviews. While journalistic interviews may have some similarities with user interviews, research interviews focus on nonjudgmentally and objectively gathering information about user needs, expectations, and interactions with a product or class of products. So, not every conversation with a user can be considered a research interview.
Unfortunately, even with the best intentions, it can be easy to bias or influence your participants’ responses. User interviews require a lot of attention to detail, and can fail for a number of different reasons, but if you focus on avoiding these common mistakes, you can ensure that your gathered data is objective, unbiased, and methodologically sound.
Common Interview-Facilitation Mistakes
1. Insufficient Rapport-Building
To some people, small talk might feel like an unnecessary chore or a waste of valuable interview time. However, rushing in and diving straight into a user interview without spending enough time building rapport will limit the quality (and quantity) of the data you’ll get from that interview.
During in-person interviews, one good way to build rapport without taking too much time is to personally pick up the participant from the waiting area (rather than delegating that task to a different person). Even if you don’t have a “waiting area,” or are doing interviews remotely, make sure to spend some time setting the tone and making your participant feel comfortable.
That said, it is also easy to take rapport building too far. Some interviewers — in their effort to build maximum rapport and seem as likable as possible — might start to share their own experiences to build solidarity and empathy with the participant. However, this approach can skew participants’ responses by shedding light on what the interviewer is interested in and may also cause participants to withdraw if there is an experience that they do not share with the interviewer.
To avoid this mistake: 

Introduce yourself and ask participants about their day, beyond the scope of the interview. This type of interaction can provide valuable additional context about your customers and the lives they live outside of using our products and services.
Speak calmly and slowly and try not to talk more than the participant. Participants will often ""mirror"" the researcher, so speaking slowly will help them remain calm as well. It also will give participants time to think about their responses. Aim to have the participant speak for roughly 80% of the interview time.
Avoid the word “interview” in your interactions with the participant; refer to the interview as a “chat” instead, to make it seem less judgmental. The word “interview” is often associated with job interviews and can increase anxiety (hindering rapport as a result).

2. Not Enough ""Probing"" Questions
Probing, or the act of asking followup questions to gain specific, in-depth information, is effective for uncovering the motivations and rationale behind certain behaviors, attitudes, and perspectives. Probing questions like “How does that make you feel?”, “Why do you think that is?”, or “Can you tell me more about that?” can get participants to share more information or to clarify what they meant in a prior statement.
These types of questions can feel a bit awkward or intrusive if you are not accustomed to asking them often; however, not asking probing questions can limit the depth and specificity of your participants’ responses and can lead to misunderstandings and ambiguity in your research data. The researchers may find themselves trying to extrapolate meaning from user quotes (rather than relying on a direct explanation from the participant).
To avoid this mistake:

Plan optional followup questions for each interview question as you’re writing your interview guide.  Even if you don’t end up asking them, it’s good to have some prepared followup questions to fall back on. These should be specific to the issue you are investigating (within this scope) and range in levels of specificity from broader (good at not assuming an answer) to very precise (something you really want to know, but don’t want to bring up yourself unless absolutely needed).
Keep a generic probes bank at the top (or bottom) of your interview guide so that you can fall back on them when needed, rather than repetitively asking “Why?” Generic questions are good at not leading the user, but you do need that bank of them, because it quickly gets annoying for a respondent to repeatedly be asked the same generic question (e.g., “How does that make you feel?”)

3. Multitasking and Note Taking During the Interview
When you are the sole researcher on your team, it can be an especially difficult to  commit your full attention to what the user is saying. Still, this is important, not only because it allows you to best interpret your participant’s words and gestures, but also because it builds the right kind of relationship with the participant. Preparation is key: the more you already know what to do next, the less you have to think about what comes next and can “live in the moment” and pay full attention to the participant.
Diverting your eyes from the participant to check your phone, answer a text message, or checking your watch can signal disinterest; so, turn off unnecessary applications and silence your devices. That said, the most common culprit of disrupted interviews is taking notes while facilitating the interview. This practice is problematic not only because it makes it hard to keep up with what your participant is saying and the participant has to wait for you, but also because it takes your attention and eye contact away from the participant and erodes the rapport you’ve been struggling to build.
To make matters worse, taking notes only when participants talk about something related to your research question can signal to them that some information is more desirable or interesting than other. Participants will often try to “please” the interviewer or be as helpful as possible, so they might alter their behavior to provide what seems to be most interesting or desirable to the researcher (rather than what is most representative of their experience).
To avoid this mistake:

When possible, record the interview and get it transcribed afterward. A recording helps avoid misquotes and gives other researchers on your team access to the same “raw” data for later analysis. Be mindful that your data-management practices respect your participants’ privacy, so anonymize the recording as much as possible, and remind participants that what is shared is confidential. (Even if this information is stated in the emailed instructions, these disclaimers can sometimes seem like insincere “boilerplate” text, so be sure to also say it out loud).
If you cannot record, assign a designated notetaker. If, for any reason, it is not possible to record the interview, have an assigned notetaker, whose primary role is to take notes on the entire interview (not just areas of interest).

4. Allowing Observers to Influence the Interview
While interest in user research is good, an observer in the interview room can hinder the ability to build rapport, which, again, can limit how willing your participant will be to share intimate details about their experiences.
An interview is meant to be a friendly one-on-one conversation; as you add observers who can insert their own questions,  it can feel a lot more like an interrogation. While it is not necessarily a deal breaker to have an observer or two present during the interview, the less noticeable they are to your participant, the less awkward your interview will be.
To avoid this mistake:

Record the interviews (as stated above) and offer recordings to any interested stakeholders or team members afterward. This tactic keeps observers completely out of the room and ensures best conditions for building rapport. That said, it is also not ideal for helping stakeholders or clients build empathy and common ground, since they may watch only fragments of the interview (or not watch it at all).
Set expectations and ground rules for observers. The interview facilitator should be the only person asking questions of the participant. Since observers are not actively building rapport with participants (and sometimes, may not be trained facilitators), the questions they may ask might not be received positively or might yield only terse responses. Set expectations and ground rules in advance (perhaps in a separate email or calendar invitation), and hold a short debrief for observers after the session (or series of sessions) so that any additional research questions can be factored into future interview iterations. (Note: If you're running the interviews via video chat, it may help to choose a meeting tool which hides the list of attendees from participants.)
Limit the number of interview staff in the room to 3 people and keep them out of sight. — either through a two-way mirror or by placing them off to the side, out of the participant’s range of vision. If running interviews on video chat and you're unable to hide attendee lists, consider having observers remove their profile picture, or rename themselves to something less recognizable, like a random string of letters/numbers. Still, participants may be affected by the knowledge that they are being watched.

As a more general point, most stakeholders are only willing to devote limited time to observing user research. Even actual members of UX team may not have time to observe all research, especially if they are not researchers themselves. In both cases, it’s better to allocate most of these people’s limited time to observing behavioral sessions (such as usability testing) where they can watch users actually use the product.
5. Leading the Participant
The most compromising of all facilitator mistakes is inadvertently skewing participant behavior or priming. Some researchers may accidentally “show their cards” and reveal the intent of the research study too early, thus biasing the participant’s contributions.  
For example, if we told the participant, “We’re studying how much word-of-mouth recommendations impact real-estate purchases,” the participant may focus their responses on recent word-of-mouth recommendations. On the other hand, if we said, “We are studying how people go about their home-buying process,” the participant might reveal that they don’t use word-of-mouth recommendations at all, but spend a lot of time independently browsing online real-estate listings.
As seen earlier, body language can also prime participants. A slow head nod changing into a vigorous one might hint to a participant that their response suddenly got interesting. Similarly, how questions are phrased can change participants’ responses. For example, the question “How important is price when looking at lawn-maintenance equipment?” might yield a predictable response of “Yes, price is very important,” because it implies that price should be considered and most people would like to respond in a way that is socially desirable and acceptable. It also reminds people of price, even though some may not necessarily think of that immediately. A better question would be “What factors do you use when selecting lawn-maintenance equipment?”
To avoid this mistake:

When introducing yourself and the study, keep the study’s purpose relatively vague, to avoid priming users.
Keep questions as open-ended as possible and start topics broadly before narrowing down with followup questions. 
Mind your body language and try to stay consistent throughout the entire interview. You don’t need to be stern or robotic, but ensure you stay relatively neutral, yet friendly and interested throughout.

Conclusion
Interviews are a method to learn about how your users view the world. To maximize the quality of your research,  spend the time to build rapport and focus on being an open-minded listener. "
12,2021-05-16,"UX professionals aim to create end-to-end customer experiences that serve the user goals as effectively as possible. To that end, we conduct user research to understand our users, their needs, and the hurdles they may encounter as they are trying to address those needs.  We usually refer to these hurdles as pain points — problems in the customer experience with a product or service. 
The phrase ‘customer experience’ is crucial in the definition of a pain point. Elsewhere we’ve defined customer experience as encompassing three levels: the interaction level, the journey level, and the relationship level. A pain point can be an issue at any of these three levels — it could be related to a particular interaction with an interface (in which case it is usually called a usability issue), to a journey as the customer is trying to accomplish a goal, or to the longitudinal experience that a customer has with a company.
Let’s first look at a few examples of pain points at the three levels:
1. Interaction-level pain point: A user is passed from support person to support person.
We’ve all been there — we call customer support, say what we need, only to be passed to another department who “will be able to handle that request.” Not only is time wasted, but we have to explain our issue over again. Even more, there are often discrepancies in the information we are told. This type of interaction causes wasted time and confusion. 
2. Journey-level pain point: A user places an order and does not receive it for months. 
I recently purchased a Peloton bike. After ordering it, I was alerted that the bike would be delivered 3 months from my order date. While this wait time was disappointing, it was somewhat expected given the high demand for the bikes during COVID-19. However, after the 3-month wait, I received a call that the bike would be delivered while I was scheduled to speak at a NN/g UX Conference. Upon calling customer support to reschedule, I was given the choice to take the assigned appointment or reschedule my delivery appointment to another 3 months from then. The pain point in this journey was the long duration between the beginning of the journey (the bike purchase) and the completion of the journey (taking my first ride on the bike). Additional point points included the lack of upfront communication about wait times, the need to call customer support to reschedule delivery, and the lack of flexibility in appointment times.
3. Relationship-level pain point: A user pays for a service but still has to watch ads. 
I pay for a subscription to Hulu to stream TV shows. However, I still have to watch advertisements periodically. This is an anomaly compared to other streaming services (Netflix and HBO Max, for example) and does not meet my user expectations or industry norms. These advertisements are a pain point in my overall relationship with Hulu — my trust in the company is diminished due to the financial commitment I’ve made, without equal reward.  Not only that, but the ads have prompted me to waste time reading forums and contacting Hulu support to solve this problem with no success.  
Pain points are diverse; they can be broad or specific, severe or relatively insignificant, and obvious or hidden. Identifying pain points is a first step to creating solutions that address users’ real needs.  
Pain Points vs. Usability Issues 
In the UX world, terminology is thrown around and inflated. This article is not meant to create yet another word for traditional usability issues, but rather define a concept that is broader than that of usability issue. In the same way in which customer experience needs to be defined as broader than usability or interaction-level user experience, pain points need to be defined as all-encompassing issues that go beyond traditional usability problems. 
Remember, pain points include usability issues (those are pain points at the interaction level), but can also include other, higher- level issues in the customer journey or in the overall relationship between customers and organizations.
The Effects of Pain Points on Users
All pain points incur a cost to users — whether it’s time and extra steps that they need to take or actual money that they lose. Some pain points translate in increased interaction cost and cognitive load. This is often the case with usability issues. For example, complicated workflows that are error-prone can result in increased interaction cost — the user will have to take additional steps to fix the error. Or, the interface may be so complicated that the user may need to call for assistance (and thus experience an increase in interaction cost).  
Other pain points will incur a time cost — for instance, if the user needs to wait for a long time for a process to complete. 
Occasionally, there is also a financial cost to the user. For example, if an internet provider has frequent outages,  users may be forced to use their cellphones as hotspots and incur extra data costs.
Last but not least, a less tangible effect of pain points on users is loss of trust and confidence. A nonsatisfactory interaction with a company often leaves users with a sense of having been betrayed; over time, these experiences erode the overall trust in the company and may cause users to terminate their relationship with the organization. 
How We Identify and Prioritize Pain Points
We can identify each type of pain point using various UX research methods, then prioritize them based on contextually appropriate criteria. 
Interaction Level 
Identify: Interaction-level pain points (i.e., usability issues), can be detected through user research such as   usability testing. Most of UX has been concerned with identifying these types of issues.
Prioritize: Traditionally, usability issues are classified according to their severity, which can be based on the issue’s impact on the user and on the popularity of the product, how often the problem occurs, and also if a user is likely to encounter it more than once.
Journey Level 
Identify:  Pain points at the journey level are found through a combination of exploratory research such as user interviews, diary and field studies and customer-journey mapping. This approach allows us to collect various data points across the entire journey and assess how successfully interactions come together to help users reach their goals.
Prioritize: Journey-level pain points often need pervasive organizational restructuring and internal process changes; they may even require a CX transformation. When prioritizing journey-level pain points, consider factors such as: 

The impact of the pain point across the journey: How much of the journey is negatively affected by this pain point? Is it contained to a single phase or widespread across multiple journey phases?  
The feasibility of solving the pain point: To what degree will your company or organization realistically be able to successfully remedy the pain point? 

Relationship Level 
Identify: Relationship-level pain points are uncovered over long periods of time. Our goal is to assess the lifetime experience that a person has with an organization and their cumulative pain points as a patron of that organization. We identify pain points at this level via benchmarking surveys (measuring brand loyalty, likelihood to recommend, and overall customer satisfaction), analytics data, or technical infrastructure that tracks and manages data of individual customers. This kind of technical infrastructure requires that customer-behavior data from across the entire journey is integrated into a single source to create a single view of the customer that includes details about individual users’ relationship with the company and their behaviors over time.
Prioritize: Relationship-level pain points are the most complex and difficult to prioritize. They require many business units collaborating to enact long-term change — internal and external to the company.  To priorize these pain points, consider:

The impact of the pain point across several journeys: How many of the journeys are negatively affected by this pain point? Is it contained to a single journey or widespread across multiple journeys?  
The churn rate caused by a pain point: How many customers abandon your company because of this pain point? 
The brand loyalty lost as a consequence of the pain point: Are customers likely to use your product less? Will they be less likely to recommend it to others? Will they choose a competitor in the future? 

Conclusion
Identifying and fixing pain points, and better yet, preventing them, is core to what we do as UX professionals. They give purpose to our work and help us focus our time and resources. Pain points, more than random feature requests, should be a driver for design changes. Pain points center the discussion around the customers. 
While pain points are never ideal, it is not cost-efficient to solve all of them. Tradeoffs must sometimes be made, and UX resources should be strategically applied. Once uncovering your users’ pain points and prioritizing them, explore potential solutions and plot the work into a future roadmap. 
 "
13,2021-04-18,"Designing for global audiences with different cultural backgrounds can be challenging.
To thrive in an international market, sites must go beyond translation and localization and gain first-hand data on how your users in your markets interact with your products. Even though the main usability guidelines stay the same across countries, testing with international audiences can reveal usability problems specific to those cultures. There are two main reasons to test with international users:

International users may interact with your products differently or rely on specific features more heavily than local audiences.
Mental models and how people interact with technology or organizations can vary from country to country.

Usability Problems Associated with Features More Likely to Be Used by International Users
International usability testing can uncover issues with important site functionalities rarely used by your major group.
For instance, domestic users may rarely if ever switch the site language. But for international users, the findability of this feature is crucial. During our user study in China, a female participant tried to shop at the mobile version of the South Korean clothes retailer Maybe-Baby. Soon, she realized the page was written in English and Korean, none of which she could read. Then she wondered if there was a Chinese version, tapping the navigation menu with no luck. She took a long time to discover the language switcher, which was in the page footer. She kept scrolling on the homepage, and after scrolling for about 13 screenfuls, she finally saw a row of national flags at the bottom of the page. Clicking China’s flag took her to the Chinese version of the site. She said, “I think it’s a good design, showing national flags for different languages, because maybe I don’t know how to say ‘Languages’ in either English or Korean. But why do they put it at the end of such a long page? I may have missed it.”

The mobile version of maybe-baby.co.kr (left) was 13-screenful long, with the language switcher in the footer (zoomed-in in the right part of the screenshot).

Clothing size is another feature that can vary from country to country, as are measurement units of choice. People shopping on a foreign site may feel they need to understand sizing and use tools such as size guides more than domestic users. A study participant tried to shop for a pair of jeans at Zara.cn, but she wasn’t able to properly access it. When she clicked the Size Guide button, an overlay with a big picture of a female wearing a pair of jeans showed up. The user was supposed to choose a size in order to have the dimensions shown on top of the image. But, unfortunately, she did not understand that: the Choose a Size label and the tiny arrow associated with it were very close to the bottom of the page. She was confused, “Where is the guide? It’s just a picture.”

Zara.cn: When a participant clicked Size Guide), she saw an image of the jeans on a model (left). She was supposed to use the Choose a Size below that image to select a size and have the corresponding dimensions overlapped on the image (right). However, she never discovered that functionality because the dropdown was too close to the bottom of her screen and the associated arrow too tiny. She thought there was something wrong with the site.

Uncovering these basic usability issues not only helps with international audiences but also benefits potential domestic users that are not typically recruited in usability tests, like minorities speaking different languages. It actually improves the accessibility of your design.
Culturally Specific Issues
In addition to general usability glitches, usability testing can also identify culturally specific problems during international usability testing related to:

Context of use
Perception of the product
Consumption of content
Interaction with your organization

Context of Use
Where do people typically use your products and who is with them? What types of device limitations do they have? You can learn about these questions better when you conduct remote international usability testing where study participants perform tasks at their preferred locations (like their homes or offices) instead of traditional in-lab usability testing.
Contextual factors refer to all artifacts (physical and digital) and people that may influence how users interact with your products or services.
While it’s neither possible nor necessary to figure out how all contextual factors impact the user experience, you may be able to identify issues such as broadband limitations.
For instance, when we conducted remote usability testing with Indian participants recently, we observed that several participants had to wait for a long time to load websites. Fancy websites full of animations, pictures, and videos only made it worse.
When a participant visited the Decathlon site on his mobile phone, he waited 20 seconds for the navigation to load (and it didn’t). The navigation was very complex, with lots of pictures and even an embedded search feature. He finally turned to the site’s search bar instead of waiting for the navigation to load fully. “The website is pretty heavy to load,” he commented.
In contrast, when he tried to shop at Paytmmall.com (an Indian ecommerce platform), the navigation’s minimal design loaded immediately, and he was happy to use it to move around the site.

Decathlon.in used a complex navigation (left). For each main module, there were several interactive submodules with a search feature. The navigation took so long to load that a participant gave up on using it. The lightweight and clear navigation of paytmmall.com (right) loaded much faster and the same participant quickly began shopping.

Your product’s compatibility with users’ preferred devices or software, like operating systems or browsers, is also vital. Since people are so used to these essential tools and have no problems browsing domestic websites, if your product doesn’t work, your users will believe it’s your problem, but not theirs.
We observed several such cases when we conducted ecommerce research study with Chinese participants. For example, a female participant tried to pick a lipstick for her friend’s birthday. She had no issues choosing a lipstick when visiting the mobile version of the Chinese MAC Cosmetics, but couldn’t check out for some reason. The cart button just wouldn’t work. She was disappointed about the site and believed something was wrong with it.
It turned out that she set the Baidu browser as her default browser, and there were some compatibility issues between that browser and the MAC site. According to Baidu Statistics, 9.5% of Chinese mobile traffic goes through the Baidu browser. MAC Cosmetics China may lose this segment of customers without even being aware of it. The researcher prompted her to try with her phone’s built-in browser instead, and the MAC site worked.
(However, you shouldn’t rely only on usability testing to find out compatibility issues; for small user segments, it can be hard to identify them during testing. Teams working on international products should consider these compatibility problems early in the design and development stages by checking platform-usage data.)
Sometimes, usability testing can also reveal how users’ interpersonal interactions and communication when using your products influence their experience.
For instance, during our testing, an Indian senior participant often asked her young daughter for help when she couldn’t find something on websites, since she wasn’t good at English and wasn’t very tech-savvy.
(Typically, we don’t allow other people to interfere with participants when they perform the tasks. However, we make exceptions in cases like this, because it’s a reflection of how the user actually uses digital technology in real life. Another example would be when parents help out young kids with tech issues.)
Perception of the Product
How do your international users perceive the visual design of your products? How do they feel about your products or services before and after performing some tasks? How much do they trust your brand?
People from different cultural backgrounds have different visual-design preferences. Researchers Katharina Reinecke and Krzysztof Gajos collected 2.4 million subjective ratings on website aesthetics from more than 39,000 participants all over the world. They found out that Russian participants significantly preferred websites with lower visual complexity, while people from Mexico and Chile gave higher scores to sites with high visual complexity. The same study also found that monochrome designs are preferred by Finns, Russians, French, and Germans, while Malaysians and Chileans tend to like colorful visuals. Similarly, our previous study in China also found that Chinese participants had a higher tolerance for high visual-complexity websites than foreign users.
Testing your visual designs with your target audiences is a good way to gauge their interests.
Beyond immediate aesthetic appraisal, usability testing can also uncover how users, especially newcomers, perceive the credibility and trustworthiness of your products or services. For example, different target audiences may perceive different design components as trustworthy.
When we tested the Chinese version of the Australian pharmacy website Pharmacy Online with Chinese participants, several people praised its credibility. They appreciated that the site had a navigation UI similar to that of Chinese ecommerce websites and also localized components such as a WeChat official account. These elements made the website look familiar to them.

The Chinese version of pharmacyonline.com.au (top) compared with Taobao’s homepage (bottom): Some participants perceived this localized design as highly credible because it used components familiar to them, such as a WeChat QR-code and Taobao-like information architecture.

Consumption of Content
Can international users easily understand the information presented on your site? Do they like its tone of voice? Do they find the content helpful and how do they make use of it?
Testing your content with your global audiences is especially important if you have multiple, culturally diverse audiences using the same language version of your site (e.g., USA, Australia, UK use the same English version).
For instance, when we tested the Chinese version of the Converse website, a young male participant complained about the labels. Instead of traditional ecommerce labels like Men, Women, and Accessories, Converse China used Dude 男的, Dudette 女的. (It is hard to find English equivalents to these two labels. In Chinese, these words often have a negative connotation, instead of the casual tone indicated by the English words Dude and Dudette.)
The participant, who shopped for fashion shoes a lot and belonged to the target audience of Converse, stated, “It says Dude and Dudette instead of Men and Women. It makes me feel uncomfortable. It’s a little bit casual, and I feel they don’t respect people.”
After translating your content for people speaking different languages, be sure to review its tone of voice as it can influence your branding. Don’t listen only to marketing experts; let several users perform tasks with your design and see how they feel about your tone.
Beyond the content itself and its tone of voice, people from different cultural backgrounds have varied preferences on how the content is organized. For instance, people in high-context cultures like China are used with websites with high information density and may prefer them to those with relatively low information density.
You can also find out whether the content you offer is helpful or whether you need to provide specific, additional content that these target audiences may be used to on similar sites.
For example, an Indian participant wasn’t satisfied with the dimension information (in centimeters) provided for photo frames on Myntra. “This is funny because when we measure the wall décor, we use inches instead of centimeters. So, the size information they provided was not useful.” (Some cultures in India prefer inches, others centimeters.)

The model size provided by Myntra (an Indian ecommerce platform) was in centimeters instead of inches, which wasn’t useful to a participant at all.

During our ecommerce study in China, a participant shopping for Nike shoes wanted a popularity-filter feature, which was available on almost all domestic retailer sites. He complained, “There are no filters on popularity! I want to know which ones sell best, so that I’m wearing the most popular style.”
Interaction with Your Organization
How do your international users typically interact with your organization or with other similar organizations? Are there any channels that tend to be used more frequently than others?
For instance, Indian participants prefer one-time passwords, but Chinese participants login via third-party services like WeChat and AliPay on mobile or by scanning a scan QR code on desktop.
When we ran an in-person university website study in China, a participant tried to figure out York University’s transfer student requirements. She wasn't happy with the website's vague information and was upset to learn that she needed to communicate with the institution through emails: “I don’t think it’s a proper way to communicate with Chinese students. I’d prefer some instant communication methods like an online Open Day through Skype.”
People from different cultures can also prefer different platforms to keep updated with their favorite brands. Open-ended and exploratory usability-testing tasks such as Check out whether any of your favorite brands launched something new can reveal their preferences and browsing patterns.
For instance, during our social media usability testing in the US, we combined such questions with a cognitive-mapping activity. We found that participants believed that organizations were more active on Twitter than on Facebook. Several used Instagram to follow different fashion-related businesses. They didn’t link use Snapchat to engage with businesses at all. In China, WeChat and Weibo (the equivalent of Twitter) are the most used social-media channels for engaging with businesses.
When to Run International Usability Testing
You should conduct usability testing with global audiences early in the product-development stage, when planning your strategy and product launch in a new market. Even paper prototyping can give you rich insights on both culturally specific and general usability issues.
You don’t have to identify all issues in one round of tests. Sometimes, running several rounds of small tests with a different focus is cheaper and easier. For instance, you can test basic usability issues, content, and tone of voice for the first round, while also observing context of use. A second round can focus on testing and prioritizing localized features and components.
After launching the product, you can do testing to research the usability of global components, used for all markets, and further investigate what other features you should localize to keep competitive in the new market.
Once your product has been known to your new market for a while, running quantitative usability testing, often used for benchmarking, can also benefit you. Keep track of your chosen success metrics and monitor them for major changes to evaluate your design work and compare your performance with industry standards.
You can (and should) combine usability testing of your products with other methods such as competitive usability testing, interviews, field studies, diary studies, and other qualitative research methods, to fully understand user needs, mental models, and expectations.
Conclusion
Conducting research with your global audiences can help you uncover issues you can’t find just by testing with domestic users. Perform these studies early and often to ensure success in your international markets.
Reference
Reinecke, K., & Gajos, K. Z. (2014, April). Quantifying visual preferences around the world. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 11-20)."
14,2021-02-14,"Any UX-research study aims to answer general questions about our design or about our users. What percentage of our user population will be able to subscribe to our newsletter? What major usability issues will people encounter on our site? Is design A more usable than design B for our target audience? But any time we set up a UX-research study, whether quantitative or qualitative, there is danger that it will not reflect the reality we want to capture because the study is poorly designed.  
There are two big types of study-design errors:

Internal-validity errors that bias participants towards a certain response or behavior
External-validity errors that capture behaviors or situations which are not characteristic for our target audience

We’ll talk about each of these separately. But before we do, let’s note that validity is separate from reliability. Reliability of a study simply means that you will get the same result if you repeat the study. In other words, findings are not random. There are plenty of statistical methods to calculate the degree of study reliability, and the main way to increase reliability is to test more participants. But reliability is no good without validity: a study with high reliability and low validity is one where you get a really good measurement of the wrong thing.
Internal Validity for UX Studies
Think of a study that compares two sites — site A and site B. You are trying to decide which of the two is  better and you always show the participants in your study design A first, ask them to complete some tasks on it, then move to design B and show them the same tasks. Is this study design likely to produce accurate results, that reflect the reality? In other words, will this study identify the better design?
Not necessarily. This study setup favors design B because, when they get to it, participants will be already used to the testing situation and with the task domain — if they’re testing car-rental sites, they will  already know what a LDW (loss-damage waver) is when they get to site B and they may have certain expectations regarding the steps of the rental process. They will also know what you expect them to do and how they’re supposed to perform the task. Therefore, this study is missing internal validity. (The usual fix to this problem is to alternate which site goes first, and have half of the users try site B first.)
Definition: A study has internal validity if it does not favor or encourage any particular participant response or behavior.
Internal validity is an issue in both qualitative and quantitative studies. With moderated qualitative studies, the facilitator may inadvertently bias or eliciting a certain response from the participants. For example, even a simple questions such as “Have you found the checkout difficult?” may invalidate the study results because the participants are primed  to think of difficulties, so they may identify more than normal (like with Richard Nixon’s “I am not a crook” statement).
With quantitative studies, lack of internal validity may produce results that skew in one direction, but do not reflect the reality. You may, for instance, in a benchmarking study, discover that your time on task is better on a redesigned version of the site than on the original and you may infer that you did a good job with the redesign, when in fact, the difference was due to different study protocols — the original test used the think-aloud protocol, but the test of the redesign didn’t. (And thinking aloud does take some extra time, so it can cause longer task times.)
In this example, the protocol is an example of a confounding variable — a hidden variable that can affect the results of your study, but that you didn’t take into account when you designed the study.
External Validity
External validity is about how naturalistic your study is.
If you’re designing a site for seniors and recruit study participants from the general population, will that study be valid? Will it tell you something relevant about your real audience? Possibly not, because younger participants are likely to behave differently than older ones. Or, if you’re testing a mobile design on a desktop, will your findings generalize to the use of the design in the wild? Maybe yes, maybe no — it’s impossible to know for sure (unless you do another study).  In both these situations, the studies are missing external validity.
Definition: A study has external validity if the participants and the study setup are representative for the real-world situation in which the design is used.
The concept of external validity also applies to both qualitative and quantitative studies — for obvious reasons.
Recommendations for Study Design
Here are some recommendations to help you set up studies that are both internally and externally valid.
Internal validity
Randomization is essential for ensuring internal validity.

Use random ordering of tasks. 

Task order can bias task responses. At the beginning of a study, people are usually new to both the study environment and to the system that they’re testing. It’s normal for them to take longer to perform the first tasks in a session and perhaps make more errors than normal. On the other hand, tasks that are shown at the end of the session might see the effect of participant fatigue.
That is why we strongly recommend that in any test, whether qualitative or quantitative, you randomize the order of the tasks as much as possible. (Sometimes, however, following this recommendation may not be entirely feasible — for example, if the tasks are Log in and Deposit check, it may not be possible for Deposit check to follow Log in).
Additionally, to mitigate the learning phase at the beginning of every session, we recommend that you prepare 1–2 warmup tasks (psychologists call them practice trials) that are irrelevant for your study and that are meant to get participants familiar and comfortable with the study environment and the study procedure. I like to pick easy tasks that bolster participants’ confidence and make them feel relaxed. But, if you do use warmup tasks, make sure that you do not include them in your analysis.

If your study contrasts two or more conditions (e.g., you want to compare your site with a competitor site) and each participant will be exposed to all conditions (i.e., within-subject design), you should counterbalance or randomize the order in which each participant is exposed to those conditions (for instance, the order in which they see your site and your competitor’s).  

This recommendation is related to the previous one — randomizing the task order. However, if you’re testing, say, 2 ecommerce sites, sometimes it may be unrealistic or unfeasible to ask the participant to shop on site one, then add an item to a wishlist on site 2, then go back to site 1 and subscribe to the newsletter, then shop on site 2 — this would be a detrimental and possibly confusing setup, if you want, for instance, to collect post-test questionnaires such as SUS and NPS for the two designs at the end of the session.
In that situation, we recommend that you group all the tasks for design 1 together and all the tasks for design 2 together. You should, however, randomize the order in which participants see the two designs — with some participants seeing design 1 first and others seeing design 2 first. And, within each design itself, the order of tasks should be randomized.

Control study setup from one session to the next and look for confounding variables — hidden factors that could affect your results.

For example, assume a researcher is interested in comparing two sites and uses a between-subject design. She decides to study site A with the participants in the morning sessions and site B with those participants coming for afternoon sessions. If she ends up finding that participants perform better on, say, site A, it could be because site A is better, or it could be because people are less tired in the morning.
Similarly, if a colleague helps you facilitate a study and you divide the sites — you take the sessions with site A and she takes site B, the facilitator is a hidden variable. It could be that one facilitator’s style is more biasing than the other or that one facilitator is a naturally a more pleasant person and participants feel more talkative and relaxed with her.
Thus, if you know that there will be any factors that will need to vary from one session to the next, ensure that they vary for all the conditions in your study.
When you put together a benchmarking program for your organization, planning carefully for internal validity is essential.  You have to document very carefully your study conditions (task wording, study protocol, whether think-aloud was used, and so on) so that they could be replicated in further studies that you will run in order to determine design improvements over time. Otherwise, a difference between a current version of a system and a prior installment may simply be due to study setup rather than to usability improvements.
External validity

Recruit participants who are representative of your target audience — both in terms of demographics and user goals.

In general, researchers are very careful with creating screeners that match the exact demographics of their population, yet that may not be enough to ensure external validity. It could be that your participants are in the right demographics but have very different goals than your users (or they’re simply not motivated enough).  Always strive to find participants who are likely to have the same goals as your users.

Replicate, to the best of your abilities, the natural situation in which participants will use the UI that they test.

Are your participants supposed to use your car-repair mobile application in their garage? Then don’t have them test it in a conference room. The environment — light, dirty hands, place where the phone is positioned, time available, tools available — are all likely to play a role in how usable this app is.
However, sometimes it may be impossible for a study to be externally valid.
Is External Validity Always Possible?
In some sense, any study will lack external validity — we rarely use interfaces with a stranger watching over our shoulder, sitting at a desk or in a lab. (To some extent, one could even argue that some remote studies are more externally valid than in-person ones because at least the participants may be in their natural environments.)  We also know that participants tend to behave slightly differently — more compliant and more persistent — in a usability-testing situation than by themselves.
Also, sometimes, it may be too cost-prohibitive to test a design in the natural environment. For example, we are great advocates of paper prototyping, but these types of tests will always lack external validity.  So, what should we do?
In these situations, some testing is better than no testing. With paper prototyping, it may be that your results are not externally valid and you will have to retest later on in naturalistic conditions. But the goal of paper prototyping is to identify any big hurdles so that you won’t spend money implementing something that is completely off. So, run a paper-prototyping study, identify the big issues, fix them, then move forward to a high-fidelity prototype that you could test in naturalistic conditions, on the device that participants will use to complete the task.
Another common situation that lacks external validity is mobile testing — most participants will not use mobile designs uninterrupted, sitting at a desk, and connected to wifi.  It can, however, be acceptable to test in that setup to identify those issues that will be encountered even in the best-case scenario of a great connection and no interruptions. Those are likely the first issues many mobile sites will need to address — if the site has problems even under ideal conditions, then the design needs to be fixed.  Once you’ve ironed out those issues, you still will need to retest under more realistic conditions.
Similarly, some quantitative-study professionals recommend to include only expert participants in certain quantitative studies in order to reduce variability (lack of variability translates into a lower margin of error for the study results and may allow the researchers to reduce the number of participants). The expert users will give you a best-case scenario and you should be fine as long as you don’t assume that the results will generalize to all your users.
In general, if you find yourself forced to sacrifice some external validity, it’s crucial that you always interpret your findings in context and realize that they may not stand true if the study were to be replicated in realistic conditions.
Conclusion
Poorly planned research will translate in results that are invalid. You may have potentially wasted time and money on running a study which doesn’t tell you anything about your product or your audience. Pay attention to your study’s internal and external validity — strive to recruit participants that are representative of your target audience and make sure that the study setup replicates how your users will use the system in real life and that it does not encourage any one behavior or response."
15,2021-02-07,"Writing good digital content requires a deep understanding of who your users are, how they think, and what they know. Testing your product’s content with users can help you to determine whether:

Your users can easily understand and process the information
The content has the tone of voice you predefined
There are jargon terms that need to be explained

You can evaluate your content using a variety of methods (including eyetracking and cloze tests), but our favorite way is through usability testing. A content-focused usability test can work much like any other such test, but there are some nuances to consider when the primary goal is evaluating digital copy.
Test Structure & Facilitation
Learn About the Topic and Content
As the researcher or facilitator, you should be extremely familiar with the content you will test and with the domain it belongs to. This is particularly important for people working for agencies, since they may be new to the content area. 
For example, let’s imagine we were hired to test the content on Investopedia.com, a site that provides investment news as well as explanations of complex financial concepts written for different experience levels.  We’d need to start by spending hours just to explore the site: learning the types of content offered, the target audience(s), and as much as possible about the content. This latter type of knowledge would be particularly important if we weren’t already very familiar with finance or investment.
We’d want to also spend time with the content creators, as well as subject matter experts. As a researcher, you don’t need to become an expert in the topic (investment), but you do need to have a rough idea of what your participants are reading.
Use Moderated Instead of Unmoderated Studies
In remote unmoderated studies, participants work on their own, with no facilitator present. Even though this variation of usability testing is cheaper, we recommend that you do not use it for content studies. When trying to discover how people research a topic, compare offerings, and make decisions, the best approach is to conduct a moderated study, where a facilitator is present (physically or remotely). 
Facilitators can ensure that participants process the content naturalistically instead of approaching the task superficially. 
Content studies tend to have long stretches of time when the user is simply scanning page after page—in silence. When left alone (such as in a remote unmoderated test), participants may feel awkward and wonder whether they’re helpful. Without proper feedback and reassurance, participants may rush through the test and approach the task in  superficial manner. This behavior is often reinforced by the shorter session times common in unmoderated testing (typically 20–30 minutes).
Additionally, having a facilitator enables specific, personalized follow-up and clarification questions, such as “I noticed you hesitated on this paragraph, can you tell me what you were thinking?” 
The table below contains more examples of valuable follow-up questions for content studies.



Follow-Up Question
Goal


What did you think about that information?

			If you could change anything about that information, what would it be?

			What was easy or difficult to understand, or why?
Encourage participants to share any problems or issues they noticed with the content


What does the word [X] mean to you?
Determine whether a technical term makes sense to participants or if it’s jargon and needs better explanation


If you had to explain this to a child, what would you say?

			Can you please summarize that information in your own words?
Evaluate whether participants understood what they just read

			(If they can easily and correctly summarize the content in their own words, it’s a good indication that they understand it. But if they have to look back at the text and read verbatim from it, they probably don’t have a clear understanding.)


Imagine a person said these words to you. Who would that person be?

			What would they look or act like?

			What job would they have?
Subtly prompt the participant to describe the tone of voice of the content



Note: Don’t use the word “content” when speaking with study participants — users don’t typically have the same associations with that word as content professionals.
Be Comfortable with Silence
Being comfortable with silence is important for any kind of facilitation, but it’s especially necessary for content testing.
Expect long stretches of quiet time while the participant focuses on processing the information. Don’t appear impatient. Avoid being interruptive or fidgety. Injecting too many questions while users work breaks their concentration and alters their behavior. 
If you need to ask a question mid-task, keep it neutral, such as “What are you thinking?” or “What are you looking for?” Once users answer, let them continue. Resist the temptation to blast questions. Wait until the participant has finished reading and is ready to provide feedback. 
Participants & Tasks
Recruit the Right Participants
You should always aim to test your designs with representative users. However, when testing content, you should  take extra care to recruit the right participants.
The people evaluating your content should truly be representative of your user population: they should have the same mindset, situation, and user goals — especially  when your tasks are content-rich, research-intensive activities.
In other words, the scenario that you give people should match a problem they need to solve in real life. Unlike UI-focused studies, content-focused studies should not ask test participants to “pretend” or “imagine” to be in a situation. The risk of invalidating the study by using the wrong participants is higher for content studies because the participants’ motivation and background knowledge is much more important for obtaining accurate insights.
Consider the National Cancer Institute, a medical reference site that describes various forms of cancer and their treatment. Some of the content is intended for patients and some for healthcare professionals. 
People who have been diagnosed with a serious medical condition are more likely to relate to the content accurately than someone asked to pretend to be interested about a disease. Beyond their different level of emotional involvement, patients  may know more about the disease from speaking with their doctor or doing their own research. In this situation, it may be acceptable to also recruit the primary caregivers for someone who was diagnosed (for example, a person whose partner had the disease), as long as that person was highly involved in the care.


To test this article about adult non-Hodgkin lymphoma on the National Cancer Institute’s site, we’d need to recruit people who had been diagnosed with the disease or were the primary caregivers for someone who had it.


It is impossible for proxy users to instantly acquire knowledge or know the situation well enough to assess the value of the content — especially when the content is scientific or technical.
In our Investopedia example, to evaluate most of its content, we would need recruit people genuinely interested in learning about investment. The odds are very high that a random person will not have the background knowledge to understand or enjoy an article titled “Understanding Preferred vs. Common Stock.” Even if the content works very well for a specific audience, it isn’t likely to be valuable for everyone in the world.
The challenge of content testing is precisely that — whether content works well depends so heavily on who it’s written for.  
Tailor Tasks to Individual Participants
In most traditional usability studies, researchers follow a prepared script and give study participants preestablished tasks to perform. Content testing often requires flexibility to ensure that each individual gets the right task.
It’s OK to prepare some generic tasks prior to the study, but be willing to modify or craft new ones on the spot as you learn more about the participant’s situation and as the session unfolds. You want to give participants the freedom to research a topic as they please, so you uncover what’s important. Don’t force an unrealistic task. The more pertinent the content tasks, the more natural people’s behaviors are when completing them.
The best results occur when study participants forget about the testing environment and immerse themselves in the activity. Participants can sometimes “fake” their way through simple pass–or–fail activities (e.g., “Find the contact name for Press Relations”), but such is not the case for exploratory tasks where having a scenario that precisely matches the person’s current situation and emotional state is critical. 
In the Investopedia example, the site’s huge collection of articles targets people with different financial expertise, from complete investment beginners (“Investing Essentials”) to intermediate or advanced users (“Guide to Technical Analysis”). Some of the specialized topics  may interest only a subset of its audience: someone who is interested in learning about how to invest in index funds may not be interested in learning how to trade options.
In a test of the site, we might spend time at the beginning of each session (or schedule a prestudy interview) to discuss the participant’s situation and make sure the task scenarios match each individual’s exact circumstance.  For example, we might ask:

How long have you been investing in the stock market?
What types of investments are you familiar with? (Stocks, mutual funds, options, etc.)
What types of investments are you interested in learning more about?

We might even gently quiz participants, asking them to define various concepts, to assess their domain knowledge. 
These questions would give us a sense for the participants’ experience level and interests. In addition, it could help us avoid presenting them with tasks that are uninteresting, irrelevant to them, or beyond their individual ability to understand (because they were written for a different experience level).
Content Testing Often Requires Open-Ended Tasks
To test content properly, write open-ended information-seeking tasks around the content of interest.
Unlike specific, directed tasks (e.g., “Find the opening hours for the Fremont public library”), open-ended tasks don’t have a definitive answer, but are meant to assess content quality and relevance. Use them to learn how people explore and research, what questions they have, how they expect information to be communicated, and whether your site meets their needs.
Open-ended tasks have vague end points, often making participants wonder how to best spend their time. At the beginning of each session, tell people to work at their own pace, as if they were by themselves, and not to worry about the time.
For example, to evaluate the Investopedia article “Smart Strategies for a Bear Market,” we should not take participants to that page and just ask what they think about it. That isn’t how people arrive to a content page. We must give them an information need such as: 

“The stock market has been declining recently and seems like it will continue to do so. You’re looking for advice on how to invest during a market downturn. See if you can find advice on Investopedia.com.”  



To test the Investopedia article “Smart Strategies for a Bear Market,” we have to write an open-ended task that might lead participants here.


Consider competitive testing. Sometimes you can get insights into your users’ needs by allowing them to search freely on the web or by letting them visit competitors’ sites rather than restricting them to your own site. Don’t worry that you’re wasting precious testing time: if users are truly representative, the insights will often be revelatory. And you can always limit the free exploration to a small part of your session.
Conclusion
When the focus of your study is to evaluate content, you may need to adjust traditional research techniques to learn how to improve your content and to ensure valid results. Take extra care to give participants tasks that are realistic and match their current situations, interests, or needs.
We have run hundreds of content studies with the methods discussed here, as well as eyetracking research. Check out our course, Writing Compelling Digital Copy, to learn what we found."
16,2020-10-04,"Effective user researchers balance project-schedule constraints with their research budget. Efficient researchers attempt to spend only as much as needed, so the remaining money can be used for more research.
When I run research projects, I try to spend enough money on recruiting and incentivizing participants, to ensure that (1) I find the right kind of users and (2) that they are motivated enough to show up for the sessions (and are also fairly compensated for it). “No-show” participants or last-minute cancelations can thwart the project schedule, make stakeholders lose interest in the research, and waste money.
Consider the expense of each testing slot in a usability study or other UX research activity:

Amortize the planning expenses across the study participants. So, for example, 20 hours spent planning a 5-participant study amount to 4 hours of expensive UX-specialist salary for each participant slot.
Look at the costs of using the testing space. For an in-person study, renting lab space that stands empty because of a no-show user is an out-of-pocket cost. Even if you’re testing in, say, a meeting room in your own building, there’s the opportunity cost of having booked the room and prevented other employees from using it.
Include team members’ time. The biggest expense of a scheduled study slot that goes unused is often the cost of the team members (including yourself) who are waiting for the participant.

Don’t forget the reduced research insights stemming from fewer research participants than the plan called for. If you follow our recommendations for small sample sizes for most qualitative studies, each lost participant represents a fairly substantial proportion of your potential insights, and thus reduces the profitability of your design project.
When to Recruit Floaters
There are some cases when it’s appropriate to throw thriftiness to the wind and recruit extra users, also known as “floaters.” This strategy makes it likely that you will fill in each research slot. Some scenarios when this approach is beneficial include:

The project schedule allows only a small time window for research. For example, a client may need a usability test and its results within a few days.
An external factor imposes time limits. For example, equipment or the location for a lab study are rented and only available for a finite period.
The participant background requires partially screening the participants on site, at the time of the study. For example, in an eyetracking study you may need to calibrate the users’ eyes with the eyetracking system onsite.
A lot rides on observers seeing a particular research session or set of sessions. For example, if a team had to be convinced to do research and is skeptical, and it finally agrees to observe at a certain time, you need that user to be there at this critical point.

To ensure that you have enough participants when you need them, you can practice “intentional overrecruiting.” Overrecruiting can manifest in a few ways:

recruit multiple people (a participant and a floater) for each scheduled slot
recruit one participant for each slot and a floater for every two slots
add more sessions (and more participants) than needed to the research schedule

One Floater per Participant
The most expensive method, and the one that also gives the most insurance, is the one floater to one user — that is, recruiting two people per slot, even though you’re testing with only one of them.
A research day might look like this:





Sessions
Times
Participants
Floaters
Recruiting cost $80 each
Incentive cost $100 each





1


8:00AM –  9:00AM


User1


Floater1


$160


$200




2


9:30AM –10:30AM


User2


Floater2


$160


$200




3


11:00AM – 12:30PM


User3


Floater3


$160


$200




4


1:00PM – 2:00PM


User4


Floater4


$160


$200




5


2:30PM – 3:30PM


User5


Floater5


$160


$200




Totals


 


 


 


$800


$1,000




If the recruited participant shows up, I pay the floater for that time slot and let her go.  
Floaters recruited for just one session are usually paid the same honorarium as participants. So, if the participant’s incentive were $100 per participant, the single-session floater’s incentive will also be $100.
With this approach, the cost for the recruit and incentives per study is twice as much, provided all users arrive for their sessions. For example, if the recruiting and incentive cost is $180 per user and you had 5 users, then the total cost without floaters would be $900 and the cost with floaters would be $1,800.
One Floater Per Two Users
A little less expensive but also a less safe approach is to recruit one floater for every two sessions. With that pattern, a research day might look like this:





Sessions
Times
Participants
Floaters
Recruiting cost $80 each
$100 each participant and single-session floater, $200 each double-session floater





1


8:00AM – 9:00AM


User1


Floater1


$160


$300




2


9:30AM – 10:30AM


User2


Floater1


$80


$100




3


11:00AM – 12:00PM


User3


Floater2


$160


$300




4


12:30PM – 1:30PM


User4


Floater2


$80


$100




5


2:00PM – 3:00PM


User5


Floater3


$160


$200




Totals


 


 


 


$640


$1000






In this case, each floater would be available for two sessions. For example, if User1 shows up, the floater would stay available to make sure that User2 also shows up. If User2 does show up, the floater is released at the beginning of the second session. This is probably the best-case scenario for the floater since she basically needed to be available for an hour and a half and got paid for being at the full sessions. 
If User1 doesn’t show up, the floater would take that person‘s place. And, the researcher would hope that User2 is going to show up because there is no floater for that session. If User1 shows up but User2 does not, the floater would take User2’s session. This is probably the worst situation for the floater because she had to wait for one session and then also be a participant. 
Floaters who must be available for multiple time slots are usually paid more. I usually pay double the incentive. (Some people pay more than that because floaters sign up to not only be available for a longer time but participate in the study if needed. Others pay floaters more only if they end up being a test participant.) So, if $100 was the participants’ honorarium, a double-session floater would be paid $200. (Note in the example above, since there is an odd number of sessions, one floater will cover just one session and will be paid $100.) If the research was remote, one might pay floaters less because they are not asked to travel or sit in some space for a period of time; rather they can do other things in their home or office while being on call.
How to Recruit Floaters
The point of having a floater is to ensure that you cover all your research sessions. It’s essential that they show up, whether in person or for a remote study. Thus, when recruiting them, they should not feel as though they are superfluous. I’m always honest with floaters. I recruit them using the same screening criteria used for study participants. Once they are deemed the right target for the study, I explain their role:
We have recruited all the timeslots for the study, but we need to make sure we actually have a person in each session. Since sometimes people have last-minute cancellations, we need to recruit more people to be available in case an already scheduled participant can’t make it.
For one-to-one: So, we would ask you to come (or log in for a remote study) as though you are participating. If the scheduled participant arrives as planned, we will pay you your $100 honorarium and release you.
For one-to-two: So, we would ask you to come (or log in for a remote study) as though you are participating. But we would ask you to be available for two sessions, starting at 8 AM and ending after 10:30AM. If the scheduled participants arrive as planned, we will pay you $200 and release you. But if either user doesn’t arrive, we would ask you to participate in the study.
For a remote research session, I ask floaters to log into the session as they would if they were a user. For in-person sessions, I inform the floaters that there will be a waiting area and wireless and drinks available and they will be asked to stay there until released. If there is no such place, I try to find a café nearby where they can go and wait as needed.
How to Dismiss Floaters
If the situation was explained to floaters when they were recruited, there should be no problem dismissing them. Still, make sure you don’t hurt their feelings or make them feel like they did something wrong and that’s why they are being dismissed. I usually say something like this:
As you know, you were recruited to ensure we had enough participants per session. The original person we scheduled has arrived, so we are all set. Thank you so much for agreeing to be part of our study but we won’t need you this time. Here is (or I will send you) your honorarium of $100 as planned. And I hope we can keep you in mind to participate in future studies. Thank you very much.
Using Floaters to Optimize Participants
Another advantage of recruiting two users for each study time slot is that, if both show up as scheduled, you could choose the one most suited for your study. In that case, you would not designate one person as the “main” participant in advance and the other as the floater.
Picking the most suited participant can be useful in studies:

Targeting a particularly difficult recruiting profile or one that is challenging to prescreen — for example, if you are testing a high-end investment product and want to see the investors’ skills before the study
Requiring a certain demographic distribution — for example, an even age distribution

If all your early test slots are filled by younger users, you would then prioritize older users for the last slots.
(However, avoid choosing participants based on random criteria — such as which participant is more talkative or seems more pleasant or likeable— because you can easily bias your study results doing that.)
More Research Slots and Cancel as Needed
A little different from scheduling floaters is to schedule extra study sessions. If you’re not on a tight deadline and you (or your team) can pivot and do other work if a user doesn’t show up, this method can be an effective way to ensure you have enough participants, and is usually less expensive than the floater models.
A schedule using this model might look like this:





Day1 Sessions
Times
Participants
Recruiting cost $80 each
Incentive cost $100 each





1


8:00AM – 9:00AM


User1


$80


$100




2


9:30AM – 10:30AM


User2


$80


$100




3


11:00AM – 12:00PM


User3


$80


$100




4


12:30PM – 1:30PM


User4


$80


$100




5


2:00PM – 3:00PM


User5


$80


$100




 





Day2 Sessions
Times
Participants
Recruiting cost $80 each
Incentive cost $100 each





6


8:00AM – 9:00AM


User6


$80


$100




7


9:30AM – 10:30AM


User7


$80


$100




Totals for both days


 


 


$560


$700




For unmoderated (no facilitator) remote research sessions, I usually recruit one extra person for every 3 sessions. For example, if I want 5 sessions, I recruit and run the study with 7 people. Once you have enough data, you can cancel the sessions you don’t need (but, of course, still pay those users in earnest). If enough users don’t show up even with the added sessions, you can schedule more sessions.
Often, though, it makes sense to run the extra sessions even if all the needed participants did show up, for two reasons:

Unusable data in some of the sessions. After I watch the recordings, I sometimes find some issue with one of the sessions and can’t use all or most of the data. Overrecruiting thus covers not only for no-show participants, but also for sessions with poor data and saves you the cost of relaunching the study and waiting for feedback.
Additional insights. Sometimes I need to follow up on something discovered in earlier sessions — if that’s the case, I can take advantage of those later session to change tasks or interview questions and investigate the new questions.

Wouldn’t just Paying a Higher Incentive Eliminate the Need for Overrecruiting?
Research participants driven by the incentive can be more likely to show up if that incentive is larger. So, you could experiment with paying a higher incentive and not recruiting floaters. You would save the recruiting costs of an additional user and could maybe pay a higher incentive to the main participant. But this strategy works only if the factor preventing participation is related to the incentive. It doesn’t account for emergencies or last-minute schedule changes, which do happen, especially when testing users in certain professions, such as system administrators or physicians. (Also, consider whether a very high incentive could bias your study.)
Isn’t Intentional Overrecruiting Wasteful?
What I like least about intentional overrecruiting is that time and money are sacrificed by the team, users, recruiters, and researchers.
Deciding to use these overrecruiting methods is a lot like deciding whether to buy optional insurance. For example, is spending $100 per month on long-term healthcare insurance worth it so that, if you need care in your old age, you have it? Likewise, what is the insurance of having a user at each allotted time worth to your team?
The table below compares the time, recruiting, and incentive costs for a 5-user study using the three overrecruiting models.





 
1 floater to 1 user per session
1 floater per 2 users
More research slots
None





Number of Research Sessions

5   
5   
5 (+ optional 2)
5   



Days Until Research is Complete


1


1


1.5


1




Number of Participants to Recruit and Pay


5


5


7


5




Number of Floaters to Recruit


5


3


0


0




Recruiting Cost @ $80 each


$800


$640


$560


$400




Incentive Cost @ $100 each participant and single-session floater, $200 each double-session floater


$1000


$1000


$700


$500




Total Recruiting and Incentive Cost


$1800


$1640


$1260


$900




Assuming the researcher facilitates five sessions:

The recruiting and incentive costs are the most ($1800) for the 1-floater-to1-user overrecruit model, and least ($1260) for the more-research-slots model.
The risk of not having enough users in the study is the lowest with the 1-floater-to-1-user model, and higher for both the 1- floater-per-2-users and more-research-slots models.
The time before research is complete is the least (one day) with the 1-floater-to-1-user and one-floater-per-2-users models and most (one and-a-half days) with the more-research-slots model, which requires additional sessions and thus time.


In the example above consider the extra cost was $900.

Is the assurance that you will get the feedback you need from target users within the specified time worth $900 to your team?
Do you have that in your budget?
What’s the opportunity cost of that $900 — that is, what else do you need to do and spend money on where the $900 could generate a higher return?

These are the types of questions you should ask yourself when you are thinking about overrecruiting."
17,2020-08-23,"You‘re more likely to encounter problem participants in a remote unmoderated study, as compared to remote moderated or in-person usability testing studies — especially if you recruit from panels hosted by dedicated testing services.
It’s important to identify people whose behavior is not representative for your user population   and exclude their data from your analysis. (Testing representative users is one of the core principles of usability testing, and unrepresentative participants invalidate many of the findings from a study.)
In this article, we’ll discuss how to identify three types of problem participants: outliers, cheaters, and professional participants.

Outliers are participants whose behavior or performance is very different from the rest of your user population, either because they are not part of your target user group or because they are in exceptional in some other way.
Cheaters are participants interested only in getting paid and moving on to the next study. They may click randomly and not even attempt to perform the tasks.
Professional participants are people who participate in too many studies too frequently.  Often, these people are not representative of ‘regular’ users because they have seen too many UX-research studies and are too attuned to researchers’ goals.



Problem participants can be professional participants, cheaters, or outliers.


Note that cheaters are often outliers — in other words, people rushing through the test without really trying usually stand out from other participants in your data in some way. 
However, not all outliers are cheaters. Some users will behave differently from the rest of your participants because they are different, not because they are trying to cheat you for incentive money. (In past research we found that 6% of task attempts were uncommonly slow, which we explained by “bad luck” since we didn’t have a better explanation for these outliers.)
The best way to identify problem participants depends on whether you’re running a qualitative or quantitative usability test. 
Qualitative Studies: Watch the Recordings
Most remote unmoderated testing platforms record videos of the participants’ screens (and sometimes their webcams) while they perform the tasks. If you’re running a qualitative test with around 5–8 participants, then you should be planning on watching all of the videos anyway as part of your analysis. 
While you watch the videos, keep an eye out for signals that you might have a problem participant.
Outlier Signals
Watch for any comments or behaviors that could tell you that the participant has a different experience level, background, or motivation from the rest of your users.
For example, if you recruit industrial engineers, but one participant sounds very confused by the terminology used in the UI, he may not actually have a background in this field. If you didn’t ask the right questions in your screener to assess knowledge, he might be a participant with good intentions who just ended up in the wrong study.
Cheater Signals
Look for participants who don’t try the tasks at all. Sometimes you’ll even see participants who  receive the task instructions, don’t read them, and then go off and read Facebook or some other site for a few minutes, before clicking to advance to the next task.
However, just because someone is impatient with your design does not make her a cheater. Many users are demanding and expect products to work perfectly and easily on the first try. If your site takes forever to load, they may try to do something else meanwhile. There’s a difference between an impatient, demanding user and one who does not attempt to perform the task at all. 
Another signal for a cheater is that your participant ignores the task instructions or parameters. 
For example,  if the task is  “Visit West Elm and find a dining chair with a metal frame,”  but the participant picks  the first item (a coffee table) he sees (and says  he is done), then he may be simply trying to get out of the task as soon as possible.
Professional-Participant Signals
Professional participants are the most difficult to identify. These are people who will (in most cases) try your tasks and give you a lot of feedback. They are often extremely good at thinking out loud. They’ve done many studies, and they’ve learned what researchers want from them.
I tend to identify professional participants more easily by their comments than by their behaviors. Listen for any terminology that betrays too much knowledge that they might have picked up from participating in studies too frequently (“SEO,” “kerning,” “mental model,”, “menu bar,” “hamburger”).
(Note: Many people these days have learned terms like “user friendly,” “usability,” or even “user experience” from ads and popular culture, so that’s not always a warning sign.)
For example, while searching for information on a nonprofit website, one of my study participants said, “Look, the wizardry of using a search engine escapes a lot of people. Their inability to form queries makes them think that search engines are useless. In my experience, learning how to rephrase your question to get to the answers you’re looking for is critical to their use. So it’s not really easy or difficult, it’s more the user’s experience with the search function.” The amount of correctly used jargon made me suspect that this person might either be a professional participant or might have worked in a UX-related field.
Quantitative Studies: Start with Metrics
If you ran a quantitative study, with more than 30 participants, watching all of the videos may not be practical. You can use metrics to help you decide which videos to check. You can also spot-check each video (by watching several minutes of each one).
Most quantitative usability studies involve collecting at least two common metrics: time on task and task success. Remote unmoderated testing tools often collect these two metrics automatically as they run the test, so you probably already have access to them. Look at these metrics to identify responses outside of the normal range of your data. Check multiple metrics to help you decide if individual participants look suspicious, and then watch the videos for those participants to confirm that they are indeed nonrepresentative.
Note that metric-based methods are good for identifying outliers and cheaters, but not usually as useful for identifying professional participants. 
Time on Task
Look at frequency distributions of task times for individual tasks, as well as the total session time for all participants, to identify those moving much more quickly or more slowly than the rest of the participants.


The two participants who completed the task in less than 9 seconds were much faster than the rest. These might be cheaters. The four participants who completed the task in more than 179 seconds could be cheaters, unrepresentative participants, or just people who needed more time to complete the task. You’ll have to investigate to find out. (The histogram shows, for each time interval, the number of participants whose time on task was in that interval.)


When participants complete tasks and sessions very quickly, they might be cheaters. When they complete tasks and sessions much more slowly than the other participants, it’s possible that they’re cheaters, outliers, or neither — just people who need a little extra time or encountered an error.
Task Success
Similarly, we can look at task success by participant for each individual task as well as the success rate per participant over the whole session. Low success rates combined with very fast task times are usually a strong indicator of a cheater.


The same two participants who completed Task 5 in less than 9 seconds have a very low success rate across all tasks. Flag these participants and  follow up by watching their videos. (The histogram shows the number of participants who had success rates in the intervals shown on the x axis.)


Open-Ended Text Responses
When planning a quantitative study, it’s also always a good idea to include a question with open-ended text field, where participants have to type a response. You can quickly scan the list of responses and identify any “lazy” answers that might signal a cheater.
Let’s look at some real-life responses to the open-ended question “If you could change something about this website, what would it be?” 




   


Example Response


Description




    


“Asiojdfoiwejfoiasjdfiasjdf”


These nonsense responses look like someone just slammed the keyboard to move forward with the study, without bothering to read the question and formulate a response. This is a strong indicator of a cheater.




    


“Its fine”


Very short non-answers with incorrect or no punctuation, like this one, can sometimes signal a cheater, but not always. In this case, the participant may have been fatigued from a long study, or just didn’t have any strong opinions to share.




    


“On the main page I would put more basic, compelling information about the ocean -- everyone [almost] has some connection to it, whether it be the beach as a kid, trade, kayaking, swimming, cruises, boats, etc. I would just stress if possible the amazing ocean animals/life and how important the ocean is to trade, military, fun etc.”


Detailed, thoughtful responses like this one are a strong signal that this is not a cheater participant.




Next Steps
When you identify problem participants, deal with each type in slightly different ways. 
Outlier
Outliers who are very different from your target audience should be removed  from your analysis entirely. 
Go back to your recruitment and try to determine how this person ended up in your study. What exactly is different about this person that makes her not fit with the rest? Was there a problem with your screener that allowed this person into the study? Learn from this mistake, to ensure better recruits in the future.
However, be sure that the participant truly isn’t representative of your user population. A UX professional once asked me if he could remove one participant’s data from his analysis, because she had given a very negative response while other participants were positive. I asked, “Well, do you have any other reason to believe she’s somehow unrepresentative?” He did not. An unfavorable response to our design is not a good enough reason to remove someone from the data.
Cheater
It’s often the case that a participant might “cheat” on one or two tasks, particularly at the end of a long session, but will actually try the others. Determine whether that’s the case with your cheater, by watching the participant’s full video. If the participant cheated only on one or two tasks, simply remove her data for that task. If they cheated through the whole session, remove them entirely.
Most remote testing tools are aware that cheaters are a problem. If you recruited the participants through the tool’s panel, many offer to replace cheater participants for free if you request it. 
If the recruiting service has a way for you to provide feedback about a participant’s performance, please ensure that you do so, as a courtesy to other researchers.
Professional Participant
Professional participants are sometimes trickier to deal with. In most cases, they haven’t done anything wrong — they showed up and participated, so they should be compensated and not reported or negatively reviewed.
The best thing is to avoid letting these people into your study in the first place. I always include a screener question that asks how recently the respondent participated in a study, and I exclude those who participated too recently (0–3 months or 0–6 months). However, there’s nothing to stop participants from lying in response to that question. Some testing tools allow you to filter out frequent participants automatically. However, these professionals are often performing tests on many different platforms (and there are a lot out there).
If you found a professional participant, look at the video and data to decide whether to throw out the session. Sometimes you’ll find that the participant made “professional” sounding comments, but the actual behaviors and data look very similar to the rest of your participants. In those cases, you can keep the data. Just make sure you flag that participant in your qualitative analysis and weigh that fact as you draw conclusions from that participant’s comments and feedback.
Conclusion
Keep any eye out for outliers, cheaters, and professional participants in your studies. Investigate multiple sources of information to help you sleuth out the situation, and decide what to do about it. If you frequently find too many problem participants in your studies, you should reevaluate how and where you recruit participants.
For more help planning, conducting, and analyzing remote studies, check out our full-day seminar, Remote Usability Testing.
 "
18,2020-07-26,"There are two flavors of remote usability testing: moderated (using a facilitator and screensharing app) and unmoderated (tasks are administered by a testing platform, without a facilitator.
One reason why unmoderated remote usability testing tends to be more popular than moderated testing is that it’s seen as cheaper and faster. While unmoderated might save some money and time, it may not be as much as you think — and it could come at the expense of findings. We compare typical costs for each type of study.
Factors that Impact Study Costs
Two of the biggest factors that influence the cost of your study is who your participants are and how they will be recruited. Monetary costs will increase if your user population is very busy, rare, or affluent — in those cases, you'll probably need to offer higher incentives. Similarly, if you hire a recruitment agency to find and schedule your users, you might pay a much higher per-recruit fee than you would if you used a participant panel.
These estimates for time and money assume that a research practice is already established in your organization. Expect a study to take longer than our estimates if:

Stakeholders or clients must be first convinced that the research is needed.
Clear research goals have not yet been defined.
The researchers are unfamiliar with the tools used during testing and must spend time learning them.
Participants are difficult to find and schedule.

Our time estimates also do not include the time required to design and build the product or prototype to be tested.
Additionally, our high estimate for an unmoderated platform subscription is $100 per month. Organizations can spend much more than that depending on the tool. Some large platforms with lots of features and tools (like UserZoom) can cost thousands of dollars per month.
We include costs for a video-conferencing application, but you may not need to include it in your own cost calculations. Ideally, you might be running many studies over time, so that cost is amortized across projects. Also, for moderated studies, your organization is very likely to already have a video-conferencing app for other purposes, so that isn’t a new cost specific to the study. You may choose not to include that aspect when calculating your costs.
Study Estimates
These are rough estimates for a typical 5-participant qualitative usability study conducted in the United States. You’ll likely find that costs vary in different locations, particularly the required incentives.
Usability testing is an immensely flexible and versatile tool, so different variations (such a quantitative usability testing, which requires 20–40 participants), will have different costs.
Table 1: Typical Cost Estimates for a Remote Moderated Study in the US




 


Low Estimate


High Estimate




Video-Conferencing App


$15 / month


$30 / month




Recruiting Fees


$0 / participant


$80 / participant




Incentives


$80 / participant


$250 / participant




Planning & Setup


16 hours


24 hours




Conducting Sessions


90 min / participant
(60 min sessions)




Analysis


8 hours


16 hours




Costs for a 5-participant study


$415 + 32 hours


$1,680 + 48 hours




Typical costs for a remote moderated study in the United States. On the low end, a moderated study for 5 participants might cost around $415 plus 32 researcher hours. On the high end, with high recruitment fees and big incentives, it might cost around $1,680 and 48 hours.
 
Table 2: Typical Cost Estimates for a Remote Unmoderated Study in the US




 


Low Estimate


High Estimate




Platform Subscription


$0 / month


$100 / month




Recruiting Fees


$0 / participant


$80 / participant




Incentives


$50 / participant


$150 / participant




Planning & Setup


4 hours


12 hours




Conducting Sessions


30 min / participant
(20 min sessions)




Analysis


4 hours


12 hours




Costs for a 5-participant study


$250 + 11 hours


$1,250 + 27 hours




Typical costs for a remote unmoderated study in the United States. On the low end, an unmoderated study for 5 participants might cost around $250 plus 11 researcher hours. On the high end, with high recruitment fees and big incentives, it might cost around $1,250 and 27 hours.
Unmoderated Can Have Hidden Costs
Based on these rough estimates, you might expect an unmoderated study to cost anywhere from 20% to 40% less than a moderated equivalent. Your researchers might also save around 20 hours of time.
But remember, there are tradeoffs associated with using unmoderated instead of moderated testing. 
If you’re using an unmoderated-testing platform’s panel, you might be at higher risk of cheaters or “professional” participants — people who just want to get paid and move on or who participate in studies on a regular basis. You might have to remove those participants and replace them. Some platforms have good policies on replacing lackluster participants at no cost to the researcher, but others don’t. To be safe, we typically recommend overrecruiting a bit. So instead of testing 5 participants in an unmoderated study, you might want to test 6.
Researchers often prefer unmoderated testing because they feel it will save them time — no need to run the sessions. But remember, particularly in qualitative testing, you should still be watching those recordings to analyze them. You might find that whatever time you’ve saved in conducting the session  will be shifted into the analysis phase of the project.
Notice a big difference between the two estimates above — the moderated sessions are planned as 60-minute sessions, while the unmoderated sessions are 20 minutes. That’s because most testing platforms have short limits on how long sessions can run. For this reason, long, complex tasks may not be well suited to unmoderated testing.  And if you are planning to test many tasks on the same site, you may need to recruit more participants (with each participant doing only a subset of the tasks).
And finally, remember that the core difference between moderated and unmoderated testing is the presence or absence of a facilitator. A facilitated session often results in more focused sessions and more insights than an unfacilitated one, and that’s something that you might miss in unmoderated testing.
Consider All Costs
Unmoderated testing can be a great option for teams with limited resources. While moderated testing can be slightly more expensive, that cost might be worth the insights to some teams. 
Just make sure you consider all the specifics of your situation (tools, schedule, users, and recruiting) and weigh those against your research goals when you make your choice.
 
For more help planning and conducting remote studies, check out our full-day seminar, Remote Usability Testing."
19,2020-05-03,"A redesign is usually intended to produce a change in the user experience. We want to make the experience better (faster, easier, or more enjoyable). But better compared to what? Establishing a UX benchmarking practice is a great way to make sure you’re moving in the right direction and that you have a clear reference point for any improvements.

UX benchmarking refers to evaluating a product or service’s user experience by using metrics to gauge its relative performance against a meaningful standard.

How Benchmarking Works
We refer to benchmarking as a practice because, ideally, once you begin benchmarking, you can track your progress over time, again and again, redesign through redesign — it’s an ongoing process.
Essentially, benchmarking involves collecting quantitative data that describes the experience.  For example, we might collect any of the following UX metrics:

Average time to make a purchase
Numbers of clicks on a Submit button
Success rate for an application completion
Average ease-of-use rating for creating an account
Eight-week retention rate for an app (percentage of users continuing to use the app after eight weeks)

You can collect those UX metrics using potentially any quantitative methodology, but analytics, surveys, and quantitative usability testing are the three methods that often work best for benchmarking. You can also use customer-service data (for example, the number of support emails about a specific task).
Once you have those numbers, you have to compare them to something. A single number is meaningless, since you have no idea whether it’s good or bad. With benchmarking, you can compare your UX metrics against four different possible reference points.




Compare Against


Example




An earlier version of the product or service


In 2019, the average time to make a purchase was 58 seconds. After our recent redesign, the average time to make a purchase is now 43 seconds.




A competitor


Our success rate for application completion is 86%, while our competitor’s is 62%.




An industry standard


The average ease-of-use rating for creating an account on our hotel’s website is 5.3/7. The average ease-of-use rating for that task in a study of the top 6 hotel websites was 6.5/7. 




A stakeholder-determined goal


Our eight-week retention rate is 8%, but we’re aiming for at least 15%.




There’s no reason you couldn’t compare against several of these reference points. When you take your first benchmarking metrics (often called the baseline), you won’t have data from earlier versions to compare against, so it’s good to compare against a competitor or industry standard at that point.
If you work in a very niche or private industry, it may be challenging to find the exact industry standards you want. Sometimes academic institutions will publish useful data. In addition, Jeff Sauro’s team at MeasuringU.com conducts industry-standard studies for major business sectors (for example, banking, airlines, hotels, etc.).
When to Benchmark
You might be familiar with a type of UX research that helps us learn what  works or doesn’t work about a design and figure out how to fix those problems. That type of research is called formative evaluation — it helps us decide how to form or shape the design. Qualitative interviews and usability testing are frequently used for this purpose. 
Benchmarking is not formative; it is a summative evaluation: in other words, it helps us assess the overall performance of a (sort of) complete design (a summary of its performance). Our designs are never really complete, we’re always improving them. But you can think of a benchmarking study as a kind of snapshot in time, capturing the experience of a specific version of the product or service. Benchmarking can happen at the end of one design cycle and before the next cycle begins.


Your design process may not look exactly like this. Your version might have different labels for the components or a different number of steps, but you’re probably doing some variation of these activities. 




Benchmarking can happen at the end of one design cycle (for example, after the evaluate phase) and before the next cycle begins (for example, before the define phase).


It’s up to your team to decide how often you conduct benchmarking studies. Their frequency may also depend on the methodologies you’re using. Quantitative usability testing can be very expensive and time-intensive, so maybe you’ll decide to perform a benchmarking study once per year. Analytics can be much quicker for collecting benchmarking metrics, so maybe you’ll benchmark after each major product redesign.
Why Benchmark
Benchmarking allows us to assess our impact and improvement. It’s helpful for reflecting on our process and design choices. There’s a lot of value that benchmarking can provide to product or service teams.
But benchmarking’s real power comes in when you show those results externally, for example, to stakeholders or clients. You can demonstrate the impact of your UX work in a concrete, unambiguous way. And you can even take that a step further by using those metrics to calculate return on investment (ROI) — show stakeholders and clients exactly how much more they’re getting in return for what they paid. That way, you’ll have no problem arguing for more funding and bigger projects in the future.
 
For guidance and hands-on practice creating your own UX-benchmarking plan, check out our full-day course, Measuring UX and ROI.
For help analyzing and interpreting quantitative data, check our full-day course, How to Interpret UX Numbers: Statistics for UX."
20,2020-04-26,"Remote moderated usability testing has a lot of advantages. Compared to in-person studies, it’s often less expensive, less time-consuming, and more convenient for participants. In cases where participants can’t travel to a testing location, remote moderated usability testing is an excellent alternative.
Perhaps the biggest downside to remote moderated usability testing is that it can require lots of advance preparation, particularly in setting up the tool you use. If you’ve never done a remote moderated study before, it might be tough to know where to begin.
Follow these 7 steps to ensure your remote moderated sessions run smoothly and yield rich, deep insights.
Study Planning
1. Choose a tool for communicating with the participant.
Choose a screen- and audio-sharing tool that will be easy to access and install for testers, but also for observers and facilitator.
You don’t have to use the same tool for all test participants. For example, those joining from their office computer may need to use a company-sanctioned meeting tool. Others may not be able to install any software — for example, due to an aggressive firewall.
There are many screen-sharing tools available, but some of the tools we use for remote testing include:

Zoom
GoToMeeting
Join.me
Skype
Lookback.io

When you choose your tool, pay special attention to the installation requirements. Are there any operating systems or browsers on which the tool doesn’t work?
For example, Lookback offers remote moderated usability testing on mobile, but  doesn’t support all versions of iOS or Android. Figure out those details early, as they may play a role in your recruitment process. If possible, you’ll want to screen participants to make sure they can use the tool. Also, be aware that many remote moderated usability-testing tools for mobile can have stability issues — it’s tough to perfectly support all the different types of mobile devices out there.



These screenshots depict scenes from a remote moderated usability study on mobile using Lookback.io. Left: The facilitator introduces the session and explains the instructions to the participant (in the thumbnail image in the lower right corner, strangely shown as a duplicate during the intro. Her face has been blurred for privacy.) Right: The participant begins her session on Google. (You might notice that this participant was sitting in a stairwell during the session – not an ideal location for a study. It’s important to emphasize to participants that they need to be in a quiet, private location with a good internet connection.)






This screenshot is from a remote moderated usability study on mobile using Lookback.io. The facilitator introduces the session and explains the instructions to the participant (in the thumbnail image in the lower right corner, strangely shown as a duplicate during the intro. Her face has been blurred for privacy.)




In this screenshot, the participant begins her session on Google. (You might notice that this participant was sitting in a stairwell during the session – not an ideal location for a study. It’s important to emphasize to participants that they need to be in a quiet, private location with a good internet connection.)



Also, think about how you’ll handle observers in this tool. Make sure that you can mute them during the session to prevent any unnecessary noise. If the tool has a chat feature, it should be possible to keep team conversations private so that participants don’t get distracted. You can also use a separate tool (e.g., a messaging app like Slack) to allow observers to communicate among themselves or with the facilitator.
Decide whether you’ll want to share webcam video. If your system and connection can handle it, it’s almost always best to share the participant and facilitator’s faces, as it improves the communication during the session.
2. Plan how to administer the tasks.
With in-person testing, you can just hand the participants a sheet of paper with the task printed on it, and ask them to read it out loud. Administering tasks is a bit more complicated with remote moderated tests, and it will likely depend on the tool you’re using.
The ideal way to deliver tasks is one at a time, and by asking participants to read the task out loud themselves. That way:

You know they’ve read the entire task.
They won’t look ahead to later tasks.
They will practice speaking out loud.

In addition, you want participants to have the instructions easily accessible as they are doing the task, to be able to refer to it in case they forget them.
There are three options for delivering the task for remote moderated testing:

Send participants a document (e.g., pdf) with all the tasks.
Use the screen-sharing tool’s chat window to send the text of the current task to the participant, during the session.
Have the moderator read the task out loud to the participant.

Send participants a document (e.g., pdf) with all the tasks (or with links to the tasks).
In this case, participants must be instructed to not read through the tasks before the session. If the document contains the task text, make sure you start with a blank page and that each task is presented on a separate page. Include page numbers to be able to guide participants to a specific task. Participants can print the document in advance or refer to it electronically during the test session.
If a digital document can be accessed during the session, you can also include links to the task text instead of the text itself. In that case, each task’s instructions can live as a webpage (e.g., a Google Doc) on the web and the participant can be asked to click on the corresponding link to read that task.

One option for delivering tasks is to use a PDF list of links, each one opening to that task’s instructions in a Google Doc.

Use the screen-sharing tool’s chat window to send the text of the current task to the participant, during the session.
For example, Zoom has a chat feature that allows attendees to send each other links and text. The chat window is a convenient way to deliver task instructions; ask the participant to read the instructions out loud before beginning.
Have the moderator read the task out loud to the participant.
Although this method is the least advisable, it may be the only option sometimes (for example, because of tool or device limitations). If you choose this option, prepare to reread the task instructions multiple times, as needed.
3. If possible, schedule technology-practice sessions.
For each participant, try to schedule a 15-minute online meeting the day before her session. Also invite one or two practice observers.
This is the time to get participants to set up the technology needed for the test. Have them install any necessary applications, and work through any hiccups with screen sharing, audio, and reaching websites.
Choose a website or app completely unrelated to the one you plan to test, and ask participants to use it, briefly. In this practice session, don’t show users the actual website or prototype that you plan to test, and don’t have them attempt test tasks. If participants need to physically sign consent or NDA forms, now it is the time to discuss the logistics and ensure that they can access the documents and know how to send them back to you before the session.
If practice sessions are not possible due to scheduling constraints, make sure you leave some extra time for setup during the actual session.
Be aware that even if you have a technology-practice session, you may still encounter tech problems during the session. We’ve had situations where a participant’s setup worked perfectly in a practice session, and then didn’t work at all the next day (for example, due to her computer automatically updating its operating system overnight). It’s always best to add a little extra time to each session to ensure you can deal with minor problems without ruining the session.
On the Day of Testing
4. Send out reminders.
As with any type of testing, send reminder emails to participants and observers, either the night before or the morning of testing.
For participants, remind them when their session begins, and of anything they need to bring or do during the session. For example, you might remind them to:

Have their laptop connected to a charging cable, or their mobile device fully charged.
Have a headset or pair of headphones to minimize feedback and improve audio quality.
Open the remote software tool they practiced using during the technology-test session.
Be connected to a strong WiFi or wired connection.
Be in a quiet place where they won’t be interrupted.

For observers:

Remind them of the testing schedule.
Tell them any rules for observing (for example, join five minutes early, or keep yourself on mute).
Give them tips for observing the study, a copy of the tasks that users will attempt, and specific issues you want them to watch out for.

During Each Session
5. Invite the team to join the session.
The exact time when observers should join the session varies depending on:

the remote-software tool
whether the technology-practice session with the user was successful
your relationship with the observers

Joining early, a few minutes before the session starts, has the benefit of being least disruptive to the participant, especially if the remote tool beeps whenever a new person has joined the meeting. But the disadvantage is that it may potentially waste observers’ time and diminish their engagement.
If the latter is a concern, it’s ok to have observers join the session once all the logistics have been cleared (usually, a few minutes after the scheduled start time). Alternatively, you may be able to keep observers busy during the down time by asking them to discuss the previous session’s findings or review the observer tips and think about specific issues.
6.  Start the session with the participant.
Once your participant has joined the meeting at the agreed time, run through some variation of the following script:

Hi [name], thanks for joining.
I am [XXX]; I work for company [YYY]
Before we begin, is [name] the correct way to pronounce your name?
OK, thanks.
I have some colleagues here with me. They’ll just be quietly observing the session, looking for ways we can improve the [site/app].
Is it ok if we begin recording?
[Wait for confirmation, then begin recording.]
[If the participant has not already signed the consent form, perform the following steps.]
	
[Share your screen.]
[Open the consent form you’d like your user to read out loud.]
This is a consent form that explains what we’re doing here today and how we will use the information. Please read this out loud, and then if you agree, please say your full name out loud.
[Wait for reading and consent.]


[Turn presentation control over to the user, for them to show their screen.]

7. End the session

Thank participants for their help.
Stop and save the recording.
Check in with your observers to discuss the major observations for that session, and any changes to tasks or procedure you’d like to make going forward.

Repeat steps 5 through 7 until your study is complete. (Step 8: Pat yourself on the back for a study well done!)

Summary of the steps involved in remote moderated usability testing

More Tips for Good Remote Moderated Testing

If you haven’t already obtained a signed consent form in advance, ask participants to read your consent form out loud, and then verbally give their consent by saying their full name out loud.
Don’t neglect your own tech setup while you’re thinking about others! Use a headset and a strong WiFi connection, just like the participants should.
Whenever in doubt, ask the user how to pronounce her first name. In in-person studies you can get the user’s attention with a gesture or sound. But if you need to interrupt the participant during a remote session, saying the person’s name can feel the most natural. But you want to make sure you know how to pronounce it.
Keep track of what time the session is supposed to end, confirm it with the user at the beginning of the session, and finish on-time.
Accept that things will go wrong and have a backup plan. Software gets updated at the last minute, firewalls interfere, apps have bugs. When will you release the observers? The user? What if you need to reschedule? Just as with in-person testing, it’s a good idea to build in a little extra time to your sessions and recruit more participants than you really need.
Do a pilot test to help refine your process, facilitation, and tasks. A pilot session is a test of the test itself, and not a test of the design, so your participant can be either a representative user or someone else.

Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote moderated user testing (including sample scripts, research plan, and tasks), check out our full-day seminar: Remote Usability Testing."
21,2020-04-12,"Under normal circumstances, few teams have enough time and resources to perform as much in-person usability testing as they’d like. Due to recent quarantine and travel restrictions, almost all UX teams are suddenly finding that in-person methodologies are not currently possible.
Acting under the (correct) assumption that any user data is better than no data, many teams are turning to remote usability testing. One of the most popular variations of remote testing is unmoderated usability testing.
Unmoderated usability testing (also known as asynchronous testing) is a popular way to get a product tested by users without traveling or breaking the bank.  It usually involves using one of the many available services (such as What Users Do), setting up some tasks, and waiting for the data to be collected. 
This method has some substantial benefits: 

No recruiting (if you’re using the built-in panels of users that the remote-testing services provide)
No moderation skills needed
Easy test setup 
Fast results
Low cost

However, the data obtained from such remote unmoderated sessions is often less detailed than that from in-person testing. This difference is accounted by the serious drawbacks of unmoderated remote testing:

Short test sessions (the length is limited by the testing service’s rules, but usually is around 20 minutes, compared with 1–2 hours for in-person studies)
No way to clarify slightly ambiguous or unclear instructions
Inability to ask users to elaborate on a comment or continue using an area of the design
Unrepresentative testers
Variability in participants’ motivation and commitment to the test 
High chance that testers will multitask or get distracted by their environment

In contrast, remote moderated usability testing combines the advantages of both methods (in-person and remote unmoderated): it can deliver high-quality findings (comparable to in-person testing), but is convenient and inexpensive (like remote unmoderated). The benefits of remote moderated usability testing include: 

The facilitator may change, skip, and reorder tasks as needed.
The facilitator may ask follow-up questions or for clarification if needed.
The participant is less likely to spend time on activities unrelated to the test.
The situation may feel more natural than talking out loud to oneself.
The test sessions can be longer (usually about an hour) and leave room for in-depth exploration of a design.
The team can watch the test at the same time and discuss the findings immediately after the session. 

We acknowledge the two most resource-intensive parts of moderated remote testing, which are:

Recruiting users: You can’t take advantage of a built-in panel. However, you can also recruit users to match your target audience. Use a recruitment agency, your own user database if you have one, your website, social networks, and friends and family.
Setting up the meeting software: It can take extra time and effort to ensure that it works for your test participants, your observers, and you.

Even with these hurdles, remote moderated testing is easier than most people think. Check out our checklist with descriptions for each step.
Three Usability-Testing Formats Compared





 


In-person moderated usability testing


Remote moderated usability testing


Remote unmoderated usability testing




Facilitator required


Yes


Yes


No (the testing platform acts as the facilitator)




Ability to ask specific questions


Yes


Yes


No (though you can set up generic followup questions after each task)




Ability to reorder or modify tasks


Yes


Yes


No




Ability to clarify instructions for participants


Yes


Yes


No




Ability to coach or encourage participant to think out loud


Yes


Yes


No (though you can instruct them at the beginning of the session)




Testing location required (for example, a usability-testing lab or focus-group facility)


Yes


No


No




Typical cost


High


Low


Low




Session scheduling


Fixed session dates and times


Flexible session dates and times (can be modified depending on participant availability)


No scheduling needed (users participate on their own time)




Typical session length


Can be short (30 min) or long (2-3 hours)


Can be short (30 min) or long (2-3 hours)


Most platforms require short sessions (around 30 min)




Risk of “cheating” or unmotivated participants


Low


Low


High (depending on how participants are recruited)










Facilitator required




In-person moderated


Yes




Remote moderated


Yes




Remote unmoderated


No




Ability to ask specific questions, reorder/modify tasks, clarify instructions, and coach participants




In-person moderated


Yes




Remote moderated


Yes




Remote unmoderated


No




Testing location required




In-person moderated


Yes




Remote moderated


No




Remote unmoderated


No




Typical cost




In-person moderated


High




Remote moderated


Low




Remote unmoderated


Low




Session scheduling




In-person moderated


Fixed session dates and times




Remote moderated


Flexible session dates and times




Remote unmoderated


No scheduling needed




Typical session length




In-person moderated


Can be short (30 min) or long (2-3 hours)




Remote moderated


Can be short (30 min) or long (2-3 hours)




Remote unmoderated


Most platforms require short sessions (around 30 min)




Risk of “cheating” or unmotivated participants




In-person moderated


Low




Remote moderated


Low




Remote unmoderated


High (depending on how participants are recruited)





When to Use Remote Moderated Usability Testing
If your team’s UX-research resources are severely limited, remote unmoderated usability testing may be your only option. This is often the case for quantitative usability studies, which often require more than 30 participants. In such situations, remote unmoderated usability testing is certainly better than no usability testing.
However, remote moderated usability testing may be a better fit than remote unmoderated testing or in-person moderated testing if:

You want deep insights and rich data
Your participants are busy, geographically distributed, or otherwise cannot travel to a testing location
Your researchers have enough time to meet with each participant individually

Give Remote Moderated Testing a Try
Remote unmoderated testing has the benefit or being fast, inexpensive, and easy. It can get some great insights, and should be part of every UX researcher’s toolbox. 
However, remote moderated testing can give you significantly more useful, interesting, detailed findings than you’ll get from remote unmoderated tests. It takes a bit more coordination, but the small amount of extra effort is well worth the beneficial impact this methodology will have on your research. If you’ve never tried it, there’s no better time than now.
Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote moderated user testing, check out our full-day seminar: Remote Usability Testing."
22,2019-12-01,"Usability testing is a popular UX research methodology.

In a usability-testing session, a researcher (called a “facilitator” or a “moderator”) asks a participant to perform tasks, usually using one or more specific user interfaces. While the participant completes each task, the researcher observes the participant’s behavior and listens for feedback.

The phrase “usability testing” is often used interchangeably with “user testing.”
(One objection sometimes raised against the phrase “user testing” is that it sounds like researchers are testing the participant — we never test the user, only the interface. However, the term is intended to mean testing with users, which is exactly the point of empirical studies.)
Why Usability Test?
The goals of usability testing vary by study, but they usually include:

Identifying problems in the design of the product or service
Uncovering opportunities to improve
Learning about the target user’s behavior and preferences



Usability testing helps us to uncover problems, discover opportunities, and learn about users.


Why do we need to do usability testing? Won’t a good professional UX designer know how to design a great user interface? Even the best UX designers can’t design a perfect — or even good enough — user experience without iterative design driven by observations of real users and of their interactions with the design.
There are many variables in designing a modern user interface and there are even more variables in the human brain. The total number of combinations is huge. The only way to get UX design right is to test it.
Elements of Usability Testing
There are many different types of usability testing, but the core elements in most usability tests are the facilitator, the tasks, and the participant.


A usability-testing session involves a participant and a facilitator who gives tasks to the participant and observes the participant’s behavior.


The facilitator administers tasks to the participant. As the participant performs these tasks, the facilitator observes the participant’s behavior and listens for feedback. The facilitator may also ask followup questions to elicit detail from the participant.


In a usability test, the facilitator gives instructions and task scenarios to the participant. The participant provides behavioral and verbal feedback about the interface while he performs those tasks.


Facilitator
The facilitator guides the participant through the test process. She gives instructions, answers the participant’s questions, and asks followup questions.
The facilitator works to ensure that the test results in high-quality, valid data, without accidentally influencing the participant’s behavior. Achieving this balance is difficult and requires training.
(In one form of remote usability testing, called remote unmoderated testing, an application may perform some of the facilitator’s roles.)
Tasks
The tasks in a usability test are realistic activities that the participant might perform in real life. They can be very specific or very open-ended, depending on the research questions and the type of usability testing.
Examples of tasks from real usability studies:

Your printer is showing “Error 5200”. How can you get rid of the error message?
You're considering opening a new credit card with Wells Fargo. Please visit wellsfargo.com and decide which credit card you might want to open, if any.
You’ve been told you need to speak to Tyler Smith from the Project Management department. Use the intranet to find out where they are located. Tell the researcher your answer.

Task wording is very important in usability testing. Small errors in the phrasing of a task can cause the participant to misunderstand what they’re asked to do or can influence how participants perform the task (a psychological phenomenon called priming).
Task instructions can be delivered to the participant verbally (the facilitator might read them) or can be handed to a participant written on task sheets. We often ask participants to read the task instructions out loud. This helps ensure that the participant reads the instructions completely, and helps the researchers with their notetaking, because they always know which task the user is performing.
Participant
The participant should be a realistic user of the product or service being studied. That might mean that the user is already using the product or service in real life. Alternatively, in some cases, the participant might just have a similar background to the target user group, or might have the same needs, even if he isn’t already a user of the product.
Participants are often asked to think out loud during usability testing (called the “think-aloud method”). The facilitator might ask the participants to narrate their actions and thoughts as they perform tasks. The goal of this approach is to understand participants’ behaviors, goals, thoughts, and motivations.


In this usability-test session, the participant sits on the left, and the facilitator sits on the right. The participant uses a special testing laptop, which is running screen-recording software. The laptop has a webcam to capture the participant’s facial expressions and is connected to an external monitor for the facilitator. The facilitator listens to his feedback, administers tasks, and takes notes. The photo captures the moment after the participant’s task, when the facilitator is asking him followup questions.


Types of Usability Testing
Qualitative vs. Quantitative
Usability testing can be either qualitative or quantitative.

Qualitative usability testing focuses on collecting insights, findings, and anecdotes about how people use the product or service. Qualitative usability testing is best for discovering problems in the user experience. This form of usability testing is more common than quantitative usability testing.
Quantitative usability testing focuses on collecting metrics that describe the user experience. Two of the metrics most commonly collected in quantitative usability testing are task success and time on task. Quantitative usability testing is best for collecting benchmarks.

The number of participants needed for a usability test varies depending on the type of study. For a typical qualitative usability study of a single user group, we recommend using five participants to uncover the majority of the most common problems in the product.
Remote vs. In-Person Testing
Remote usability tests are popular because they often require less time and money than in-person studies. There are two types of remote usability testing: moderated and unmoderated.

Remote moderated usability tests work very similarly to in-person studies. The facilitator still interacts with the participant and asks her to perform tasks. However, the facilitator and participant are in different physical locations. Usually, moderated tests can be performed using screen-sharing software like Skype or GoToMeeting.
Remote unmoderated remote usability tests do not have the same facilitator–participant interaction as an in-person or moderated tests. The researcher uses a dedicated online remote-testing tool to set up written tasks for the participant. Then, the participant completes those tasks alone on her own time. The testing tool delivers the task instructions and any followup questions. After the participant completes her test, the researcher receives a recording of the session, along with metrics like task success.



In remote unmoderated usability testing, the flow of information changes because the facilitator does not interact with the participant in the same way as an in a moderated test. The testing platform takes on the role of the facilitator, administering tasks to the participant. The researcher designs the study and upload task instructions on the platform, and then reviews the data after it’s collected, usually by observing video recordings of the tasks.


Cost of Usability Testing
Simple, “discount” usability studies can be inexpensive, though you usually must pay a few hundred dollars as incentives to participants. The testing session can take place in a conference room, and the simplest study will take 3 days of your time (assuming that you have already learned how to do it, and you have access to participants):

Day 1: Plan the study
Day 2: Test the 5 users
Day 3: Analyze the findings and convert them into redesign recommendations for the next iteration

On the other hand, more-expensive research is sometimes required, and the cost can run into several hundred thousand dollars for the most elaborate studies.
Things that add cost include:

competitive testing of multiple designs
international testing in multiple countries
testing with multiple user groups (or personas)
quantitative studies
use of fancy equipment like eyetrackers
needing a true usability lab or focus group room to allow others to observe
wanting a detailed analysis and report about the findings.

The return on investment (ROI) for advanced studies can still be high, though usually not as high as that for simple studies.
NN/g Resources for Usability Testing

Qualitative Usability Testing (Study Guide)
User Testing: Why & How (Video)
How to Conduct Usability Studies (Report)
How to Set Up a Desktop Usability Test (Video)
How to Set Up a Mobile Usability Test (Video)
Turning User Goals into Task Scenarios for Usability Testing (Article)
Usability Testing for Mobile Is Easy (Article)

Facilitating a Usability Test
For hands-on training and help honing your facilitation skills, check out our full-day course on usability testing.

Talking with Participants During a Usability Test (Article)
User Testing Facilitation Techniques (Video)
Team Members Behaving Badly During Usability Tests (Article)
Thinking Aloud: The #1 Usability Tool (Article)

Recruiting Participants

Recruiting Test Participants for Usability Studies (Article)
Why You Only Need to Test with 5 Users (Article)
How Many Test Users in a Usability Study? (Article)
Usability Testing with 5 Users: Design Process (Video)
Usability Testing with 5 Users: ROI Criteria (Video)
Usability Testing with 5 Users: Information Foraging (Video)
Employees as Usability-Test Participants (Article)
Using Usability-Test Participants Multiple Times (Video)
Obtaining Consent for User Research (Article)

Remote Usability Testing
For detailed help planning, conducting, and analyzing remote user testing, check out our full-day seminar: Remote Usability Testing.

Remote Usability Tests: Moderated and Unmoderated (Article)
Remote Moderated Usability Tests: How and Why to Do Them (Article)
Remote Unmoderated User Tests: How and Why to Do Them (Article)
Tools for Unmoderated Usability Testing (Article)

Special Usability Testing Studies or User Groups

Quantitative vs. Qualitative Usability Tests (Article)
Conducting Usability Testing with Real Users’ Real Data (Article)
How to Conduct Usability Studies for Accessibility (Report)
Paper Prototyping: Getting User Data Before You Code (Article)
Paper Prototyping 101 (Video)
Beyond the NPS: Measuring Perceived Usability (Article)
International Usability Testing (Article)
Usability Testing with Minors (Article)

Printable Usability Testing Poster
You can download and print a poster that explains usability testing (available below for your preferred size printer paper: A4 size or US letter size, or you can scale the printout for bigger sheets)."
23,2019-10-27,"Many UX teams rely on remote usability testing to efficiently get design feedback from users. There are two types of remote user testing:

Moderated remote testing involves a researcher meeting with a participant via remote screen-sharing software, which allows the researcher to provide instructions, observe the user’s interaction with the design in real time, and ask followup questions specific to that participant’s session.
Unmoderated remote testing does not require a researcher to attend each test session; instead, a software application provides instructions to users, records their actions, and may ask them predetermined followup questions.

Is Unmoderated Testing Right for Your Project?
Unmoderated studies do not include any direct interaction between the researcher and the study participants, which is both their biggest benefit and their greatest drawback.
Because there’s no need to schedule an individual meeting with each participant, unmoderated testing is usually much faster than a moderated study. It may be possible to launch a study and receive results within just a few hours. Unmoderated studies also allow you to collect feedback from dozens or even hundreds of users simultaneously. And for international studies, you don’t have to get up at an ungodly hour to match users’ time zone.
However, there are important limitations of unmoderated usability testing: 

Early-prototype testing is difficult without a moderator to explain and help participants recover from errors or limitations of the prototype.
Without a moderator, participants tend to be less engaged and behave less realistically in tasks that depend on imagination, decision making, or emotional responses.

To better understand this second limitation, think, for example, about the difference between shopping and purchasing. Shopping can include many different types of research and comparison — there’s no single ‘right’ way to do it. In order to shop realistically, participants must first imagine themselves needing that product, then pay attention to details and make comparisons. A participant who just pretends to shop and isn’t very motivated often will glance only at a few products and quickly select one that seems reasonable. But, in real life, consumers who are spending their own money on a product they actually need behave very differently. Of course, a moderated study is not totally realistic either, but because participants are aware that the moderator is observing them, they will be socially motivated to fully engage with the task. Thus, this social pressure compensates for the lack of personal motivation in moderated studies — and even more so in in-person studies.
Unmoderated studies work best for evaluating live websites and apps or highly functional prototypes. They are appropriate for studying activities that don’t require a lot of imagination or emotion from participants.
Unmoderated research requires even more meticulous planning that a moderated study, since you can’t rely on human judgment to adapt the study procedures on the fly. For an unmoderated usability study, you’ll need to go through all the steps below:

1. Define Study Goals and Participant-Recruitment Criteria
Choosing software should not be the first step in unmoderated research. Before you decide which testing software to use, you should get a clear idea of what you hope to accomplish by doing the study. Then you can select a tool which has the capabilities best suited for your research goals, rather than limiting your study to fit within the technical constraints of a particular tool. Clearly articulated study goals allow you to identify must-have requirements for the testing software.




If the study goal is to…


The study tool must be able to:




Compare how long it takes people to complete a signup and checkout process on your site vs. your competitors



Measure time on task
Aggregate time on task data
Display individual times to inspect outliers
Export time data to a standard spreadsheet format (e.g., .cvs)





Help a large team understand why users struggle to complete the checkout process within a mobile application



Record screen and audio of native mobile applications
High-quality video capture, to see detailed interactions
Easily create clips and compilations of videos to share with the team




Examples of how study goals may affect tool selection 

Goals vary from study to study, and a tool that is well suited to one study might be not at all effective for another. Knowing the study objectives is essential in order to make a good tool choice.
In this stage, you will also need to think about the types of participants you want to include in your study. What should their demographic be? Where do they need to be located? Will they be users new to your system or experts? Will they match a particular persona or user group in your target audience? These questions usually are determined by your study objectives and can also inform your tool selection.
2. Select Testing Software
For unmoderated studies, the software that administers the test is absolutely crucial to getting useful results. The software must guide the participants through the session and record what happens. It may also control the selection of study participants.
Fortunately, there are many different unmoderated testing services available, with varying combinations of functionality at different price points. The plethora of choices means that you can now be more critical in selecting a tool that suits the requirements of your project. (Features and pricing change frequently for many of the unmoderated testing tools, so be sure to compare them to your needs at the time of your study.)
It’s definitely worth your time to thoroughly investigate and pilot test tools, because migrating a study to a different system due to a technical limitation discovered after you launch a study is not fun! (Neither is trying to integrate data collected by two different tools.)
3. Write Task Instructions and Followup Questions
Many unmoderated testing services include study templates with generic example tasks. Don’t blindly copy them. The tasks that you give participants to do on your site or application should be highly specific to your situation. Generic tasks, such as “What is the purpose of this site,” are unlikely to give you good insights: to really assess the usability of your system you will need to write your own tasks.
In our experience training other companies to run their own remote usability tests, writing tasks is where most researchers fail in getting the results they need from their studies.
To write good task instructions for an unmoderated study, first articulate what it is you want the user to accomplish (such as: use the help section to answer a question, upgrade an account, or save an article to read later). Then, describe that objective with instructions that are specific, realistic, and actionable — without including hints that make the task too easy. You’ll need different types of task instructions depending on whether you’re doing a qualitative or quantitative study.
In unmoderated studies, the activities that you want the participants to conduct have to be even more carefully written than the tasks for moderated sessions. Participants cannot ask for clarification if they don’t understand the instructions and you can’t ask them to try again if they do the wrong thing. If users misinterpret your instructions and perform the wrong task, your test is wasted. Unmoderated task instructions should also explicitly tell users when they should stop; remember, the moderator won’t be there to ask them to move to a different task.
You should also meticulously plan any followup questions. These can include quantitative questions, in which participants rate the subjective difficulty or satisfaction of an activity. Or you may ask open-ended questions which prompt users to describe specific parts of the experience. Carefully choose how you phrase your questions; broad wording such as “How would you describe this brand?” may lead unmoderated participants to talk about their past experiences instead of the system they just used.
4. Pilot Test
A pilot test is a trial session that you run before your actual study begins, in order to discover any problems with your study design or procedure. Pilot testing is a good idea for all user studies but it’s especially important for unmoderated studies, because there won’t be a moderator available to fix problems while the study is running. Even the most thorough pilot testing can’t catch everything, but you can often detect and fix problems with:

Task instructions that people misinterpret because the wording is ambiguous


Tasks that are missing, or presented in the wrong order (especially in complex studies with many tasks)
Prototypes that are missing functionality or content necessary for the study
Incompatibilities or technical limitations which prevent your testing software from capturing the data you want to record — especially if you are trying to test an intranet or native mobile application
	(If your testing software records data via a web-browser extension, then browser or website restrictions may prevent it from capturing entire pages or sites or lead to low-quality recordings that are difficult to analyze. If you’re using a testing service for the first time, do a quick test of the recording process before you even bother setting up your tasks.)

You can discover some of these problems by going through the study yourself as though you are a participant, but others will not become apparent until you have real participants using their own equipment. Make sure to analyze the data collected in your pilot study!
5. Recruit Participants
There’s no point in watching people use your system if they don’t match your target audience. Make sure you have some control over who participates in the test, either through screening questions, or by recruiting your own participants.
Some tools only offer unmoderated user testing with participants from their panel, while others will provide you with a URL to distribute to your own pool of testers.
Using a provided participant panel is fast and easy (especially if your product is something that is relevant to a broad consumer audience). Panel participants are familiar with the study software, too, and can participate in your test as soon as they have time. Almost every panel includes some basic demographic filtering, but most studies get better results if you screen participants using questions about their behavior, not just about their age or gender. If the experience you’re testing is relevant only to people who meet specific criteria such as driving a car or shopping online regularly, make sure to use a tool which allows you to write your own custom-screening questions.
The downside of using panel participants is that many do these studies so frequently that they’ve learned to focus on certain aspects of the design and look for things to critique. To compensate for possible “professional testers,” recruit extra participants and exclude data from people who didn’t seem honestly engaged with the activities.
Being online means that you can test users on the other side of the globe as easily as people on the other side of the street. Many unmoderated testing services now have panels which include participants from all around the world. If your site targets international customers, unmoderated testing is a great way to reach a wide variety of locations. Just remember that if you recruit participants to complete the test in other languages, you’ll also need a researcher or translator fluent in that language to interpret the results.
In summary:

B2B sites, sites that target elite or rich customers, and other services with narrow target audiences usually can’t use panels and must recruit the test participants themselves.
B2C sites and other services that target a broad audience usually get faster and cheaper results by using a panel.
You don’t always need the biggest panel, but if you require users from a particular region some panels may take a very long time to fulfill your study. (If you really need a specific audience, discuss your requirements with the testing service or panel manager in advance, and if they’re at all vague about whether they can do it, consider using a different service.)

6. Analyze Results
Unmoderated studies can quickly accumulate a LOT of data, so you’ll need an organized, analytical approach to turn this data into actionable insights about your design.
If you collect qualitative data, such as video and audio recordings of participant actions and comments, you’ll need to review each session recording. Users’ verbal and written comments can be misleading, so you have to watch their behavior in order to understand what works or doesn’t. In a moderated study you can follow along with the participants as they conduct activities, but during unmoderated studies you need to be able to watch a recording afterwards.
Screen recordings are helpful, but, in the absence of an audio recording, it is easy to miss why certain behavior occurred. If nothing is happening on the screen, is it because the participant is reading, or is she thinking about where to click next? The audio recording of participants verbalizing their actions is essential. (Recordings of the participants’ webcam to capture their facial expressions are also nice to have, but not essential. It can be more difficult to recruit participants who have a webcam and are willing to be recorded, so make it a secondary requirement.)
In each recording you’ll want to identify problems, questions, and both positive and negative reactions to the design. This process can be relatively quick if you have only a few recordings to review, but for large studies with dozens of participants, video analysis becomes extremely time-consuming. If you expect to analyze large studies (or to carry out several different smaller studies), look for an unmoderated testing tool which offers robust features video analysis, particularly:

Tagging videos with timestamped notes as you watch them
Aggregating, exporting, sharing, and visualizing the notes you’ve added to your recordings
Producing short clips or highlight compilations of important moments in your recordings

Of course, you can make notes and video clips even if this functionality is not built in to the unmoderated testing tool — but unmoderated testing services which include these features don’t necessarily cost much more than barebones tools that lack them. If you will analyze more than a few hours of recordings, it’s well worth it to pay slightly more for a tool that speeds up your data analysis.
For extremely large qualitative studies, consider tools which can collect some quantitative measures or which offer automatic transcription. These features don’t eliminate the need to carefully review your recordings, but they can certainly expedite the process by directing your attention to specific recordings that are likely to be significant (such as recordings where users had low satisfaction ratings or recordings where particular keywords were mentioned).
If your study is primarily quantitative, your analysis will be quite different. Metrics such as success rate, task time, and subjective ratings will be automatically collected by your study tool. But to ensure your conclusions are accurate you’ll need to review your data and:

Clean the data by identifying and excluding inaccurate values. For example, if a few task times are much shorter or longer than the others, investigate why, and exclude the outliers from your analysis if the values are inaccurate because the participants didn’t fully complete the task, or did the wrong task.
Perform statistical tests to assess the significance of your results (especially when your goal is to compare multiple designs or tasks).
Generate data visualizations to help communicate your findings to others.

Some unmoderated testing tools can automate the process of excluding outlier data points and many have built-in data visualizations charts. But, since cookie-cutter charts don’t always show the most important results, if you plan to do quantitative analysis, make sure your tool includes the ability to export your data so you can perform your own analysis using Excel or specialized statistics software.
Summary
Unmoderated research requires less work than moderated testing during the session, but it requires meticulous advance planning before the study begins. You can learn more about remote user testing and the relation between this method and in-person testing in the full-day Usability Testing training course, which includes hands-on details on writing tasks, facilitating sessions, and more."
24,2019-09-22,"These days, we’re spoiled for choice when it comes to remote user research. The vast array of tools available at many different price points can be overwhelming ⁠— especially since many of the descriptions of tools are virtually indistinguishable.
Every remote-research tool promises to deliver user insights, but they do so in very different ways. If you’re trying to choose a tool, this list can help you understand exactly what you’re getting and make sure the service you pick is a good fit for your research needs.
All of the user-research tools compared in this article allow you to do studies that are:

Remote: Participants can be located anywhere, the entire study is completed online.
Unmoderated: Participants complete the study on their own, without a researcher guiding the session.
Task-based: Participants receive instructions to complete specific tasks.
Behavioral: Users’ actions are recorded by the tool so you can tell what people did and whether they successfully completed the tasks.
Interactive: Participants can test on live sites or interactive prototypes rather than just seeing a static image.
Do-it-yourself: You can plan and carry out your own studies, without using the tool’s research-consultancy services.

Together, these qualities allow you to conduct studies that are similar to in-person usability testing, but without the moderator meeting individually with each participant. Unmoderated testing is often a good option when you have limited time or budget or when users are geographically dispersed.
2 Types of Data in Unmoderated Usability Testing
It’s important to understand that different types of data that can be collected by various tools. Some tools record unstructured qualitative data in the form of video recordings; some tools collect highly structured quantitative data about tasks, and some tools can gather both of these types of data.



Type of Data
Qualitative
Quantiative




How it is collected
Video recordings capture sceen activity and think-out-loud narration by the test participant
Metrics are recorded for dimensions such as time spent, success rate, satisfaction, and perceived difficulty


What it reveals
What participants did and why they did it
How common certain problems, behaviors, and opinions are among participants


Challenges
Unstructured video recordings are time-consuming to watch and analyze.
Metrics do not reveal causes of behavior; low participant motivation or inaccurate self-reported data can cause misleading metrics


Useful when you need to...


Understand why a problem is happening and get ideas for how to fix it
Evaluate new designs when you have no idea what problems users might encounter
Inspire empathy for users within your team or organization




Track usability over time
Quickly and accurately assess precise frequency of problems
Quickly assess subjective participant reactions of a large group of participants
Persuade stakeholders who prefer quantitative data





Make sure you have a clear idea of what you hope to achieve through your research. Then you’ll be able to decide whether you need qualitative recordings, quantitative data, or both.
Tools and Data Types
The chart below lists 15 different tools which can be used to conduct unmoderated usability testing. The position of each tool in this chart indicates both the type of data collected by the tool and how long the tool has been in existence. Generally speaking, tools which have been available longer are more mature, with more robust features. Also, though there is never any guarantee, a longer-lasting service is less likely to go under in the middle of your study and make any already-collected data evaporate into a lost corner of the cloud.

The diagram above indicates several unmoderated-testing tools which combine both types of data collection. The features listed for these tools are quite similar, so it can be difficult to distinguish between tools by reading their descriptions. Despite these surface similarities, there are important differences between these services, which are easier to understand if you’re aware of the history of each system. UserZoom and Loop11 initially focused on quantitative metrics, and later added qualitative recordings; while UserTesting, Userlytics, and Userfeel initially focused on video recordings and later added quantitative metrics. As you might expect, the tools’ original functionality tends to be more robust, while the newer features are more limited. (These distinctions are represented in the chart above by the placement of each tool’s name, which is positioned closer to its original data type.)
It’s also worth noting that the metrics-only tools included in this diagram, Maze and KonceptApp, are both designed to be used for testing prototypes and are not suitable for testing live websites or applications. Although they can simulate interactions, such as letting test participants click a link and move to another screen, this behavior requires you to actually build or import an interactive prototype.
Feature Comparison
Once you’ve determined the type of data you want to collect, review the precise capabilities of the tools you are considering. Some features which may be important to the design of your study are listed in the table below.



Recruiting
Study Design & Setup
Qualitative Data
Quantitative Data






Participant panel
Set quotas for multiple types of users
Custom screening question(s)
Multiple Languages
Bring your own users
External panel integration




Test websites (desktop, mobile, and prototype)
Test native mobile apps
Test static wireframes or screens
Separate instructions for each task
Persistent access to task instructions
Custom welcome & final screens
Copy a past study
Branching (skip logic) to personalize tasks
Randomize task order
Professional research services available
Shared projects for team collaboration
Supports moderated testing




Record screen & audio
Record face
Timestamped notes
Export individual session notes
Export all project notes
Download individual recordings
Download entire project
Share recordings via url
Produce video highlights compilation
Automatic transcription
Browse video thumbnails




Simple rating questions
Custom ratings and written questions
Task time
Filter out speeders and cheaters
Rate of task abandonment
Data export (csv or xls)
Data-visualization charts
Success rate by url or click location
Click heatmaps
Clickpath across screens





Features that you may need to consider when selecting remote unmoderated usability-testing software
As a starting point for your comparison, we’ve prepared a list of tools and features we were able to confirm for each tool. This spreadsheet provides a detailed feature comparison for 15 tools for unmoderated user testing. 
When to Use NONE of These Tools
This article has focused on tools for unmoderated usability testing, but that's not always the right research method. For example, moderated usability testing (whether in-person or remote) is more appropriate for evaluating an early-stage prototype or to identify usability issues in interface or tasks that are so complex that it’s necessary to provide personalized directions and ask followup questions to fully understand users’ behavior. Also, all participants in unmoderated studies are people who were willing and able to install a browser extension or application and carry out a fairly complicated online interaction. If your target audience includes a lot of users who wouldn’t opt to participate in this type of research, you’ll need to use other methods to find and observe these people.
Finally, some research questions are better answered with a completely different type of study, such as an A/B test, 5-second test, interview, field study, card sort, or tree test. (Some services — most notably UserZoom — support a wide range of such research methods.) You should always figure out which research method best addresses your question before choosing any tool.
Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote unmoderated user testing, check out our full-day seminar: Remote Usability Testing."
25,2019-08-18,"Eyetracking Research
Eyetracking equipment can track and show where a person is looking. To do so, it uses a special light to create a reflection in the person’s eyes. Cameras in the tracker capture those reflections and use them to estimate the position and movement of the eyes. That data is then projected onto the UI, resulting in a visualization of where the participant looked.
This research can produce three types of visualizations:

Gazeplots (qualitative)
Gaze replays (qualitative)
Heatmaps (quantitative)



This gaze plot shows how one participant processed a web page in a few minutes. The bubbles represent fixations – spots where the eyes stopped and looked at; the size of the bubble is proportional with the duration of the fixation.



Sorry, your browser does not support the video tag.

This video clip is a gaze replay — it shows how one participant’s eye processed a page on Bose.com.


This heatmap is an aggregate from many participants performing the same task. The colored areas indicate where people looked, with red areas signifying the most amount of time, followed by yellow and green, respectively. To get this type of visualization, we recommend having at least 39 participants perform the same task on the same page.


We use this eyetracking data to understand how people read online and how they process webpages. Our eyetracking research has yielded major findings such as:

Banner blindness: People avoid elements (like banners) that they perceive as ads.
Uncertainty in the processing of flat UI elements: Extremely flat UIs with weak signifiers require more user effort than strong ones do.
Gaze patterns: Users tend to process different content in different ways. Two of the most common patterns are the F-pattern and the layer-cake pattern.

In an eyetracking study, the tracker has to be calibrated for each participant. Every individual has a different eye shape, face shape, and height. As a consequence, the tracker has to “learn” each participant before it can follow their gaze. Once the machine is calibrated, the participant has to stay roughly in the same position — moving too far side to side or leaning in or out can cause the tracker to lose calibration.
Materials List
In this desktop eyetracking study of how people read online, we used the following materials:

Desktop eyetracker with built-in monitor (Tobii Spectrum)
Powerful PC desktop tower
Large monitor for facilitator and observer
Two keyboards
Two computer mice
External speakers
External microphone
Printed task sheets
Printed facilitator script
Printed consent forms
External hard drive for backing up data
Two tables, side-by-side
Two chairs
Envelopes with incentives for participants (cash)

Lab Setup
Room
For this specific study, we rented out a 4-person office space in a WeWork coworking facility. This office provided enough space for a participant, a researcher, and 1–2 observers, without getting too crowded. 
PC, Monitors, & Eyetracker
We used a powerful PC desktop tower, connected to two monitors:

Participant’s monitor (with the eyetracking cameras attached)
Facilitator’s monitor (showing the participant’s gaze in real time)

The participant and facilitator each had a separate mouse and keyboard, so they shared control of the PC. The facilitator controlled the PC only for setup, calibration, and to stop and start the recording.


The facilitator’s monitor, keyboard, and mouse are set up to the left of the participant’s monitor, keyboard, and mouse. In this room, we chose to place the eyetracker in the corner because it was out of the range of direct overhead lights (which can sometimes cause problems with the tracking). The facilitator’s monitor was angled away from the participant, to prevent her from seeing it.




During each session, the participant (right) completed tasks using what looked to her to be a normal monitor. Meanwhile, the screen was shared on the facilitator’s screen with real-time gaze data. The facilitator (me, left) monitored the gaze calibration, watched user behavior, and administered tasks and instructions as needed. I also took some notes, but as eyetracking facilitation requires multitasking through many activities, those notes were very light. Primarily, I used my notes to record any issues I saw in the gaze data or to remind myself to go back and rewatch particularly interesting incidents. Human eyes move fast, so the bulk of eyetracking analysis work has to happen by slowing down the videos and watching them several times.


Using a separate monitor for the facilitator was optional, but had two major benefits:

Space: Having a separate monitor allowed the facilitator to observe the task without sitting too close to the participant.
Real-time gaze data: The facilitator’s monitor showed a red dot and line representing the participant’s gaze; these were useful for monitoring the participant’s calibration. (If the participant shifts in her seat, the tracker can lose her eyes. Lost calibration means that the gaze visualization won’t show what the participant was looking at — making the data unusable. By monitoring the gaze data in real time, the facilitator can catch the problem and recalibrate as needed.)

I’d recommend using a large, high-definition screen for the facilitator’s monitor, in order to easily see which words the participants were (and weren’t) reading on the screen.


This screenshot shows the facilitator’s view during a session. The white dots in the upper right corner represent the position participant’s eyes as seen by the eyetracker. If the dots disappear or move too far from the center, the facilitator knows she needs to intervene to save the calibration. The real-time gaze data is shown on the screen as red dots and lines (center). This provides another piece of information for monitoring calibration. For example, if the participant seems to be reading a headline, but the red dots are appearing a half-inch below that headline, that could be an indication that the calibration is off.


Tables and Chairs
The monitors, keyboard, mice, and tasks sheets were spread across two tables that we pushed together. The facilitator sat in a rolling chair, so she could easily move closer to the participant to adjust the eyetracking equipment as needed or to hand him a task sheet. The participant sat in a fixed (not rolling) chair. This little detail won’t necessarily matter in a normal usability test, but matters a lot in eyetracking — you don’t want to give participants any reason to move out of range and ruin the calibration.
Task Sheets
Task sheets are another detail that can sometimes cause problems in eyetracking studies. When participants look down at a task sheet, they’re turning away from the eyetracker. When possible, it’s nice to have the task instructions delivered either verbally or through the eyetracking software itself. 
In the past, we’ve found that referencing task sheets can break the calibration, but we did not have a problem with it in this study: when people looked back up at the screen to perform their task, the tracker was able to refind and track their eyes. Be aware that this capability may differ depending on the tracker you use.
Eyetracking Now vs. 2006
The setup for a desktop eyetracking study hasn’t changed very much in the past 13 years. Compared to a photo of our setup in a 2006 eyetracking study, our 2019 version looks quite similar —  two monitors, an eyetracker, and a PC tower.
However, even though the structure of the system may be similar, the technology has definitely changed from 2006 (check out those little low-resolution monitors!). Compared to 2006, eyetracking tools have certainly improved the calibration process and they’ve gotten better at hiding the eyetracking mechanisms in the eyetracker (thanks largely to smaller cameras).


In 2006 Kara Pernice (right) facilitated an eyetracking study with a very similar setup to our 2019 study.


Tips for Your Eyetracking Study
Think through your goals for the study. What data are you looking to gather?

Gaze replays and anecdotes: If you’re looking for video clips and qualitative insights, a lightweight tool might work for you. Instead of the complex setup we used for this study, you could consider using lightweight USB-connected eyetracker systems or special eyetracking goggles (particularly for testing mobile designs). Those types of studies can be much easier to run than full-fledged quantitative eyetracking studies. Be aware, though, that those products are often not capable of producing gazeplots or heatmaps. Lightweight systems also tend to be less precise —  instead of a little dot showing you which word someone is reading, you might get a big bubble that just shows you which paragraph he’s looking at.
Gazeplots: If you want static visualizations of where individuals looked on a page, you could use a setup similar to ours, but you wouldn’t need as many users. You could collect data from 8-12 participants. (For regular qualitative usability testing, it’s usually best to test with around 5 users, but for a qualitative eyetracking study you’ll want to recruit a few extra test users to account for calibration problems and other technical issues.)
Heatmaps: If you want static visualizations that summarize where many people looked at a page on average, you’ll need to run a quantitative study like we did. We usually recommend having 39 participants complete the task you want to use for a heatmap.

If you’re planning an eyetracking study, it’s important to think through all the little logistics details. Running a day or two of pilot testing is a good way to work through all the potential hurdles you’ll encounter. Based on our experiences, you should absolutely expect technical difficulties.
I also highly recommend dedicating 1–2 days just to set up your equipment, before your pilot testing. Traditional eyetracking tools are complex, delicate systems. You’ll want plenty of time to think through and experiment with your study setup.
 
For more details, check out our free report on how to run eyetracking studies."
26,2019-07-14,"Finding the right participants for a user-research study can be challenging, even if you have clearly defined your target audience. You can learn something from almost any user, but you’ll get more meaningful insights if your study participants have the same behaviors, interests, and knowledge as your actual users.
Using Screening Questions to Select Participants
To recruit study participants, you should ask screening questions which assess their background and characteristics. For example, if you’re designing a website about online games, you might need people who are interested in gaming. To assess this, you could simply ask them, “Do you play online games?”
But screening questions that make the purpose of the study obvious can backfire. If people can easily guess what the study is about, some will be tempted to exaggerate their responses just so they can participate (and receive the incentive, if the study offers one). When part of the screening process for a research study, the question “Do you play online games?” will clearly signal to respondents that the study is about gaming. It will be easy for people to guess that, if they admit that they do not play online games, they’re unlikely to be invited to participate in the study.
Therefore, screening questions have two conflicting goals:

They must elicit specific information about users.
They should also avoid revealing specific information about the study.

Achieving both goals is tricky, but it can be done by carefully preparing screening questions using two main techniques: open-ended questions and distractors.
Open-Ended Screening Questions
An open-ended question asks people to answer in their own words instead of choosing from a list of predefined answers. Since there are no answer choices provided, it’s difficult for people to guess which answer is ‘right.’
Open-ended questions can be used to elicit details about past experiences. Rather than asking whether someone plays online games, you can ask “What activities do you do online?” and select participants who mention games.
Open-ended questions are also good for assessing:

Disqualifying occupations: If there are specific categories of people you need to exclude from the study, an open-ended question will be better than a multiple-choice one with answers that list the excluded categories. For example, unless your design is for expert users, it’s best to exclude those whose job relates to the design you are evaluating, because their professional experience makes them too different from a normal user. If you’re evaluating a website about travel planning, people who work in the tourism industry would likely have a completely different perspective from the average traveler. The open-ended question, “What is your occupation?” is more likely to get accurate responses than a question which lists all the excluded occupations and relies on people to self-identify whether any of those describes their job.
Level of experience or interest: If you need to recruit people who are highly familiar with a certain topic, open-ended questions can elicit authentic details that tell you whether respondents have the relevant experience. If you ask people how often they play online games, it’s easy for someone who plays a few times a year to exaggerate and claim that she plays several times a week. But if you ask her to describe some of her favorite games and why she likes them, you will quickly be able to distinguish the hard-core gamer who can immediately list the names and details of many games from the person who can barely remember any. (If you are writing screener questions that someone else will be asking in an interview, request that respondents’ answers to these open-ended questions be written down word for word.)

However, open-ended questions alone are not enough to ensure effective participant screening, because they have some critical limitations. If you are recruiting for a very specific niche behavior, an open-ended question may fail to elicit relevant information because some people who do engage in the behavior may not mention it in their response to a general question. Also, in purely practical terms, responses to open-ended questions require more time to produce and also more time to collect and to analyze. Because the answers are free text, you need to read through each respondent’s statement and evaluate its meaning. This extra step may not be possible in some contexts (such as unmoderated usability studies, which are often designed to allow people to instantly proceed to the study after answering the screening questions, with no time allowed for a researcher to review the screener responses).




 DON'T rely on 'yes or no' questions:
""Would you consider using a short-term scooter rental service again in the future?""


 DO ask open-ended questions to elicit authentic experiences:
""Please describe the last time you rented a scooter.""




Distractor Answer Choices
Multiple-choice questions can be instantly evaluated, making them suitable for unmoderated recruiting; they also allow you to assess specific behaviors that people might not think to mention in an open-ended response. But, it’s still important to avoid revealing the purpose of the study. To do so, borrow a technique used by teachers for multiple-choice tests: include distractors among the answer choices. Distractors are incorrect answer choices which camouflage the right answer by surrounding it with incorrect responses. Good distractors look like they could be correct responses — they help distinguish between people who truly know the correct answer, and those who are just guessing and are likely to pick one of the appealing distractor answers.
You can incorporate distractors into multiple-choice screening questions by providing answer choices that include both your target response and several equally plausible responses. The distractor answers should be realistic both as activities that people would do and as research topics for an organization. For example, if you’re recruiting people who are interested using a scooter-rental app, you might ask about scooters and include walking, ride sharing, and taxis as distractors. But hang gliding would not be a good distractor, because it’s obviously not a reasonable transit option in an urban area, and also not a behavior so common that companies would conduct research about it.
For some questions, it will be appropriate to allow people to select more than one answer. Make sure to exclude respondents who select all the answers to one question — (especially if they do so repeatedly for several questions). Selecting all choices is a warning sign that the person may be too eager to participate, and it would be safer to choose someone who selected fewer choices.
If you’re using a testing platform which allows asking only one or two screening questions, even a single, well-written screening question with good distractors can be effective at identifying which potential participants match your target audience.




 DON'T ask easy-to-guess multiple choice questions:
If you needed to get to a meeting on the other side of downtown, about 2 miles away, which of the following would you consider doing to get to your meeting?
			A. Walk
			B. Hang glide
			C. Rent a scooter


 DO use plausible distractor answers to conceal the subject of the study:
If you needed to get to a meeting on the other side of downtown, about 2 miles away, which of the following would you consider doing to get to your meeting?
			A. Walk
			B. Rent a bike
			C. Rent a scooter
			D. Take an Uber




Conclusion
Finding the right test participants is important for any user-research project, but it becomes essential when you need to identify a specific type of user from within a large pool of general consumers. Screening is especially important if you’re looking for:

Potential future users, who aren’t yet customers, but could realistically become customers in the future
Highly motivated users, whose interest in a particular topic or activity is so strong that their knowledge and behavior significantly differ from those of the  ‘average’ person

These types of users are often desirable target audiences, but the attitudes and experiences that make these audiences valuable are often difficult to assess accurately with multiple-choice questions. Carefully planned screening questions discourage exaggeration and guessing, and identify those users who truly fit your target audience.
Learn more about writing screening questions and other tips for recruiting user-research participants in our free report How to Recruit Participants for Usability Studies. This is also one of the topics covered in the full-day course on Usability Testing."
27,2019-05-12,"When a design serves a large and varied population of users, stakeholders sometimes direct that the design must target “everyone.” This approach may feel more inclusive than focusing on certain categories of people, and is especially attractive for organizations that have a strong need to attract more customers or users.
But avoiding any definitions of the target audience actually leads to a less usable experience for most people. Without an understanding of who the users are, you risk getting biased research findings and incoherent design choices.
You don’t always need precise categories of user characteristics, but you do need some idea of who will be using the design, and what they’ll try to do with it.
Designing for Anyone Is like Packing for a Trip to Anywhere
Before you pack for a trip you probably gather at least a little bit of information about:

How long you plan to stay
What kinds of activities you’ll be doing
What the weather will be like at your destination

Imagine trying to prepare for a trip if you had no idea whether you’re heading for a weekend jaunt to a luxury resort or a six-week expedition to Antarctica. How could you possibly bring the right supplies for a great trip if you have no context for what you’ll need?

Designing a product without understanding your users is like packing for a trip without considering your destination — both may end in an unpleasant surprise that could have been avoided with a little forethought.

The exact same principle applies to design. Ensuring our users have a great experience doesn’t necessarily mean we need to define everything about them. But we do need to define a lot about the trip they’re about to embark on: what activities will they be doing, and what content and features will they need to make those activities successful and enjoyable?
Testing with Anyone Is Not Testing with Everyone
One big problem with failing to define your users occurs when you evaluate the design. If you plan a research study with no restrictions on who can participate, you could easily end up with all your participants being quite similar to each other. This is no problem if your system is only intended to serve one type of user, but if you’re aiming for a mass-market audience, you can miss out on a lot of insights and opportunities.
Imagine you’re designing the website for a city-transportation system used by both local commuters and tourists. If you plan a usability study with 10 people and let anyone participate, you could easily end up with 9 commuters and 1 tourist. (Or even worse: 10 commuters, if you recruit a convenience sample of users who live close to your office.) You might learn a lot about commuters but very little about your other important audience — tourists who encounter unique challenges due to their lack of familiarity with the local geography.
You can learn a lot from doing usability testing with just 5 users. But if your audience includes a range of people who have fundamentally different tasks, needs, and expectations, then 5 users will not be enough. Instead you will need 4–5 test participants from each major type of user. (You can often get away with slightly fewer users in each group if there is still some overlap between the tasks done by each group.) Before you can find those people, you must have some idea of how to distinguish different types of users.
Getting past ‘Everyone’: Strategies for Meaningful Descriptions of Large Audiences
A natural approach for describing a large group of people is to turn to demographic facts such as age, gender, occupation, and income. When recruiting participants for user research, it’s definitely good to have some demographic diversity. But these facts don’t usually describe what you should really be focused on — those distinctions among your target users which actually affect how they will interact with your design. Travelers on a city-transportation system could be divided into groups by age — such as 16–24, 25­–55, and older than 55. This type of diversity will probably reveal some differences because people at different stages of life tend to have different schedules. But we could still miss out on the perspective of people who are just visiting or who have recently moved to the area and aren’t yet familiar with the city.
Designers and researchers need to understand how people behave. So the best way to organize information for them is to group people according to their actual behavior, not just their demographic categories.
With a large audience it may seem overwhelming to try to describe the full range of user behavior. For example, different people may use a transportation system to travel between hundreds of different locations, for an endless number of reasons such as job interviews, parties, dates, school, doctors’ appointments, work, and so on. The trick is to organize all these different usage scenarios according to what parts of them are actually likely to make people interact differently with the system.
If you design for large audiences, frequency of use is a good starting point for identifying meaningful distinctions between different types of users.
Frequency of Use Often Distinguishes Different Groups
Often, frequent users behave differently than occasional users. But these important differences are easily overlooked if you think only about how ‘everyone’ acts. By dividing your audience into distinct groups of frequent and infrequent users, you can begin to detect important patterns and trends.

Imagine you work on an application with a huge population of users. Chances are some people use the application more than others. Variations in frequency of use is both a cause and an effect of important differences between types of users:

Frequent use causes familiarity with the interface and quick completion of repeated tasks.
Frequent use is an effect of needs and goals: people use it more because the tasks or content are important to them.

And vice versa — infrequent use implies less familiarity and less importance to the user. Of course, there are exceptions: some usage scenarios are infrequent but very important (such as buying a house), while some interfaces may remain a mystery even to people who use them every day. But for many designs, grouping people according to how frequently they perform a task or interact with a system will reveal valuable insights about their needs and challenges.
Even if you need to serve both frequent and infrequent users, it’s still important to distinguish between them, so you can understand and support the cluster of behaviors associated with each group.
If you want to increase the overall size of your audience, then it’s imperative to define extremely infrequent users as a distinct group, because these are your potential future customers. Ideally you should have some specific information about which new markets you hope to expand into; but even a group defined as ‘people who don’t currently use our product’ helps design and research by forcing consideration of users who aren’t already familiar with the design.
Besides usage frequency, there are many other ways to segment a large audience into more specific groups:



Audience Differentiator
Example





Goal Priority:
			Is the task inherently more important to certain types of users?

People may be more motivated to find information about getting a flu shot if they are in a high-risk group for flu complications.



Domain Knowledge:
			Do some people know much more than others about the industry or topic?

Current homeowners will know more about mortgages than first-time homebuyers.



Likelihood of Use:
			Are some people more likely to become future users than others?

People who own a car are less likely to become frequent users of public transportation.



Applying User Categories to Improve UX Outcomes
When you’ve identified important traits for your audience, you can apply them in several ways:

Data driven: review analytics or survey data to estimate cutoff points which divide clusters of users. For example, you might find that 10% of users visit daily, 70% visit weekly, and 20% visit less often.
Simple cutoffs: even if you don’t have quantitative data, pick a cutoff point based on anecdotal user comments — such as, people who visit at least once a week are frequent users, anyone who visits less than once a week is an infrequent user. Your groups may not exactly match the real distribution, but you’ll still be much better off than if you just throw up your hands and treat all users as though they are all the same.

More important than finding the exact cutoff point between groups is to make sure they become part of your research and design planning. Depending on the depth of the difference between the groups and the implications for usage and your business, you might consider the groups sufficiently distinct to require separate usability studies with different tasks. Or, with less-important distinctions, you could simply make sure to include people from each relevant group when you recruit participants for a study, so you can observe any differences in what they expect and how they behave. Share the categories with your design team and make them visible in brainstorming and planning sessions (for example, by creating personas to represent each category).
While it may seem overwhelming at first glance, identifying meaningful segments from a very large audience doesn’t have to be complicated. Even a little bit of effort in this area can yield substantial improvements in the outcome of your research and design efforts."
28,2019-05-05,"The Disgust for Hush: A Universal Pattern
Silence.
Dead air.
Crickets.
Even the simple act of reading these words might cause a prickly, uneasy feeling.
The fact is, lack of back-and-forth chatter makes us uncomfortable. Research by Koudenburg, Postmes, and Gordijn has shown that, in the United States, it takes only four seconds before an extended period of silence becomes uncomfortable during conversation. Four seconds! Why the disgust for hush? Long story short, humans equate silence with rejection. We have an evolution-driven desire for conversation because it makes us feel connected and accepted. So why would we want to intentionally create periods of “awkward” silence with participants in workshops or research activities?
The power of intentional silence is well-known and utilized among many professional groups: Sales people pause after their pitches for dramatic effect. Counselors practice waiting five seconds after a patient stops speaking before responding. Nurses and physicians employ intentional silence in order to demonstrate compassion and respect. And negotiators adhere to the saying: “He who speaks first, loses.”
As UX professionals, we, too, can harness the power of intentional silence. If we can just become comfortable with that brief period of unsettling silence during our user interview sessions, usability tests, and workshops, we’ll get more out of our participants. Intentional silence, used strategically, can create space, invite response, and signal interest. And it is in periods of silence where participants often offer crucial and most-poignant information.
How to Be Intentionally Silent: Count to Seven
I use a personal guideline when practicing intentional silence: When I find myself in the depths of a poignant pause, I count to seven before speaking. It is an attempt to coax the participant into filling the silence first. Let the first four seconds tick away — remember, this is where the awkwardness will spike — and then allow a few more seconds to pass. The additional three seconds provide space for the participant to collect his or her thoughts and continue speaking after the awkwardness has peaked. The longer you keep quiet, the more the other person will want to fill the void.
When to Be Intentionally Silent: User Interviews, Usability Testing, and Workshop Facilitation
Here are some situations relevant to UX professionals where intentional silence is particularly powerful, and some examples of appropriate and inappropriate usage.
User Interviews
When interviewing users, apply intentional silence to create space for participants to think and respond in a thoughtful, unhurried manner. Show you are interested in what they have to say by allowing them time to articulate their thoughts. Don’t finish interviewee’s sentences in an attempt to read their minds in order to fill silence. Participants are likely to simply agree with you and discontinue their thought.
 
Poor Example: Interjecting During Pauses Shuts Participants Down
In this audio clip of a user interview, the interviewer jumps in to finish the interviewee’s thought when there is a brief pause in conversation. Effectively, the participant discontinues his train of thought and potentially valuable insight is lost.

 

Transcript:
Interviewer: “Tell me a little about your design process.”
Participant: “Ok, so, there’s a process we have here that we’re trying to use to timebox our design process. It’s like a way to…”
Interviewer: “...to scale Agile?”
Participant: “Oh, yeah, SAFE. You know about it then.”
 
Good Example: Intentional Silence Allows Participants to Process Their Thoughts
In the next clip, rather than fill the silence when the participant struggles for a few seconds to find the right words, the interviewer uses intentional silence to allow the participant time to gather his thoughts. In this example, the use of intentional silence results in a valuable insight regarding the difficulty of including UX in the company’s Agile design process.

 

Transcript:
Interviewer: “Tell me a little about your design process.”
Participant: “So, there’s a process we have here that we’re trying to use to timebox our design process. Like...it’s like a way to…”
[Interviewer uses intentional silence]
Participant: “...well, it’s a way to make sure our designers are working with different roles. But there’s really no clear way to insert UX into the process. I mean, it’s been a challenge for our team.”
By the way, how long did that moment of intentional silence feel to you as you were listening to this clip? Did you start to feel uncomfortable? The silence was only five seconds (though it might have seemed longer due to our innate discomfort with that length of pause in conversation)!
 
Poor Example: Interrupting Silence Cuts Off Participants’ Thoughts
Even when interviewees appear to have finished what they had planned to say, it’s a good practice to wait a few seconds. Often, they will add on interesting information, as their first utterance is often simply thinking initial thoughts out loud.
In the following clip, the interviewer rushes to fill the silence after the participant appears to have made a complete statement, moving on to a different topic.

 

Transcript:
Participant: “We’re really good at going fast. Like, the teams are just clipping along and it really makes it so we hit our milestones...”
Interviewer: “Ok, then. Let’s talk about how your team uses tools.”
 
Good Example: Intentional Silence Helps Participants Dig Deeper
Compare the above clip with the following example: The interviewer uses intentional silence to encourage the participant to elaborate, and the participant continues his thought to reveal poignant information that actually conflicts with his first statement.

 

Transcript:
Participant: “We are really good at going fast. Like, the teams are just clipping along and it really makes it so we hit our milestones.”
[Interviewer uses intentional silence]
Participant: “...I mean, well, we maybe miss one milestone per month, and that’s always because marketing slows down the process, so it doesn’t really count. It’s like it always feels like they’re operating on a different set of information than the rest of us. I don’t know if they need to be in the standups or what, but ... it’s bad.”
Qualitative Usability Testing
When conversing with participants during qualitative usability testing, employ intentional silence to allow participants time to read, make judgements and generally reflect on their experience. Filling in periods of silence by barraging the user with questions or making inappropriate commentary about the design is often a sign of a nervous facilitator.
 
Poor Example: Filling Silence with Useless Commentary Distracts Users from the Task at Hand
Falling into general back-and-forth chatter with the usability-test participants invites side conversation and bias. In the audio clip below, the moderator cannot resist filling in the break in user speech with biased commentary. An interjection such as this one would influence the user’s behavior and distract him from the task he is trying to complete.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
Moderator: “Yeah, that’s pretty bad.”
 
Good Example: Intentional Silence Encourages Natural Progression in Task Flow
Instead, resist the urge to eat up silence with comments or a barrage of questions. Don’t answer rhetorical questions or questions where the participant is simply thinking out loud, and don’t interrupt the thought process. Allow participants the time and space they need to complete the task at hand as they would normally.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
[Moderator uses intentional silence]
Participant: “I just want to see when my headphones will get here.”
[Moderator uses intentional silence]
Participant: “I’m just going to click on the cart and see if there’s a way to get to my past orders.”
 
Poor Example: Too Much Intentional Silence Can Backfire
Beware of letting intentional silence linger for too long. Users might conclude that they are being unhelpful or that you are disinterested in the session. In the following clip, the moderator’s prolonged silence frustrates the user, who is lost in the task.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
[Moderator uses intentional silence]
Participant: “Uh...If I click here, will it take me to my profile?”
[Moderator uses intentional silence]
Participant: “I’m lost. I have no idea what to do.”
[Moderator uses intentional silence]
Staying completely silent throughout a test session is often a sign of a nervous facilitator. When the participant grows agitated, such as in the above clip, intercede with rhetorical questions (e.g., “What do you think?”) or neutral responses, such as “What are you looking for?”
Workshop Facilitation
Finally, intentional silence can be a powerful facilitation technique in group meetings such as workshops. Don’t be afraid of silence in a group setting! Participants are more likely to share if there is space in the room for commentary. Remember, your role as a facilitator is not to share your knowledge with the group; it is to collect knowledge from others and align the many different perspectives in the room. Participants are more likely to speak up if there is a pause in the room.
 
Good Examples: Use Intentional Silence to Invite Contributions of Varied Perspectives
Precede your periods of silence with a question that invites response. Ask an open-ended question, then wait. Use the count-to-seven rule to ensure anyone with an idea has had ample time to contribute before moving on. Here are some examples:

“I’m going to pause here so we can hear some additional perspectives.” [Count to seven.]
“What are some reactions to that?” [Count to seven.]
“Anyone want to play devil’s advocate on that?” [Count to seven.]
“Has anyone experienced something similar?” [Count to seven.]

 
Poor Usage: Intentional Silence After a Participant Contribution Can Feel Exclusionary
While using intentional silence to encourage contributions from workshop participants is a useful technique, be careful not to give the impression of a cold shoulder with misplaced intentional silence. Do not follow participant contributions with intentional silence if they have completed their thought. This signals rejection or disagreement to the contributor. Instead, acknowledge the comment and ask for reactions or additional thoughts.
What to Do Instead of Talking
You may be wondering, “What do I do during these silence periods instead of speaking?” Use your body language to allow time for people to articulate their thoughts. Maintain eye contact and focus. Do not say anything or even nod your head. Wait patiently and relaxed, giving the person time to speak. If you can’t resist filling the silence, use soft encouragers to invite the participant to elaborate, such as, “Tell me more about that,” or, “What did you think about that?”  Another trick: If you must do something, take a sip of water, continuing your count as you do.
Cultural Influence on Comfort Level with Silence
It is interesting to note that research suggests a higher degree of discomfort with prolonged silences in Anglophone cultures, such as the United States. For example, the same study by Koudenburg, Postmes, and Gordijn mentioned in the beginning of this article found that, while English speakers became uncomfortable with silence after just 4 seconds, Japanese speakers were comfortable with a silence lasting twice that long (8.2 seconds). Additionally, research by Petkova has found that Finnish speakers are also more comfortable than Americans with extended periods of silence in conversation due to the high value attributed to privacy in that culture. However, regardless of the exact number of seconds that leads to discomfort in conversation, one fact remains true: Being heard is one of the deepest human needs, across all cultures. And being heard requires silence.
References
Koudenburg, N., Postmes, T. & Gordijn, E. H. (2010). Disrupting the flow: How brief silences in group conversations affect social needs. Journal of Experimental Psychology.
Petkova, Diana P. (2015). Beyond Silence. A cross-cultural comparison between Finnish “Quietude” and Japanese “Tranquility”: Eastern Academic Journal. 4. 1-14."
29,2019-04-28,"Recently we published the 4th edition of the UX Design for Children report and the 3rd edition of the UX Design for Teenagers report, which covered the usability of sites and apps for minors aged 3–12 and 13–17, respectively.
Conducting user research with young users requires additional considerations compared with studies with adult participants. For example, from a legal perspective, children and teens cannot give their own permission to participate in the study and therefore we must work with their parents or legal guardians to get formal permission. Additionally, children and teenagers tend to get distracted more easily than adults, they often read at a lower proficiency level, and their research capabilities are inferior. With these problems in mind, we share some tips for recruiting, study preparation, and facilitation with minors.
Recruiting
Effective participant recruiting is always essential to a successful usability process. But when working with youths, we need to give special care to the number and type of participants we recruit, as well as to the compensation that they receive. The differences between a 7- and 17- year old are much more pronounced than, for example, a 37- and 47-year old. For this reason, segmenting an accurate user group is particularly important.
1. Recruit a few more participants than you need.
Approximately one in nine recruited users fail to show up to a study. When we’re working with minors, there are now at least two people that can affect the attendance of the participant: the parent and the child. (Sometimes, there could be even four, since some sessions involve friendship dyads.)  
In addition to no-shows, working with youths may also result in more interruptions throughout the study. More interruptions mean less time working on tasks and perhaps fewer research findings in the long run. Interruptions can be due to environmental distractions (discussed later), water or restroom breaks, or simply the participants’ need to get comfortable in their chairs.
Absent participants and study interruptions are unavoidable. However, you can protect yourself from rework and lost time by recruiting a few more participants than what you’d normally need in a similar adult-based study.
2. Don’t group all minors together. Segment users based on varying levels of maturity.
The power law of learning shows that task time decreases with the number of repetitions of that task. This law is as true for youths as for other users. Each year, youths get more experience with interfaces, so they get a little faster and a little better at using the web. In addition to the skill-level spectrum, there is also variation in content interest across ages.
When designing for kids, there are certain cognitive and physical considerations to keep in mind. For example, children aged 3 to 5 may be comfortable only with touchscreen interfaces; however, by the time they’re 6, they will likely be able to use a trackpad. This difference is important to the way we set up our labs and prepare tasks.
In the case of teenagers, a 13- or 14-year old likely has different interests than a 16- or -17-year old — presumably because 13- and 14-year-olds have yet to reach high school, a pivotal change in perceived maturity.
To get relevant study findings and design for the right audience, recruit appropriate age and interest groups.
3. Determine an age-appropriate incentive.
Cash is always appreciated as an incentive. However, to ensure that minors are content with their compensation, consider alternative or additional incentives.
Gift cards to frequently visited vendors can be an appropriate substitute to cash incentives. For example, if you’re testing a site specifically aimed at children or teenagers, a gift card to the site or a free membership would make sense. However, this gift is best for older minors that can shop online on their own.
When testing with young children, we’ve offered a toy bin that contains small toys, erasers and stickers; participants enjoyed selecting a gift from the bin. (Note: this incentive was paired with a gift card. Although, if the toy being offered was comparable to that of a cash or gift card incentive, it could be appropriate on its own.)
 Remember that testing with minors requires effort from the parents, too. Monetary incentives may be controlled by a parent, so it’s nice to supplement a cash incentive with a toy or another tangible item.
4. Emphasize the need for sociable participants.
Youths are likely to feel uncomfortable talking with an unfamiliar adult in a lab setting. It’s a unique and strange experience for them.
As researchers, there are things we can do in task preparation, environment setup, and facilitation to make youths feel more at ease, but the first step is getting the right participants. Reserved participants are difficult to facilitate. This is another reason we recommend recruiting more participants than you need — just in case you get several shy participants.
Task Writing and Study Prep
Writing good usability tasks is an art form. When it comes to working with youths, task writing needs to be varied, age-appropriate, and simple, without providing too many hints. 
5. Use age-appropriate language in written tasks. Aim for simple tasks.
If you’re working with an age range comfortable with reading, be sure to keep the language simple. Avoid using complex terms and write at an age-appropriate language.  Alternatively, if you’re testing with kids at a prereading age, provide instructions verbally.
In addition to writing clearly, don’t create tasks of unrealistically high complexity. For example, asking a 7-year-old to create an account and go through an ecommerce checkout workflow is not realistic and might make participants feel like they’re being tested (even though they’re not).
Although task difficulty and wording might be different for a child or teen compared to an adult, it is important to make sure the task is still realistic and actionable.
6. Avoid providing clues.
Be careful not to use UI-specific language in task writing. Tasks that include terms used in the interface bias users.
For example, in our work with teenagers, they often wrote words or phrases from the task directly into search bars. This habit, though present in all age groups, tends to be more frequent in younger groups due to participants’ less developed research abilities. 
7. Prepare a plethora of varied tasks because participant interests vary.
When testing any interface, it is important to prepare different types of tasks. If you only have one or two types of tasks prepared and a user gets bored and unengaged, it’s almost a waste of the session. Children and teens are even more likely than adults to get bored and stop providing valuable insights.
For example, in our teen research, although we wanted to test games, not every participant was interested in playing. In this case, we had plenty of other types of activities (ecommerce, entertainment, school, etc.) that the user could engage in.
Additionally, children and teens might work very quickly through tasks. In our studies, youths tended to be more likely to stop researching once they’ve found an answer rather than continuing research to confirm they’ve found the right answer.
Prepare various tasks to keep minors engaged and interested so that they will continue to provide valuable responses and interactions with the system. Likewise, formulating extra tasks can guarantee you don’t end a session early because your participant ran out of things to do.
8. Schedule sessions no longer than 60–90 minutes and leave enough time for breaks in between.
Kids can get tired quickly and lose motivation if they’re working for too long.  In our study of children aged 3–12, we conducted 60-minute sessions. This was a good amount of time to give an introduction to the study, conduct a brief interview, and work through multiple tasks.
With teenage participants, we conducted 90-minute sessions. This lengthened time accounted for tasks of a higher complexity, more talkative teenagers, and more device switching (laptop, tablet, mobile).
9. Consider conducting sessions in friendship dyads with children 6- to 8-years old.
Paired usability testing is more effective for kids between ages 6–8. With children younger than 6-years-old, pairs didn’t work as well because one child would often get bored watching the other child control the device. On the other hand, children older than 8 tended to be comfortable enough with a device to not need someone else there.
10. Design a lab that’s child friendly.
As mentioned earlier, usability labs can be intimidating to youths.  The testing environment should be as pleasant as possible, while also devoid of potential distractions.
For example, a large window overlooking a busy street might seem appealing to you (as someone stuck inside all day), but to a child, it’s a distraction waiting to happen. Either choose a different windowless room or ensure that the participant is facing away from the window (while also avoiding screen glare).
Other considerations for a child-friendly lab include having tissues on hand to wipe runny noses, a chair without wheels to prevent rolling around, and perhaps a booster seat (if you’re testing with users that may not be able to see the device from their seat easily).
Facilitating
One of our challenges in testing youths was encouraging them to think aloud as they worked on the tasks. Children and teens tended to be much less talkative than adults, mainly because they were shy or self-conscious. Facilitating minors requires extra work to get the insights you need. For example, facilitating children and teens often requires being slightly friendlier and praising than in an adult usability test, still without biasing the participant.
11. Confirm that parents signed the consent forms.
Working with minors requires parental approval; minors cannot sign consent forms on their own behalf. In our usability testing with older teenagers (16–17-year-olds), it was not uncommon for them to show up alone. Providing the consent form ahead of time allowed them to have their parent complete the form beforehand. (Note: In theory, a participant could forge a signature. In our process, a recruiter spoke with a parent first and then with a child.) 
12. Be prepared for siblings and parents to come along.
Youths frequently needed someone to bring them to the study. In many cases, the participant arrived with a parent and a sibling. In these circumstances, it is important to note that the participant may not be the only nervous one. Parents may also be uneasy about usability testing if they’ve never participated in research themselves. For this reason, we offer parents the option to wait outside of the usability lab (this could be behind glass or just outside of the room) or sit in the room (in that order).
If parents decide to wait in the room, place them in a seat outside of the participant’s view and remind them to be quiet as the session begins to avoid distracting the participant throughout the study.
13. Be thoughtful when asking minors to think aloud.
Getting youths to think aloud is often challenging. For some participants, it might feel unnatural; others may simply forget to do it after a few minutes.
In our work with children, we told them that they were the experts, and that we wanted them to teach us how kids use websites and apps and what they think about them. This method encouraged most children to share, though its effectiveness still varied depending on the child.
During our research with teenagers, we asked them to read a think-aloud document out loud. This document is an alternative to a demo video. Reading the document served as a warmup to the actual study and helped teens understand what was being asked of them. Although this method was generally successful, some participants were still quieter than others and needed periodic reminders to think aloud.
14. Dress casually.
Suits, lab coats, or any other authoritative apparel can make young participants uncomfortable and less willing to share. Dress casually but professional. For example, nice jeans and a sweater, button-up shirt, or blouse can work well.
15. Be friendly and establish a relationship, but remain neutral.
When conducting usability tests with minors, the facilitator should not look or act too authoritative. This presence could alienate minors and make them feel uncomfortable and hesitant about sharing their thoughts and feelings. In addition to being friendly and approachable, remind users that they’re not being tested, and there is no right or wrong answer.
For example, young children (under 5), tend to be disrupted by the “poker face” technique that’s appropriate for adult participants. If they see that the facilitator isn’t reacting to what they’re doing, the child tends to become quieter and less responsive.
Effective usability testing with children requires being cognizant of all your responses and keeping them neutral and meaningless. Generic praises like “you’re doing a great job at teaching me about apps” or “all of this is really helpful to see” make the child more confident and willing to speak.
16. Start off easy.
It is common for usability participants to feel pressure to perform well, no matter how many times you tell them that you’re not testing them. By starting with something simple, you build confidence.
In our research with youths, we often began by asking them to talk about themselves, show a website they like, or talk about their online experiences. This initial sharing, where they were in control, served as a warmup and got them comfortable talking. Once they were relaxed, we moved on to a simple, age-appropriate task.
Conclusion
Usability testing with minors requires special considerations to ensure effective results. Begin by recruiting participants based on their level of maturity and interest- or age-group to obtain relevant findings. Thoughtfully consider incentives that your child or teen participants would enjoy. Be sure to prepare an abundance of age-appropriate tasks. Design a child-friendly lab and anticipate parents or siblings staying in the room. Finally, on the day of the study, dress casually and present yourself as a friendly yet professional figure."
30,2019-02-24,"Usability testing with a small number of participants is an incredibly efficient way to improve an interface. By observing real people attempt to use a design, we see the interface from the users’ perspective. Since average users lack insider knowledge about how the system is “supposed” to work, they often encounter problems that the creators of the design didn’t foresee.
Discovering these unexpected problems is the point of doing usability testing. But the very fact that they are surprising, and not problems that stakeholders anticipated, often leads to doubts about the whether the issues observed are actually representative of what ‘real’ users would encounter.
Stakeholders who have doubts about whether they should trust findings from usability- tests often assert that test participants are not representative or that there aren’t enough test participants to take the results seriously.

When you encounter these objections, here are some techniques you can use to help teams understand why they should take qualitative usability testing seriously. In the simplest cases, skepticism may be simply a consequence of a lack of experience with the methodology, and can be successfully addressed by clearly explaining the method, and communicating findings in a compelling format.
Prepare Stakeholders with Proactive Explanation
First and foremost, make sure to explain your research method before conducting the usability sessions. Describing the process in advance is much more persuasive than trying to defend it after people have already become skeptical. Prepare your own explanation, or share videos and articles that cover what usability testing is and why you need only a few participants. Acknowledge the limitations of this method upfront: mention that some questions, like whether people believe a certain brand is reliable, could not be accurately answered by talking to just 5 users. But usability testing isn’t about beliefs or preferences — it’s about discovering how normal users behave  in the specific context of using a particular design for a particular task.
Prepare stakeholders for surprising findings by explaining:
“We’ll probably see people encounter problems that we don’t expect, because our familiarity with the design makes it hard for us to spot some issues.”
“Since all users lack our team’s insider knowledge about the design, it’s likely that problems caused by this difference of perspective will affect many users and will show up within the first few test sessions.”
“Rather than trying to prove exactly how bad each problem is by repeating tests with a lot of users, we will focus on quickly discovering common issues, so we can fix them and improve the design.”
Share this context with members of the team in the early planning stages, especially if they are not familiar with usability testing. Even teams that are familiar with usability testing benefit from reminders that they are not like their users.
Hearing Is Believing
Sometimes stakeholders dismiss test results because they view them as just one more opinion — the opinion of the researcher who wrote the report.
The whole point of usability testing is to hear from users. Insights and analysis from a skilled researcher can add great value to a report, but always make sure that users’ direct feedback, in their own words, comes through loud and clear. You can  prominently incorporate direct quotes from your study participants; video clips are even more compelling (if you are able to record sessions and have time to edit the footage).
The most persuasive method of all is to arrange for teams to actually directly observe the usability-testing sessions. This practice allows them to see for themselves that the test participant is a real person and develop empathy for the user’s struggles with the system. When team members directly observe sessions, they can also see that the researcher is not leading or influencing users.
Addressing Objections that Participants Aren’t Representative
To avoid objections that test participants are not representative of “real” users, make sure to test with users who are actually representative — and share how you recruited them in advance with your stakeholders.
Test with Real Users
If you are testing a live website or application, recruiting actual visitors or users is an effective way of finding representative participants. Today’s sophisticated analytics tools (such as HotJar or Ethnio) make it easy to target specific types of users, such as people who are using a particular section of a website or a certain feature of an application. Lists of past or prospective customers are another great source for realistic test participants.
Get Buy-In on User Profiles Before Recruiting
If you can’t recruit participants directly from the actual system, you may need to recruit participants from other sources. But you can still ensure that your participants are representative by carefully defining which traits are important characteristics of your target audience. If you have personas, refer to them. Ask the stakeholders who will be receiving the research findings for their input about who the target audience is. Then screen potential test participants by asking them questions which selectively identify people with all the essential characteristics of the target users.
Whichever method you use to find participants, describe it to your stakeholders every step of the way: during the planning stages of the study, by distributing participant profiles to stakeholders who observe test sessions directly, and as part of the final report.
Objections that There Aren’t Enough Participants to Prove a Problem
Proactive explanations of the usability-testing methods go a long way towards building credibility and confidence in test findings. But there may still be concerns that a small number of people experiencing a problem doesn’t necessarily mean it’s a common issue — especially if only a single participant in the study encounters the problem. That participant could be an “outlier,” who has unusual expectations or habits that are not shared by many other users.
The truth is, seeing at least 2 people struggle with the same issue does inspire more confidence in the validity of that problem. But that doesn’t mean you should discount issues that only one participant experienced. (Statistically, there’s not much difference between 1-of-5 vs. 2-of-5 in terms of confidence intervals.) Instead, gather more information to better explain the context of that single instance.
Analyzing Findings from A Single User
When only a single test participant experiences a problem, first consider the following:

How much would it cost to fix it? If there’s an easy fix, it may be less expensive to just fix it rather than invest time in trying to document the problem, let alone spending time in team meetings debating the issue endlessly. (This option works best when you already plan to test of the next design iteration, where you can check whether the ‘fix’ introduces any new problems.)
How serious was the problem for that one person? Serious issues are typically those that prevent user from completing a task, cause great frustration, or relate directly to key design goals. If any of these are true, but it’s not an easy problem to fix, then further investigation is warranted before committing to a design change.

Gathering more information about single-participant usability findings does not necessarily require more user testing. One starting point to consider is competitive analysis: if a review of competitor designs reveals that most do not have this problem, then fixing it may be important in order to meet user expectations.
Check Independent Data Sources to Estimate Frequency
Existing data sources such as usage analytics or support requests are excellent for estimating the frequency and potential impact of a problem.
For example, review support requests and compare how many of them relate to the ‘outlier’ issue, compared to other issues observed in testing. This information can provide a relative estimate of the real prevalence.
If you don’t have specific data about how many users encounter a problem, you may be able to determine what proportion of users interact with that feature and how valuable those users are to the organization’s goals. For example, imagine one participant in a study misunderstood how a sorting feature worked, and because of that she failed to find a product. If you have analytics with event tracking set up for that sort function, a quick check of that data can reveal what percentage of real customers use the sort function. (Since not all users will experience the problem, the percentage of people affected will be somewhat lower than the percentage who use the feature.)
Compare Test Observations with Usability Theory
Consider the extent to which your single-user usability issue can be explained by existing knowledge about user behavior and what makes user interfaces easy or difficult to use. For example, check the 10 usability heuristics which have proven themselves over decades. Or look at published usability guidelines based on substantial user research with a broad range of other designs. While such broad UX theory doesn’t specifically address your individual design, you can generalize from previous research.
If the issue you observed with a single test participant can be easily explained by existing usability theory, you have a stronger case for believing that it’s a real usability problem. If, on the other hand, the theory predicts that your design ought to be easy and not cause problems, then it’s possible that your unlucky test participant was indeed an outlier and the issue would be unlikely to repeat with any frequency.
Unfortunately, the state of UX theory is such that it can’t conclusively decide the matter one way or the other. Human behavior is so variable, and the usability of a design is further dependent on a myriad of small contextual details, that we can never know with 100% certainty whether something is good or bad. But this doesn’t mean that we know nothing about UX. We have substantial amounts of conceptual insights from decades of prior research about what makes user interfaces easy or difficult for certain categories of users and certain types of tasks, and you can use this knowledge to interpret individual empirical observations.
Also, relating your test observations to the body of externally validated UX knowledge can be a useful way for you to explain your findings to your team. Make it clear that it’s not just you who think that the design has a problem, but that many others have found similar problems in testing other designs. And put a name to the problem. These tactics help you rise about the “tiny sample” objection.
Conduct Quantitative Usability Testing
For potentially serious problems that cannot be placed into context with existing data sources, testing with just 5 users may truly not be adequate to confidently recommend a course of action. In this case a quantitative usability study can provide more confidence in what proportion of users are likely to encounter a problem. If the problem can be tested in an unmoderated remote test, collecting data from 20 users needed for an accurate quantitative study can be reasonably affordable. 
Quantitative research may also be worthwhile if an organization has deeply rooted skepticism about qualitative methods with small groups. Measuring the incidence of qualitative findings with a larger sample of users can prove the reliability of qualitative methods within the specific organization’s designs and users. To capitalize on the investment in qualitative research, thoroughly document and publicize the project within the organization, with particular attention to the correspondence between qualitative and quantitative findings.
Conclusion
It’s pointless to discover design problems if the team doesn’t believe the findings and therefore doesn’t do anything to fix the issues. Effectively addressing doubts about usability test findings is an essential part of user research, and should be incorporated into every research plan, from start to finish."
31,2018-05-13,"When you want to compare several user interfaces in a single study, there are two ways of assigning your test participants to these multiple conditions:

Between-subjects (or between-groups) study design: different people test each condition, so that each person is only exposed to a single user interface.
Within-subjects (or repeated-measures) study design: the same person tests all the conditions (i.e., all the user interfaces).

(Note that here we use the word “design” to refer to the design of the experiment, and not to website design.)
For example, if we wanted to compare two car-rental sites A and B by looking at how participants book cars on each site, our study could be designed in two different ways, both perfectly legitimate:

Between-subjects: Each participant could test a single car-rental site and book a car only on that site.
Within-subjects: Each participant could test both car-rental sites and book a car on each.

Any type of user research that involves more than a single test condition has to determine whether to be between-subjects or within-subjects. However, the distinction is particularly important for quantitative studies.
Experimental Design in Quantitative Studies
Unlike qualitative studies, quantitative usability studies aim to result in findings that are statistically likely to generalize to the whole user population. How the data from such studies is analyzed depends on the way in which the study was designed (that is, on the study’s experimental design).
Often, the main goal of quantitative usability studies is to compare — a site with its competitors, two different iterations of a design, or two different groups of users (such as experts vs. novices).  Like in any scientific experiment in which we want to detect causal relationships, a quantitative study involves two types of variables:

Independent variables, which are directly manipulated by the researcher
Dependent variables, which are measured (and expected to vary as a result of the independent-variable manipulation)

(If the study produces statistically significant results, then we can say that a change in the independent variable caused a change in the dependent variable.)
Let’s go back to our original car-rental example. If we wanted to measure which of the two sites, A or B, is better for the task of reserving a car, we could choose Site (with two possible values or levels — A and B) as the independent variable, and the time on task and the accuracy for booking a car could be the dependent variables. The goal of the study would be to see whether the dependent variables (time and accuracy) change when we vary the site or they stay the same. (If they stay the same, then none of the sites is better than the other.)
To plan our study, the next question that we need to answer is whether the study design should be between-subjects or within-subjects — that is, whether a participant in the study should be exposed to all the different conditions for the independent variable in our study (within-subjects) or only to one condition (between-subjects). The choice of experimental design will affect the type of statistical analysis that should be used on your data.

It is possible that an experiment design is both within-subjects and between-subjects. For example, assume that, in the case of our car-rental study, we were also interested in knowing how participants younger than 30 perform compared with older participants. In this case we would have two independent variables:

Age, with 2 levels: under 30, over 30
Site, with 2 levels: A and B

For the study, we will recruit an equal number of participants in each age group. Let’s assume that we decide that each participant, whether under or over 30, will make a car-rental reservation on both site A and on site B. In this case, the study is within-subjects with respect to the independent variable Site (because each person sees both levels of this variable — that is, both site A and site B). However, the study is between-subjects with respect to Age: one person can only be in a single age group (either under or over 30, not both). (Well, technically, you could pick a group of under-30-year olds and wait until they turn 30 to have them test the sites again, but this setup is highly impractical for most real-world situations.)
Some independent variables may impose the choice of design. Age is one of them, as seen above. Others are Expertise (if we want to compare experts and novices), User Type (if we want to compare different user groups or personas — for example, business traveler vs. leisure traveler), or Gender (assuming that a person cannot be of several genders at the same time). Outside usability, drug trials are one common case of between-subject design: participants are exposed to only one treatment: either the drug being tested or a placebo, not both. And sometimes the manipulation itself changes the state of the participant: for example, if you want to see which of two curricula is more effective for teaching reading, you could not have the same student be exposed to both, because once she’s learned how to read, she cannot unlearn it.
Which Is Better: Between-Subjects or Within-Subjects?
Unfortunately, there is no easy answer to this question. As seen above, sometimes your independent variables will dictate the experimental design. But in many situations, both designs may be possible.

Between-subjects minimizes the learning and transfer across conditions. After a person has completed a series of tasks on a car-rental site, she is more knowledgeable about the domain than she was before. For example, she may now know that car-rental sites charge an extra fee for drivers under 21, or what a collision-damage waiver is. That knowledge will likely help her become more efficient on a second car-rental site, even though that second site may be very different from the first.

	With between-subject design, this transfer of knowledge is not an issue — participants are never exposed to several levels of the same independent variable.

Between-subjects studies have shorter sessions than within-subject ones. A participant who tests a single car-rental site will have a shorter session than one who tests two. Shorter sessions are less tiring (or boring) for users, and can also be more appropriate for remote unmoderated testing (especially since tools like UserZoom usually require a fairly short session length).
Between-subject experiments are easier to set up, especially when you have multiple independent variables. When the study is within-subjects, you will have to use randomization of your stimuli to make sure that there are no order effects. For example, in our car-rental study, we need to make sure that participants don’t always start with site A and then move on to site B. The order of the sites needs to be random for each participant. This is easy with just two sites: randomly assign 50% of users to start with each site. But as you increase the number of independent variables and of levels for an independent variable, randomization becomes more difficult to implement within some of the existing platforms for quantitative usability testing.
Within-subject designs require fewer participants and are cheaper to run. To detect a statistically significant difference between two conditions, you’ll often need a fair number of a data points (often above 30) in each condition. If you have a within-subject design, each participant will provide a data point for each level of the independent variable. For our car-rental study, 30 participants will provide data points for both sites. But if the study is between-subjects you will need twice as many to get the same number of data points. That means twice the cost.
Within-subjects design minimize the random noise. Perhaps the most important advantage of within-subject designs is that they make it less likely that a real difference that exists between your conditions will stay undetected or be covered by random noise.
	Individual participants bring in to the test their own history, background knowledge, and context. One may be tired after a long night of partying, another one may be bored, yet another one may have received a great news just before the study and be happy. If the same participant interacts with all levels of a variable, she will affect them in the same way. The happy person will be happy on both sites, the tired one will be tired on both. But if the study is between-subjects, the happy participant will only interact with one site and may affect the final results. You’ll have to make sure you get a similar happy participant in the other group to counteract her effects.
In practice, researchers won’t be able to assess such differences between participants — although they may match the gender, the experience, and the age across groups, it will be difficult to predict or detect other factors specific to each participant.


Randomization: Essential for Both Types of Design
Whether your experimental design is within-subjects or between-subjects, you will have to be concerned with randomization, although in slightly different ways.
Above, we discussed why randomization is important in within-subject designs: it counteracts the possible order effects and minimizes transfer and learning across conditions.
For between-subject designs, you must make sure that participants are allotted randomly to conditions, because you want to ensure that your participant assignment does not affect your study results. Thus, if a researcher decides that all the participants that he likes should interact with site A and then he finds that site A performed better than site B, he won’t know whether he’s discovered a true difference between the sites or whether the result simply reflects his assignment (for example, because people who sense that they are liked tend to return the favor, and may be more patient or have a positive mindset during the test).
Even without such an obvious bias as your personal preferences, it’s easy to get randomization wrong. Say that you run a study across four days, Saturday through Tuesday. You might decide to have the first half of the test users start with site A and have the second half of the users start with site B. However, this is not a true randomization, because it’s very likely that certain types of people are more likely to agree to a study during the weekend and other types of people are more likely to sign up for your weekday testing slots.
Conclusion
User research can be between-subjects or within-subjects (or both), depending on whether each participant is exposed to only one condition or to all conditions that are varied within a study. Each of these types of experimental design has its own advantages and disadvantages; within-subjects design requires fewer participants and increases the chance of discovering a true difference among your conditions; between-subjects designs minimize the learning effects across conditions, lead to shorter sessions, and may be easier to set up and analyze."
32,2018-03-18,"Part of making a site easy to use is organizing information so that people find what they’re looking for. Too often, content is structured based on what makes sense to the company, not to the users. (This was the #1 usability problem in our recent study of 43 websites.) One of the primary ways to figure out an organization scheme that best matches users’ mental model is through card sorting.
Definition: Card sorting is a UX research method in which study participants group individual labels written on notecards according to criteria that make sense to them.  This method uncovers how the target audience’s domain knowledge is structured, and it serves to create an information architecture that matches users’ expectations.
Let’s imagine that you’re designing a car-rental site. Your company offers around 60 vehicle models that customers can choose from. How would you organize those vehicles into categories that people can browse to quickly find their ideal car rental? Your company might use technical terms such as family car, executive car, and full-size luxury car. But your users might have no idea of the difference between some of those categories. This is where card sorting can help: ask your users to organize vehicles into groups that make sense to them, and, then, see what patterns emerge.

Hertz.com: In recent user testing on ecommerce websites, participants saw a dropdown list of Rental Car Types, but they weren’t sure what categories such as Dream Cars or Prestige Collection meant. Fortunately, the site included a photo and a brief description of each category, but comparing the different car types still required a fair amount of effort. Card sorting can reveal what kinds of cars users expect to find on a car-rental site.

Conducting a Card Sort
Generally, the process works as follows:

Choose a set of topics. The set should include 40­–80 items that represent the main content on the site. Write each topic on an individual index card.
Tip: Avoid topics that contain the same words;  participants will tend to group those cards together.
User organizes topics into groups. Shuffle the cards and give them to the participant. Ask the user to look at the cards one at a time and place cards that belong together into piles. Some piles can be big, others small. If the participant isn’t sure about a card, or doesn’t know what it means, it’s ok to leave it off to the side. It’s better to have a set of “unknown” or “unsure” cards than to randomly group cards.
​Notes:

There is no preset number of piles to aim for. Some users may create many small piles, others may end up with a few big ones. It all depends on their individual mental models.
Users should be aware that it’s OK change their mind as they work: they can move a card from one pile to another, merge two piles, split a pile into several new piles, and so on. Card sorting is a bottom–up process, and false starts are to be expected.


User names the groups. Once the participant has grouped all the cards to her satisfaction, give her blank cards and ask her to write down a name for each group she created. This step will reveal the user’s mental model of the topic space. You may get a few ideas for navigation categories, but don’t expect participants to create effective labels.
Tip: It’s important to do this naming step after all the groups have been created, so that the user doesn’t lock herself in to categories while she’s still working; she should be free to rearrange her groups at any moment.  
Debrief the user. (This step is optional, but highly recommended.) Ask users to explain the rationale behind the groups they created. Additional questions may include:
	
Were any items especially easy or difficult to place?
Did any items seem to belong in two or more groups?
What thoughts do you have about the items left unsorted (if any)?

	You can also ask the user to think out loud while they perform the original sorting. Doing so provides detailed information, but also takes time to analyze. For example, you might hear the user say, “I might put card Tomatoes into pile Vegetables. But wait, they are really a fruit,  they don’t really fit there. I think Fruits is a better match.” Such a statement would allow you to conclude that the user did consider Vegetables a decent match for Tomatoes, even though Fruits was even better. This information could push you into crosslinking from Vegetables to Fruits or maybe even assigning the item to Vegetables if there are other reasons leaning in that direction.
If needed, ask the user for more-practical group sizes. You should not impose your own wishes or biases upon the participant during the original sorting (steps 1–3), but once the user’s preferred grouping has been defined, and after the initial debrief, you can definitely ask the participant to break up large groups into smaller subgroups. Or the opposite: to group small groups into larger categories.
Repeat with 15–20 users. You’ll need enough users to detect patterns in users’ mental models. We recommend 15 participants for card sorting: with more, you’ll get diminishing returns for each additional user; with fewer, you won’t have enough data to reveal overlapping patterns in organization schemes.
Analyze the data. Once you have all the data, look for common groups, category names or themes, and for items that were frequently paired together. If you see that some items were frequently left off to the side, determine whether it’s because the card labels weren’t clear or the content seemed unrelated to the rest of the topics. Combine the patterns you see with your qualitative insights from the debrief, and you’ll be in a better position to understand what organization system will be most successful for your users. (There’s a lot more to the analysis of card-sorting results, but that’s a topic for another article.)

Variations in Card Sorting
Variations in card sorting involve whether or not users can create their own category names, whether a facilitator moderates the session, and whether the study is conducted with paper or a digital tool. Each has its own benefits and disadvantages, which we’ll briefly outline below.
Open Card Sorting vs. Closed Card Sorting

Open card sorting is the most common type of card sort and what we described above. Generally, when practitioners use the term card sort, it’s implied that it will be an open card sort. In an open card sort, users are free to assign whatever names they want to the groups they’ve created with the cards in the stack.
Closed card sorting is a variation where users are given a predetermined set of category names, and they are asked to organize the individual cards into these predetermined categories. Closed card sorting does not reveal how users conceptualize a set of topics. Instead, it is used to evaluate how well an existing category structure supports the content, from a user’s perspective. A critique of the closed card sort is that it tests users’ ability to fit the content into the “correct” bucket ­— to users, it can feel more like solving a puzzle than like naturally matching content to categories. The method does not reflect how users naturally browse content, which is to first scan categories and make a selection based on information scent. Instead of closed card sorting, we recommend tree testing (also known as reverse card sorting) as a way to evaluate navigation categories.

Moderated vs. Unmoderated Card Sorting

Moderated card sorting includes step 4 in the process outlined above: the debrief (and/or think-aloud during the actual sorting). This step is a highly valuable opportunity to gain qualitative insights into users’ rationale for their groupings. You can ask questions, probe for further understanding, and ask about specific cards, as needed. If it’s feasible for your schedule and budget, we recommend moderating your card sorts to get these insights.
Unmoderated card sorting involves users organizing content into groups on their own, usually via an online tool, with no interaction with a facilitator. It is generally faster and less expensive than moderated card sorting, for the simple reason that it doesn’t require a researcher to speak with each user. Unmoderated card sorting can be useful as a supplement to moderated card sorting sessions. For example, imagine a study involved highly distinct audience groups, and the research team decided to run a card sort with 60 users: 20 users for each of 3 different audience groups. In this case, it can be cost-prohibitive to run 60 moderated card-sorting sessions. Instead, the team may decide to do a small study of 5–10 moderated sessions for each audience group, followed by unmoderated card sorting for the remaining sessions.

Paper vs. Digital Card Sorting

Paper card sorting is the traditional form of card sorting. Topics are written on index cards and users are asked to create their group on a large workspace. The biggest advantage to paper card sorting is that there is no learning curve for the study participants:  all they have to do is stack paper into piles on a table. It’s a forgiving and flexible process: users can easily move cards around or even start over. It’s also easier for people to manipulate a very large number of cards on a big table than it is to manipulate many objects on a computer screen that often can’t show everything within a single view.  The downside of paper card sorting is that the researchers have to manually document each participant’s groups and input them into a tool for analysis.
Digital card sorting uses software or a web-based tool to simulate topic cards, which users then drag and drop into groups. This method is generally the easiest for researchers, because the software can analyze the results from all the participants and reveal which items were most commonly grouped together, what category names users created, and the likelihood of two items being paired together. The downside is that the usability of the tool can impact the success of the session — technology problems can cause frustration or even prevent users from creating the exact groups that they want.

Card sorting is a well-established method within the information-architecture field. In fact, a card-sorting study done today will look exactly the same as the photo of a study we did 23 years ago — at least if using physical cards and not a software solution. There are many ways to run a study and the variations above are by no means a comprehensive account of all the possible types of card sorts, but they are the most common. When planning a card-sorting study, you’ll want to pick the approach that is best suited to your goals and resources.
Conclusion
Card sorting is a highly useful technique in information architecture; it is used to understand how users think about your content. It can help you organize content so that it suits your users’ mental models, rather than the point of view of your company. Card sorting can be supplemented with other information-architecture methods to identify issues in your category structure."
33,2018-02-18,"Have you ever involved stakeholders in design critiques, ideation sessions, or meetings devoted to analyzing research findings only to find them uninterested? And were the ideas and research findings ever ignored or forgotten? UX workshops have numerous benefits: educate people and make them empathic towards users, make stakeholders feel involved and responsible for ideas and research findings, create awareness of usability issues and design challenges, build common ground across all parties involved, and bring together many types of backgrounds and expertise.
However, in such UX workshops, it can be challenging to engage the team, and create order among diverse ideas and facts.
One method that helps teams collaboratively analyze research findings as well as ideas from ideation sessions is affinity diagramming. Often used in UX, affinity diagramming is adapted from the KJ diagramming method (named after author Kawakita Jiro).
Definition: Affinity diagramming refers to organizing related facts into distinct clusters.
Affinity diagramming is also known as affinity mapping, collaborative sorting, snowballing, or even card sorting. (However, in UX, ‘card sorting’ stands for a very specific research method for determining the IA of a site or application. In it, users sort index cards with category names and commands written on them.)

Affinity Diagramming in UX
While affinity diagramming as a method can be used by individuals as well as groups, in UX, it is used primarily by teams for quickly organizing:

observations or ideas from a research study
ideas that surface in design-ideation meetings
ideas about UX strategy and vision


Affinity diagramming in UX usually involves two steps:
A. Generating the sticky notes. In this step, team members write down ideas or facts on separate sticky notes.

During a usability session, the facilitator and the observers write observations, findings, or ideas — each on one sticky note.
During an ideation workshop, attendees or the workshop facilitator write each idea on a sticky note.

B. Organizing the notes in groups. After the test or the ideation session, the team has a workshop devoted to analyzing the notes by:

sorting them into categories
prioritizing each note, and determining the next steps in design or further research

Workshop Leader
It can be tiring to interpret and sort many notes, and attendees can lose interest if the meeting drags on. So, it’s important to have a leader to keep the meeting moving quickly and productively. This person’s job is to:

Communicate the agenda and goals.
Describe what people should be doing at each phase of the workshop.
Track and communicate time for each step.
Get all the notes on the wall.
Facilitate moving past any issues that may arise, such as helping the person who seems lost, or pairing people when one is idle and another is very busy.

Steps for Affinity Diagramming in UX
Before the Affinity-Diagramming Workshop

Choose a room that has walls on which sticky notes can be attached (e.g., glass or whiteboard). Alternatively, tape flip-chart paper on the walls and stick the notes on top, or use easels if walls are not tape-friendly.
Just before the meeting begins, enlist some help and put all notes on the wall, in no order.


All the sticky notes are posted on a wall in an affinity-diagramming workshop. (This picture shows notes from testing sessions; each note color corresponds to a different test participant. The color coding makes it easy to tell from which test each finding originated.)

3. Consider creating a few category names to help get the sorting started. For webpages, some common categories are: Search, Global navigation, Homepage, Legibility, Footer, Page layout. Stick these category names high on a blank part of a wall, so many notes can fit below.
4. Add a “?” category name for notes that are not understandable.
5. Put some pens or markers and blank pads of sticky notes on a table.
At the Start of the Meeting: Announce the Steps and Timing Guidelines
The leader should announce what the different steps of the process are and how long the workshop will take:
6. The general steps we’ll follow are:

Sort into top-level categories.
Sort each of those categories into subcategories.
Summarize those categories.
Determine priorities (e.g., by taking votes).
Plan subsequent design meetings as needed.

No talking as you sort, please. It gets distracting and makes the process take longer. This meeting should take not more than 3 (usually 2) hours. We’ll all need to work hard and quickly to make that happen.
Sort Notes into Top-Level Categories
The leader should describe the process and announce the time for this step:
7. Pluck a note off the wall and read it.
8. Look for a category under which it would make sense.
9. Stick it there.
10. If there is already a note that says the same thing, stack the notes on top of one another, so the most descriptive note is the only visible one.
11. If the note doesn’t fit under any category, think of a new one. Use markers and blank sticky notes to write down any new categories.
12. Yell out the names of new categories as they are stuck on a blank section of the wall, so everyone knows when a new category has been created.
13. If you can’t understand a note, move it to the “?” category. We’ll try to make sense of that in the end.
14. Let’s work to get these sorted within 30 minutes (or however much time you feel it will take).
15. I’ll periodically yell out how much time we have left.
16. Questions? Let’s go!

Your browser does not support the video tag.

In most browsers, hover over the video to display the controls if they're not already visible. Affinity-diagramming workshop for analyzing findings from user research on Marriott International's intranet


Sort Notes from Each Top-Level Category into Subcategories
Here are instructions for this step:
17. Everyone choose a category.
18. Categories with a lot of notes may require two people to sort them. Some people will be fast and will be able to sort multiple categories.
19. Look for related themes and sort those notes into subcategories.
20.  Write the names for the subcategories on sticky notes.
21. When you finish sorting a category, look for another category that needs sorting, or for a person who could use help. Don’t be idle.
22. Questions? Let’s go!
Present Each Category
23. Anyone who sorted a category can start. Stand by it and read each finding, then summarize the category.
24. It’s okay for the rest of the attendees to make related points or ask questions now. (But these comments add time to the meeting, so you may choose to skip this instruction.)
25. One-by-one, each person stand by her category and present it. Go clockwise around the room until all categories are done. If multiple people sorted a category, they can present it together, or choose one delegate.
26. Questions? Let’s go!
Determine Priorities
Slightly different methods are used depending on whether the purpose of the workshop is to analyze usability-test findings or organize new ideas.
A.     Usability-Test Findings
There are two common ways to determine which usability-testing issues have the highest importance: either by going through all findings and having people vote on their severity (high, medium, low), or by having people assign a finite set of points to those items they think are important and need to be fixed. The latter method often can be more effective and more immediately actionable than the former (which can result in many problems rated as “high importance”).
Severity
27. The meeting leader reads each finding again and asks people to vote about its importance by show of hands: high, medium, and low. Whichever rating gets the most votes is the rating assigned to the problem.
28. Write the rating (H, M, or L) on each note.
OR
Points
29. Each attendee gets a certain number (usually 3) of importance points which she can assign to the items she thinks most need to be addressed.
30. Then, people walk around and stick a sticker (or draw a dot with a colored marker) on the items they wish to cast their votes for.
31. The voting rules differ by team and local preferences, but usually people are allowed to assign multiple importance points (up to their allocated maximum) to a single issue if they think it’s crucial enough. For example, an attendee with 3 votes could assign two votes to one issue and one vote to another.
B.     Design Ideation
Design ideas from ideation workshops can also be voted on, or assigned value in some other way. Two common methods are the one-hundred-dollar test and the NUF test.
One-Hundred-Dollar Test
32. When there are several design ideas for the same problem, give the group $100 (any currency will do) to split among the ideas generated, such that the total for the group of ideas equals $100. This method forces people to think in terms of each design’s worth, and usually one design emerges more valuable than the other(s).
NUF
33. NUF stands for: New, Useful, and Feasible. Each design idea is rated on a 1- to-7 scale for each of these three attributes: 1) whether the team has used that design before (usually newer items are considered better); 2) whether the idea is useful and it solves the problem; and 3) how feasible it is for the team to implement the idea. The ratings are totaled, and the overall score is used to rank the ideas.  
Capture
The resulting clusters and the determined priorities are the end product of the affinity diagramming. To record it:
34. Write the main points on flipchart paper so all can see them.
35. Take photos of the walls. By now the stickies are usually falling down, which signals the meeting’s end.
End the Meeting
36. Assign follow-up tasks (or announce when these will be assigned).
37. Announce that a written report and (or) video recordings will be available.
38. Announce that design-workshop meetings will be scheduled, if appropriate, to design based on the outcome from affinity diagramming.
Timing for Each Step
The timing for each affinity-diagramming step can vary depending on the:

Number of notes
Number of meeting attendees
Attendees’ experience with user research and affinity diagramming

Even with an abundance of notes, we suggest limiting the meeting to about three hours, and having a scheduled 10-minute break. It’s also helpful to bring out some treats at key times — for example, while people are sorting categories into subcategories.
We like to keep most meetings between 90 minutes and 2 hours. Below are some general timing guidelines for each step:
 

Variations on the Affinity-Diagramming Process

In ideation workshops, teams often sort ideas after they generate them, within the same workshop.
For affinity diagramming of research findings, some teams prefer to sort the sticky notes after each participant’s test session, and again after all (four to eight) sessions. Here are some pros and cons of this variation:

Benefits
Benefits of sorting after each session are that:

It helps the team gain a shared understanding after each user.
It reminds the team members of what they saw, right when their memories are fresh, which can help with further discussions and with observing future test sessions.
It may ultimately be faster if your team members tend to take a lot of notes.
It enables you to summarize results quickly when you plan to make design changes between user tests — for example, for rapid prototype testing with very early design iterations.

Drawbacks

It takes a lot of time, and the same benefits may be achieved with just a quick discussion after each test.
It can deter stakeholders from coming to the final, posttest affinity-diagramming meeting.

Who Should Sort Notes
Anyone can come to the affinity-diagramming meeting. In particular, anyone who:

Attended all or some of the usability-test sessions or design workshop
Has an interest in the design that was tested, or in any changes that will be made to that design

Conclusion
I used to think that, as a user researcher, my job was to make the information I collected about users as understandable and digestible as possible for my team. And as a designer, I was supposed to come up with the best design solution. While I still feel these things are true, I have learned that insulating stakeholders from all the false starts, design ideas, details and messy notes taken in tests does them a disservice. To educate other team members, involve them in design, build consensus, and increase sensitivity to usability issues, get them involved in taking notes, sorting them, and discussing them rather that protecting them from these processes.
Reference
Karl Ulrich, 2003. KJ Diagrams. The Wharton School, University of Pennsylvania, Philadelphia, PA
Raymond Scupin, 1997. The KJ Method: A Technique for Analyzing Data Derived from Japanese Ethnology. The Society for Applied Anthropology"
34,2018-01-28,"Sometimes, as UX professionals, we must perform seemingly wasteful activities for the greater good. One of them is conducting usability studies when we can predict the outcome.
Usability studies serve multiple purposes. The most obvious benefit is to figure out how to best address user needs, by identifying those design elements that do or do not work.
However, an equally powerful, and often neglected, benefit to usability testing is consensus building. Usability studies are a persuasion tool. When your design recommendations meet resistance, it can be better to show rather than tell.
Why Conduct Usability Studies
Below are common situations in which you should conduct usability studies, even if you think you know the answer:
Build Camaraderie and Trust
You may know the answer, but your team members might not. They may come from different disciplines and lack the domain experience that you possess. If you are new to a team or organization, you many need to build their confidence in your abilities.
Usability tests are an opportunity to give teams a shared learning experience and to establish common ground.  The experience of witnessing people’s reactions firsthand alters the conversation from “what I think” to “what customers think.” In the process, you garner trust by making the research process transparent.
Teams that participate in frequent usability studies are more cohesive and successful than teams that don’t because their decisions are based on behaviors  witnessed together, as a group.
Usability tests provide insights that are the foundation of team discussions. When you make a design recommendation, it’s based on shared knowledge, not what some people might misconstrue as your opinion.
As a UX professional, you obviously know that your personal preferences are irrelevant for the design, since you’re designing to satisfy the target audience and not to satisfy yourself. However, since it’s normal human behavior to want to satisfy one self, you can’t really blame your colleagues for suspecting that this could be the motive behind some of your design suggestions, even you’re truly basing them on user needs and not your own likes.
Bridge Divergent Ideas
Divergent thinking can generate creative solutions. However, once multiple alternatives are proposed, they must converge to a single solution. While ideas are easy, consensus is hard. If everyone is right, what should you do?
Rather than participate in unconstructive debates over what’s best, test divergent ideas to get the answer directly from users. The point of testing multiple ideas is not as much to determine a winner, but to identify elements that work best in each design and consider them for future iterations. Usability studies can help diffuse arguments and set teams back on track because it’s difficult to argue with users. Let users be the voice of reason, your referees.
Manage Awkward Requests
Many UX practitioners have experienced the phenomenon in which an executive descends from the heavens with a wild idea and commands you to implement it. When saying no is awkward (or not an option), one way to mollify a tense situation is to offer to research the idea.
Let’s say the request is to build this new shiny app that you know will fail. A diplomatic response might be to see if you can gather data to support or enhance that idea. And in the process you might find an alternative solution that better meets the goals of the request.
In many cases, you can test the idea without creating something from scratch. Most likely the shiny app, or a version of it, already exists in the real world. Get user feedback on these existing designs. Usability tests often result in colorful user quotes and highlight videos that can be extremely compelling.
Avoid Delivering the Bad News Yourself
You might have a strong feeling that a proposed idea is terrible — if released it will fail miserably. No one wants to be the bearer of bad news. And people might not take your assessment seriously, especially if the team is far down the development cycle.  When we tinker at something for a long time, it becomes our baby. Accepting criticism is hard.
Soften the blow by running a usability study of the proposed idea. Allowing actual users to respond to the design removes the perception of subjectivity. Instead of delivering the bad news yourself, let customers do it for you.
Just in Case You’re Wrong
Even the best UX designers can’t predict precisely how people will respond to an interface without testing it. Having experience helps, but it’s not foolproof.
Even though I’ve been conducting usability studies for two decades, something new always surprises me. I’ve experienced situations in which I’ve advocated for a design changed vehemently, only to discover in testing that the issue was not as severe as I anticipated.
It’s not feasible to test everything, but when you have an opportunity to test, you should — especially in highly critical, low-certainty situations. In the course of testing, include other research questions that might help the team so you don’t feel as if you’ve wasted resources.
Conclusion
Usability testing is never a waste of time, even if you think you know the answer. Besides gaining design insight, use it as an opportunity to bring team members and stakeholders along for the ride. Show, not just tell. In the process, you’ll garner more support. When in doubt, just do it — it can take less time to run a simple study than the hours consumed by having a full team meet endlessly to argue over something.
"
35,2018-01-21,"A carefully crafted set of tasks is necessary for success in any type of usability testing. There are some characteristics your tasks should always have, regardless of the specific methodology of your research. However, what precisely counts as a good task depends on whether you’re running a quantitative (quant) or a qualitative (qual) usability study.
Background: Quantitative vs. Qualitative Studies
There are many differences between quantitative and qualitative usability testing. Each one is appropriate for different research goals.
Quant studies closely resemble scientific experiments, with rigorous well-controlled conditions intended to enable the capture of metrics (such as success or time on task).
In contrast, qual studies attempt to discover problems in the UI. Qual research aims to understand the thinking and the difficulties experienced by individual users. It often relies on presenting users with open-ended activities that have the potential to expose issues in the UI.
As a rough summary, the goal of quant user testing is numbers, while the goal of qual testing is insights.
Since the goals and characteristics of these two methodologies are quite different, you’ll need to write tasks differently depending on the type of study. In many cases, a task that would be perfect for a qualitative study wouldn’t work at all for a quantitative study, and vice versa.
Let’s first look at the basic principles of writing tasks for any user testing — quant or qual.
Writing Tasks for Any Kind of Study

Look at what your users need to do with your product for inspiration for your tasks. Remember that tasks must be as realistic as possible. You don’t want to force participants to do something they wouldn’t do in real life, or you’ll measure something that’s irrelevant to your project. User interviews, search-log data, customer-service calls, and analytics data are good places to find out what people want to use your product for.
Avoid accidentally providing any clues in your task. Don’t describe the exact steps users need to take — let them figure it out on their own. Avoid the exact language used as labels in your UI. You don’t want to prime users to look for a specific word.

Bad: Sign up for the Alertbox.
Better: Sign up for this site’s newsletter.

Try to keep the task emotionally neutral. Avoid making assumptions about your user’s personal life or relationships when you’re writing tasks. When possible, it’s better to write a task referencing a “friend,” rather than “your father,” “your spouse,” or some other family member.

Bad: Find a gift for your mom’s birthday.
Better: Find a gift for your friend’s birthday.

Always pilot test your tasks. This is a critical step that far too many researchers skip. You may think your tasks will work well, but you don’t know until you try them out with one or two representative users. Always plan for a pilot in your study budget and schedule. It’ll save you from wasting your resources by accidentally using a bad task and from getting bad data.

Writing Tasks for Qualitative Studies

Open-ended tasks are OK. For qualitative studies, it’s OK to leave the task open to interpretation. While sometimes you’ll want  to give users specific criteria, you can also leave tasks open to see what they care about, and what factors into their decision-making.

Specific qual task: Open a Cash Rewards Credit Card.
Open-ended qual task: Find a credit card that fits your needs.

Provide enough detail to set the scene, but don’t go overboard. It’s important to make sure you’re establishing the participant’s motivation for doing this task, but you don’t have to write a novel. If you’ve made sure this participant is representative of your actual users, and you’re sure this task is realistic for them, then you shouldn’t need to provide too much context.
If you aren’t getting the insights you need, change the task. In a qualitative study, it’s OK to change your task wording mid study, or even throw out or add a new task altogether. Qualitative research is all about getting useful observations, and it’s fine if you don’t have every participant do the exact same tasks. (In fact, better than wasting precious participant time on repeating a useless task.)

Writing Tasks for Quantitative Studies

Make sure there’s only one way to do the task. In contrast to qualitative tasks, quantitative tasks must have only one possible interpretation and solution. They can’t be ambiguous.
	If the instructions are too broad or open-ended, each participant may do essentially different tasks and take different paths through the interface. The metrics you collect won’t describe the same thing, and you’ll have a lot of variability in your data. And how will you know what counts as a “success” if there are many different possible solutions?

Bad: Find a credit card that fits your needs.
Better: Open a Cash Rewards Credit Card.

Provide as many details as necessary to keep the task narrow and focused. In some cases, this means providing a level of detail that would feel excessive in a quantitative study.
	Often, you might think you have enough detail, but then in a pilot test you’ll realize you forgot something. For example, it isn’t enough to tell users which hotel to book at which time period — you also need to tell them what kind of room to book. This level of direction would not be required in a qualitative study, but it’s necessary in a quantitative test to make sure everyone will perform exactly the same task.

Bad: Book a room for 2 people at the Hyatt Regency in Chicago from January 17 to January 19.
Better: Book a Queen Room for 2 people at the Hyatt Regency in Chicago from January 17 to January 19.

Provide fake credentials (dummy data) for login, checkout, or any other task that requires entering personal information. The fact that everybody will enter the same information minimizes variability. All participants will type the same strings. Additionally, some people may be more hesitant than others to share their data. It may take up extra time to come up with an acceptable solution. (For many qual studies, you may want to have participants use their real data instead.)
Each task should stand alone. Ideally, you want to be able to randomize the order of tasks in quantitative studies. If you have two tasks that build on each other, those tasks must always be presented in the same order. Additionally, someone who fails the first task will automatically fail the second.

Bad:
Task 1: Find the personnel file on Joe Smith.
Task 2: On Joe Smith’s personnel file page, find his direct manager.
Better:
Task 1: Find the personnel file on Joe Smith.
Task 2: On the page provided at this link, find who the direct manager of Jim Grant is.

Each task should have one single success criterion. Avoid combining two tasks into one. In quantitative studies, you need a single, clear end point to help you determine time on task and whether the user was successful. If you have multiple success criteria in a single task, what happens if a user finds one piece of information but not the other? Fix stacked tasks by splitting them up.

Bad: Find the museum’s address and holiday hours.
Better:
Task 1: Find the museum’s address.
Task 2: Find the museum’s holiday hours.

Once you’ve begun the study, don’t change the tasks. It’s important that each participant is doing the exact same task in quantitative studies. These studies should hold all conditions constant except your independent variable (for example, your app vs. your competitor’s app). That means you don’t want to get halfway through your study and realize you want to change the wording of a task. Doing so may contaminate your final result. Pilot testing is critical for any research, but particularly important for quantitative testing, when you don’t have the flexibility to change things mid study.
Focus on top tasks. Quant studies are expensive, and it does not pay off to test tasks that aren’t high priority for your users and your organization. Whereas qual tests may have more flexibility to try tasks that are realistic edge cases, quant studies must focus on the most important, core tasks.

Recap: Qualitative vs. Quantitative Tasks




 


Qualitative


Quantitative




Task characteristics


Open-ended, exploratory


Concrete, focused




Details in task wording


Provide as necessary to establish motivation


Provide as necessary to ensure one single way to do the task successfully




Randomization of task order


Nice to have but not required


Required




OK to change tasks midstudy


✓


✗




Realistic tasks that are based on user research


✓


✓




Task wording that does not give any clues


✓


✓




Emotionally neutral tasks


✓


✓




Dummy data


✓


✓




Pilot testing of tasks


✓


✓




 
Regardless of the type of study you’re planning, writing good tasks takes practice.
 
For hands-on practice writing good qualitative tasks and tips for facilitating a qualitative usability study, check out our full-day seminar Usability Testing."
36,2017-12-17,"Quick quiz. Which phrasing of a follow-up question will result in more accurate results in your next user research?

“I saw you were having difficulty with the navigation. What happened?”
“Why did you have difficulty with the navigation?”
“What was easy or difficult about getting to the content you wanted?”

The third question will get you the most reliable response, but why? Because each of the first two questions is leading – meaning that it includes or implies the desired answer to the question in the phrasing of the question itself.
Let’s see what is problematic about the first two questions.
Question 1: “I saw you were having difficulty with the navigation. What happened?”
Problem: The interviewer rephrases what was observed, which may not be an accurate representation of the user’s experience. To the interviewer or observer, it may have looked like the respondent was struggling with navigation, but she may have been deciding what information was most important, confused by the task, or exploring various areas of the site. The question also names a user interface element — the navigation — which is a term that users may or may not fully understand, relate to, or normally use.
Question 2: “Why did you have difficulty with the navigation?”
Problem: Again, this question implies the answer and assumes that navigation was the problem. It also puts the blame on the user, rather than on the site. It focuses the question on the user’s actions as opposed to the elements in the site that may have contributed to the user’s actions.
Question 3: “What was easy or difficult about getting to the information you wanted?”
Improvement: This question steers the user to the topic of interest — moving around the site and finding content — without suggesting terms or feelings to the user. The user can say it was simple to move around or difficult, without disagreeing with the interviewer.  Here the interviewer offers a general frame for the topic of the question, rather than suggesting a response.
Much of user research is observational — we watch what users do. But we also listen to what users tell us, and in many instances, we request clarification about what they tell us. We ask follow up questions after tasks. We may prompt users to share more information in the moment. We start a session by asking for or confirming some basic information about users.
Honest, unbiased participant feedback is critical for user research. When we ask questions, we want to learn more about the user’s actions. Why was this piece of content clear? Why did an interface element cause difficulties? Leading questions are a problem because they interject the answer we want to hear in the question itself. They make it difficult or awkward for the participant to express another opinion. This is particularly true in a usability-study interaction, where often the interviewer is the “authority” in the room and many participants will not want to disagree.
Leading questions result in biased or false answers, as respondents are prone to simply mimic the words of the interviewer. How we word these questions may affect the user response and also may give them extra clues about the interface. We may end up with inaccurate feedback that may or may not truly reflect the user’s experience, mental model, or thought process. Or, even worse, we may alter that user’s behavior for the rest of the session. For example, an unexperienced facilitator asked “What do you think this button does?” in a session and made the user realize that the text she was pointing to was in fact an active link.
Leading questions ultimately rob us of the opportunity to hear an insight we weren’t expecting from the user. The more leading our questions are, the less likely the user will comment in a way that surprises or intrigues us, or makes us think about a problem or solution in a different way. They may be good for “validating” designs, but are definitely bad for testing designs.
Keep in mind that sometimes the best question is not a question at all, but a redirection to help users continue their thoughts. When we do want to ask questions, how can we avoid leading the user?
What Makes a Question Leading
You may be familiar with the idea of leading questions from courtroom dramas, as lawyers call out: “Objection! Leading the witness!” There are many ways in which we can prime a user to simply repeat or confirm our bias, agenda, or assumption. Some practitioners may do it intentionally, trying to get confirmation for their own theory about what does or does not work in a design. However, most of us do it unintentionally. To gather user insights, we should ask open-ended questions — questions designed to elicit explanations from users, rather than single-word yes, no, or multiple-choice answers. How we ask these questions is essential to the value and validity of the feedback we receive. Here are some common traps to avoid:

Do not rephrase in our own words.

	
Participant: “I notice this picture here…”
Researcher: “You mentioned that the picture was helpful. What about it did you like?”
Improvement: “You mentioned the picture…?”


Do not suggest an answer.

“How well would this save time for you during your workday?”
Improvement: “How might this affect your efficiency, if at all?”


Do not name an interface element.

“The related links on the side of the page here — where would those lead?”
Improvement: “This area on the side of the page… [point to area]. What is that?”


Do not assume you know what the user is feeling.

“When you were struggling with this task, what was happening?”
Improvement: “What was easy or difficult about completing that task?” 



Practice Makes Perfect…or at Least Better
Some questions that we ask participants are prepared ahead of time — for example, we may start or end all research sessions with a list of standard questions. In that case, we can easily review those and rewrite them as many times as necessary to make them neutral.
It is much more difficult to make up questions on the fly without impacting what the user says. Everyone makes mistakes in phrasing now and then, and it is difficult to ask a question that is unbiased in every way. However, being aware of the problem is a good way to start fixing it. Watch out for instances where you ask leading questions, or observe others doing so, and see how it impacts the user’s response."
37,2017-10-01,"Introduction
All usability-testing studies involve a participant performing some assigned tasks on one or more designs. There are, however, two types of data that can be collected in a user-testing study:

Qualitative (qual) data, consisting of observational findings that identify design features easy or hard to use
Quantitative (quant) data, in form of one or more metrics (such as task completion rates or task times) that reflect whether the tasks were easy to perform

Qual Research
Qualitative data offer a direct assessment of the usability of a system:  researchers will observe participants struggle with specific UI elements and infer which aspects of the design are problematic and which work well. They can always ask participants followup questions and change the course of the study to get insights into the specific issue that the participant experiences. Then, based on their own UX knowledge and possibly on observing other participants encounter (or not) the same difficulty, researchers will determine whether the respective UI element is indeed poorly designed.
Quant Research
Quantitative data offer an indirect assessment of the usability of a design. They can be based on users’ performance on a given task (e.g., task-completion times, success rates, number of errors) or can reflect participants’ perception of usability (e.g., satisfaction ratings). Quantitative metrics are simply numbers, and as such, they can be hard to interpret in the absence of a reference point. For example, if 60% of the participants in a study were able to complete a task, is that good or bad? It’s hard to say in the absolute. That is why many quant studies usually aim not so much to describe the usability of a site, but rather to compare it with a known standard or with the usability of a competitor or a previous design.
While quant data can tell us that our design may not be usable relative to a reference point, they do not point out what problems users encountered. Even worse, they don’t tell us what changes to make in the design to get a better result next time. Knowing that only 40% of the participants are able to complete a task doesn’t say why users had trouble with that task or how to make it easier. Often researchers will need to use qual methods to supplement quant data in order to understand the specific usability issues in an interface.
Statistical Significance
One advantage of quant over qual is statistical significance. When quant data are presented in a sound way, they come with a certain protection against randomness: usually, mathematical instruments such as confidence intervals and statistical significance will tell us how likely it is that the data reflect the truth or whether they may be just an effect of random noise —  perhaps an artifact of the specific participants that we happened to recruit or of the conditions in which the study was run. Although seasoned qual researchers will deploy an arsenal of good practices to protect themselves from chance and to make sure that their results are not biased, we have no formal assurance that the findings from a qual study are indeed objective and representative for the whole target population.
Differences Between Qual and Quant
Qualitative and quantitative data need slightly different study setups and very different analysis methods. They are rarely collected at the same time — hence the distinction between qualitative and quantitative user studies. Both qualitative and quantitative testing are essential in the iterative design cycle. Although qual studies are more common in our industry, quant studies are the only ones that allow us to put a number on a redesign and clearly say how much our new version improved over the old one —  they are the essential instrument in calculating return on investment.
The table below summarizes the differences between the two types of research. In the rest of the article we discuss these differences in detail.



 
Qual Research
Quant Research





Questions answered


Why?


How many and how much?




Goals


Both formative and summative:

inform design decisions
identify usability issues and find solutions for them



Mostly summative:

evaluate the usability of an existing site
track usability over time
compare site with competitors
compute ROI





When it is used


Anytime: during redesign, or when you have a final working product


When you have a working product (either at the beginning or end of a design cycle)




Outcome


Findings based on the researcher’s impressions, interpretations, and prior knowledge


Statistically meaningful results that are likely to be replicated in a different study




Methodology



Few participants
Flexible study conditions that can be adjusted according to the team’s needs
Think-aloud protocol




Many participants
Well-defined, strictly controlled study conditions


Usually no think-aloud





The Iterative Design Cycle: Goals for Qual vs. Quant
The basic user-centered design cycle starts with an evaluation of an existing design, followed by a redesign intended to address the current system’s usability challenges. Once the new version is complete, it can be evaluated and compared against the initial version.

The iterative design cycle: Steps 1 and 3 involve summative research (done with either quantitative or qualitative methods), while step 2 involves formative research (done with qualitative methods). 

The first and third stage of the iterative design cycle are summative — they are intended to provide an overall assessment of a design. In these steps both qual and quant research methods (or combinations such as PURE) can be used to evaluate the design. However, when the goal is to link the entire redesign effort to actual financial savings or explicitly figure out how much the redesign has improved, quant studies must be used. Organizations with mature UX often have such a quant usability-tracking process in place. (Sometimes this process of quantitatively evaluating each version of a design and comparing it with previous versions is called benchmarking.)
During the redesign stage, user research has a formative role: it is meant to inform the design and steer it on the right path. In this phase, designers and researchers need to get user data relatively quickly and cheaply in order to be able to choose among different design alternatives and create a usable UI. At this stage, qual studies are usually the most appropriate. We know that, with 5 users, a qualitative study is likely to uncover 85% of the usability problems in a design (provided that the design is not already close to being perfect), so, in the redesign step, it makes sense to run one quick study with a few users, determine the big issues, fix them, and then test the new version again with another small set of users.
When to Use Qual vs. Quant
Qual studies are well suited for identifying the main problems in a design: for example, we can easily run a qualitative study to see what (if anything) prevents users from submitting a form successfully, and, based on that study, we may determine that we need to lengthen the form fields, present password requirements, or use labels outside the fields rather than inside them.
In contrast, most quant studies are done on a complete version of the site, with the purpose of evaluating the usability of the site, rather than directly informing the redesign process. This is not because one could not employ quantitative methods during the redesign iterations, but rather because quant usability studies would be too costly if used often and early in the design process. Quant studies usually involve a large number of users, and most organizations cannot afford to spend a lot of money on such studies to investigate whether the page copy is clear or whether a button is findable. However, the numbers obtained from quantitative testing can be invaluable when it comes to convincing upper management that your site is in need of a complete redesign.
Outcome: Qual vs. Quant
Qual data usually will consist of a set of findings, which identify (and prioritize according to severity) the strengths and weaknesses of a design. These findings are estimates — they are based on the knowledge and level of experience of the researcher facilitating the task and interpreting the meanings of the users’ actions. Different practitioners will often identify different issues in the same user-testing session (a phenomenon known as the evaluator’s effect). Plus, even if we’ve been very careful in recruiting participants who match our target demographics, when we include only a few people there is always a chance that they are not truly representative of the whole user population, and so our findings may be skewed.
Quant studies usually involve a relatively large number of users (often more than 30) and use statistical techniques to protect themselves against such random events. When reported correctly, quantitative studies will include information about the statistical significance of the results. For example, a margin of error will help you understand how much you can trust the results from the study.  Or, if the difference in task-completion time between your site and the competitor’s site was statistically significant, you will know that, even if you were to recruit a different set of users and rerun your study, your results will point in the same direction, even if the exact averages may be slightly different.
Thus, when quantitative studies are carried out and analyzed correctly, you can have confidence that their results are sound. Namely, that they are not due to a lucky or unlucky throw of the dice.
These types of analyses are based on statistics and usually involve other types of skills than you will find in a qual usability researcher. That is why many companies have separate job requirements for quant and qual UX researchers.
Methodology: Qual vs. Quant
On the surface quantitative and qualitative user testing can look quite similar (i.e., they both involve users performing tasks on a design). Both types of studies need to follow the basic rules for good experiment design, by making sure that they have:

External validity: participants are representative of the target audience and the study conditions reflect how the task is done in the wild. For instance, testing a mobile site on a desktop simulator lacks external validity because people would normally use that site on a touchscreen phone.
Internal validity: the experiment setup does not favor any one condition. For example, if design A is tested in the morning and design B is tested in the afternoon, it is possible that fatigue play a role in how participants use design B.

But, because quant studies strive to obtain results that are meaningful statistically, there are some important differences between the two types of studies:

As discussed above, quant studies involve more users than qual studies.
Because differences in the session setup and in participant backgrounds can increase the measurement noise and lead to larger margins of error, quant studies aim to minimize variability as much as possible. Thus:
	
The conditions in quant studies need to be stringently controlled from session to session. That is, you need to make sure that your participants are all run in pretty much the same environment as possible: you cannot have two sessions done in person, and three sessions done remotely.
Quant studies often start with a practice task intended to make all participants familiar with the study setup and with the site being evaluated. In this way, possible individual differences between, say, expert and novice users are ironed out, as novices get a chance to learn the interface.
The think-aloud protocol is the de facto method in qual studies, but is sometimes not recommended in quant studies. Researchers are split as to whether the think-aloud protocol can be soundly used in quant studies. To some degree, because some people are more talkative than others, it is likely to increase the measurement noise. As a result, many quant studies do not ask participants to think out loud.
Personal information such as names, addresses, or birthdates will increase the variability of the study, because different people have different data. Whereas for a qual study, you want people to enter their own, real information, in a quant study everybody should have the same experience and thus should type in the exact same strings. That is why participants should be provided with a set of made-up data that they can all use. (This constraint will sometimes create backend difficulties for a live system.)


Conversely, for qual studies, it’s okay to vary the study conditions between sessions. For example, if you discover that a certain task doesn’t give you the insights you need, by all means rewrite it before running your next user. Changing the task would make it invalid to average measures across users who had performed the different tasks, but in a qual study you aim for insights, not numbers, so you can take liberties that will ruin numbers (which are not your research goal anyway).
For quant studies, tasks need to have a single well-defined answer. Thus, while a task such as “Find the phone number and the address for John Smith” may be appropriate for a qual study, it is not good for quant studies because it is hard to code success for it: if the participant finds the phone number but not the address, should that be considered a failure?
	Moreover, all participants should understand the same thing when they read the task. A task such as “Research the requirements for obtaining a drone-flying permit in California” is too vague for a quant user study, as different people may understand different things by the word “research,” but can be okay in qual studies if you’re trying to figure out what types of information people may be interested in.

Whereas it is good practice to use task randomization in both qualitative and quantitative experiments, often qual studies won’t be completely randomized. In quant tests, randomization ensures that the order of the tasks does not bias the results in any way.

Conclusion
Qualitative and quantitative user testing are complementary methods that serve different goals. Qual testing involves a small number of users (5–8) and directly identifies the main usability problems in an interface. It is often used formatively, to inform the design process and channel it in the right direction. Quant usability testing (or benchmarking) is based on a large number of participants (often more than 30); when analyzed and interpreted correctly, results from quant tests come with higher protection against random noise. Quant studies offer an indirect, summative evaluation of the usability of a site through metrics such as task-completion rate, task time, or satisfaction ratings and are typically used to track the usability of a system over design iterations.
For an overview of popular quantitative research methods, guidance on which to use each one, and how to calculate return on investment, check out our course Measuring UX and ROI."
38,2017-09-17,"For some usability studies, there may be an obvious and limited set of interface elements that should be tested. In many cases, however, the system is complex, new to the researcher, or the list of candidate features for testing is long. This article shows a 7-step method that helps everyone focus on the most important elements to test first, while also prioritizing everything of concern for later research.
The 7 Steps

Determine the most important user tasks.
Discover which system aspects are of most concern.
Group items from 1 & 2, then sort issues by importance to users and organization.
For each top issue, condense the information into a problem statement.
For each problem statement, list research goals.
For each research goal, list participant activities and behaviors.
For each group of goals, write user scenarios.

Our checklist for planning usability studies describes the larger process.
It’s important to involve stakeholders in these planning steps. After conducting user studies, researchers can sometimes encounter resistance to the findings: someone says the researcher recruited the wrong participants or tested the wrong elements. Going through part of the test-planning process as a team ensures that everyone understands and supports the research goals, priorities, and methods.
If you have only limited access to your stakeholders, you can do steps 1 and 2 in advance, steps 3–5 can be done in a one-day workshop with the stakeholders, and then the UX researcher or UX team can do steps 6 and 7 alone.
For each product, you might do this end-to-end process only once, because the process generates important mutual understanding, many useful research issues, and potential metrics. New features and issues that come up from time to time can be easily added into the artifacts resulting from this one-time process.

This process flow depicts the steps discussed below.

1. Determine the Most Important User Tasks
What do people need to do? Make a list of the tasks that users must be able to accomplish with the system.
Normally, every organization has a list of the top user tasks, because this list is instrumental not only in evaluating the usability of your website, device, or application, but also in tracking improvement over time. Ideally, you should use a systematic way of compiling the set of top tasks, by having users rank all possible tasks that the system can support.
But even if you haven’t yet formally engaged in this exercise, you can still determine the top tasks by looking at traffic data from website analytics, search-log queries, survey data, a competitor analysis, or other types of user data. And sometimes the important tasks are well-known or obvious. For example, on an ecommerce website, one of the most important tasks is buying a product. This task is composed of a number of subtasks:

Top task: Buy a product
	
Look for a desired product.
Compare similar products and decide which one to buy.
Put product in the shopping cart.
Complete the checkout process.
Be sure the purchase is complete.
Understand when the product should arrive.



Other important tasks on an ecommerce website likely include: return a product, ask customer service for information, and look up a previous purchase.
2. Discover Which System Aspects Are of Most Concern
What are stakeholders most worried about? Often organizations have concerns that motivate them to seek usability testing. Examples include making sure users understand new features, reducing training and support costs, or discovering why people do (or don’t do) something. Look for issues that make users unhappy, problems that cost the organization money, and tasks that take too long to accomplish.
If stakeholders are not nailing these lists to your door already, you can discover top concerns and burning questions by interviewing members of product teams, customer-support staff, sales people, social media representatives, documentation specialists, and trainers. Other evidence may exist, such as search logs, FAQs, journey maps, and materials made by support and training staff that point to issues that might be useful to explore in testing. Doing a design review of the system is another good way to find issues that would be useful to test.
3. Group Items from 1 & 2, then Sort Issues by Importance to Users and Organization
Group related user tasks, stakeholder concerns, and questions. Sticky notes, a whiteboard, spreadsheet, or card-sorting system can help with this process. Your list of grouped issues will probably be too much for one research study, but it will be useful over time, so keep everything. It’s important to include user tasks, because stakeholder concerns don’t always encompass what users want and need to do; yet top tasks must be usable and often need testing.
If possible, prioritize issues as a team, with representative stakeholders.
Name each group of related issues, then put their names in a spreadsheet to sort and prioritize. You can use various methods to decide priorities; here’s a simple example that takes into account both user and business goals.




Issues


Importance Ranking




Name


Users


Organization


Total Score




Customer-service quality complaints


5


5


10




Search-results quality issues


5


3


8




Comparing stereos is difficult


2


5


7




Sharing feature is not being used enough


0


5


5




Few people listen to the podcast


0


4


4




A spreadsheet and a point system can help with ranking: Sort the named issues from most important to least important, considering both the concerns of users and the costs and benefits to the organization.
Rank issues by importance to both organization and users. When there are many stakeholders or many issues, consider voting with tokens (sticky dots or coins, for example) or using a survey platform’s ranking system.
The objects of this step are to:

Capture everything of importance
Prioritize the list of issues quickly
Create consensus

Don’t obsess over the method of scoring. List the top issues in a spreadsheet, with one column for importance to users and another column for importance to the organization. If the number of issues is large, rank only the most-important 10 or 15 issues in the table. Add up the user and organization numbers for each issue, and then sort the rows by total score.
Together, choose the issues to focus on for the next usability study. Keep in mind that although the total scores are important for prioritization and consensus, sometimes a test should include issues that score 0 for users but high for the organization — for example, expensive problems or new features.
Tips: If you require more accuracy — for example when the list of tasks is very large, the frequency of testing is very low, or changing the product is very expensive — you could survey users and stakeholders separately in order to discover and rank top tasks and concerns. (See our 2 min. video about ranking techniques for UX projects.)
4. For Each Top Issue, Condense the Information into a Problem Statement
Problem statements help everyone focus on what’s important and help you tie research goals to issues everyone wants to solve. Maintaining a list of known problems helps justify and get buy-in for research activities. Problem statements also suggest metrics you can measure to track improvement over time. Sometimes these statements will also include questions. Related issues can be combined into one problem statement, so you may end up with fewer problem statements than top issues. If that is the case, you will need to prioritize the problem statements — using either the issue priorities determined in step 3 or some other method.
Encourage stakeholders to participate in this step as well. It’s much easier to get people to accept and act on the results of a user study when they have been part of the process.

Explain that the purpose of the meeting is to help decide what is most important to test first, and how.
Share the detailed list of issues from step 3 with everyone.
Write the issue names on a big whiteboard (or share a document for writing text together, if your team is distributed), leaving blank space under each for the problem statements.
Combine related issues into problem statements; for example:

Problem: People call customer service because they can’t find instructions on how to set up their networked lighting system or because they don’t understand the instructions. Calls are expensive for the company, and customers are angry when they call.
	 
Prioritize problem statements for testing, either by combining issue scores (as in step 3) or by voting.

5. For Each Problem Statement, List Research Goals
Problem statements suggest goals for the study. For example, given the problem statement above, here are some corresponding study goals:

Discover where in the process people need instructions.
Learn why people can’t find the instructions.
Determine the best locations to put the instructions.
Find instructions that need improvement.
Find opportunities to improve the product in order to minimize instructions.

Some research goals can’t be addressed with usability testing. In those cases, note which methods would be better to use (for example, a survey or analytics) and set those goals aside.
6. For Each Research Goal, List Participant Activities and Behaviors
List the user activities and behaviors that you need to observe in order to address each of the research goals.
Research goals and problem statements can be quite abstract and high level. In order to tackle them in usability testing, you will need to observe users attempting to complete activities that are related to these goals and problems. This step helps you identify (1) types of user activities that are related to your research goals; (2) user behaviors that signal success or failure for a particular activity; (3) possible steps and problem-solving strategies that people could use in doing the activity.
For example, for the goal “Discover where in the process people need instructions,” some likely activities for the process would be:

Unbox the lighting system
Assemble the lighting unit
Connect the unit to the network

Some possible participant behaviors that signal success or failure might include:

Asking questions (note their exact wording)
Getting stuck for more than a minute
Wanting to contact someone for help (note who and how)

Some problem-solving strategies people may use to complete the activity include:

Looking for instructions in or on the box
Looking for instructions on the website (note search queries and navigation paths)

After you have done this breakdown for each goal, group goals that have all or many of the same activities and behavioral observations so you can try to write one scenario that covers several goals.
In this example, it’s likely that the listed activities and behaviors also cover the goals: “learn why people can’t find the instructions,” “determine the best locations to put the instructions,” and “find instructions that need improvement.” The final goal for this problem statement, “find opportunities to improve the product in order to minimize instructions” will probably be met by ideas that occur to you and observers during the study.
Planning at this level of detail has four main advantages:

Focuses scenarios (step 7) on what you hope to observe
Helps observers focus on what’s important to note during the sessions
Allows you to decide on the granularity of the success and failure criteria
	For example, instead of having one, monolithic user activity (set up the lighting system), you can consider whether to assign points to (or just track success and failure for) some or all of the subtasks that the participants complete successfully.
Helps you decide in advance how far you want to let the participants go, how extensive your test environment needs to be, and what you might need to prompt participants for

For example, in this situation, you need to decide whether you want to:

Listen to participants make calls and find out how well that process works for them
Watch them create support tickets to discover how difficult filling out the form may be
Capture the questions they want to send to support (how?)
Let them look through the support website (for how long?) or just find the support landing page
Prompt or encourage the participant to do or not do any of these things (what’s the best way to phrase what the facilitator should say?)

7. For Each Group of Goals, Write User Scenarios
Scenarios are the instructions that your test participants read during the study. For our example list of research goals (step 5) and the corresponding activities and behaviors (step 6), here’s a good user scenario:
Please show me how you would set up your new Acme Home Illumination Kit, and explain what you are thinking and doing in detail while you put it together.
With any luck, participants who attempt this scenario will find and read instructions as needed and they will think out loud by way of explaining, so you can understand their difficulties with both the system and instructions.
Because the example situation is quite straightforward, this scenario seems obvious and unworthy of the steps to arrive at it. (However, all scenarios should encapsulate your thinking and planning process into a simple statement that participants can understand quickly.)
Nonetheless, this method can untangle big and complicated systems, tame and educate a room full of stakeholders having competing priorities, generate a prioritized list of research problems to work on over time, help organize the test plan, the facilitator script, and observer study guide, generate usability metrics, and help you think through the logistics and contingencies for your usability sessions.
Tips:

Some goals might lead to several scenarios. Some scenarios might cover several goals. As you work through this process, think about how you can combine goals into scenarios without making them too long or too complicated.
Similarly, across problem statements, some duplication will naturally occur when making lists of goals and activities. After you make the lists, consider whether it makes sense to remove some steps or to combine or sequence scenarios in order to reduce repetitive activities or to reduce biasing effects (when doing one scenario teaches people a lot about another scenario). Sometimes it makes sense to watch each person do many searches, but you might need to watch other activities only once or twice for each participant.

Guidelines for Writing Scenarios
We wrote entire articles to help you write effective scenarios and avoid common errors in the specific task instructions. Briefly:

Focus scenarios on outcomes your users would be motivated by.
Write scenarios in a way that should cause people to do the things you need to observe.
Don’t instruct people to operate the user interface.
Don’t mention labels found in the interface. (An exception might be “search,” when you’re testing search, but consider whether you want to find out if people notice or locate the search tool on their own before you mention it.)
Use general words your participants should understand, not technical or branded terms.

Next Steps
Once you’ve generated scenarios for your research goals, the next steps usually involve:

Engage stakeholders in observing the study sessions to help you focus on the most important issues and to build consensus and buy-in for your user research activities.
Decide what to measure and how to score it.
Pilot test your session to debug the scenarios and your process, and to determine how many scenarios to attempt per session.
Include the information generated from these 7 steps in your research plan for all of the scenarios in your study. Document the rest of the information for future research studies.
Invite stakeholders to your study sessions and encourage them to help you take notes.

Conclusion
This 7-step procedure makes it easy to decide what to test, how to test it, and how to write comprehensive user scenarios. The lists generated at each step help you to prioritize research goals, to get buy-in for your research sessions, to organize your study sessions and materials, and to track important issues over time."
39,2017-09-10,"All too often people use the word ""validate"" to justify their UX work. As in, ""Let's test the design to validate it,"" or “Let’s do an expert design review to validate this iteration.”
I strongly oppose using ""validate"" in this context. Call it a pet peeve. Call it semantics. Call me rigid. But let me tell you why I feel this way.
User research is as much a mindset as it is a science and an art. In a user study, attitude greatly affects participants, your team’s reactions to their behaviors, and follow-up actions on study findings. Planning the testing, observing users, analyzing findings, and describing the research process all require a delicate balance of curiosity, realism, diplomacy, honesty, and a profound ability to welcome and withstand criticism.
The Effect on the Test Participant
What you say from the inception of the study to its conclusion — how you recruit, write tasks, ask questions during a test, and lead posttest interviews — can all subtly (or not so subtly) affect how the test participant reacts. It’s a simple matter of priming.
For example, imagine that, as you are briefing the test participant just before you begin a usability test, you say, ""We would like to watch how you do things so we can validate the design.""
This statement implies to users that they are not supposed to point out problems with the design. It also suggests that, if they don't understand something, they are inept because this design is finished and just in the phase of being validated, not in the stage when making changes would be expected and acceptable.
The Effect on Your Team
Saying to your team that you want to validate a design suggests that you know it works and are simply looking for concrete proof. It implies that you are resistant to learning that the design doesn’t work or that you may need to fix it in major ways. And when the team is thus biased, it can reason away major design issues as minor bumps, because the unsaid implication of “validate” is that the design is at a final stage.
When both the team and the study participants are thus primed by the word “validate,” user research turns into a complete waste of time and money.

User Research Should Uncover Many Negatives and Some Positives
A usability test should always find both positive findings and a substantial set of problems.
It is important to find and discuss good things about any design because:

They help the team realize which design aspects work, so they don’t change them. (The old saying, “If it ain’t broke, don’t fix it” requires you to know what’s good in the UI.)
The team can learn about what makes a design good and may even be able to reproduce those positive traits in future designs by constructing design patterns and usability guidelines on top of instances of good usability.
They can boost the team's morale.
They solidify your credibility as a researcher. If you are always look at the negative, your perspective can seem imbalanced and not valuable.

The negative findings in a user study are obviously what drives design change and improvement. Any study should find some issues with the design. If it doesn’t, here are some probable reasons:

The test was not set up well, and it would behoove you to look at your methodology to see if you structured your study appropriately. Jakob Nielsen has said many times, “If a usability study found nothing to improve in a design then that only proved one thing: that the test was done wrong.”
The scope of your study was tiny. For example, you tested whether people notice a cursor change. In a case like that, the hypothesis is so confined that the study could, indeed, yield no negative findings — people did notice the cursor change and that was that. Studies like this happen a lot in inexpensive remote sessions. But efficiency dictates that an in-person study looks at more design aspects to offset its cost.
Your team is inexperienced at analyzing user behavior. Maybe problems did occur in the test, but the team ignored them or did not know how to analyze them and thus found no issues.
The team’s eyes are closed to negative findings. Because the stage was set before the test that the design is almost complete, the team did not notice usability issues, or the team members were afraid to bring them up.

In short, if your user study did not find any issues, something is wrong with your study or with your team. The perfect user interface has not been seen yet, and it’s unlikely to make its first appearance in world history during your current project.
If you are going to take on the cost of collecting and analyzing user data, you should also plan to support the cost of changing the design based on the study findings. Many teams don't plan for that last step, but they should.
Denying Issues Is Not the Same as Deferring Issues
Of course, sometimes a tight schedule or limited resources make it impossible to implement all the design changes suggested by the user study. But this reality doesn’t grant us license to say that there were no problems found in the study. There is a big difference between deferring design issues and denying their existence. It's better to discuss the problems found, rate their severity, and triage what can be done now. Even with a tight schedule, you may be able to try some of the following actions:

Tweak the design slightly.
Edit documentation.
Tailor training.
Forewarn support teams about usability issues expected to come up.
Set analytics metrics to check the anticipated problems.

In the spirit of continuous quality improvement, consider your new release to be a prototype of the release to follow. By documenting known weaknesses in the new release, you have a leg up on the work of designing its replacement. Record these issues (in a database) and feed them into the requirements for the next release of this design.
Instead of “Validate”
What should we say instead of ""validate""?
Try these:

“Test”
“Research”
“Examine”
“Study”
“Analyze”
“Watch how people use”
“See where the design is successful and unsuccessful”

If “validate” is a permanent fixture for you or your team, consider balancing the possible priming by pairing it with “invalidate,” as in “Let’s test the design to validate or invalidate it.”
Conclusion
A sentiment better than “Let’s validate this design,” is ”Let’s learn what works and what doesn’t work well for users and why.”
Strictly banning the word ""validate"" from user research is an inflexible and rudimentary solution to a simplified problem. The greater point of focus is that subtle words inspire meaningful attitudes. These attitudes result in positive or negative actions and habits. A viable UX research program in the short and long term means: 1) making a habit of changing designs based on user feedback, and 2) using concise, descriptive, and constructive words when discussing research.
(Remember: user research can be cheap and fast: learn how in our full-day Usability Testing course.)
"
40,2017-04-30,"Introduction
Teams should collaborate on user experience activities in order to be sure to make products that are usable and desirable. Collaboration and team ownership of the user experience has many other benefits, in particular saving time by helping people design more-usable software to begin with, once everyone understands the users, their goals, and difficulties. A very effective way to get people involved is to invite them to observe user-testing sessions. Observing can be a great introduction to the value of user research, and gathering data together can bring benefits to all concerned, including:

Gaining insights and developing high-quality solutions, as observers interpret incidents through their diverse professional points of view
Engaging the team constructively, as everyone understands usability problems together
Generating top findings together to get consensus quickly and motivate the team to fix problems
Demonstrating how fast you can gather actionable information through UX research

A practical number of notetakers is 4 to 10 per session including designers, researchers, technical leads, and other stakeholders. Decide who needs to watch most or all of the sessions, then rotate a few other stakeholders into available slots. Observers should watch at least 2 sessions in a study, so they won’t be tempted to form judgments based on 1 user.
During user-testing sessions, most researchers take notes alone or in pairs, but when more people are involved, a bit more structure is necessary. In most UX research situations, it’s best to use the simplest tools for the job, and reduce reliance (and load) on the network. You can take group notes in a word processor, in a spreadsheet, on sticky notes, or on colored paper.
Each person should take notes separately. Later, the group or the researcher can combine and sort the observations. Shared collaborative documents can work fine for a couple of people, but for larger groups, shared documents can distract from the main task of observing, becoming unwieldy when everyone tries typing at once. Worse, seeing others’ notes can lead to groupthink or can diffuse responsibility and discourage people from taking notes, because someone else is already doing it.
Group notetaking involves 2 steps:

Capturing notes: each individual writes down observations during the session.
Collecting insights and observed problems: after each session, the main findings are summarized on a whiteboard. This is a group activity.

Techniques for Capturing Notes
There are 2 main ways of recording user data: chronological logs and topical notes. Both methods work well, so choose one based on your situation, or combine them.  
Chronological Logs
Chronological logs are lists of observations made during a user test, in the order in which they occurred. Typically, notetakers use a document template that includes the facilitator script (with user scenarios) and provides space to take notes. Each notetaker uses 1 copy of the document per participant.
Below is an example of a chronological log for a device-based study. This template could be presented in a word processor or as a spreadsheet. The facilitator reads aloud scenarios with the bold words or hands the user these scenarios one at a time, printed on paper (or in remote studies, by pasting them into chat or SMS).



Scenario A: Look at this product and tell me who would use it and what you can do with it. Please comment on anything that springs to mind.
			[5 minutes max.
			Tasks:
			1. Who/what is this for
			2. Read instructions]
Now show me what you would do with it.
			[3. Assemble and turn it on. – 10 minutes max.]


P
Scenario/Task
Observation
Category
Importance


1
A1
“This is for people like me, who want something that looks nice and works well.”
Q
*****


1
A1
“This box is actually elegant. I’d keep it. It would fit easily in my closet.”
Q
****


1
A2
Read product features in Getting Started guide
Doc
**


1
A2
Started with first instruction
Doc
Good


1
A2
She looks confused, hesitant
Doc
**


1
A2
Maybe putting the feature list on the box would be better.
Me
?


1
A2
Said she can’t understand #7, that it should be combined with #8
Doc
?


1
A3
 
 
 


1
A3
 
 
 


1
A3
 
 
 



[4. Discussion]
Q: Have you used something like this before?
			[Circle answer]   Y   N 
[If yes] Please tell me about that.
			[If no] How do you normally do this [task] now?



P
Scenario/Task
Observation
Category
Importance


1
A4
 
 
 


1
A4
 
 
 


1
A4
 
 
 



Notetaking rows contain the participant’s number (P1, P2, etc. — no names), the scenario and task identifiers, the note, categorization of the observation, such as topic (Category column in our example), and importance. Researchers typically invent their own abbreviations for quick logging. Some of the information can be added before or after the session (for example, the first 2 and the last 2 columns). Some fancy systems and spreadsheets can add timestamps, or you can just manually note clock times for key quotes or events that you want to review later in recordings. Your own comments, suggestions, or insights about the interface should be marked as such and be distinguished from user-based observations. (The example above uses “me.”) A separate Comments column can sometimes be helpful.
Two quick shorthand notations can be entered into the comments or Category column when appropriate:

V for Video: this segment might make for a compelling video clip to include in a highlights reel after the study.
M for Methodology: we may want to revise our study methodology to handle situations like this better in the future. For example, the facilitator might have asked a question in a manner that biased the user’s response.

In theory, you could provide paper copies of the notetaking template, but in practice it makes little sense to do so. As a result, chronological logs are used mostly in digital format — each notetaker records observations on a laptop or another portable device. 
Advantages of digital chronological logs:

Observers can be in several locations.
Handwriting legibility doesn’t matter.
Time data and sequences of actions can be compared or recreated, and click paths can be used to analyze navigation problems and solutions.
You can combine notes from all the observers and then sort the rows in useful ways.
Notes are immediately searchable.
Digital raw data can be stored with your project plan.

Disadvantages of digital chronological logs:

Everyone has a distraction machine in front of them.
Facilitators may have to travel with 2 laptops (1 for taking notes, 1 for the user) when testing away from the office.
People have to type very fast during the test sessions, which may be fatiguing as well as noisy.
You have to categorize, rate, and count everything afterward in long, scrolling, digital documents.
When people type, they tend to write more, which can cause extra data-analysis time if paragraphs contain many issues.

Tips: Remind everyone to use session time to capture as many observations as possible rather than to write neat stories. During data analysis, you can add more columns to put additional categories in, or you can import log data into a database or mindmap.
Topical Notes
If your team is in one location, using square sticky notes or small notepaper for group notetaking can be a great choice. With this method, each observer writes 1 incident per note. When doing qualitative studies with a small sample size, each participant is represented by a different paper color. Each note also includes the scenario letter and task number and the notetaker’s initials, with optional note topic.
Some observers can be asked to focus on capturing specific types of information, such as quotes, search queries, or click paths. After the research sessions end, the group sorts notes into emergent topical categories and prioritizes them through voting.
This method is commonly called affinity diagramming or KJ Analysis.

Topical notes are short observations that the team sorts into groups. Colors represent different participants.

Advantages of topical notes:

This method is easy to explain and to learn.
Some people prefer to take notes on paper rather than typing.
Observers don’t need  laptops.
Paper is less distracting than internet-connected devices can be, so people stay focused on observing.
There’s no typing noise to distract participants or observers.
Using small notes encourages people to change paper when they change topics, making it easy to sort the notes later.
Hand writing allows people to sketch and diagram if they want to.
The group can sort notes after the study, shortening the work of analyzing the data.
Color-coding makes it easy to see at a glance how many users had which kinds of issues.
Sorting notes into topics immediately shows which issues generated the most notes.
It’s easy to get team agreement quickly on a top-findings list right after the test. If a report is not necessary, then sometimes that’s all you need to move forward.

Disadvantages of topical notes:

Hand-written notes can be difficult to read.
Although you gain in categorization, you can lose in sequence.
The process may appear to be less scientific to some stakeholders.
Important data must be digitized or transcribed later; it’s harder to document the process or the resulting artifacts.
A lot of paper is wasted.

Tips: Assign one of the observers the duty of collecting all the notes and distributing the next color to all the notetakers after every session. Sticky notes are good for grouping into topical categories on walls or windows. Small index cards or nonsticky paper notes are easy to sort on big tables.




          SF              A4          Navigation
 
 
She never touched the hamburger menu.
 
 
 
 








        SF                   C2              Search
 
He scrolls the search field to check for typos.
 
 
Me: The search box is too short.
 
 




Two examples of topical notes: The top contains the notetaker’s initials, the scenario letter and task number, and maybe a topic. (Topics can be added at any time, or notes can just be sorted into categories later.) Notes typically contain observations, questions, quotes, or remarks and ideas from the observer. Colors represent particular participants. (The reason we use color-coding to differentiate participants instead of notetakers is that it’s more important to see at a glance how many users encountered a certain usability problem than how many observers spotted that issue.)
Collecting Insights: Whiteboard Top-Findings List
Regardless of your chosen notetaking method (logging or topical), after each session schedule a short team-debriefing period to gather key findings quickly. Ask observers to describe the most important things they noticed during the preceding session. Capture those key observations in view somewhere, on a whiteboard or on a series of giant sticky notes, for example (but make sure that participants cannot see them).
The facilitator can field questions, provide insights and context, and help observers think constructively and objectively about the findings. Keep a tally next to items that are found again in additional sessions.
At the end of the research study (or anytime you run out of room), photograph the preliminary top findings for the record, because sticky notes tend to fall off the wall overnight, and cleaning staff may erase your whiteboard.
After the notes have been analyzed, the details and variations of the top findings, along with issues that weren’t raised in the debriefings, can be added to help complete the picture.
Advantages of the running top-issues list:

Important findings stay visible and top of mind during the research study.
Stakeholders can get up to speed quickly just by walking into the room and reading the board.
Preliminary top findings can be transcribed, sorted, and conveyed to the larger group almost immediately after the study ends. For this reason, the whiteboard method is particularly suited for fast-paced Agile environments.
Everybody collaborates on making the list, so the findings aren’t controversial or mysterious. At the end of the study, everyone knows what happened and what’s most important to focus on in the next design iteration.

Disadvantages of the top-issues list:

Writing on a board is erasable and can be difficult to read.
The top findings must be transcribed.
It’s common to run out of room and have to photograph the board, erase, and continue.
Issues on whiteboards can’t be searched or sorted, so finding an existing issue to add a tally number to it can take more time.

Tips: The UX facilitator should write on the board, because top issues need usability-oriented phrasing, and the process requires a leader to keep the team focused and to answer questions. Ask 1 of the observers to transcribe the board into a spreadsheet or mindmap throughout each day, in order to make the information digitally usable as soon as possible. It’s tempting to make 1 top-findings list per participant, but that makes it more difficult to tally issues that recur over the study. Rely on the notes to separate findings by participant, instead.




How to Decide Which Observer Notetaking Method to Use for Your Study




Situations

Observer Notetaking Methods


Observers are in multiple locations or remote from the facilitator
Chronological logs


Sequence is important to data analysis
Chronological logs


Observations need to be detailed, technical, or made by subject-matter experts
Chronological logs


You’re measuring time on task
Chronological logs with timestamps or a dedicated timekeeper


It won’t be possible to hold debriefs between sessions
Chronological logs or topical notes


The group has a lot of mutual trust, cooperates well, and is doing the research as a team
Chronological logs or topical notes


You’re training facilitators
Chronological logs or topical notes


There are few observers
Chronological logs or topical notes


There are many observers
Topical notes and whiteboard


Observers are easily distracted
Topical notes and whiteboard


Some people don’t have laptops
Topical notes and whiteboard


Sketching is important
Topical notes and whiteboard


There are different observers each session
Written notes and whiteboard


The research outcome may be contentious, observers are skeptical, or a lot of discussion might be needed in order to come to consensus
Allow extra debriefing time and rely on whiteboard process to raise and answer questions


Observers resist writing or sharing notes or insist on using long-form notepads
Rely on facilitator notes and gather stakeholder observations on whiteboard


Time is the most important factor
Use any note format but run with top findings on whiteboard


The research is aimed at finding big issues without much detail
Use any note format but run with top findings on whiteboard



Choose the most-useful method for engaging observers in a given project, considering the people you have and the data you need. It’s common for UX researchers to log data while observers write notes by hand, but it’s best when all observers stick to one method in order to simplify data analysis. One person with many two-sided pages of notebook longhand can slow you down considerably when everyone else is producing sortable notes. The whiteboard method should be used whenever debriefing occurs, regardless of the primary data-capturing method. 
Conclusion
Taking notes is a great way to bring teams together to solve usability problems. As stakeholders learn more about users and UX methods by participating, UX researchers can spend more time generating actionable data and less time explaining. You can’t just throw people into a research situation unprepared, however. Here are some guidelines for observers with notetaking instructions that you can modify for your situation."
41,2017-04-09,"Qualitative usability studies are dependent on a few key pieces: a design to test, a participant to test it, and (often) a moderator to run the session. The other essential element: the tasks.
A task needs to accurately and adequately reflect the researcher’s goal, as well as provide clear instructions about what participants need to do. Good tasks are essential to having a usability study that results in accurate and actionable findings.
Writing tasks for a usability test is not easy. As any experienced usability researcher can tell you, how the task is written directly impacts the success of the study. If you give study participants bad instructions, you can bias them and completely change the outcome of the study. At best, you won’t learn that much, and the study won’t reflect real-world use very well. At worst, your “findings” will be directly misleading and cause you to make the product worse, rather than better.
After you’ve written your tasks, take another pass through them, looking for common mistakes that can impact the value or depth of your findings, or the well-being of your participant.
1. Telling Users Where to Go
Do words from the interface appear in your task? If so, you’re priming your participants and testing their reading comprehension and ability to find matching words, rather than your labels and navigation. Rewrite the task to remove any words that appear in your interface, so you give yourself a fair chance to see if users can find their way around the site.
Task goal: Use the location finder tool (labeled Find a Branch)
Leading user task: Find a branch near you and see when it is open tomorrow.
Improvement: When is the bank location that’s most convenient to you open tomorrow?
2. Telling Users What to Do
As part of a task, users may need to go through several steps, such as registering for the site, installing software, or downloading a document. Take the opportunity to gather more about the process by not warning study participants about what they will need to do. When your task includes prompts to register, install, or download, you may miss out on the valuable feedback that users might offer when encountering that step in the process. For example, participants may be surprised or annoyed by an additional or unexpected step.
Task goal: Find the price for consulting services
Overly-structured task: Locate information about consulting services, provide details about yourself and your company, and set up a time to talk to a consultant about pricing.
Improvement: Find out how much a consulting project costs.
3. Creating Out-of-Date Tasks
Often, we write tasks only a few days before the usability study is scheduled. Even so, consider the timeliness of your tasks. If your task includes a future event, make sure that event is going to still be in the future during testing. If the task is to find a flight leaving February 20, don’t run that test on February 22. A task about the latest news on a site should be updated the day before or the day of testing to include current content. Be cautious if tasks include information that is typically relevant or updated only in specific months or seasons. Users may think the site has out-of-date information or that the tasks aren’t realistic.
Task goal: Find sports teams’ scores (Assume testing is taking place in February. In the United States, baseball is played from April to October. Hockey is played October through April.)
Outdated task: Find out how the Cubs did in their last baseball game.
Improvement: Find out how the Blackhawks did in their last hockey game.
4. Making Tasks Too Simple
If you want to know if people can effectively use charts, graphs, or information on the site, don’t just test if they can navigate to it. Create tasks that make study participants work a bit for the information. Your goal isn’t to make tasks unnecessarily complex, but to give users a realistic task that requires processing, rather than just locating information.
Task goal: Find and use player statistics (Points Per Game is the first item listed in player statistics, sorted from highest to lowest)
Too-easy task: Who scored the most points, averaged across games, in the league?
Improvement: Who scored more points, averaged across games, during the season: Russell Westbrook or LeBron James?
5. Creating an Elaborate Scenario
Some tasks may benefit from a small scenario to give the activity some context. A short description may help study participants understand the reason for such a task or clarify the exact information you would like them to find. You may suggest a genre of music that the participant should investigate, a reason for looking for particular information, or provide a name and address for a purchase. For instance, you may include a detail such as when a gift recipient’s birthday is, to see if users can find a shipping option that will ensure a gift is delivered on time.
Scenarios can be helpful, but be cautious when using them. They are not always necessary. They may add complexity to a task that could be straightforward. They can increase the number of details users must read through and remember. Sometimes such scenarios are used to justify an unusual or abnormal activity. If it takes a long story to explain why a user would want to do an activity, it’s likely not a realistic task to test.
Task goal: Find and use nutritional guidance information
Unnecessary backstory: You are helping babysit your friend’s 3-year-old boy for a week and want to know more about healthy diets for kids. Find out how much grain should be in his diet.
Improvement: Find out how much grain should be in a 3-year-old’s diet.
6. Writing an Ad, not a Task
Don’t let marketing language or internal lingo sneak into tasks. Make sure your tasks don’t include marketing phrases like “exciting new feature,” business phrases like “thinking outside the box,” or mysterious corporate acronyms. Use user-centric language, not maker-centric language. For specialized audiences, it may make sense to use technical terms or audience-specific language, but that is the exception, rather than the rule.
Task goal: Use the new social sharing feature
Promotional wording: Check out the exciting new feature that lets you quickly and easily share articles with colleagues.
Improvement: Send an article to a colleague.
7. Risking an Emotional Reaction
While writing a task that revolves around someone’s mother may seem harmless, you never know the specific circumstances of your study participants. Mentioning a specific relationship in a task may add unnecessary emotion to the user test. What if the participant has a difficult relationship with the person you’re referencing, or that person has passed away? Don’t risk upsetting a user and derailing a task or even an entire session. Part of the responsibility of running a usability test is to ensure the well-being of your participants. Stick to harmless and vague relationships instead – friend, colleague, a friend’s child.
Task Goal: See how participants shop for gifts.
Potentially upsetting task: Mother’s Day is coming up. Find a bouquet to send your mother.
Improvement: Send your friend flowers to celebrate her new job.
8. Trying to Be Funny
Don’t joke, use famous names in tasks, or otherwise try to lighten the mood. Doing so can backfire and make some participants feel awkward or, even worse, as though you are making fun of them. Even using gender-neutral names, such as telling the user to register as Kelly or Jesse, can be a distraction from the task.
Task Goal: Identify problems in the gift subscription checkout flow.
Distracting joke in task: Send a subscription to your friend for her birthday. Her name is Ima Customer and she lives at 826 Main Street in Tempe, Arizona, 85280.
Improvement: Send a subscription to your friend for her birthday. Her name is Jen Smith and she lives at 826 Main Street in Tempe, Arizona, 85280.
9. Offending the Participant
Avoid potentially offensive details in tasks. Societal issues, politics, health, religion, age, and money all have the possibility of offending a participant.
Task goal: Find and use information about exercise and calories.
Potentially offensive task: You need to lose a few pounds. See what types of exercise will help you lose the extra weight.
Improvement: See what types of exercise burn the most calories.
10. Asking Rather than Telling
While you want to be polite to your participants, don’t overdo it. Don’t ask participants “how would you” complete a task — unless you want them to talk you through what they theoretically would do on a site, rather than doing it. The point of usability testing is to see what users do, not to hear what they would do.
Task goal: Find the symptoms of the flu
Instructing the user to talk instead of perform: How would you find the symptoms of the flu?
Improvement: Find out what the symptoms of the flu are.
Tip: Start with the End Goal
Sometimes, with all the things you need to avoid in a task, it can feel like you’re writing a riddle that the participant needs to solve. It can be hard to avoid navigational labels, steps, stories, or marketing language in your tasks. This is why task writing is more of an art than a science.
If you find yourself struggling to write a task, consider the user’s end goal rather than the task’s end goal. Rather than focusing on the section or the feature you want to test, consider why people would use that section or feature. What would they ultimately try to accomplish?
To test your checkout process, give the user a task to buy something. To review your newsletter subscription process, ask the user to sign up to receive information via email. To see if a user can understand content, write a task with a question about the information contained in the content. Starting with the users’ end goal helps streamline task writing, and reviewing these common mistakes can finetune the tasks.
Learn more about task-writing in our Usability Testing course."
42,2017-03-19,"In 1967, the Russian psychologist Alfred Lukyanovich Yarbus watched people as they looked at the same oil painting with different goals in mind. He noticed that the eye-gaze movements depended on the activity being performed and he concluded that people attended to those areas of the scene that were more likely to contain relevant information for the current task.
Our recent eyetracking studies build on Yarbus’s research, and reinforce the idea that tasks greatly impact user behavior on the web and therefore drastically change the outcome of eyetracking gazeplots and heatmaps. Better get the tasks right, or any eyetracking you do will be more misleading than helpful for driving your design.
Methodology
In usability eyetracking studies, we uncover the most helpful and realistic findings for improving a website when we allow people to click, search, and type as much as they want, since this is how they normally use the web. In other words, the users have to try to actually do something realistic with the website or application, as opposed to being told to “just take a look at this page.” Doing something, of course, almost always requires users to move between different screens where they don’t know in advance which pages will be useful to them.
However, because this study’s goal was to determine how users examine the same webpage as they attempt different tasks on that page, we adjusted our recommended eyetracking methodology: We gave users a task and, instead of allowing them to navigate by themselves to the page of interest, we took them to that page, allowed them to complete the task on the page, then closed the page for them. We examined a few different pages, including 2 pages that we will discuss in this article: a page displaying women’s dresses on Bebe.com, and a page displaying vacation packages on jetBlue.com.

One page in our study included images of dresses on www.bebe.com.

 

Another page in our study included a list of vacation packages offered on www.jetBlue.com.

For each page, we gave users several different tasks. (The Bebe tasks will be described later.) These were the tasks given on the jetBlue page:

Take a look at the page. (Test facilitator closes the page after 8 seconds.)
To which places do these getaways go?
Which place looks the nicest to you?
Which is the least expensive?
Which is the highest rated?
Imagine you were going to get a quiz about this page. Study the page enough that you feel you could pass the quiz.

To be clear: the approach we used in this study is not how we recommend that you test your website. The goal of our research was to capture the effect of the various test tasks, not to improve the websites. If Bebe or jetBlue had hired us for a consulting job, we would have run very different studies.
Gaze Plots
In eyetracking studies, researchers record participants’ eye movements and track the order and duration of their gazes (or fixations). Then they aggregate the number of fixations, fixation durations, or sequence of fixations in graphical representations such as gaze plots and heatmaps. In this article we illustrate our findings using gaze plots.
A gaze plot is an overlay superimposed on a static screenshot that demonstrates where one or more users looked on that page. The elements in a gaze plot include the following:

Dots = fixations: In a gaze plot, each dot represents one gaze (or fixation). In other words, a dot says that the user looked at that spot on the screen (or closely around it). Most eyetracking usability studies operate under the eye–mind hypothesis — namely, if a user looked at an item, that item was attended and processed cognitively. That said, an attended item may still not be properly interpreted or remembered. It’s important to understand that the dots show the only parts of the page that the user saw sharply enough to read text or understand the details in images.
Larger dots = longer fixations: The size of the dot is (roughly) proportional with the duration of the corresponding fixation. Thus, larger dots represent a longer time spent looking at that location on the page. Long fixations signal that the user spent more time processing the corresponding item, because 1) she is interested in it, or 2) she is confused and has a hard time understanding what it means.
Numbers = order: The numbers within the dots represent the order in which the user looked at the items on the page.
Lines = saccades between fixations: Each dot (fixation) is connected to the fixation preceding it and to the one following it. Thus, the lines between dots make it easier to follow the eye movements (called saccades) and the sequence of the fixations. (The numbers provide enough information for determining the order, but, without lines, it becomes too challenging to hunt for the next fixation.) Because the eyes move extremely fast between fixations, the person is effectively blind during an eye movement and doesn’t register the visual landscape that the gaze speeds across.


This gaze plot from a study participant doing all the given tasks on jetBlue’s vacation-package page includes more than 440 fixations.

Interpreting User’s Fixations on a Page
The gaze plot of the jetblue.com page above includes more than 440 fixations on various page elements, ranging from vacation listings to UI components such as logo and navigation. Where did the user look and why?
We could come up with several reasonable interpretations, such as:

The participant looked in the area in the upper left a few times to see the logo and confirm the site she was on.
She looked at the navigation within the first few moments on the page to get a sense of what was offered on the site.
Many, long fixations on each package name, image, rating, price, and short description possibly indicate that she is interested in all of them and trying to decide which to pick.
After she looked at everything on the page, she checked the footer to see what else was offered on the site.

Each of these possible interpretations are logical. But, because we know nothing about what the user was trying to achieve, these assumptions are unfounded. Simply zooming in on the top of the gaze plot shows that the fixation numbers are high and did not happen within the user’s first few moments on the site, demonstrating that the second interpretation is wrong. But that’s about all we can conclude from looking at the static image alone.

Fixations on the menu and logo happened relatively late in the task, after more than 200 fixations on other places on the page.

Watching the user’s full eyetracking session, not just studying gaze plots, is the best way to uncover meaningful insights. Watching slow-motion replays enabled us to create more telling gaze plots that represent time segments on the page (as opposed to the entire visit on the page) related to the task the user was doing at the time. We’ll examine those in this article.
Task and User Motivation Affect Eye Movements
Task 1: Free Examination (“Take a Look at the Page”)
The first task was simply to look at the page. The task did not specify an amount of time, but the facilitator closed the page after 8 seconds. This task tested the concept of “free examination.” Free examination is rare in the real world — people usually visit websites for a reason. In usability testing, free-examination tasks should be reserved for special cases when you want to understand how users behave when they’re simply interested in your company or brand. (Even then, people are usually trying to satisfy an information need — for example, to see what’s new, or what the company does.) The main reasons free-examination tasks are not usually recommended in usability testing are: 1) they are unrealistic; and 2) they cause users to study the page more carefully than they normally would, and, as a result, may bias their behavior in subsequent tasks.
User behavior. In these first few seconds on the page, the user looked at the largest piece of text in the content area (the price of the first vacation), at the corresponding thumbnail, and then she moved her gaze to the upper left corner of the page. That spot is traditionally reserved for the company logo, but on jetBlue’s page the logo was pushed to the right. Centered logos are unexpected and harder to locate.
After not finding the logo, the user returned to the content area and began to scan the location names.

The gaze plot for the task “Take a look at the page.” The image is cropped to exclude areas where the user did not fixate.

Task 2: Scan Section Titles (“To Which Places Do These Getaways Go?”)
The second task was to find something more specific: the locations of the vacations. For this task and the ones that follow, we will describe the information on the page that was necessary to complete the task, the user’s behavior, and, if applicable, the design elements that supported the task or hindered it.
Information needed to complete the task:

the names of all the destinations
the destination thumbnail images, which may have been a secondary source of information

User behavior. Once the user learned the look of the destination names and how much vertical space existed between them, she quickly adapted her scanning to extract the information needed for the task (i.e., destination names) as efficiently as possible, without wasting extra fixations. Although occasionally the user glanced at secondary elements, such as the thumbnail and the description, the majority of her gazes were directed at the destination names. Our participant was able to complete the task in 38 fixations.
The gaze plot that shows her eye movements exhibits the layer cake pattern of scanning, during which users scan to headings and subheadings but don’t read the information below these, usually because the headings contain enough information to either answer their question, or indicate that the text under the heading is not relevant for answering it. The physical scan path resembles the horizontal sheets found in a layer cake. (This and other scanning patterns are described in our How People Read on the Web: The Eyetracking Evidence report.) The scanning pattern in the gaze plot is an instance of efficient scanning: it is highly focused on the current task and ruthlessly ignores content unrelated to the user’s goal.

The gaze plot for the task “To which places do these getaways go?”; The image is cropped to exclude areas where the user did not fixate.

Design elements that support the task:

consistent presentation of the different vacation packages
large, bold text for location names, juxtaposed against the smaller description text
vertical white space between vacation packages
light, thin, subtle grey lines separating vacation packages
consistent vertical spacing between different package names

All these design elements allowed the user to quickly figure out how to find the most important piece of information needed to complete the task: the destination names.
Task 3: Scan the Images (“Which Place Looks the Nicest to You?”)
The third task involved gathering impressions about each destination.
Information needed to complete the task:

the thumbnail photographs
the location name for the nicest place

User behavior. Again, the user quickly limited her scanning only to information that was needed to complete the task. Each thumbnail got 1–6 fixations, and some destination names (presumably of places that looked nice to her) also got fixations. When the user didn’t care for the photo — as with those for Grand Cayman, Charleston, and Fort Lauderdale — she didn’t bother reading the location name. And she scanned nothing else on the page. She was able to complete the task in 37 fixations. (However, had the images been clearer, she likely would have been able to succeed with even fewer fixations.)

The gaze plot for the task “Which place looks the nicest to you?”: The image is cropped to exclude areas where the user did not fixate.

Design elements that hindered the task:

photographs that were too small given the amount of detail in them
inconsistent thumbnail image types, with different subjects, camera angles, times of day, and level of detail

Task 4: Scan the Prices (“Which Is the Least Expensive?”)
The second vacation package, Saint Maarten, cost $349 and was the least expensive.
Information needed to see to complete the task:

price for each location
location name for the cheapest destination

User behavior. Our study participant confidently scanned all prices on the page, then she scrolled up and gazed at the name of the second destination in the list to find her answer. That is highly efficient scanning. She completed the task in 28 fixations.

The gaze plot for the task “Which is the least expensive?”; The image is cropped to exclude areas where the user did not fixate.

Design elements that supported the task:

price size larger than all the text items in the content area
whitespace surrounding prices
short numbers
bold font for prices
same price position for each location in the list

All these elements made the prices easy to locate.
Task 5: Scan Details (“Which Is the Highest Rated?)
Information needed to complete the task:

star ratings


location name for the highest-rated destination

User behavior. Our participant optimized her scanning procedure along the way, as she learned about the structure of the page: she started by looking at all the stars in the ratings appearing early in the list, but, as the scan progressed, she understood that all locations got at least 4 stars, so she fixated on only the stars to the right. Finally, she determined that Charleston, the 7th entry, and Aruba, the last entry, each held a 5-star rating. She completed the task in 36 fixations.

The gaze plot for the task “Which is the highest rated?”; The image was cropped to exclude areas where the user did not fixate.

Design elements that supported the task:

conventional pattern for ratings (stars)
star icons easily distinguishable from the other items in the destination description
consistent positioning of items across destinations

All these made the stars easy to locate.
Design elements that hindered the task:

small stars
little visual difference between dark blue and light blue stars, or between half-full stars and full stars

These issues required the user to spend more time on each star rating to parse the details.
Task 6: Motivated Reading (“Study the Page Enough That You Feel You Could Pass a Quiz About This Page”)
This task required the user to pay attention to all the information presented on the page and attempt to memorize it so that it could be recalled later.
Information needed to complete the task:

everything on the page

User behavior. Our participant looked at navigational elements and at the content, and in fact she fixated many of these elements multiple times, presumably trying to memorize them. She used 228 fixations to complete the task.
As we saw in the previous tasks, for the sake of efficiency, users scan web pages, focusing only on the relevant bits of content. But when their motivation or engagement are high (or when all the content on the page is relevant, like in this task), they might read almost everything. This commitment can be simulated in a lab setting by telling people that they will be quizzed about the content of the page.

The gaze plot for the task: “Imagine you were going to get a quiz about this page. Study the page enough that you feel you could pass the quiz”; The image is cropped to exclude areas where the user did not fixate.

Test Your Eyetracking-Analysis Assumptions
Our analysis of the gaze plots for the jetBlue tasks probably gave you some insights into how users adjust eye movements to respond to the task. Let’s see if you can apply these to a new set of data.
Below is a list of 3 tasks we asked people to try on www.bebe.com, and 3 gaze plots (labeled A through C) of segments from one user’s visit to the same page.
Your job: Match the task to the corresponding gaze plot.

Which dress is the prettiest?
Estimate the average age of the models.
What is the price range for the dresses?

 
A.

 
 
B.

 
 
C.

 
The answers appear at the end of this article.
What Did We Learn from This Study?
The jetBlue examples and the Bebe quiz should have convinced you that Yarbus’s findings hold for web reading: Users fixate on those elements on the page that are relevant for their task. The same page will be processed differently by the same user when her goal changes.
And, more importantly, this behavior is another embodiment of the principle of least effort and minimum interaction cost: in performing an activity, people will try to be as efficient as possible and will avoid spending any unnecessary effort. Like our study participant, they will always try to find an optimum algorithm for getting the information that they need without working unnecessarily hard.
Design for Efficient Scanning
The 6 jetBlue tasks discussed above encapsulate 3 general scanning behaviors:

getting the lay of the land (task 1)
comparing items (tasks 2–5)
motivated reading (task 6)

The first two are most common on the web, and indeed all our Bebe tasks are also comparison tasks. Designers should support users in their attempt to minimize the amount of effort involved in reading and extracting relevant information from a webpage. As we saw in the previous gaze plots, predictable, unambiguous patterns help users get to an optimal scanning algorithm fast and allow them to easily focus on the essential, while skipping those content elements that are unnecessary for their goals. Remember that sometimes too much variation or detail (like in the case of star ratings and photographs) can slow down the eye, make the task harder, and ultimately increase user frustration.
Here are some design recommendations for supporting efficient scanning of list pages:

Be consistent with the position and layout of items in the list.
Use short, recognizable words and digits when possible.
Use large, bold text and white space surrounding it to attract the eye and showcase the most important information.
Consider using comparison tables to support this behavior.

If displaying photos:

Present consistent types of photos.
Present a level of detail that is recognizable given the size of the photos.

If presenting star ratings:

Consider showing a number with the star ratings.
Make the selected number of stars very simple to glean in one fixation. For example, use a saturated color filling with a thick outline for the selected stars, and white filling and a medium outline for the stars that are not selected. Use high color contrast between selected and not selected stars (and remember color blindness when choosing colors).

Tasks in Usability Studies Focus Test Participants on Particular Site Areas
One of the greatest drawbacks of giving people tasks to perform in a usability test is that tasks affect behavior. Even the act of asking people to do an activity usually clues them that the activity is possible and that the answer exists somewhere on the site. If the users had not been given a task, they may have never discovered certain site features related to that task.
If tasks change how people look at and interact with a design, then why give a task at all in a usability test? Although giving tasks has some drawbacks, it also has many benefits:

Most important, if users don’t have a reason to use a website or application, then they would just flail around to no purpose, which is a completely unrealistic way of approaching most designs. It’s better if we present people with a purpose than to waste our time observing a style of use that doesn’t happen in real life and has no business value.
If the tasks reflect our audience’s main user needs and goals, then it makes sense to ask study participants to perform those tasks in the lab, so we can optimize the design for those high-priority activities.
Tasks allow teams to focus on sections of the design that are new or important for the business. Task-based studies enable us to fix issues before a design goes live, and learn from the research so we can become better at predicting which new designs will work for users and why.

Clearly formulated tasks enable us to figure out what design elements work for which user goals. Still, it’s helpful to employ a variety of research methods in addition to task-based usability tests. Particularly useful are open-ended observation of users in their natural environment (e.g., field studies, contextual inquiry) where they have their own reasons to use the computer or phone. But these research methods are not practical for all projects, budgets, development schedules, or design stages. If we did a field study every time we wanted to see users interact with a design, we could spend days watching people engage in site activities that are irrelevant for our current design project. And this method doesn’t work for sketches and prototypes that need to be tested early in the design lifecycle. Also, when researchers observe users in the field, they may not have a clear understanding of their goals and motivations.
When designing a usability study or analyzing user data, remember that tasks may change user behavior: they may tell users that a piece of information or a functionality are available in the design, leading them work harder than if they had not suspected that the task was doable.
(Learn much more about how to design good test tasks in our full-day Usability Testing course.)
Eyetracking Requires Realistic Tasks
While good test tasks are important for traditional usability testing, the tasks are essential for the validity of an eyetracking study. A heatmap of where users looked while performing a nonrealistic task will be misleading. Any design decision made on the basis of such data is more likely to hurt your business than to improve your metrics. Write good tasks for the most useful eyetracking studies.
 
Quiz Answer
These are the answers to the quiz in this article:

Which dress is the prettiest? Image C.

After a few initial fixations needed to locate the dresses on the page, the user looked directly at the dresses. Consistent size and layout of the dress images made it easy for the user to find the dresses. Unlike the jetBlue photos, the dress images are consistent and each shows a model standing in front of a plain light background.

Estimate the average age of the models. Image B.

Most of the fixations were on the faces of the models, which would be most telling of their age. But there were also a few fixations on the body, which can also be expressive of age. Again, the consistent pictures used in the design helped the user quickly locate the faces.

What is the price range for the dresses? Image A

The first 2 fixations were used to finding the way to the prices, then the user scanned only the text below the images, where the prices appear. The consistent positioning of the prices made the task easy. But, unlike the large jetBlue prices, the Bebe prices were smaller and positioned too close to the dress name, forcing the user to spend longer time or more fixations to register the information.
"
43,2017-02-26,"User research offers valuable opportunities for VIPs, product owners, managers, customer support representatives, marketers, designers, developers, and other team members to learn how users interact with products and services firsthand. Without users, you can’t properly be said to do “user” experience work: as we’ve said before UX – U = X (with the second “X” meaning, “don’t do it.”). With users, almost everything in a UX project improves, but only if the remaining stakeholders (besides the researchers themselves) know what was done.
Having an audience for your research is a wonderful opportunity for UX practitioners to gain allies and get buy-in for problems that need to be solved. Stakeholder participation in both research and design is best, because you can show the value of UX activities and get the whole team focused on the people you are trying to serve. Collaboration doesn’t reduce the need for UX expertise and guidance. Instead, involving stakeholders helps everyone appreciate the user-experience effort.
Collaborating on the User Experience Benefits Everyone
UX-research participation can remove obstacles to doing future research. It can build organizational support for UX activities by demonstrating their value. It can also show how user testing in the early stages of design can prevent integration and maintenance problems more efficiently. And when everyone understands research activities and results through direct participation, less time is needed to explain methods and results.
Participation in design and research projects can motivate stakeholders to fix the user interface or to modify the requirements to align better with customer and user needs. It will also encourage empathy for users, create group ownership for usability outcomes, and solve problems faster. You don’t need to argue when you can test. (In fact this one advantage can sometimes provide full cost-justification for a round of user testing: the cost of recruiting 5 users and having a UX researcher spend a day or two testing the main design options can be less than the cost of a big team spending endless meetings arguing over what to do.)
Teams create better design patterns and make better decisions when team members are immersed in user behavior and typical usability difficulties. User-interface designs improve when everyone understands how to prevent problems in the future.
Motivating Stakeholders to Participate in Research and Design
User research reduces the likelihood of building something that doesn’t meet user needs, but only when everyone knows what those are. Here are some ways to increase motivation to participate in user research:

Evangelize UX research. Show how various UX-research methods can be used to reduce risk and align products with user expectations. Dispel the notion that you’re the graphic-design syrup on the top of the product stack or a time-consuming form of quality assurance. Point out that you’ll save work (and rework) for developers by helping as early as possible with research, design, and validation. Make sure everyone knows how deep user experience design needs to be and what you can accomplish. Explain UX concepts, terms, and capabilities by having UX practitioners discuss their skills, typical methods, and outcomes.
Make time and space for collaboration. Schedule regular UX office hours and design clinics. Keep the chat channel open with your team so you can listen and share information. Invite everyone (regardless of their role) to use you as a resource for user-research insights and design solutions. Create a physical space for team members to interact with UX R&D staff and information.
Educate everyone about the benefits of participation in UX research. Teach upper management about the benefits of stakeholder participation. Find a champion who can encourage other people with her own participation stories.
Align your research goals with your stakeholders’ goals. Run strategy meetings with stakeholders to understand their goals and establish the best ways to measure success. Then set a design vision and UX strategy before development begins, so everyone understands how to ensure good usability for users. Later on, at the end of the project, give credit for successes to stakeholders. They are more likely to help projects that make them look good.
Demonstrate utility with small pilot projects. Start with small, easy, yet vital opportunities, such as soliciting advice and concerns while sharing user insights and data.
Make participation easy. Schedule key UX activities (such as customer-site visits, usability testing, and participatory design sessions) when stakeholders are available, and plan for that time in the development cycle. Personally invite each person and emphasize their importance to obtaining good outcomes.
Create accessible user-testing deliverables that reflect your audience’s interests and technical understanding. Share UX research, process, and prototypes widely, in person if you can. Brown-bag lunches are a good way to build interest. Tell compelling stories, communicate risks, and show visual information everyone can understand quickly. Share data and insights as soon as you can with your immediate team, using a lightweight system, such as bullet lists and screenshots in a shared document. Annotate higher-fidelity prototypes and designs rather than writing a separate spec. Show interactions rather than talking about them. Summarize key insights and implications for larger audiences. Track UX issues and metrics in a visible location.
Make UX deliverables useful for other processes within the organization. Create design patterns for reuse and make living style guides for issues that recur. Make usability test cases for QA that adhere to the main design principles. Use storymapping and project-management tools to integrate user tasks and UX tasks into developer tools, stories, and tasks. Generate needed user stories for the product backlog.
Increase the visibility for UX-research and design artifacts, such as personas that change as insights deepen, user-journey maps, hi-fi prototypes, and user-task flows. Post these artifacts on walls in public locations. For remote team members, build a project room in a shared environment to display digital UX artifacts and data.

Getting Started with UX Collaboration
Involving stakeholders in gathering data is a good way to begin. It’s best to make the whole research effort into a team activity, however. Ideally everyone should be involved in planning the study, writing scenarios, creating participant profiles and screening questions, and in reviewing potential recruits for participation.
Start with a short study, such as two days of user sessions. Invite stakeholders to attend any of the sessions they can make time for. Encourage them to attend at least two sessions, so they will be less likely to take a single session at face value. Explain your research plan and method, to set expectations correctly. To prepare stakeholders for participating in user-research sessions, give them observer guidelines and explain how to take the most effective notes. After the study, circulate the top findings widely and publicly thank your collaborators for their valuable input.
Besides user testing, other UX group activities such as design-thinking exercises can create team ownership of the design vision and concept, through generating designs and critiquing them together. Schedule regular design reviews and group walkthroughs. Point out quality nuances that make a big difference, such as microinteractions, streamlined forms, and helpful messages. Show competitor products that do a better job at task flow, fit, and finish.
Conclusion
User experience outcomes improve when UX is a team activity. User research should be an essential part of design and testing processes in order to refine designs before development and to help everyone understand what needs more work. Collaborating in user research provides many points of view and motivates everyone to make products and services more usable.
Want to learn more about creating a great user experience through collaboration? Join us at the Nielsen Norman Group’s full-day Lean UX and Agile course or Engaging Stakeholders to Build Buy-In for more in-depth recommendations.
"
44,2017-01-29,"It’s a familiar frustration to usability-test moderators: You watch a user struggle through a suboptimal UI, encountering many errors and obstacles. Then, when you ask the user to comment on her experience, all she can talk about is the site’s great color scheme.


During usability testing, one user encountered many issues while shopping on the FitBit site, ranging from minor annoyances in the interaction design to serious flaws in the navigation. She was able to complete her task, but with difficulty. However, in a posttask questionnaire, she rated the site very highly in ease of use. “It’s the colors they used,” she said. “Looks like the ocean, it’s calm. Very good photographs.” The positive emotional response caused by the aesthetic appeal of the site helped mask its usability issues.


Instances like this are often the result of the aesthetic-usability effect.
Definition: The aesthetic-usability effect refers to users’ tendency to perceive attractive products as more usable. People tend to believe that things that look better will work better — even if they aren’t actually more effective or efficient.
In other words, users have a positive emotional response to your visual design, and that makes them more tolerant of minor usability issues on your site. In most cases, this is a positive thing from your perspective. This effect is a major reason why a good user experience can’t just be a functional UI — designing an interface that’s attractive as well as functional is worth the resources.


Apple’s success is an excellent example of the competitive advantage of paying attention to aesthetics.


The aesthetic-usability effect was first studied in the field of human–computer interaction in 1995. Researchers Masaaki Kurosu and Kaori Kashimura from the Hitachi Design Center tested 26 variations of an ATM UI, asking the 252 study participants to rate each design on ease of use, as well as aesthetic appeal.
They found a stronger correlation between the participants’ ratings of aesthetic appeal and perceived ease of use than the correlation between their ratings of aesthetic appeal and actual ease of use. Kurosu and Kashimura concluded that users are strongly influenced by the aesthetics of any given interface, even when they try to evaluate the underlying functionality of the system. In his 2004 book Emotional Design, our colleague Don Norman explores this concept in depth as it applies to everyday objects.
Bear in mind that the aesthetic-usability effect has its limits. A pretty design can make users more forgiving of minor usability problems, but not of larger ones. (As the first law of e-commerce states, if the user can’t find the product, the user can’t buy the product. Even great-looking sites will have no revenue if they suffer from poor findability.) Form and function should work together. When interfaces suffer from severe usability issues, or when usability is sacrificed for aesthetics, users tend to lose patience. On the web, people are very quick to leave.

Arcadis, a consultancy, uses large background photos on many pages throughout the site. During a usability test, a participant responded positively to the site’s aesthetics: “First thing, popping into this webpage, I see this beautiful, colorful image.” After struggling to complete several tasks, however, the participant revised his opinion of the same design detail: “I feel like the whole screen being taken by this is pretty awesome once… And probably annoying the second time.”

Interpreting Positive Comments About Visuals During User Research
Understanding the aesthetic-usability effect is critical for decision making in resource allocation during the product-planning phase, but it also has implications for interface evaluation. User researchers must know how to spot this effect during usability testing, and how to interpret it.
When the aesthetic-usability effect happens in real life for your users, it’s a good thing. It means your team’s investment in creating a beautiful UI is paying off and connecting with your target audience. However, when the aesthetic-usability effect happens during user research, it can prevent you from discovering usability issues.
You can identify instances of the aesthetic-usability effect during user research by paying close attention to what users do and how it relates to what they say.
So let’s imagine we’re facilitating an in-person qualitative usability-test session. We observe the participant struggling through a few tasks on a site, but his final feedback is a vague comment on the attractiveness of the interface.
Whenever we hear this kind of feedback that seems out of place, we need to consider three possibilities.

The participant might feel pressure to comment on something… anything. Users (especially novice ones) often find it easier to give feedback on the visual design of sites.
The participant might feel pressure to say nice things about the site. These empty compliments tend to happen when participants believe you had a hand in creating the site.
The aesthetic-usability effect is interfering. Let’s say we rule out the first two possibilities — we think our user is comfortable in the session and doesn’t feel pressure to say something or give empty compliments. This may be a true instance of the aesthetic-usability effect. Clearly, there are usability problems to be fixed, but it’s a sign that our visual design may be doing its job.

Once we determine why our user is giving positive feedback on visual design after a negative experience, we can try to work around the problem.
Pressure to Comment
Some people are naturally uncomfortable with silence. Reduce the pressure to say something on your participants by establishing a low-stress vibe early in your session.

Reassure participants frequently that what they’re doing and saying is helpful.
Remember that the communication between a moderator and a participant isn’t the same as a regular conversation —  the right amount of silence is part of the process.
Give participants plenty of opportunity to comment during the session by asking open-ended questions, but don’t push them too hard if they don’t have anything to say.

Pressure to Be Nice
Again, some people are naturally eager to please, and that will show up during sessions. But you can avoid this problem by distancing yourself from what you’re testing.

Before beginning each test session, emphasize that you didn’t design the site (even if you’re a researcher technically on the design team), that you’re there to learn from the participant, and that negative comments won’t hurt your feelings. It’s more valuable to hear hard truths than false praise!
Try (as much as possible) not to register emotional reactions to their comments in your face or body language. This is harder than it sounds, and takes practice. Try to keep a consistent demeanor of pleasant, nonthreatening, mild interest.

Aesthetic-Usability Effect
Sometimes you can work around the aesthetic-usability effect by probing users to think beyond the visual layer of the UI. But be careful not to lead the witness. Use vague questions like, “Do you have any comments about how easy or difficult it was to find this information?”
You might also return the user to a page or stage in the process that seemed particularly challenging, and ask them to describe what happened.
Sometimes probing questions will help, but not always. Be willing to let it go and move on to the next task.
“Great Color Scheme” Doesn’t Mean Your Visual Design Is Working
Bear in mind, when you hear positive feedback about a visual design during a test session, it doesn’t necessarily mean that your visual design is working. As described above, it’s possible that your users feel pressured to make a comment or to say nice things about your site.
Additionally, your visual design may be attractive, but it may not support usability. When that’s the case, users might still make positive comments. The visual hierarchy of the site may not help users understand its content, or a lack of signifiers may degrade interaction.
Conclusion
Aesthetically pleasing interfaces are worth the investment. Visual designs that appeal to your users have the side effects of making your site appear orderly, well designed, and professional. Users are more likely to want to try a visually appealing site, and they’re more patient with minor issues.
However, this effect is at its strongest when the aesthetics serve to support and enhance the content and functionality of the site. Additionally, this effect often influences user comments during research. As always, listen to what users say, but, first and foremost, take into account what they do.
References
Kurosu, M., & Kashimura, K. (1995). Apparent Usability vs. Inherent Usability. Conference companion on Human factors in computing systems - CHI '95.
Norman, D. A. (2004). Emotional Design: Why We Love (Or Hate) Everyday Things. New York: Basic Books."
45,2017-01-01,"Usability testing in the field is an effective and quick way to learn about users and their context of use. Like other types of field research, it typically takes place where users live, play, or work. The following lessons learned from our own studies can help you avoid common problems.

Do a pilot study session in order to debug your materials and understand how much you might get done during the allotted time.
In the likely case that you have too many questions or tasks and not enough time, either prioritize and make some of the last ones optional or plan how to best rotate questions and tasks among participants so you get good coverage of your research issues.
Consider making an editable script for each session, so that you can take notes into the script. A tablet with an ink app can be handy, or bring a superlight laptop. Use a checklist for important things to take with you.
Your questions should evolve or change over the sessions as you learn. Prepare any stakeholders and observers for this process and explain why in this situation (unlike for other types of research such as surveys or field studies) consistency is not essential. (These are decidedly not measurement studies.)
The onsite project manager may be able to arrange incentives and a host gift for you. This help can be extremely useful when your research takes place in a different culture or country than your own.
Monitor recruiting closely to make sure you aren’t getting all superusers, trainers, administrators, or support-escalation people when you actually need normal users. Don’t let well-meaning helpers stack the deck with “good” (but unrepresentative) or expert participants in a misguided attempt to ensure that people perform well in the study.
Recruit diverse participants in terms of ethnicity, role, gender, and experience with the task domain and the system. Try to include at least one person whose native language is not the language of the interface.
Everyone will want to watch. Limit the number of observers per session. Don’t overwhelm your participants by crowding them out of their cubes. If needed, schedule the observers too. The more the better, but make sure you are running the show so they won’t prevent you from getting the data you need.
Weigh locations carefully. Think about whether you need to conduct research sessions in each person’s normal workspace or to move it to another venue, such as a conference room — for example to get enough privacy and quiet. Sometimes a noisy environment is essential in order to recreate all the distractions that people will normally have to deal with when using the system.
If you are able to pay the incentive at the time of the research session, the consent form could also serve as your signed receipt. That way you can pay cash, so that no one will worry whether you’ll really send them a check in the mail or not.
Observers and stakeholders will want to talk to the participants. Provide guidelines for observers, including when and how other people can ask questions. You may need to reword some questions before they get asked in order to remove biases. Be ready to alert people who cause problems through a prearranged signaling system. (Passing notes works well for communicating with observers during sessions.) Don’t let your research be derailed by other people if at all possible. Reserve 5 minutes at the end of the session for observer questions. Sometimes a quiet discussion after a session is necessary in order to regain cooperation. Stakeholders will very likely have other (and better) opportunities to find out more, but you might have only one shot at doing the research. If power struggles appear during the planning phase, build in extra participant-question time for the stakeholders at the end of each session and extra debriefing time.
When conducting research at a business location, reserve a conference room or another private area, if possible, for the researchers and observers to occupy when not in sessions. A whiteboard and a projector might come in handy too. Make snacks and drinks available. Ensure that research participants can’t overhear the team talking.
Encourage and welcome observers, especially if they are usability research skeptics. Stakeholders who observe research studies can become your biggest advocates for change later, and it saves a lot of explaining time to have them onsite. Having a few extra people along can also be quite handy when problems crop up and you need people working in parallel with you, running errands, or intervening when political or emotional issues intrude.

During the Study

Make sketches. Consider sketching notes and ideas on copies of the user interface screens (and even taking environmental photos).
Keep separate copies of the original images and documents, so you can have as many of them as you wish to annotate.
Date documents for version control.
Number participants and their documents so you won’t be attaching names to data.
Take good notes, even if you are allowed to make recordings. Recordings take just as long to listen to (or longer) than the original session, and recordings sometimes fail. Capturing observations and insights in real time can be crucial.
Don’t rely on people to remember to send you promised material after the session. Get permission for someone to email them one reminder if needed.
Pay attention to everything in the environment.
Don’t rely on your memory for anything. Note your questions, ideas, insights, to-do items, and concerns as they arise.
Debrief observers and any onsite research team after each session.
Make debriefing notes so they can become the source for preliminary top findings.

After the Study

If you have recordings that need to be shared with stakeholders, add data-confidentiality instructions and warnings at the beginning of each video or audio file. Release recordings only to a responsible person who fully understands the need to keep research participant data safe and anonymous and the need to destroy raw data and personally identifiable information as soon as it’s no longer needed.
Set expectations for when you’ll deliver results.
Compile and share preliminary top findings as soon as possible, while everything is still fresh in your mind.
Thank everyone who helped make the research effort successful. If allowed, bring a business gift for the host(s), such as a UX book or great office supply item (or make plans to go out for a meal together).

Conclusion
When preparing for field research projects, collaborate with stakeholders to make a research plan, ensure you take the equipment and supplies you need, review these tips, and then relax and have a great usability study.
We can teach your team a full-day course on how to do ethnographic field research. (Also available as a 2-day course, for further depth.)
"
46,2016-12-18,"What Is a Testable Prototype?
A user interface prototype is a hypothesis — a candidate design solution that you consider for a specific design problem. The most straightforward way to test this hypothesis is to watch users work with it.
There are many types of prototypes, ranging anywhere between any of these pairs of extremes:

Single page vs. multipage with enough menus, screens, and click targets that the user can completely finish a task
Realistic and detailed vs. hand-sketched on a piece of paper
Interactive (clickable) vs. static (requiring a person to manipulate different pages and act as a “computer”)

The choice of prototype will vary greatly depending on goals of the testing, completeness of the design, tools used to create the prototype, and resources available to help before and during the usability tests. But, whatever prototype you may use, testing it will help you learn about users' interactions and reactions, so you can improve the design.
Why Test a Prototype?
Ripping up code is very expensive. Ripping up a prototype is not, especially if it’s just a piece of paper.
Let’s first consider common arguments for  not  testing a prototype. These are:

Waiting for a design to be implemented before you test it means that the design actually works, and test participants can use it in a natural way.
There is no adjustment to be made to the Agile or Waterfall processes to accommodate UX and iterative design.
Some Lean proponents say that, with no prototype testing, there is no prototype to throw away when it (inevitably) tests badly, so there is no waste.

These arguments may seem good at first glance. But in reality,  testing final products is uninformed and risky. Enlightened teams create prototypes, have users test them, and iterate the design until it’s good enough. (Note: We also test final products to benchmark the usability at the end of a project or at the beginning of a new one, to assess  ROI, to run  competitive studies, and to do a final check and make small tweaks.)
Interactive vs. Static Prototypes
Work needs to be done to bring a prototype to life for usability testing. To make it respond to user actions, we can spend time implementing the interaction before the test or we can “fake” the interaction during the test. Both methods have benefits and drawbacks.
Interactive (Clickable) Prototypes
With interactive prototypes, the designer must set a response for each possible user action before the test happens. Even with the right tool, building an interactive prototype can be time consuming: you have to get all the click targets right, and make the system respond only when the user interacts with a clickable target.
Static Prototypes
With static prototypes, the responses to users’ actions are given in real-time during the test by a person who is familiar with the design. There are several methods that can be used with static prototypes:

Wizard of Oz.  This method is named after the famous Frank Baum book (and more famous movie) with the same name. (If you’re not familiar with the book or the movie: in it, the great Wizard of Oz is impersonated by an ordinary human hiding behind a curtain.) The “wizard” (the designer or someone else familiar with the design) controls the user’s screen remotely from another room. None of the user’s clicks or taps really do anything. Rather, when the user clicks, the “wizard” decides what should come next, then serves up that page to the user’s screen. The “wizard” may even create the page on-the-fly and serve it up. Users don’t know what produces the response. In fact, they are usually told very little beyond that the system is “unfinished and slow.”

	Wizard of Oz testing is particularly useful for testing AI-based systems before you have implemented the artificial intelligence. The human who controls the computer can simulate the AI responses based on natural intelligence.

Paper-Prototype “Computer.”  The design is  created on paper. A person who knows the design well plays the part of the “computer” and lays the papers out on a table, near the user’s test table but not in her line of sight. As the user taps with the finger on a paper “screen” in front of her, the “computer” picks up the page (or modular part) representing the response and places it in front of the user. (In this article, we use the notation of “computer” to refer to the human who’s implementing the user interface during the test session.)
	TIPS: 

The “computer” should indicate to the users when “it” has finished working and they can proceed with the interaction. This can be done either by using a designated gesture consistently (e.g., hands folded in front of you) or by using a special “Working” or hourglass-icon printout that is shown to the users while the “computer” is looking for the appropriate response and that is removed as soon as the “computer” has finished working.
The facilitator should avoid overexplaining the design elements or the process.


Steal-the-Mouse “Computer.”  This method is a version of the Wizard of Oz technique in which the “wizard” is in the same room with the user. (The “wizard’s” role could be played by the facilitator.) The prototype is shown to the user on a computer screen. As the user clicks, the facilitator asks the user to look away from the monitor for a moment and the “wizard” navigates to the page that should appear next. The user is then prompted to look at the monitor and continue.

Criteria to help you decide which type of prototype is right for your project:


The fidelity of the prototype refers to how closely it matches the look-and-feel of the final system. Fidelity can vary in the areas of:

Interactivity
Visuals
Content and commands

A prototype may have high or low fidelity in all or some of the above 3 areas. The table below explains what high and low fidelity mean in each of these areas.



 
HIGH-FIDELITY PROTOTYPE
LOW-FIDELITY
			PROTOTYPE


Interactivity


Clickable links and menus

Yes: Many or all are clickable.


No: Targets do not work.



Automatic response to user’s actions

Yes: Links in the prototype are made to work via a prototyping tool (e.g., InVision, PowerPoint).


No: Screens are presented to the user in real time by a person playing “the computer.”



Visuals


Realistic visual hierarchy, priority of screen elements, and screen size

Yes: Graphics, spacing, and layout look like a live system would look (even if the prototype is presented on paper).


No: Only some or none of the visual attributes of the final live system are captured (e.g., a black-and-white sketch or wireframe, schematic representation of images and graphics, single sheet of paper for several screenfuls of information). Spacing and element prioritization may or may not be preserved.



Content and Navigation Hierarchy


Content

Yes: The prototype includes all the content that would appear in the final design (e.g., full articles, product-description text and images).


No: The prototype includes only a summary of the content or a stand-in for product images.




Benefits of High-Fidelity Prototypes

Prototypes with high-fidelity interactivity have realistic (faster) system response during the test.  Sometimes it can take extra time for the person playing the computer, whether online or on paper, to find the right screen and respond to a user’s click. Too long of a lag between user’s action and the “computer’s” response can break the users’ flow and make them forget what they did last or expected to see next.

	A delay also gives users extra time to study the current page. So, with a slow prototype, usability-test participants may notice more design details or absorb more content than they normally would with a live system.
TIP: If the page supposed to appear next is hard to find in a paper prototype or slow to load in a clickable prototype, take away the current screen the user is looking at, so she is instead looking at a blank page or area. When the next page is ready, first display the previous page for a few moments again so the user can get her bearings, then replace that screen with the next one. The test facilitator can help this process by saying just a few words to help the user recover the context — for example, “Just a recap, you clicked  About Us.” 

With high-fidelity interactivity and/or visuals, you can test workflow, specific UI components (e.g.  mega menus,  accordions), graphical elements such as affordance, page hierarchy, type legibility, image quality, as well as engagement.
High-fidelity prototypes often look like “live” software to users. This means that test participants will be more likely to behave realistically, as if they were interacting with a real system, whereas with a sketchy prototype they may have unclear expectations about what is supposed to work and what isn’t. (Though it’s amazing how  strong the suspension of disbelief is for many users in test situations where not everything is real.)
High-fidelity interactivity frees the designer to focus on observing the test instead of thinking about what should come next. Nobody needs to worry during the test about making the prototype work.
Interactive-prototype testing is less likely to be affected by human error. With a static prototype, there is a lot of pressure on the “computer” and a fair chance of making a mistake. Rushing, stress, nerves, paying close attention to user clicks, and navigating through a stack of papers can all make the “computer” panic or just slip during the test.

Benefits of Low-Fidelity Prototypes

Less time to prepare a static prototype, more time to work on design, before the test. Creating a clickable prototype takes time. Without having to make the prototype work, you can spend more time on designing more pages, menus, or content. (You still need to organize pages before the test so the “computer” can easily find the right one to present. But doing this is usually a lot faster than preparing a clickable prototype.)
You can make design changes more easily during the test. A designer can  sketch a quick response, and erase or change part of design between test sessions (or during a session) without worrying about linking the new page in the interactive prototype.
Low-fidelity prototypes put less pressure on users. If a design seems incomplete, users usually have no idea whether it took a minute or months to create it. They may better understand that you are indeed testing the design and not them, feel less obliged to be successful, and be more likely to express negative reactions.
Designers feel less wedded to low-fidelity prototypes. Designers are more likely to want to change a sketchy design than one with full interaction and aesthetics. Once we invest more time and sweat in a design, it’s harder to give it up if it does not work well.
Stakeholders recognize that the work isn’t finished yet. When people see a rough prototype, they don’t expect it to ship tomorrow. Everybody on the team will expect changes before the design is finalized. (In contrast, when a design looks very polished, it’s easy for an executive to fall into the trap of saying, “this looks good, let’s make it go live now.”)

Interaction with the User During Any Prototype Test
In prototype tests, facilitators often talk a bit more with participants than they do in tests of live systems, mainly for the following good reasons:

They need to explain the nature of the prototype medium (not how the design itself works) to the user, before the study starts.
They occasionally may need to explain the state of the system (e.g., “This page doesn’t work yet”) and ask “What were you expecting to happen?”
They may have to find out whether users who sit idle are waiting for a response (from a slow system) or think that the task is completed.

Even with the above necessary interactions between the test facilitator and the user, the test facilitator’s ultimate goal should be to quietly observe the person interacting with the design, not to have a conversation with the participant.
TIPS:

If users click an item for which there is no prepared response yet:
	
Say: “  That isn’t working.”
Ask:  “What were you expecting to happen when you clicked that?”
Present the second-best page if there is one, and say something as an explanation. For example, “You clicked the compact cars link. We don’t have those screens today. So please pretend you clicked midsize cars. Okay?” After the user confirms, present the midsize-cars page. Then say as little as possible, and stay neutral.


If the wrong page appears after the user clicks, the “computer” should take that page away as soon as possible and revert to the previous page. The facilitator should tell the user immediately that the page was wrong, then repeat what the user did on the current page, “You tapped About Us.” Then the “computer” presents the right page.

“Computer” Errors Have a Negative Impact
Note that “computer” errors can seriously impact the test. As screens appear, users form a mental model of how the system and the research method work. If the wrong page appears, don’t assume that you can make users forget what they just saw. (Brain wipes only work in Star Trek.) Even if you take the screen away or try to explain the error, users may infer that the wrong screen is related to the task or get some more knowledge from your explanation, and may be later influenced by that information. Seeing the wrong page also breaks the users’ flow, and can confuse them. Finally, later in the test, if a screen appears that is unexpected, they might think the prototype is just malfunctioning again. This impacts the users’ expectations, trust in the research method, and ability to form a consistent mental model.
Since computer errors can negatively impact the study, take the time to pilot test and fix issues with the prototype before any sessions occur.
Conclusion
Make no mistake: You cannot afford to not test prototypes. Your design will be tested, whether you plan for it or not. Once your system goes live and people begin to use it, they are testing it. And rather than collecting feedback in a low-risk research setting, where you can learn, and then react and change the design, you’ll have actual unhappy customers on your hands. With them will come a litany of problems such as lost sales, abandoned orders, lack of understanding of content and products, alienation due to poor tone of voice, returned products, increased support calls, increased training costs, social shares of bad experiences, low Net Promoter Scores, and brand abandonment. The business will have to figure out how to fix all these. The development team will react by scrambling to fix the design, taking out working code, visuals, and content, and trading it for rushed, only marginally better replacements. All will come at a great cost. Redesigning, taking code out, coding again with the new design, quality testing that code, and, if applicable, changing marketing and documentation materials, is far more expensive than discarding a prototype.
Test prototypes, whether clickable or static, whether high- or low-fidelity. Aim to learn how to change and improve your design. That way, your customers will never see your design failures.
 
Read the companion article: IA-Based View of Prototype Fidelity.
For more information about preparing testable prototypes, get our Paper Prototyping training video, or attend our “Wireframing and Prototyping” course.
"
47,2016-11-20,"Why Plans Matter
Creating a project plan structures your thinking around the research activity. Plans keep stakeholders involved and informed, while reducing the need for calls and meetings. Documenting your research-project planning can help prevent misunderstandings, unwanted method variation, and unnecessary rework.
A research-project plan is a living document that is shared and updated as needed. After your study, edit the plan to serve as the record of your research method. Plans take the work away from your limited memory and provide a convenient place to keep track of the many documents generated by each project.
The benefits of checklists are well known to people doing complex tasks, such as surgical staff and pilots. Busy people in distracting situations often forget to do something important, despite having many years of experience with a procedure. This checklist is meant to ensure that you don’t overlook any important elements when  planning research studies.
What to Include in Your Research Plan
In the beginning, your research project plan should include not only the information needed during the actual study sessions, but also various information that the team may make use of beforehand. Most plans should cover:

Purpose of the plan
Information about what you will be conducting research on, such as basics about the product, method, and rationale
Purpose and goals of the research study
User tasks, metrics, and usability goals, such as maximum time on task and other success criteria (if applicable)
User profiles (characteristics of target participants for the research)
Screening questionnaire and recruitment plan (if applicable)
Expectations about deliverables and timing
Team
	
Expectations and roles
To-do lists


Note-taking and question-asking guidelines for observers, including how to collaborate in effective data gathering without  biasing the sessions 
Schedule
Facilitator script with questions for participants, including user scenarios (if applicable) and consent forms
Location information and contact details
Test setup, including equipment and supplies

Important Planning Considerations

Get stakeholders’ signoff   on plans, so everyone understands what’s in scope and how you will meet the research goals. Provide written goals, user profiles, tasks, and participant scenarios as soon as possible, so any concerns and questions can be discussed well in advance.
Research participants can sometimes be controversial, so keep stakeholders informed  periodically by sharing the details of the recruits as they are scheduled. But don’t include participants’ names and identifying information in plans — use numbers (P1, P2, etc.) instead to preserve their privacy. If screening criteria need to be adjusted during recruitment, having this shared understanding can make rapid changes easier to accomplish.
Think through all the logistics:
	
First, consider the  schedule  for research sessions, building in time for pilot session(s), lunches, and debrief and regroup time between participants, so you can have solid appointment times for recruitment.
Choose the  location  where you will conduct the research. Gather logistic information (such as maps, nearest hotels, and travel contacts) to help the research team have a great experience.
Arrange any needed onsite  support, such as IT staff, equipment, chairs, desks, tables, catering, beverages, and snacks.
Decide how to compensate study participants. If you will pay them directly at the time of the study, plan to obtain the needed cash, gift certificates, or checks.
If you have  observers, consider how many you can accommodate and how they can best help with data capture.
Make a   checklist  of items needed for a user study.
Share contacts, including everyone’s mobile phone numbers for urgent issues.
Consider  information sharing: when and how to disseminate information during and after the study.


Consent forms  are required for all participants in order to inform them of the purpose of the study and get their agreement in a way that protects everyone legally. The exact wording may require approval in advance by stakeholders and any businesses you may visit for the research. Include at a minimum: what the study involves, who is conducting the research, why you’re doing it, the date, any incentive paid (phrase this part to double as a receipt), which information will be collected and how (including recording details), and which information will be shared with whom (including any NDA requirement).
Provide tips for observers. Coach people in what to look for, how to take good notes, and how best to participate without interrupting. Set expectations about what to do and not do, and explain why that’s important to the research. Print observer guidelines to help everyone stay on track.
Plan data capture, analysis, and reporting. Consider carefully how you will analyze the data and report it. Then plan a data-capturing method (and a fallback method) that meets your needs. Consider the abilities of the observers, the situation, and concerns of your participants. Plan to keep data confidential, and identify who will be responsible for protecting it appropriately.
Make a Plan B. What could possibly go wrong? Don’t assume that you’ll be able to record audio or video, even if that’s the plan. Plan around not being able to print, get text messages, or use your devices. Decide what to do if anyone runs late or cancels at the last minute.
Make a detailed facilitation script  that includes introductions, instructions, any needed prompts for materials exchanges, timing cues for switching tasks and turning tools on and off, task scenarios, questions you plan to ask, and so on. Plan how to edit scripts between sessions as needed. Scripts usually include some of these components:
	
Introduction 

Thank the participant.
Introduce yourself and explain why you are doing the study.
Set expectations for how the session will proceed.
Get consent from the participant in writing.
If applicable, give the participant the compensation for the study.
Go over any instructions and, if you’re using a  think-aloud method, read the think-aloud instructions to the participants and maybe show them a quick  demo of what’s expected. Alternatively, ask people to read the instructions to you, and then ask if they have any questions about the process.


Initial questions 

Depending on the type of research you’re doing, you may want to ask some questions at the beginning of the session. For example, you might want to ask  open-ended questions about the person’s role, experience with the task domain, industry, product, competitors, and so on. It might be helpful to ask what participants expect and what they hope for when using a system like yours.


Interview or observations 

Depending on your research, you might be asking structured interview questions or watching people do tasks. Your script should contain planned wording for you to read as needed, so that you won’t accidentally introduce variations that might bias the answers.
It’s often necessary to change wording, or to add or remove questions in the script between sessions. Some situations could require you to invent probing questions on the spot, as well, when interesting things occur. Set expectations with stakeholders that some variation will occur.


Scenarios 

If your research will have a hands-on component, ask participants to read  scenarios (one at a time) and show you how they would do the activities, while you watch quietly and take notes.
Optional: After each scenario is complete, you might want to ask people to rate how easy or difficult it was for them.   The Single Ease Question is designed for this purpose. The best reason to use rating questions in qualitative research is to ask, “Why did you give it that rating?” afterward, so be sure to ask that each time too. You might also want to compare ratings over many research projects having the same scenarios. People are notoriously inconsistent at assigning numbers to ease of use, however, so you may find that the numbers or the reasons people state for their rating are at odds with what you observed in the session. Even so, the rationale for the rating may point to top-of-mind concerns.


Final Questions 

Near the end of the research session, ask about anything that you didn’t understand or need more information about. Other useful questions include:
			
What did you like the most and the least about [X], and why?
How do you normally do [Y activity] today; what’s easy and difficult about that? What could make that easier to do?
Do you have any questions for me?


Leave time for any observer questions and answers.
Consider asking for permission to follow up with an additional question later. Ask people to contact you if they think of something else they want to tell you. You could also ask if the participant would like to be contacted for future research projects. If you  used an outside recruiter, however, you should talk with the recruiter about that, not the participant.





What to Include in Your Final Research Plan
The final version of your project plan should serve as a record of the research at the level of detail required to replicate the study in the future. Start with the plan you used for the study and remove unneeded detail or add new sections as needed. Be sure to update anything that changed, such as script and scenario wording.
Include information about:

Team and product basics
Research purpose, method, and goals
User profiles and anonymized participant profiles
Screening questionnaire and recruitment plan (if applicable)
Checklists, schedule, script, documents, and forms
Anything that might be helpful to know about the location and travel
Any tested prototypes, any photos, screenshots, or other important artifacts from the research study
Deliverables list (file names, formats, responsible person, location), for the raw data, recordings, and report, if any

What to Deliver

Preliminary findings list
Bug list (for the developers)
Report, slide deck, or end prototype
Recordings and other raw data
Final research plan

Conclusion
Research-project plans are essential tools that help keep stakeholders informed. Planning helps you to remember what to do and to keep track of where you are in the research process. Each research project is a little different, so plans differ in structure and content. Make a research-plan template that you can modify as needed to save time in the future.
"
48,2016-10-30,"These guidelines and instructions can be modified for your research projects to help you manage observers. (Please give NN/g credit as described at the bottom of this article.)
There are two main reasons to have guidelines for study observers:

To prevent observers from inadvertently messing up your methodology
To gain better insights from all the additional people who supply extra brainpower and diverse perspectives to your analysis

How to Observe Research Sessions
Everyone

Audio in the research room (not the observation room) is being recorded. Gently remind anyone who seems to have forgotten this, including people who may stand outside the room to talk.
Bathrooms and hallways are especially hazardous for discussions about participants or the research sessions. Even during a session the next participant may be waiting nearby.
Please avoid laughing or shouting during sessions. Rooms are not completely soundproof.
Turn computer notifications and phone ringers off. If you must take a call, please do so where you can’t be overheard or accidentally recorded.
Please don’t distract anyone by talking during the session. If you must make a comment to someone, pass a note.
When you think of a question you’d like to ask the participant, please write it down. Near the end of the session, the facilitator will collect these.
We’ll be taking structured notes together, and your participation is very welcome. Please review the note-taking instructions and materials. We’ll compare notes between sessions and at lunch.

Observing in the Room with Participants

Please sit out of the participants’ line of sight (behind them) and try to be completely silent.
Smile in a friendly way and watch carefully while taking notes.
During and before the sessions, don’t engage participants in conversation, offer advice, correct them, or answer their questions, because introducing new information or giving unintentional clues can bias the research results and even invalidate that session’s data. It’s difficult not to respond when someone asks you questions, because it seems impolite, but this is really important. Say something like, “I’ll be glad to talk with you about that later” and look to the facilitator.
Answer only the questions that the facilitator asks you directly. It’s very common for facilitators to ask participants questions that they already know the answers to, questions to help the user relax, and questions to find out what the participant understands. Don’t answer those questions either. It’s important to understand what the participant believes to be true, even when that’s not technically correct.
Be careful not to distract anyone. Your fidgeting, sighing, posture, facial expressions, noise from phones or clothing, and so on, can easily change the participant’s behavior and comfort level. If you need to, please leave the room quietly. No in and out.
Typing noise is fine if you’re taking notes, but try to type constantly so participants will ignore it.
It’s okay to laugh if the participant makes a joke, but not at any other time.

Notetaking Instructions
Make many notes. Write about everything, because you don’t know what might prove valuable during data analysis. Get more paper (or copy/paste more table rows) if you run out.
Even when you see others taking notes, write your own observation too, because others may overlook what you noticed, and your particular point of view and expertise will be very helpful.
Write one observation per note, so we can sort them into categories later. Don’t write paragraphs with several ideas; just keep notes in sequence when they go together. With paper notes, don’t write on the back, in case we need to cut them into sections.
When you think of a question you’d like to ask, write it down. The facilitator will collect questions near the end of the session.
Include the scenario letter on each note. Please also add your initials to your notes or put your name in the document, so we know who to ask if we don’t understand something.
Each participant was assigned one color of note card or one notetaking document. Changing documents and colors between participant sessions is mission critical, so help ensure that it happens.
Examples of Effective Notes
Note whatever seems important:
Mistakes

He skipped the City field on the address form.
She didn’t click Save before closing the window.

System errors and error messages

The menu didn’t drop down the first time she clicked it.
Error message: “Database error on line 55.”

Click-paths (navigation sequences) can be especially helpful:

Home > New Car > Vehicle Type > Back > Model > Back (looking for “vans”)
Searched “ford truck” > Ford.com > Trucks & Vans > F-150 > Models

Strategies and tools

He said he always chooses two items because shipping costs less.
She chose About Us and said she was looking for “contact us.”
He used a calculator and Google Maps, then entered the data into the desktop app.

Search terms and results

Search 1: lexis (nothing useful)
Search 2: lexus (correctly chose 3rd result)

Quotes

“This is great!”
“I expected this to work like Amazon.”

Anything that seems overlooked, misunderstood, ambiguous, or confusing
Suggestions, questions, and comments (including your own)
Conclusion
Giving your study observers guidelines like these will increase the value your company gains from its investment in UX research. You can email the guidelines to scheduled observers in advance along with the time plan for the study. It’s also good to have printed copies of the guidelines available to give to unexpected observers who show up at the study, as well as a printed copy on each desk or table in the observation room (if you’re using a lab with a separate room for the observers).
Feel free to copy our list and give it to your observers, with the following credit line:
Observer guidelines from Nielsen Norman Group, used by permission. Original: https://www.nngroup.com/articles/observer-guidelines"
49,2016-09-04,"In an effort to save time and money, some companies use their own staff as user-testing participants. Although this method is convenient and cheap, for most situations it is inappropriate and leads to incorrect results that cannot be trusted to reflect your real audience’s needs and behaviors.
Why Some Companies Recruit Employees for User Testing
Here are a few reasons why it is tempting to engage in this practice:

Recruiting convenience: Internal participants are more easily accessible than external users and also more willing to volunteer their time in order to benefit their company. You won’t need a recruiter to find participants — you likely know where to post an ad for a study and you can trust the responders to be responsible and show up for the planned session. After all, their reputation is at stake. Plus, you probably have a myriad of internal tools that you can use to communicate with coworkers.
Lower Cost: Companies have different rules about paying internal employees, but often the compensation process is simple and may not even involve money. (Cafeteria vouchers seem popular.)
Confidentiality: In some situations, conducting a usability test on an unreleased product creates confidentiality issues, especially for revolutionary new products that you don’t want anybody to know that you’re even working on. Employees have already signed nondisclosure agreements and are less likely to breach them than external participants.
Attracting attention: Inviting employees to be usability-test participants could increase interest in the project. Those test participants that become familiar with the process can advocate for your work and possibly lead to more UX support.

Why You Should not Recruit Employees
Asking employees for feedback can affect both how sound your data will be and how the employees might feel about the company. Your data may reflect employees’ biases instead of real-user needs and behaviors:

Employees have more prior knowledge about the company than external participants. They know the company’s jargon and its business goals better than nonemployees, and can use that information in a user test to perform better — or for sure differently — than an outsider would do. You won’t observe authentic behavior, which is the main goal of user research.
Study participants may have prior knowledge about the project: they may have been directly or indirectly exposed to the project and may have already defined attitudes toward it.
Employees have higher motivation to use your product than many outside prospects might have. Employees are likely to think that whatever your company does is meaningful, whereas outsiders might dismiss your company — or even your entire industry — and have very little motivation to engage with your design.
Employees’ perspectives of the company are clouded by their opinions on the best course of action. They might make judgments about how feasible and desirable your project is, and how much time, and how many resources it would take to implement any changes suggested by your research. The feedback obtained from the study will likely reflect these opinions.
Employees might feel pressure to complete the task; they may spend extra time on each task because they are invested in the welfare of the company.
Internal participants may directly or indirectly know people involved in the study. Their feelings for these team members may influence their behavior during the study. Even when participants don’t personally know the team, they might feel a strong connection to it and fear disappointing the designers or insulting their work.
An employee’s relationship and history with the company can influence the results of the study. Employees may have a strong positive or negative view of the company that may bias their opinion of the product.
Participants may feel uncomfortable being watched by their coworkers or even feel the need to impress high-level executives observing the study.

The last 30 years’ experience with usability studies has conclusively demonstrated the methodological benefits from testing with customers instead of employees. A recent study conducted by Yahoo! confirmed our recommendations: its results indicated that employees are more critical of competitor products and spend less time on competitor sites when compared with outside usability-test participants. These behaviors are consistent with brand loyalty: employees rated the company’s own site 2% higher than a competing site, whereas outside users rated the company’s site 15% lower than the competing site. The outsiders were 18% slower than the employees in performing tasks on the company’s site. Thus, both preference and performance data were skewed, and the research would have been misleading if conducted purely with employees.
All these reasons point to why studies with employee participants will lead to inconclusive results and biased data. The irony is that, in spite of their poor reliability, results from such studies may actuality enjoy higher credibility in the company, because they reflect views of people that the company already trusts and perceives as “its own.” Don’t fall into that trap: by taking employees studies ad litteram and acting upon their findings, you risk wasting a lot of time and money without fulfilling the needs of your actual audience.
When Should You Test with Employees?
There are a few cases where it’s appropriate to recruit internal participants for a usability study:

If you are testing an intranet or an application developed for your own employees, then you can legitimately use your employees as test users, because they are the exact target audience. Even in this situation, you want to minimize biases, so find people that are not involved or familiar with the project and minimize discomfort by asking that all direct managers or high-level executives observe in a different room.
If you want to pilot a user test, recruiting internally is a great method that can point out weaknesses in your test plan or tasks. If you invite observers or stakeholders for the pilot session, be clear about the scope of pilot testing: to give you feedback about the test structure (to improve on the subsequent test sessions with real users), and not to draw conclusions about the interface.
If your company is resistant to provide UX support, this method could be a skunkworks-style technique suited for the transition from UX maturity level 2 to level 3, to demonstrate the need for usability testing. In this case, it is better to recruit people with little existing knowledge of the company history such as interns or new hires. Human-resources people or support staff are preferable to developers, researchers, designers, or marketers who already may have a lot of knowledge about interfaces. Conduct the study to highlight challenges that could be resolved through usability testing with a few target users.

Recruiting employees as test participants is not a replacement for recruiting externally. Limit its use to these few scenarios because it can prevent your organization from recognizing the benefits of recruiting externally. When it comes to usability testing, there is nothing that can truly compare to recruiting real users.
(Learn how to do your own user testing with sound methdology in our full-day training course Usability Testing.)
References
Joanne Locascio, Rushil Khurana, Yan He, and Jofish Kaye. 2016. Utilizing Employees as Usability Participants: Exploring When and When Not to Leverage Your Coworkers. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York, NY, USA, 4533-4537. DOI: http://dx.doi.org/10.1145/2858036.2858047
"
50,2016-05-29,"Involving stakeholders and team members in usability testing is essential in order to foster collaboration and buy-in. At all project stages, the design team should be able to participate in a variety of research activities, ranging from setting goals or defining the target audience to selecting the methodology to use for research and observing the test sessions.
While team involvement is vital in usability research, sometimes stakeholders behave in ways that violate good practices of user testing. Even though inappropriate requests or behaviors are often well intentioned, UX researchers have an obligation to handle these situations delicately.
Ethical User Research
The norms of conduct for working with human participants help us distinguish between acceptable and unacceptable behavior at all stages of research. Adhering to the standards help the UX field uphold the quality of the research and protects the organization from potential legal implications. While codes of conduct vary for each profession, the following serve as guides for helping UX researchers make the right decisions when faced with unethical requests:

Avoid harming people. Do not expose participants to unreasonable physical or mental stress.
Respect people’s dignity. Do not make derogatory remarks about clients, participants, or colleagues.
Act with integrity. Conduct research and report findings in a fair and truthful manner.
Maintain confidentiality. Respect the privacy and anonymity of the participants.

The Dilemmas and Resolving Them
The above bullet points surely sound reasonable. Who would be so nasty as to violate these basic principles for treating research participants ethically? Most of us, that’s whom, unless we stiffen our spines and recognize the slippery slopes that may sound reasonable at first but end up in the wrong place.

Below are examples of difficult situations and how to handle them:

Team insists that participants continue the study even when they don’t want to. This behavior may sound extreme but unfortunately it does happen. Study sponsors are motivated to get the most out of their money, and letting a participant leave before the scheduled end time means less or no data.
What to do: Hopefully, you will never experience this situation. Users should ALWAYS be allowed to take a break or leave for any reason. At Nielsen Norman Group, we explicitly tell people at the beginning at each session that they can stop at any time.
A person who is no longer interested in participating should never be coerced in staying because doing so is unethical (and probably illegal). Plus, their attitude will taint the findings anyway.

Team wants you to lie to users. UX Researchers are sometimes pressured to lie to users, often with good intent. For example, we are asked to tell participants that no one else is watching when observers are in the other room (or over the internet). The assumption is that white lies are harmless and can benefit the participant by making them feel more at ease.
What to do: In general, do not lie to users. Trust is difficult to earn and easy to lose. Better to be transparent about the setup and testing procedures. Don’t try to ignore awkward situations and keep people wondering. Acknowledge that usability testing might feel unnatural.
If you are recording the session, point out where the cameras are. If your lab has a one-way mirror, come straight out and tell users what it is. And if your colleagues are dialing in, just say so. Don’t put yourself in a situation when a colleague forgets to mute the telephone and the participant hears.
(While you shouldn’t lie to users, there are times when withholding the truth is acceptable. For example, if you’re conducting a competitive study and the user asks who’s sponsoring the study. To avoid biasing the findings, it’s okay to tell the user that you can’t reveal the sponsor at this stage of the study, but that you will inform them at the end of the session.)

Team wants to rescind payment when the participant shows up but is a poor fit. Sometimes you discover that a participant doesn’t fit the profile during the session. In some cases, you might conduct the study anyway to salvage what you can. However, if the participant is too different from the recruitment requirements it’s better to cut your loses and allow them to leave gracefully.
What to do: Regardless of the reason for the recruitment mishap, it’s best to pay the participant the entire incentive. Give participants the benefit of the doubt and assume an error occurred during the recruitment process. The participant put in effort and time to show up at the test location and that should be enough to warrant the incentive. Learn from what happened, make necessary changes to the recruitment procedures, and move on.

The participant’s boss wants to observe the session. Internal studies are nice because you can usually find participants easily. However, be aware that employees might feel uncomfortable being observed by their managers or colleagues. New hires in particular might fear being judged by their superiors or might believe their performance in usability studies will affect their performance review.
What to do: When conducting intranet research, make it a policy to NOT allow bosses to attend sessions of their subordinates. Asking employees for permission is sometimes insufficient because they might feel obligated to conform to the request. Also, make sure any notes, research findings, and reports are anonymized.

Team becomes defensive when users don’t understand the interface. For many designers, it’s difficult to sit quietly (and stare blankly) while someone struggles with the interface you’ve created. It’s tempting to reach over, grab the mouse from the person’s hand and show them how use the site correctly. This is why we usually recommend that you don’t test your own design
We’ve witnessed observers interrupt studies to pitch their ideas to participants in the hopes of changing their mind. This is bad practice because it defeats the point of getting user input. Also, when the team becomes defensive, respondents are less likely to give honest feedback, or, worse, they might feel intimidated. Usability testing is the team’s opportunity to receive feedback, not give it.
What to do: Set clear rules for proper “observer” etiquette. Review the rules with the team before you begin, especially if observers sit in the same room as the participant. Remind observers about the purpose of the study and why it’s important to remain silent and keep an open mind. You can also include these rules as you are explaining the session to the participant: “There are some team members observing the study, but they will be silent for the duration, except at the end of the study when I will see if there are any questions.” Including a phrase like this in the introduction serves as a reminder to all observers on how they should behave.
Allow observers to ask questions at the end of each session. Ideally these questions should be communicated to the moderator, who can reformulate them as needed and present them to the participant. Observers are less likely to interrupt if they know there are opportunities to ask questions later.

Team is distracting during test sessions. When conducting usability tests with participants and observers in the same room, it’s sometimes difficult for observers to remain unobtrusive for the entire session. Sometimes it may be hard to keep a neutral face if the participant makes a funny comment, or, in their excitement, members may want to discuss what they just saw immediately. Such reactions or discussions, even in quiet whispers, distract the participant from their tasks, and even worse, make them feel self-conscious.
What to do: You don’t need a fancy usability lab to conduct usability studies. Often, a simple conference room will do. However, if you have observers sharing the same room, make sure everyone is aware of the rules for participating. Remind observers to remain quiet throughout the session and stay for the entire duration of the study. Keep any movement or sounds to an absolute minimum.
Only allow up to 3 observers in the same room. Users will feel more comfortable when they’re not overly outnumbered. Set expectations by letting participants know about other observers BEFORE they arrive. Try to position observers away from the user’s line of sight. We usually ask team members to sit on the opposite end of the boardroom table and observe the screen using an external monitor.
If you must have more observers, consider broadcasting the session to a second conference room where observers could all gather.

Team jokes about users. When testing shows major flaws in the design, a common reaction from observers is to blame the user. Team members sigh in disbelief as a participant does something completely unexpected and wonder if there’s something wrong with the participant. Occasionally, observers might call the participant undesirable names, such as “guinea pig”, “stupid”, “clueless”, and so on. Name calling is not a joking matter.
What to do: Any negative undercurrent of name calling and labeling is unprofessional. Help team members get in the habit of reframing conversations to what is wrong with the interface, not what is wrong with the user. Create a work culture where all participants are seen as partners and treated with respect.
Ask team members to try being a user sometime. Nothing is better at demonstrating how it feels to be in the participant’s shoes.
Conclusion
Upholding research ethics is everyone’s responsibility. Familiarizing everyone on the team with the code of ethics helps you preserve the integrity of the profession and keep you (and the company) from potential litigation. When you protect the participant, you protect yourself and the company. And you get more valid research results.
Learn how to conduct usability studies and manage challenging ethical situations at our conference.

More on code of conduct, check out these websites:
American Psychological Association
User Experience Professionals Association
"
51,2016-04-17,"Every usability study is different, depending on your specific goals and constraints. But one thing is common for all user research: even though the basic methods are easy enough, if you jump into a study without proper preparation, you won’t get nearly as high a return on your research investment. Below is a checklist of activities to consider when planning a usability study.
1.    Define Goals for the Study
Meet with the stakeholders to determine what they want to learn. Identify the questions, concerns, areas of interest, and purpose for the research. The goals will determine which UX research methodology to choose.
Usability studies are well suited for gathering qualitative or quantitative behavioral data and for answering design-related questions (e.g., is the content presented in a way that is easy to find and understand? Can people complete a task successfully?). If your goal is to collect attitudinal feedback, then consider alternative research methods better suited for those purposes.
Don’t commit to too many goals: for every additional question you want answered, the quality of your insights on the other research goals will drop. You only have so much time with users: make the most of it by focusing on the research goals that’ll truly move the needle on your product ROI.
2.    Determine the Format and Setting of the Study
Below are some considerations for determining which research approach is appropriate for your situation:
In lab or in field: Should you conduct the study at your facility or go to the participant’s location? For convenience, most face-to-face usability studies are conducted in-house, in a lab setting. However, if the users’ actual environment is critical or if it’s difficult to represent the users’ setup, then the travel time might be worth it.
Moderated or unmoderated: Moderated studies tend to provide you with richer design insights and opportunities to probe and ask for clarification. They also are a better source of open-ended comments from the participants. On the other hand, unmoderated studies can be cheaper, quicker, and may provide better access to hard-to-recruit participants.
In-person or remote: In general, we recommend in-person studies whenever possible. When you are in the same room as the participant, the interaction feels more personable and you are able to detect subtle cues, such as body language, much easier. However, sometimes in-person testing may not be feasible — for instance, because you have no travel budget, because you work within an Agile environment, or because users can’t come to you.
3.    Determine the Number of Users
For traditional qualitative studies, we generally recommend 5 participants for the best return on investment. If your research involves more than one target user group, then you may need to adjust the number of participants to 2–5 per group, depending on the level of experience and attitudinal overlap between the groups.
Quantitative studies and eyetracking require a larger sample size to obtain meaningful conclusions. Expect to increase the number of participants by at least 4 times. You may need at least 20–30 participants in each target user group
4.    Recruit the Right Participants
A foundational rule in conducting user testing is to get representative participants. The greatest insights are derived from gathering feedback from real users. Identify people who match your demographics (or even better: match your personas) and then screen for behavioral traits, attitudes, and goals that match those of your users.
Asking proxy users to pretend or imagine a scenario might lead to invalid results. You may have some leeway when testing general sites, but for specialized websites, you must find people who fit your exact circumstances, especially when testing content-rich sites and B2B websites.
(For more information on participant recruitment, see our free report).
5.    Write Tasks that Match the Goals of the Study
In usability testing, researchers ask users to complete activities while using the interface. The activities (or tasks) are usually written in the form of scenarios and should match the goals of the study. The scenarios can range from general to specific, and usually come in the form of two main types:
Exploratory tasks: These open-ended tasks answer broad, research-oriented goals and may or may not have a correct answer. These tasks are meant to learn how people discover or explore information, and they are not appropriate for quantitative testing.
Example: You are interested in booking a vacation for your family. See if the site offers anything that you might suit your needs.
Specific tasks: These tasks are much more focused and usually have a correct answer or end point. They are used for both qualitative and quantitative testing.
Example: Find the Saturday opening hours for the Sunnyvale public library.
Writing solid tasks is critical in conducting a valid study. Strong tasks are concrete and free from clues that might prime people’s behavior. Vague instructions might cause users to evaluate areas that are not particularly important for this study. If clues are present, such as when the task contains the same word as on the screen, the activity is no longer a usability study, but a word-matching game.
6.    Conduct a Pilot Study
After you have written your tasks, make sure to run a pilot study to help you fine-tune the task wording, anticipate the number of tasks you can give per session, and determine the order in which to present them. Pilot studies can also help you refine your recruiting criteria, to ensure you are testing with the right participants. Better to catch problems early than during a session when all eyes are on you.
Note: Pilot studies are especially important when conducting online unmoderated studies because you are not present to give clarification or make corrections if study participants misinterpret the instructions or task descriptions.
7.    Decide on Collecting Metrics
The main purpose of qualitative usability studies is to gain design insights, and, with few users, metrics are unlikely to be representative for your whole user population  Therefore, measuring usability is usually not a high priority. In a quantitative study or in a study where you have well defined tasks and a fairly large number of users, however, this step is important.  Common usability metrics are: time on task, satisfaction ratings, success rate, and error rate.
If you decide to collect subjective measurements (e.g., ease of use or satisfaction questions about the task, satisfaction about the system), decide on when you will give the questionnaires: after each task, at the end of the session, or both.
8.    Write a Test Plan
Once you’ve figured out how you’re going to conduct the research, document your approach in a test plan and share it. This document serves as a communication tool among team members and a record for  future studies. The document doesn’t need to be lengthy, but should contain key information such as:

Name of the product or site being testing
Study goals
Logistics: time, dates, location, and format of study
Participant profiles
Tasks
Metrics, questionnaires
Description of the system (e.g., mobile, desktop, computer settings)

9.    Motivate Team Members to Observe Sessions
A great benefit of usability studies is fostering collaboration and buy-in. Nothing is more convincing than witnessing how users respond to the interface design. Having stakeholders observe moderated-testing sessions establishes common ground and reduces the amount of time required to communicate and document the findings. Teams spend less time guessing and debating, and more time designing.
Invite stakeholders and team members, and give them plenty of reasons to participate. Food is always a good motivator!
Conducting the study in a traditional usability lab or a simplified usability lab is ideal, but if your team members work in different locations, then offering remote viewing options keeps the activity inclusive.
Check out our course on Usability Testing for hands-on training on how to plan and facilitate user tests.
"
52,2016-03-20,"Last week, I attended the Games User Research Summit (GamesUR or GUR), which happened in connection with the Game Developer Conference (GDC), but was hosted separately at Electronic Arts (EA) in Silicon Valley — as you can tell, these guys love their acronyms.
With EA being the gracious hosts, the conference happened under the watchful eyes of an enormous dragon and the break room was festooned with large posters of the classic Star Wars characters. It was clear just from the surroundings that we were not in Kansas anymore. (Or, rather, not in the realm of mainstream UX. Here really be dragons.)
It was clear from the terminology bantered around in the talks that games are different from other design projects. Take, for example, the game Rainbow Six Siege (inevitably abbreviated as R6S, because they do like acronyms). One of the UX metrics tracked during the testing of this game was the kill/death ratio, which, admittedly, is not one of the things we teach in our otherwise comprehensive Measuring User Experience seminar. (This ratio is the number of opponents you kill divided by the number of your team members who die during a death match. Another term we don’t use much in mainstream design projects.)
Much Remains the Same
Despite the dragon and the death matches, I actually saw many similarities between the games user-experience (GUX) world and the mainstream UX world.
In a brilliant opening talk, Brandon Hsiung (Director of Insights at Riot Games) explained how he has organized his department of 70 people. A main takeaway was the benefit of embedding UX researchers within product teams, both at the feature-design level, but ideally at the higher level of the complete game. Twenty years ago, at Sun Microsystems, we made the same key point of using a matrix organization where researchers report to a central, specialized group but sit with a product team in a dotted-line relationship.
Since Riot Games has close to 2,000 employees, a department of 70 insight professionals might seem too low for the recommended share of 10% of project teams being allocated to user research. However, because Riot is both a studio (designing and implementing games) and a publisher (distributing and selling games), much of the total staff must be allocated to the publishing side. So an insights team of 70 may actually be close to the recommended 10% of the people actually building the new products.
The main innovation I got from Hsiung’s talk lies in the very name of his department: the Insights Team. The wording may seem like superficial propaganda, but in reality it makes a profound point: the goal of research is to increase company profitability by improving products and raising conversion rates. We can achieve these profitability goals only if the UX teams deliver actionable insights and drive the company’s development activities at both the tactical level (better design) and the strategic level (discovering customer needs and building products to meet and exceed these needs). Most companies’ UX maturity is not even at the tactical level yet, but to reach the strategic level, we do have to don that “insight team” hat.
I was pleased to hear that Riot’s Insights Team encompasses the company’s user research, as well as their market research and analytics. Analytics and UX should be joined at the hip, but are too often separated in different departments. And market research is usually kept even further from UX. This despite the many benefits of integrated customer insights that triangulate findings from multiple methods.
Another presentation that elicited some déjà-vu moments came from Laura Hammond from UEgroup, who talked about testing gesture-based games. She recommended avoiding swivel chairs when testing young children, because kids get too easily distracted by moving around on the chair. True, but an observation we made in 2001 in the first edition of our report from usability testing of children using websites. The kids who were 6 years old in 2001 are now 21 and thus qualified to participate in our current user research with young adults/Millennials. It’s nice to know that the next generation of children is the same, at least when it comes to swivel chairs in the usability lab.
To record the test sessions and get the gestures on video, the researchers recommended using 3 video cameras: from above, from the side, and facing the user. Exactly what we did 20 years ago in the hardware human-factors lab to record system administrators installing hard drives in servers. Testing 3D user interfaces requires more equipment than studying 2D websites.
Of course, Hammond’s talk had also new observations, specific to games for the Intel RealSense camera (which requires users to control the game by moving their hands in front of the camera). For example, the researchers needed to include the users’ hand size as one of the screening criteria when recruiting test participants. We certainly don’t ask about hand size in our screeners, and apparently, it’s a challenge to get it right.
Another insight from Hammond’s talk was that testing 3D gestures introduces yet another opportunity for the study facilitator to bias the user: the very way you sit or move may prime the user to copy aspects of your body language in their gestures.
Multiuser Testing
Unlike mainstream user testing, game research often involves testing with many users in parallel — either because the game takes a long time to play or because it involves multiple players.
(On occasion we do test with multiple users at the same time even in traditional UX projects — for example when running usability studies with young children, but mostly we run one user at a time, because we want to pay close attention to every detail of the user’s behavior. Also, a website visit usually only lasts 2–3 minutes, with a typical page view lasting maybe 30 seconds, so we should aim to study everything in detail.)
Hardcore gamers will often play for hours at a stretch, with much of their time spent repeatedly shooting at something. As a result, playtesting labs around the world seem to be uniformly designed to accommodate 12–20 game testers (or more, for big companies) who play the same game, each at their own console.
Sebastian Long from Player Research in the UK described his company’s playtesting lab: The observation room included a big projection display with reduced versions of 12 users’ screens, as well as a pushbutton switch for observers to select one of the 12 screens to be magnified on a separate monitor for high-resolution observation when one of the testers did something interesting in the game. This need to alternate between surveying many peoples’ broad behavior and detailed attention to a single person’s specific interactions is rare outside games research.
The multiplayer component of many modern games is the second reason for multiuser sessions in game research. Whether several players play together in the same room or across the network in real time, researchers must understand their processes of communication and collaboration. In contrast, in mainstream UX, even when taking into account social media and omnichannel experiences, people rarely work together at the same time with the same interface to solve the same task.
Games researchers often have access to data at true scale: in the case of the R6S kill/death ratio I mentioned above, Olivier Guedon from Ubisoft measured the ratio across 440,000 games during alpha testing and 182M games in two beta-testing rounds. In the alpha, the defending team won 61% of the time, resulting to tweaks making it easier to attack. As a result, the attackers won 58% of the time in the first beta test. Further redesigns finally made the game balanced in the second beta. A great example of iterative design and the common observation that fixing one UX problem (too hard to attack) sometimes introduces a new problem (too easy to attack), which is why I recommend as many rounds of iteration as possible.
Professional Users
In the gaming domain, some companies have to accommodate two classes of users: normal users (who buy the game and play for fun) and professional users who are paid to play the game as an “eSport.” eSports are a big business with huge audiences watching the championship games. (In 2014, Amazon.com paid almost a billion dollars for one eSports site.)
Of course, one of the oldest lessons in traditional user experience is that we need to design for both novice and expert users. Each have different skill levels and need different features. (More on this in our UX Basic Training course — it’s that fundamental a concept.) But professional users take this distinction to an entirely different level and require separate research of what happens when operating a user interface becomes its own goal and the focus of somebody's career.
Designing Fun
User satisfaction has always been one of the 5 main usability criteria: people will most definitely leave a website that’s too unpleasant. Even in enterprise software, you want users to like your design to reduce employee turnover. That said, mainstream UX research spends much time on other criteria, such as learnability and efficiency, because users are so goal oriented: they go to a website to get something done (say, buy something or read the news), not to have fun with the user interface per se.
In strong contrast, a game has no purpose other than fun. The stated goal may be to kill the boss. (No, not your manager, but a nasty gremlin or alien invader — these game enemies are referred to as bosses.) But the real goal is to have fun while doing so. That’s why it’s important to study the kill/death ratio: if designers made the game interface too good at killing bosses, that would be efficient, but not fun. (Good traditional UX; bad GUX.) Gamers need just the right level of challenge, because it’s also no fun if you die immediately and don’t get to off some bosses.
In an attempt to pinpoint exactly when users are excited or bored, some GUR researchers employ esoteric biometrics sensors. For example, they measure skin-conductance levels (sweat activity), which is related to physiological arousal. Pierre Chalfoun gave a good overview of biometrics at Ubisoft, and he emphasized that these physiological sensors are not always directly connected to user emotions, which is what we really want to design for. (The goal is engaged users, not sweaty users, even if there is a correlation.)
Chalfoun presented an interesting study of game tutorials, which showed that users’ levels of frustration, as indirectly measured by biometrics, mounted every time they failed to understand a game tutorial. First failure: somewhat frustrated. Third failure in a row: very frustrated. While this finding makes intuitive sense and may not be worth the cost of a biometrics lab, Chalfoun stressed that good visualizations of such data convince management and developers to take research seriously and invest in fixing the bad designs that caused such growing user frustration. (Without quantifiable data, it’s easier to dismiss user frustration as a minor matter that can’t hold up the release schedule.)
More Tech
Across the conference presentations, it was striking how many GUX teams make use of custom-written software. Anything from running the playtest lab to game telemetry (“calling home” with data about live play in a beta test) requires the company to allocate software developers to build special features just for the researchers.
I think there are two reasons that GUX teams seem to be more tech heavy than mainstream UX teams:

The game researchers are embedded in highly geeky companies with legions of programmers, and it’s their company culture that if you need something, you go build it.
The many game genres are widely diverging in needs, and thus require custom software to study seriously. In contrast, all websites are built on top of the browser and require the same types of interactions. This means that it’s actually possible for third-party solutions to offer, say, cloud-based analytics tools that collect most data needed to study a website, thus eliminating the need for custom software.

Age and Gender Differences
The best (but very data-dense) presentation at GamesUR was by Nick Yee from Quantic Foundry. Yee has collected data from 220,000 gamers who completed a survey about what motivates them to play computer games. Motivations clustered into 6 groups: action, social, mastery, achievement, immersion, and creativity. Obviously, different games speak to different motivations: a death-march game will attract gamers motivated by action and social play, whereas a simulation game would be preferred by people interested in immersion and creativity.
One of the main components of the social cluster is competition. In this cluster gamers care about beating other players and being acknowledged as a high-ranking player (even if they don’t take it to the eSports extreme). The following chart shows the average score on the competition metric for men and women at different ages:

Average gamer scores, expressed as standard deviations from the overall mean across all ages and genders. High scores indicate people who are more motivated by competition. Source: Quantic Foundry, reprinted by permission.

Two observations from the chart:

Men are more competitive than women. (Or, more precisely, men like competitive games more than women do.) Maybe not a big surprise.
Competitiveness decreases drastically by age. In fact, the difference between young and old gamers is more than twice the difference between men and women, and by age 50 there’s no real difference between men and women anymore. (Older women might even be more competitive than older men, but there’s too little data in this research to say for sure.)

We sometimes find differences between young and old users in mainstream UX research, but our effect sizes are usually much more modest than those in the Gamer Motivation Study: as users age, task performance using websites declines by 0.8% per year. And it’s almost unheard of to see any reportable differences between male and female users. Say you want to study menu design: the difference between how men and women use any given menu is so negligible that is has zero practical meaning compared to the difference between a design that complies with menu UX guidelines and a poorly designed menu.
In conclusion, the Games User Research Summit was a great conference with many insightful talks by top professionals. Both they and we probably think that mainstream UX and GUR are more different than they really are, but all of us should periodically reflect on the notable similarities between the two fields to make sure that we don’t unduly limit our methods to those traditionally employed in our UX niche. For sure, as persuasive web design becomes increasingly important, mainstream user researchers will need to adopt (and adapt) methods from games user research."
53,2016-03-06,"It is common for usability studies to focus on a particular design detail, and it’s tempting to start the session by asking the test user to go straight to the page you want to test. After all, why spend time watching the user navigate to this location in a study that doesn’t aim to research your site’s navigation design?
Some common examples of this situation:

A new feature on your site
The product page for a new product
A competitive study where you don’t care about the overall design of the competing sites but only about how they do one particular thing, such as specifying what lot of shares to sell in a stock trade
A measurement study of how long it takes users to accomplish a certain task, such as checking out from an ecommerce site after they already placed something in their shopping cart

Why You Shouldn’t Take Users to Target Pages
In cases like these, why not take the user directly to the feature you want to test or to the first page of the workflow in question? There are three main reasons why this is usually a bad idea:

Users behave differently when they are explicitly sent to a target page. How users think about something (here: the feature you want to test) is primed by what they’ve seen recently. (Each prior experience changes the state of a person’s brain.) Participants who don’t see all the pages leading to your new feature will interpret this feature differently than real-life users.
Getting to something is a major part of using that thing. Quite often, navigation problems can be the biggest hindrance to the use of a feature.
In real life, there’s no silver bell that rings when users finally arrive at the right page. Users treat every page with a healthy dose of skepticism: job #1 upon arrival at any page is for the user to judge whether it’s even worth the time to stay on the page and engage with it more deeply. If the study facilitator designates a particular page as the right one, you will never observe the common scenario of users arriving at the right page only to reject it because it doesn’t immediately communicate its purpose.

A common mistake in usability testing is to define the study scope too narrowly. You usually gain invaluable lessons from broader tasks and having users approach the problem from scratch instead of taking them to an artificial starting point.
As an example, we once tested a group of embedded small applications on websites. Each of these apps performed a narrowly targeted function, such as calculating the amount of laminated flooring needed for redecorating a kitchen. We didn’t care about the websites themselves (let’s say, whether the site was good at describing the difference between various types of floor covering), we only cared about the apps. This would seem like a case where it would be best to take users straight to each of the applications we wanted to study. Luckily we didn’t do so; we started each task at the homepage. As it turns out, users failed 36% of the tasks for the simple reason that they never got to the applications. Those uses who did get to an app certainly faced various usability problems and sometimes failed the task. Even so, the single biggest problem with these applications was the way they were presented on the websites, not the interaction with the features themselves. We would have missed this big insight if we had taken the study participants directly to each application.
When You May Take Users to Target Pages
After spending 440 words convincing you not to take test users directly to specific locations, let me spell out for you the legitimate reasons for leading users to a specific page in some studies:

You know for sure that you have search and navigation problems (so you don't need more data about those issues), and you want to identify other in-depth problems related to specific pages.
You are in the process of (re)designing your site and certain functionality either does not work yet or will (later) be significantly redesigned. You want to bypass these design elements and go straight to the stuff that's currently available in the design you care about.
Navigation and search design are beyond your control.

For instance, one of our clients was interested in the design of article pages and knew already that the company’s website had huge navigation and search problems (these problems, however, were outside the scope and budget of the project we were involved in). In user testing, after confirming these problems with 1–2 users and noticing that people spent the majority of the precious session time locating the article of interest, we decided to lead people to a specific article to get more feedback about the design of the article page and understand how it could be improved.

DBS.com.sg: the main page for the PayLah! service we tested during a recent trip to Singapore.

Shortcut to Target Pages
What’s the best way to lead people to a specific location in those tests where you do want to take a narrow view of an individual feature?
As an example, last month we ran a test of the PayLah! individual-payment service from Singapore’s DBS Bank. If we had done this as a consulting project with DBS as our client, we definitely should have taken a broader view, to find out how customers view the service in the context of the entire website. Or, if we had been doing a competitive study for another bank, we would also have wanted to understand how people viewed PayLay! as part of DBS. But we were conducting independent research (for our courses on Persuasive Design and Compelling Digital Copy) on how to best explain a complex new service. We didn’t care about the bank, only about the service description.
Furthermore, we had many other things to test and limited research time available in Singapore. So we decided to take a shortcut and bring the study participants directly to the PayLah! page, at http://www.dbs.com.sg/personal/deposits/pay-with-ease/dbs-paylah (shown above).
It’s a bad idea to ask users to input such a long URL: typing is error-prone (especially on mobile) and takes a long time during which you don’t learn anything except for observing the user’s typing skills. It’s also a bad idea to have users locate the page through search. While search is usually the way people find stuff on the Internet, it’s also error-prone and risks having users select a different search hit than the one you want to study. (Having users search as they please is great when you take the recommended broader research view, but not when you have chosen a narrow study.)
On the web (or on an intranet), the best way to get users directly to the destination is simply to bookmark it in the browser. Then, as the first part of the task description, ask users to “Please use the browser’s bookmark menu and select the entry called Bookmark A.”
If you want to test multiple destinations, bookmark each one and rename the bookmarks “Bookmark A”, “Bookmark B”, and so on.
Why change the the bookmark names? First, the default name may be too revealing and may prime people towards a certain behavior. Second, if you test several sites, the set of bookmarks may give participants advance warning of the different activities that they will be asked to do later in the study. Plus, it’s common that not all users will complete all tasks — because a user might be too slow, because you have so many tasks to test that you split them up among users, or because your study involves several user profiles with different sets of tasks each. Whatever the reason, you don’t want users to be overly conscious of the parts of the study in which they are not invited to participate.
To conclude, it’s best to let users wander their merry way around a website to get the most realistic data about how they approach a task. But in some studies, you can save a lot of time (in return for weaker data about the big picture) by bookmarking specific destinations and asking users to go straight to a bookmark.
(There are a lot more intricacies to running a great user study and getting optimal research insights, so we need a full-day course on Usability Testing for these additional issues.)
"
54,2016-02-28,"Along with satisfaction-rating scales or qualitative-comment analysis, the Microsoft Desirability Toolkit (also known as the Microsoft Reaction Card Method) can help designers gauge users’ responses to the aesthetic qualities of a site.
This desirability study methodology was first introduced by Joey Benedeck and Trish Miner in their 2002 paper, “Measuring Desirability: New methods for evaluating desirability in a usability lab setting.” Benedeck and Miner set out to develop a practitioner’s methodology suited to the challenges of measuring intangible emotional responses like desirability and fun. They created 118 physical cards with different product-reaction words written on each card. At the end of usability testing sessions, participants were given the deck of cards and asked to select the five words that best described the product.
(The product reaction cards are called ""Microsoft reaction cards"" because the 2002 study was done at Microsoft.)
A major benefit of this methodology is that it introduces a controlled vocabulary for participants. The variability in word choice that naturally occurs during free-form qualitative evaluation can be problematic for data analysis. For example, a researcher must determine what exactly a participant meant when she said the site design was ‘intriguing’. (A controlled vocabulary can’t remove this problem entirely, however — I encountered one study participant who seemed to have a uniquely negative interpretation of the word ‘sophisticated’.)
The full list of product reaction words is quite large and comprehensive. Taken together, the words cover a wide variety of possible reactions to features ranging from the design’s visual appeal, the microcopy used, the functionality, to the user experience as a whole. Here are a few example words:

Empowering
Approachable
Disconnected
Friendly
Irrelevant
Patronizing
Stable

To adapt this methodology to focus on visual appeal, we recommend modifying the list of product-reaction words. There are a few points to keep in mind if you compress the word list:

Which words you include depend on what you’re interested in measuring.  Include words that you expect your users to use for describing the interface, as well as words that would be relevant for your study goals. Don’t be afraid to add your own words to the list as needed.
To measure aesthetics, remove any words that are only relevant for functionality, content, or performance (such as, “Controllable,” “Too Technical,” or “Slow”).
A list of 118 words takes a long time to read and consider, even in laboratory usability studies. If you plan on using a survey format (particularly an online survey), reduce the number of words in the list. Around 25 or less would be best, and the fewer the better, as long as you still have enough words to cover the range of possible reactions. Keeping the number of words down will help decrease respondent fatigue and ensure a better response rate.
If possible, randomize the order in which the words are presented. This will help dilute the impact of lazy respondents who fail to read and just choose the first five words presented to them (which may be more of a problem in online surveys).
Include a variety of words: negative, positive, and neutral. Your results won’t be useful if you let participants choose only flattering words to describe your UI. Benedek and Miner aimed for at least 40% negative words in their original set.
 Users don’t need to interact with the design. Simply showing participants a screenshot or a design mockup can reduce distractions that could arise from their evaluations of content or functionality.

In our online survey to measure the attractiveness of flat designs, we used this modified product reaction list.

Boring
Busy
Calm
Cheap
Creative
Cutting-edge
Exciting
Expensive
Familiar
Fresh
Impressive
Innovative
Inspiring
Intimidating
Old
Professional
Trustworthy
Unprofessional

To analyze your participants’ responses, determine the percentage of participants who selected each individual word, then rank the words to identify the most frequently selected ones.

Report the top most-selected words (for example, ‘calm,’ ‘expensive,’ ‘innovative,’ ‘fresh,’ and ‘intimidating’).
Use percentages rather than raw frequencies to report the number of times each word was selected. (For example, you may report that 71% of participants selected the word ‘fresh’ to describe the design.)
If you have multiple user groups and can identify those in your participant responses, include them in the presentation of your results. Meaningful differences between the sets of words preferred by the two groups may give you insight into their different attitudes. (For example, you may report that 54% of experienced users described the design as ‘exciting’ while only 13% of novice users selected the same word.)
If you’re evaluating multiple designs or multiple versions of the same design (for example, old and new), look at the differences between the sets of words chosen to describe the different designs. (For example, you may report that 83% of the users described the redesigned app as ‘professional,’ compared with only 20% using the same word for the older version of the app.)
If the site is intended to communicate specific brand attributes, decide in advance what words correspond to your brand positioning. Then count how many users include at least one of those words in their top-5 list.
Use a Venn diagram to present how your results map to design direction words, how different designs are described differently, or how different user groups describe a design differently (see the example below).



This Venn diagram plots the words chosen to describe a flat design, as selected by two user groups — young adults (18–25 years old) and older adults (35 years and older). The diagram was created by looking at the top 5 descriptive words chosen by each age group. Words chosen by both groups are displayed in the center segment, where the two circles overlap.


References
J. Benedek and T. Miner. 2010. Measuring desirability: New methods for evaluating desirability in a usability lab setting. Proceedings of UPA 2002 Conference
C. Rohrer. 2008. Desirability studies: Measuring aesthetic response to visual designs. xdStrategy.com
"
55,2016-02-07,"Designers often rely on icons to save space and to take advantage of the speedy recognition of visuals. With increasing popularity of small-display devices — smartphones, wearables, and so on — the use of icons has likewise increased. But, how usable are these icons? The only way to know whether a particular icon will work is to test it with users.
What Do You Test When You Test an Icon?
Different testing methods address different aspects of icon usability. But what makes an icon usable? Here are 4 quality criteria for icons:

Findability: Can people find the icon on the page?
Recognition: Do people understand what the icon represents?
Information scent: Can users correctly guess what will happen once they interact with the icon?
Attractiveness: Is the icon aesthetically pleasing?

All of these issues will be critical for the success of the final design, but must be considered separately to determine how to improve an icon.
Methods for Icon Testing
There are several techniques for evaluating icon designs, and which one you use will depend on your goals and on your stage of design. The methods can be separated into 2 main categories: out-of-context and in-context testing, depending on whether the icon is shown to the user in isolation or in the context of the actual interface. More importantly, however, is choosing a method based on what you need to learn in order to move forward with your design confidently.
Keep in mind that, even with methods where the icon is presented out of context, your test participants should always be part of the intended target audience and thus familiar with the overall industry and with relevant concepts.
Findability Methods
To gauge findability, icons must be shown in their native habitat — in the context of the full interface. In-context testing can help you determine if multiple icons appear too similar and users will have a difficult time distinguishing among them, or if the icon is hidden under a false floor or in an ad-rich area and is thus overlooked.
Time-to-locate tests are the best measurement of whether or not users can easily find an icon (or some other interface element) among the crowd of the full design. In these tests, participants must click or tap the UI element to achieve a given task. Measure how long it takes people to successfully select the correct icon, as well as the rate of first-click selections (that is, how often their first click is on the right icon: wrong selections indicate that the icons are not suitably differentiable, while slow-but-correct selections are a discoverability issue).
Recognition Methods
Testing for recognition is best done out-of-context: the icon is shown in isolation, in the absence of a text label or of other interface elements. Users presented with an icon must guess what that icon symbolizes. In some ways, this is the icon version of a Rorschach inkblot test. The purpose of this test is to make sure that icons are recognizable, and that people can easily deduce the object that it depicts.
Look for common phrases and terms to gain insight into people’s initial interpretations. If users’ guesses don’t relate to what you intended to represent, ditch that icon idea and start over.
If you know that your icon will be accompanied by text, you may think that it would be reasonable to show users the label and ask them to select the icon that best represents that label among several possible options. However, we don’t particularly advocate this method, because in real life some users may actually ignore the label in the final UI and look only at the image. This testing method therefore only makes sense in cases where users would somehow already know to look for a particular functionality within an interface, and are simply trying to locate a matching graphical representation (which is not a common circumstance).
Information-Scent Methods
What matters in the end is not only whether users can recognize what real object the icon resembles, but also if they can infer what functionality that icon may stand for. In fact, as long as people understand what function a symbol represents, it doesn’t matter if they don’t know what the object is — we needn’t worry about finding an old floppy disc to present to youths as long as they continue to understand that the strange square symbol means Save.
The same out-of-context testing method used to assess recognition can also be applied to judge information scent. However, rather than simply asking people what the icon may represent, instead ask what they would expect to happen if they selected that icon. Unlike for recognition tests, you should provide some minimal contextual information about the type of system where that icon will appear. For instance, study participants may be told that a suitcase icon is part of an e-commerce website and asked to guess what the icon may denote in the context of that type of website. (Note however that no specific information about what that website may look like, nor any hints to possible functionality are actually provided to the users.) This general reference framework allows researchers to understand whether the mental model behind the iconography corresponds to users’ expectations.
Later down the road, A/B experimentation can be used to help designers assess which out of several candidate icons may have the stronger information scent. With this form of A/B testing, a percentage of users are exposed to one version of the icon on the live site, while others see another. Measure any difference in interaction rates between versions of the icon, as well as whether users click on the icon and go back to the original page very quickly. This behavior is called probing, and usually is a signal of poor information scent; it indicates that users were disappointed in the content behind the icon and hence returned to the prior page. Be sure to maintain the same position and label for the icons when testing for the optimal graphic, to make sure that no other variable produced the change in user behavior.
Attractiveness Methods
Besides testing for recognition, icons should also be tested for attractiveness, both individually and as part of an icon family. One of the common reasons to use icons in the first place is to add visual appeal to a design, but not all icons are equally good-looking.
The simplest attractiveness test is to ask people to score each icon on a 1–7 scale. If you have alternative designs of the same icon, you can also ask people to pick the most attractive from each set of alternatives and explain why they like or dislike particular images. Finally, you can show people an entire icon family and ask them to pick out the one they like the best and the least. This last test can help you avoid the common problem where most of your icons are fine, but there are one or two less attractive ones that require a do-over to better match the aesthetic of the full design.
Standard Usability Testing and Icon Testing
Standard usability testing can also reveal issues with an icon. However, keep in mind that there can be many reasons why an icon may be ignored in a standard usability test, some unrelated to the usability of the icon per se. For example, users may get distracted by some other elements in the interaction or in the site design and may not get to ever complete the task. And even if the icon design is at fault, it’s hard to pinpoint exactly which of the features of the icon are problematic: is it the case that people cannot recognize the icon, cannot understand what its meaning is, or maybe they simply cannot find it?
For example, no participant interacted with a clock icon representing Recently Viewed Pages during a study of a controversial site redesign, but it’s hard to know whether the reason was because they did not notice the icon in the header or because the clock did not have a clear meaning in that context.
Because of the multitude of factors involved, you should not rely on standard usability testing as the only way to determine the usability of your icons.
Phases of Product Development
Like with all UX research methods, when choosing a testing method for your icons consider the current stage within the project lifecycle.

Strategize: In this early concept phase, focus on methods to ideate and explore numerous design options. Out-of-context techniques for recognition and information scent are the most applicable during this time, to determine the feasibility of using icons at all and to hone in on the appropriate mental models for the icons.
Execute: During this design–and–implementation stage, focus on research methods that will continually guide you toward the best icon design for your system. Once icon designs are recognizable, focus on out-of-context testing for attractiveness until clear winners emerge. Once more of the UI is designed, transition to in-context icon-testing methods. Time-to-locate testing is helpful to quantify the findability of an icon and its placement within several potential variations of the interface. Usability testing (beginning with paper prototypes and transitioning to higher fidelity versions) can give you some additional insights about the expected meaning for icons and their discoverability.
Assess: Once the system or feature is launched, methods that measure success and allow for incremental improvements are the most applicable. Benchmark testing with usability studies and time-to-locate tests can be conducted periodically to track performance. To continually improve an icon, A/B experimentation is the best method to measure performance and determine the optimal design.

Other Considerations
As with all methods of research, be sure to avoid introducing biases in tests. Pay particular attention to the terms used in the phrasing of tasks, as they can easily prime the imagery associated with the icons. Especially for out-of-context testing methods, consider conducting the study multiple times with various ways to phrase the survey question (using synonyms, omitting branded terms, and so on) to ensure that task wording did not influence the response selection.
Not all these testing methods need to be used in order to reach a usable icon design, but they are each helpful for different purposes and at different stages of the design process. Additionally, each method should be used iteratively, to incrementally move toward a meaningful icon–label relationship, and an optimal placement within the interface.
For more information on icons and their usage in mobile designs, consider our full-day training course on Visual Design for Mobile & Tablet.
"
56,2016-01-24,"In usability testing, we use realistic tasks to reflect what users do on the site. To complete some of these tasks, users may have to provide personal information during testing. This personal information may be as simple as a name or email address, or as complex and potentially sensitive as financial or health information.
Users are often hesitant to give personal information to unknown websites, even in a testing scenario. And you as a moderator need to respect the user’s privacy. Couple this with the fact that one or more moderators and observers may be watching the users’ sessions, and that those sessions may be recorded, and the users’ hesitations can be magnified.
So, what to do?
Consider if It’s Necessary
The need for participants to use their own information varies with each design and with the goal of the testing. In quantitative testing, we rarely ask users to use their own data, as it can make the experience too variable and increase measurement error. The goal of quantitative testing is obtaining measurable results. But in qualitative testing, researchers need to assess their needs and decide on a case-by-case basis whether there is any benefit in using real user data.
If entering personal information is only a means to an end (for example, because the user needs to gain access to a section of the site), but does not determine what information is shown, using fake information will usually suffice. For instance, in a test of a telecommunication site, providing users with an address that qualifies for the services being tested may be more important than using participants’ own information.
However, do not dismiss the need for real data in user testing too quickly. Watching users type their own information can reveal more diverse issues in the interface than if all users enter the same fake information. For example, variations in the users’ names or choice of passwords can inform the design. Users will also be much more aware of any errors in the data or interface if the information used is their own.
Another reason for using real data is to make study participants more likely to relate to the assigned tasks and more involved in the study. In usability testing, we ask users to suspend disbelief and complete specific tasks in a specific order. Fake information can remove them even more from the process and make them less invested in the task. The more realistic the experience for users, the better the chance that they will engage with the design.
Provide Fake, but Realistic Information
If the information isn’t essential to the testing or if you’re running a quantitative user study, provide users with information necessary to proceed through the site tasks. This information should be as realistic as possible.
Don’t try to be funny. Providing a silly name or a celebrity name in place of a real name can be a distraction.
Consider how you provide the information for the user. For instance, in a real purchase, a user needs to look at a credit card in order to enter the credit-card number, expiration date, and a security code that may be on the back of the card. In testing, if this information is presented in a labeled list, you may make it easier to enter data than it would be in reality.
Determine what information can be the same for all users and what information needs to differ. Signing up for a newsletter on a live website won’t work if each user enters the same fake email address, unless you can set up an account on the backend that won’t trigger duplicate-email errors.
Think about what needs to be set up ahead of the study. A fake email address won’t work if users must check that email account and respond in order to activate a registration.
Ask Users for Consent During Recruiting
If participants will have to provide personal information, make sure they agree to it during the recruiting process.The more sensitive the information requested from the users, the more details you should offer at recruiting. For example, if the user is asked to fill in only a first and last name, special notice will likely not be needed. If a user will be asked to provide a financial, health, or family-related information, ask about their willingness to do so while recruiting, with an explanation of why the information is needed, how it will be used, and what you will do to protect the data.
Ask Users to Bring Real Data
In some tests, users may have to enter more information than they can be expected to remember or know. For example, in a test of an application for automobile insurance, a user’s driver-license information may be necessary. In a test of a healthcare application, participants may need to know the names or dosages of their current prescription medications or names of their healthcare providers.
In such cases, make sure to ask users during recruiting if they are willing not only to use their personal data, but also to bring the additional information they’ll need for the study.
Provide a Reasonable Alternative
In some situations, it may be possible to provide a workaround to avoid the need to share information if users are reluctant to do so. In an ecommerce study, we gave participants a budget to purchase an item of their choice.
During recruiting, we asked users if they were willing to use their own credit card, and told them it was not a requirement for participation. At the time of the study, before starting the purchasing task, we asked users again whether they were ok with using their credit card. Some who had previously said no now consented, as they had better understood what the study was about. Others decided against using their own information. For anyone who did not want to use personal data, we provided a gift card to use in the study.
Follow Through on Privacy
If a participant uses personal information in a study, ensure that the information remains private. In highlight videos, blur personal information. In reports, blur user data in screenshots or examples. At the end of the usability session, clear all cookies and cached information, including form data from the browser; do this in front of the participant, explaining what you are doing. If, during the study, users printed anything that contained their information, let them take it with them or shred it at the study location.
Better Results with Real Data
Results from qualitative usability testing will be more robust with realistic user data than with fake data. While interfaces can be tested with dummy information, the level of insight can deepen with users’ real information. Details about the success or failure of processes, forms, feedback, and information can be discovered through the variety of information used when participants enter their own data.
 
Update added March 2016. After this article was published, two papers that reach similar conclusions were brought to our attention. While we were not aware of these papers when this article was written and reviewed, it’s nice to have an independent confirmation of some of our recommendations: 
T. Zazelenchuk, K. Sortland, A. Genov, S. Sazegari, M. Keavney. 2008. Using participants' real data in usability testing: lessons learned. In CHI '08 Extended Abstracts on Human Factors in Computing Systems (CHI EA '08). ACM.
A. Genov, M. Keavney, T. Zazelenchuk. 2009. Usability testing with real data. J. Usability Studies 4, 2, 85-92."
57,2015-08-30,"We often advocate using a parallel and iterative design process to explore design diversity, because it produces much better user experiences. A cheap way of implementing this type of design process is rapid paper prototyping. Paper prototyping is not only efficient for progressing designs quickly and allowing designers to find their best-of-breed with minimal investment, but it is also fast, fun, and easy to learn.
In this article we show how Mozilla used paper prototyping as well as user research and data mining to quickly advance the UX-focused redesign of a major part of its website. (A previous article documented how this redesign had high return on investment.)
If anybody says that design iterations will break your ship schedule or that user testing is too much bother, point them to this article, because some of the prototypes progressed through 7 versions during 2 weeks. Testing with users before even breaking out the HTML editor was cheap and it showed which alternative designs worked best.
The Iterations
One of the central goals of Mozilla’s redesign effort was to improve information discoverability and findability in order to enable users to find the information they need quickly. A key measure of that success was to reduce the number of questions in its support forums.
Product-help landing pages were the top entry points for the support website, because they were accessed via the Help menu in the products. As the most-viewed page on the support website, the Firefox Help landing page pointed to a lot of useful information, but, with this design, too many people ended up in the forum asking questions. More than 30 articles were linked from the homepage, but when someone wanted information on a problem not listed, searching was the only way to find it. Instead, users needed to be able to choose a path related to their issues and find the few articles applicable to them.


The Firefox Help landing page before the redesign


Although there is no single answer to the question of flat vs. deep hierarchies, our many years of usability research indicate that with too many choices, people easily get overwhelmed. When people click the right thing first, they are almost 3 times as likely to succeed at their task.
With that knowledge in mind, in the first iteration of the design we focused on limiting the number of choices. The new design allowed people to start with either their task or their product or service, and it offered 5 most-wanted links in a box in the bottom left corner of the page. These distinct task categories (getting started, installing, protecting, customizing, and repairing) allowed people to find what they needed or to determine that the information didn’t exist, quickly.


The first version of the paper prototype for the Mozilla Support homepage: Users could start with a task (1), a product or service (2), or choose a hot topic (3).


The designer made the prototypes using OmniGraffle, and we printed them onto tabloid paper and cut them to size. Because there was no code to change, this prototyping method allowed us to make design changes quickly during testing. Firefox users helped progress the designs rapidly toward better usability, by allowing us to watch them try to find answers to top questions. Where they got stuck or confused, we redesigned.
In this early design stage, the intent was not to focus on graphical or layout issues, but rather to find out which choices we needed to present on each page and to test comprehension of labels. Any of these elements might have been ultimately expressed using other interaction widgets, such as menus or accordions.
In a later iteration of the design, shown below, task groupings for the help material were eventually moved under the products on the next layer down in the IA, in part because all tasks were not available for all products and services, and that order of layering helped manage that necessary difference gracefully. To avoid overwhelming and distracting users, we also used progressive disclosure: the different Mozilla products were now hidden under a collapsed accordion.


A later-iteration design for the Support homepage: People could choose a task (2) or expand the software row (1) to choose a product or service first. When someone clicked the question in the middle, we showed them another piece of paper like this, but with the middle section expanded (compare with the previous screenshot).


In this iteration, users loved the big icons (2), but they were confused by wording in some of the choices (3): “Participate: How you can help others” (too general) and “Feedback: Give us your suggestions via Firefox input” (too specific). We tested several other phrases until we found the wording that worked best.
Note about writing for the web: Throughout the research, we wanted discover how much (actually, how little) information needed to be shown in order for people to find the most-important things easily, because one of the fundamental principles of reading onscreen is that less is more. Reading onscreen is more difficult than reading on paper, and low-literacy is a challenge as well. Because people tend to scan at first rather than reading right away, they perceive fewer words as being more informative when they read online, and concise pages are more usable. Also, because Mozilla website visitors speak all the languages of the world, we wanted to make word translation and concept localization as straightforward as we could.
After the paper-prototype testing sessions ended, the designer made the next version in HTML. This design used strong grouping and background colors to differentiate among activity and information types. It was still a bit too complex to scale gracefully, however, so it was simplified before implementation as products, services, and tasks were added.


Early HTML design for the Mozilla Support homepage: Although this is clearly a much higher-fidelity representation, it’s important to note that the earlier low-fidelity versions (as shown above) were sufficient for usability testing of many design questions.


The final design started with the choice of products, then showed the task-based navigation after the first click, following good progressive-disclosure principles. In addition, a number of participation options were consolidated under one button, Volunteer for Mozilla Support, that allowed people to express their general interest in open-source contribution via this one entry point, then figure out exactly how to do that further downstream, where more choices and explanation could be displayed.


The Mozilla Support homepage as launched after the redesign project


Mozilla’s staff measure website interaction constantly, so the support website’s design and content continues to evolve over time, based on that data analysis.
Lessons Learned

Frequent changes are better when testing prototypes. Frequent iterations give each design change more time in front of more users, which allows more evolution to occur. In-house teams should test a few new design ideas each week.
Test alternative designs early to allow the best design components to be used on fewer pages as testing progresses.
Be consistent with fidelity, size, and colorspace. Showing a few high-fidelity screenshot cutouts of interface elements didn’t work well, because the colors attracted too much attention on predominantly grayscale pages.

We’d like to thank Mozilla for allowing us to share this experience with the UX community.
The UX Team

Susan Farrell, Senior User Experience Specialist, Nielsen Norman Group. Susan conducted the research, did data discovery, analyzed data, and made design-change recommendations.
Crystal Beasley, Product Designer, Mozilla. Crystal led the project, coordinated with Mozilla stakeholders, and played the computer during paper-prototype research sessions.
Bram Pitoyo, Web User Experience Designer, Mozilla. Bram designed the task flows and prototypes and supervised the interaction-design changes to the website. He also tested the old IA so we could compare results with the tests of the proposed new IA.

See also: full-day course with hands-on training in using paper prototyping in the UX design process."
58,2015-08-02,"One of the more frequent questions that we get asked by our clients or by our seminar attendees is: ""Is a redesign for usability worth it?"" In other words, what is the return-on-investment (ROI) of a redesign?
In recent years, we've seen a decline of the ROI for usability, most likely due to approaching a ceiling of usability improvements: With more than 20 years of web-design experience under their belt, designers have learned a few things and fixed quite a few problems. But a redesign for usability can still save you a significant amount of money.
In this article we tell the story of Mozilla's support site, which was able to get a 233% improvement out of a redesign for usability. (Here's an explanation of how the improvement score was computed.) Thus, we can say that the redesigned site was about 3 times better than the original site.
The cost involved in this redesign was 14 person-weeks or 560 person-hours.
Is it worth spending 14 weeks to become 3 times better? This depends on the hourly rate for your staff and the value of your site and thus cannot be answered in general. However, for Mozilla, which gets huge amounts of traffic, the improvement is certainly worth the trouble, as it would be for almost any big site or company that does substantial business online.
So how did Mozilla do it? What was involved in this redesign?
Who is Mozilla
Mozilla is a large, open-source, worldwide, software organization staffed by both employees and volunteers. It makes one of the most popular web browsers (Firefox), along with other useful products and services.
Pain Points
Millions of people come to Mozilla's support website every year to get help with Firefox and other products and services. When users cannot get an answer from the information existent on the site, the Mozilla staff aims to help by answering each person's question in the user-support forum, and to respond to questions as quickly as possible.
As Mozilla's website had grown organically, users were having difficulty finding information and the support staff couldn't keep up with the number of questions in the forums. Specifically:

At about 400 pages or so, the help documentation had become a difficult place in which to locate particular information quickly.
The forum staff (employees and volunteers) were having trouble responding to questions as quickly as they wanted to, because of the increasing number of incoming questions for the rapidly updating Firefox.
The forum overload was also making it difficult for staff to find time to write new help articles for frequently asked questions. More articles could help, but the growing number of articles also caused more findability problems.

Action Plan
Mozilla decided to invest in discovery and iterative usability testing in order to improve the IA of its support site. The research questions aimed to understand (1) how people (users and staff) used the support system; (2) which types of information were really important.
Top research questions

How do users and staff interact with the support system?
Which problems are the most important to address in the website redesign?
What is the most-wanted information?
Which words do people use when they search?
Which desired information seems to be missing?
How can the information best be organized and presented in order to match what users most want to do on the website?

The UX Team
The UX team consisted of 3 members:
Susan Farrell, Senior User Experience Specialist, Nielsen Norman Group. Susan conducted the research, did data discovery, analyzed data, and made design-change recommendations.
Crystal Beasley, Product Designer, Mozilla. Crystal led the project, coordinated with Mozilla stakeholders, and played the computer during paper-prototype research sessions.
Bram Pitoyo, Web User Experience Designer, Mozilla. Bram designed the task flows and prototypes and supervised the interaction-design changes to the website. He also tested the old IA so we could compare results with the tests of the proposed new IA.
The Steps
We employed a variety of research methods intended to help us understand users' needs and also to redesign the IA and the workflow on the support site:

Doing data discovery and analysis, to understand how users behave on the support site and why
In particular, we looked at a variety of data sources to identify users' top tasks, as well as difficulties that they had with the current site. The table below summarizes the methods that we used.




 UX data


Usability reports and user profiles done by others




 Behavioral data


Frequently asked questions
Traffic and search analytics




 Content analysis


Information organization and connectivity
Structure: titles, headings, groupings
Task-flow evaluation, form usability
Gaps: missing most-wanted information




 Interviews with staff 


Conduct video calls with subject-matter experts about pain points, known problems, and redesign hopes and concerns






Testing old and new navigation schemes using card sorting and tree testing to improve the information architecture
Testing variations of the most-important task flows for desktop and mobile web designs, using paper prototypes

We conducted the paper-prototype research with users in Portland, Oregon, USA. Bram designed the evolving prototypes in Jakarta, Indonesia after watching each day's sessions on video.
Bram changed the designs and sent them back to us as PDF files, which we sent to an office supply store to print for us on their large-format printer. Because of the 14-hour time difference, it was possible to work around the clock as a team most days.
By revising designs between test days, we were able to progress key pages in the design through 7 versions in only 2 weeks of testing. Prototype-design time, including testing and final revisions, took place over 9 weeks.
Although not every project can test 7 design versions in 2 weeks, this example is certainly proof that usability can be agile (whether with a lower-case a or an upper-case A) if the team is sufficiently efficient and decides to emphasize throughput and fast research methods.
Redesign Results
A 70% decrease in support questions means forum staff are less overwhelmed and able to respond more quickly.
After prototype testing, but before implementation of the new design, Mozilla staff implemented a temporary quicklinks navigation item on the homepage to test whether direct access to the identified, most-wanted content would decrease the volume of new questions coming into the forum. Quicklinks are often a workaround for poor navigation structures, so we don't recommend them, but in this case it was important to test those key research findings at scale before implementing the new navigation scheme.
As a result of the website redesign and content improvement effort, the Mozilla Support help documentation was expanded to sufficiently address the most general questions we discovered. Because articles on basic and frequent issues are more findable now, visitors ask fewer questions, and their questions are about more-specific topics.

This graph from the Mozilla KPI Dashboard shows the volume of questions (in solid blue) in the Mozilla support forum before, during, and after the 3-month UX activities (red box).

Providing easy access to the most-wanted information caused the volume of new help questions in the forum to drop immediately by about 70% (from about 7000 to about 2000 questions per month), allowing forum staff to exceed their quality goals. A month after the UX activities, Mozilla staff revised the support documentation to make it more findable, and ongoing data analysis allows the hot topics to change as needed.

The redesign increased the 24-hour response rate remarkably, from 40-60% of questions answered to 80-90%. This view of the 4 months during the UX activities and the initial redesign implementation shows the incoming forum questions in blue with a shorter, overlapping, green, answer volume. The drop of 5000 questions per month is good in itself, but having question and answer rates converge on the right (as they do) also shows a good match between user needs and customer-support capacity after the redesign.

Lessons Learned
Support websites are made of answers to frequently asked questions, which are a moving target. By periodically analyzing user data of various kinds, customer service can document the standard answers that users need, allowing support staff to focus on new issues and questions that require unique answers.
Data is key to gaining needed resources. Data mining proved the problems existed, and analytics proved the solutions worked. It was easy for support-site stakeholders to gain the resources and momentum to implement recommended changes once some of the fixes measurably addressed their pain points. This early success led to more support for UX activities and new hires: an information architect and a content manager, who now optimize content, navigation and search.
Collaboration over time helped the UX team, support staff, and developers, who were located in many places around the world, to share knowledge and develop a shared vision. This unification of purpose and concerns while working closely together to solve problems resulted in a great website design, one that helps meet everyone's needs.
Clearly, Mozilla's investment in UX design for its support website has paid off beautifully, and it showed measurable improvement almost immediately. We hope this example helps you make the case for user-experience ROI and paper prototyping — for iterative, user-centered design.
We'd like to thank Mozilla for allowing us to share this experience with the UX community.
Learn more about ROI in our ROI for Usability report or in our course on Measuring UX.
Related courses

ROI
Prototyping
Analytics
User testing
Information architecture
"
59,2015-04-05,"It’s usability-study day. Your first participant walks in. You brief him, explain the session, and dive into the study. Only to realize that the participant does not understand what you’re asking him to do. It’s not a problem with the site design — it’s a problem with the study itself. You’ve just begun your study and you’re already digging yourself out of a hole, trying to quickly determine how to salvage the session. Now what?
When running qualitative usability studies, we often recruit only a handful of users. Small numbers of users can lend great insights into design and usability strengths and weaknesses. So if users struggle not with your site design, but your study design, it can be difficult to get the necessary results. Session data may need to be thrown away, depending on the severity of the problems.
Pilot testing can go a long way to alleviating such problems. In a pilot test, the usability practitioner (and team, if possible) runs through a session or two in advance of the main, scheduled study. Typically only a small number of sessions are needed to prepare for the full study and make sure everything is in order. The point of the test is to test the study itself, running 1–2 sessions to help ensure that the full study goes as smoothly as possible
Pilot testing is particularly important if you are:

New to running a usability test. Better to have your first try with a session that you can throw away if needed.
Testing in an unfamiliar subject area. If this is your first test of a site aimed, for example, at rocket scientists or nuclear physicists, and you aren’t an expert in the area, pilot tests can help prepare you. (Also do work ahead of the study to familiarize yourself with the topics and terminology, for instance by meeting with subject-matter experts as well as designers and developers.)
Running a remote, unmoderated study. Any time that study instructions need to stand on their own, they need to be tested to try to limit possibilities for misinterpretation. If you are running a diary study, unmoderated online study, or study conducted via email, all communications, from recruiting information to confirmation emails to study instructions to follow-up questions, should be pilot tested. These instructions need to stand on their own, as no one will be there to answer questions or make clarifications if a participant runs into a problem.
Running a quantitative study. Larger scale studies are typically done so that statistically significant results can be calculated. In such studies, each session needs to be run the same way. As such, a solid script must be created and tested.
Testing a high-visibility project. Even if you’ve worked in usability for years, there may be the occasional high-visibility project that requires some extra care. Maybe the results are going straight to the CEO. Maybe the test is on the company’s premier product. Of course, every usability project is important, but some may be a bit more important than others.
Doing a one-shot research project. If you’re doing many rounds of iterative testing, then the damage is limited if you make a few mistakes in the first test. The second study will be better. But if a design project only gets a single dose of user research, you want to get it right the first time, because there won’t be a second time.

Even veteran usability practitioners can benefit from running pilot tests. The longer you work in the field, the better you can get at writing strong tasks and instructions in the first place, but it never hurts to run a test plan past a participant in advance. Does every usability study require a pilot test? No, but it’s extra insurance that the study will run smoothly, leaving the team able to focus on the results, rather than the study itself.
Benefit #1: Dress Rehearsal
Pilot testing is a dress rehearsal for the study. It’s a great dry run to make sure the facilitator and team are prepared for the study. Are the materials printed? Consent forms copied? Payment prepared? Site ready and functional? Checklists can help, but running through the study in a low-pressure setting first is a good way to double check that the team is prepared.
Benefit #2: Test the Tasks
If a user is derailed by a poorly written task, it takes precious time away from studying the interface. The facilitator first needs to realize that the participant didn’t understand the task as it was intended, and then has to spend time redirecting or reinstructing the user. Any on-the-fly changes run the risk of accidentally giving users clues about how to complete an activity, or making participants feel they did something wrong. Tasks that can seem perfectly clear to you and your team can be confusing or misleading to participants. Find and fix issues with tasks before your study to help ensure a smoother test and stronger results.
Benefit #3: Timing
It can be hard to know how much time to schedule for user testing. Will participants complete 3 or 7 tasks during your testing session? Running through the tasks yourself and allowing more time than it took you is a very rough way to estimate task time. Running through the full study with 1 or 2 pilot users can help you better estimate how long users might take, allowing you to prioritize tasks effectively for the real study.
Benefit #4: Data You Can Use (Maybe)
If all goes well, or pretty well, in a pilot session, you’ve gotten a jump on the rest of the study. Run the session as though it’s the real thing, and if there are no major hiccups, the data can be used. Even if one or two tasks go awry, you may still be able to salvage information from the sessions. Don’t be too quick to dismiss any learnings just because it was a pilot test. At the same time, don’t be overeager to use the session as part of your final learnings if the session does not go well or something goes wrong at the beginning that might affect the reliability of the rest of the session (such as leading the user or overexplaining an offering or interface.)
Tips for Pilot Studies
To get the most out of a pilot session, schedule it at least 1 day in advance of the scheduled study, and longer if you are testing instructions for an unmoderated study where you might need to pilot instructions several times before they are solid enough to use. This allows you and your team time to make any necessary changes. (Assume there will be changes, because there always are.)
Recruit participants who match your target profile for pilot studies. This means that any feedback — about the study or about the site being tested— will be more relevant. In a pinch, recruiting someone who doesn’t quite fit the profile is typically better than not running a pilot test at all, but the results from those sessions then would not be applicable in the final study.
Does running a pilot study take extra time? Of course. Materials need to be ready earlier, extra participants need to be recruited, and time needs to be scheduled to run the sessions. The payoff, however, is that the final study will run more smoothly, making it easier to get the results that matter to your team and, ultimately, your product.
"
60,2014-12-07,"So, Your Site is Ready for a Redesign
Where should you start? There are many UX activities that can help your project start off on the right foot. One of them is competitive usability testing. That is, usability studies of the existing design along with several competitor websites. Resist the urge to jump on your computer and start designing right away.
Even if a redesign (or refresh) is required, don’t be so quick to throw away the existing design. You can learn from it. Use it as a starting point for your new project by gathering user feedback and feeding it into your new design. Before you begin, know how aggressive a change you need. For Agile teams, competitive testing is a worthwhile activity to perform during sprint zero.
Your old site is the best prototype of your new site: it’s already fully implemented and it solves exactly the design problem you’re targeting: a website for your business.  For sure, the old site doesn’t solve the design problem perfectly, but it’s not enough to say that it’s bad. You need to know in which specific ways it’s bad. And you also need to find out what aspects of the old design are loved by users. The short history of the Internet is rife with examples of major web properties that redesigned only to be met by a storm of user requests for great features that had been inadvertently removed.
Competitive usability studies provide a way for you to assess different design patterns and user flows, and determine which concepts might work and not work for your audience. They allow you to test ideas without having to build the designs yourself and to discover new interactions for your site and avoid mistakes made by others. Usability studies lay the groundwork for the redesign process and keep you and your team focused on the right issues. You feel more confident about your decisions because you’ve witnessed people interacting with websites in similar situations.
We’ve all wondered about our competition — How well are they presenting information? How are they better than us? It is good to browse websites for inspiration. However, don’t stop there. Test them. If you find concepts and ideas you like, don’t rush to copy them. The design might look good but might not be usable. Minimize risk by testing designs and know for certain whether or not they work before it’s too late. Resist the temptation to emulate without proper knowledge. This is why we often find bad design replicated across many different websites.
Learn From Competitive Usability Tests
Teams waste much time debating over design solutions and making decisions based on personal preference or bias. Rather than continue the debate, competitive testing allows teams to get feedback from target users early, so they can make informed decisions. It’s much easier to argue over opinions than user data.
Competitors’ sites are the second-best prototypes of your new site: they solve almost the same design problem as you have, and they are fully implemented instances of designs that these other companies have invested major resources in polishing.
Competitive tests can help you with the following:

Evaluate future features. Before you offer or build a new feature, learn whether customers consider it valuable or how it could be designed better. Seeing how customers react to features on the competitor’s site can help you determine whether they’re worth the effort.
Examine similar features. Your site may offer features similar to those on competitor sites, but one process might work better than the other. By examining varying designs of similar features, you can quickly identify the elements that work and make your design better while avoiding mistakes made on other websites.
Discover better ways to doing things. Your site might test well, but testing other sites might reveal features and interactions that you haven’t thought of. Having people react to different designs gives them an opportunity to compare and contrast. Participants are often better at articulating their thoughts and retrieving memories when they have several examples to which they can refer.

Conducting Competitive Usability Tests
Competitive studies don’t need to take long; usually 2 days are sufficient. Spend a few days studying your website along with 1-2 competitive websites and the study will yield fruitful findings that answer complex UX questions and provide inspiration for new ideas. 2 days is quick—a small price to pay in the context of a project timeline.
We run most competitive usability tests based on the thinking-aloud methodology. In this type of study, each participant performs the same tasks on every website (e.g., Site A, Site B, and Site C) while thinking out loud. We observe how people interact with each website and note verbal feedback. At the end of each session, we may ask users which version they prefer and why. The key is to understand the rationale behind people’s behavior.
To combat order effects, alternate the sequence of the websites that participants evaluate. For each participant, keep the number of websites they evaluate at 3 or less. When evaluating too many sites, tasks become monotonous and difficult to track for participants.
Select sites with interesting features. The sites you select do not need to be direct competitors. It’s best to aim for diversity by comparing sites that have features and designs distinctly different than yours. Don’t waste your time testing sites that you know are bad. The study will generate more valuable ideas when you include sites that might outperform your site. It’s OK to lose now you will win later.
Conclusion
Before you redesign your site, make sure that you understand the strengths and weaknesses of your current design. Garner design ideas and alternatives by studying your competitors. The focus of competitive tests is not to crown a winner, but to gain deeper insight into why design elements work or fail so we can make informed decisions moving forward."
61,2014-09-01,"Doing user testing by asking users to think out loud is the #1 usability method. If you only perform one user research activity, then run a thinking-aloud study.
Basically, you ask the test participant to continuously verbalize his or her thoughts and keep up a running monologue as they use the computer. Easy. And yet. People can do it, but it’s a bit unnatural to just sit and talk and talk and talk — having to say anything that comes to mind without it being part of a conversation. One of the main things we teach in our Usability Testing course is how to facilitate a test and prod the user to keep talking. (Use neutral prompts that don’t bias users’ behavior — important but not my topic today.)
Most users need some prompting from time to time, but first you’ve got to get them to start talking in the first place. A great way to do so is by showing the test user a short video of somebody else thinking aloud. Demonstrating something by a concrete example gets the point across better than most abstract explanations.
What to Put in the Demo Video
Some criteria for a demonstration video of thinking aloud:

The video should be short, because you have a tight time budget in any research study and want to spend almost all of it observing the user’s actual behavior and very little time on administrative overhead. 1 minute is a good video length. (We tried a 2-minute video, but it was too long and didn’t add any real value over a 1-minute video.)
It should show a different user interface than the one you will be testing in the study. Otherwise you’ll bias the user. Some people like to do the demo with a design that’s not even a computer: for example, how to refill a stapler. I prefer showing a screen-based UI, because then we can also demo the kinds of comments we’re looking for.
In order not to intimidate the study participant, we don’t include a picture-in-picture of the person doing the demo. While we definitely should disclose to study participants if they’re being video recorded, we don’t want to belabor the point and make them overly conscious of having their image appear in the study recording.
The person recording the demo should obviously be an outspoken “good participant” and keep talking throughout the demo. It’s worth rehearsing your demo think-aloud a few times before recording it. For privacy reasons, don’t use a real user; use your own staff to record the demo. Plus, use somebody who speaks with a clean, easily understood, audible voice.
Demonstrate key points you want to learn from a usability study, such as
	
This is what I expect will happen if I click here
This page or resulting action is not what I expected
This text/instruction/image/etc. is confusing
This is easy/difficult
This is what I think this text/instruction/image/etc. means
I like/dislike this
This is not what I need — I need XX instead


Don’t bias the user by focusing on a specific point you want feedback on. Say you’re interested in the style of images on your site. Sure, have your demo include a comment about some image, but don’t limit the demo to discussing the images or diving deep into an image analysis. If you prime users to think in a certain way, they’ll do so for the duration of your study, but your findings won’t represent real use.
Don’t be funny. Even though it’s tempting to joke and produce a lighthearted video, stay serious because you want users to take the test tasks seriously and perform them as if they were visiting the site for real.

Sample Thinking Aloud Demo Video
Here is an example of a demonstration video that illustrates what it means to think aloud during a usability study. Feel free to show our video to your own study participants if you don’t want to make your own.
Having an online demo video is particularly useful if you're doing remote user testing with narrowly-targeted study participants who are recruited from a screener but haven't tried a think-aloud test before.

 
In most browsers, hover over the video to display the controls if they're not already visible.

"
62,2014-08-10,"A website’s (or product’s) success depends on how users perceive it. Users assess the usefulness and ease of use of websites as they interact with them, forming their conclusions in seconds—sometimes milliseconds.
Users base their decisions on whether to engage with a site based on questions like, “Does it have value to me? Is it easy to use? Am I delighted by the experience?” A good user experience leaves users answering ‘yes’ to all of these questions.
What is User Experience (UX)?
User Experience (UX) is a common term in the design community, but its definition is somewhat elusive, even amongst the UX community. The founders of Nielsen Norman Group, Don Norman and Jakob Nielsen, defined UX as follows when they started the company in 1998:

The first requirement for an exemplary user experience is to meet the exact needs of the customer, without fuss or bother. Next comes simplicity and elegance that produce products that are a joy to own, a joy to use. (See full UX definition)

In other words, UX teams and practitioners should strive to create products that users want and need, and design them in a way that is easy and joyful to use. User experience is concerned with everything that affects users and their interaction with the product.
Of course everyone wants to achieve an exemplary user experience. However, in practice, many organizations fall short of understanding what is required to make this happen. Although the field of user experience has gained popularity, bad design practices still exist in many organizations.
The User Experience is Everyone’s Responsibility
Having a UX department or UX title does not mean you are practicing UX. To achieve an exemplary user experience, coordination must be achieved among multiple disciplines, including product management, development, marketing, content, customer service, graphic design, and interaction design. In other words, everyone is responsible for looking out for the user. Take users’ needs into account during every step of the product lifecycle, by keeping your users at the center of your design efforts.
An orchestrated approach across many disciplines and stakeholders must be achieved to create a truly effective user experience and for the company to thrive. For a product to be truly successful, user-centered design must complement (or even drive) business objectives.
Bridging Business Goals with User Goals
Business priorities often lack the reality of user needs and decisions are made based on what we “think” is awesome rather than what is “truly” awesome. UX has strategic aspects that involve a deep understanding of the business, the users, and context in which they operate.
One of the biggest complaints from UX professionals is the lack of support from their organizations for UX-related activities, such as user research. The problem is amplified with a quick-release product-development framework such as Agile. Even worse, Agile development methods such as Scrum typically do not include UX designers as a core role, leaving many organizations to assume that UX is nonessential. This is a tragic mistake.
My latest interviews with Agile team members reveal that most teams are not performing user research on their concepts or designs. Respondents cited time constraints and lack of UX resources among the top reasons for this trend. Lack of user research could also happen with any product development model, including waterfall. Regardless of the method, organizations are shipping products without knowing their true value to the customer. It doesn’t matter how many products we release. If they’re junk, we’re simply shipping a lot more of it.
Paying UX Lip Service
Projects bring with them pressures of all magnitudes. We’ve all had to take shortcuts and rely on our own intuition. However, a user-centered process is something to be valued long term as it can help us achieve our goals.
We must actually support design decisions and business strategies based on real user data, not on our own experiences or preferences. When user-centered practices are executed, designers and researchers can find the design “sweet spot” where business needs and user needs overlap.
Empower UX designers and researchers to influence business priorities so your project does not fall into the “just ship it” trap. Also, structure the organization and cultivate its culture so that everyone—including executive management—feels connected to the goal of solving customer needs as a way of achieving business priorities.
There are many ways to evangelize usability. One is to let research participants do it for you. Find ways for naysayers to observe a usability study. After seeing the pain that your customers go through, it is difficult to challenge best practices or to continue designing blindly.
Do the Right Research, Correctly
Sometimes research is done with good intentions, but the methodology or execution is flawed. Far too frequently, companies contact Nielsen Norman Group in a panic to help them figure out why their new website is failing. When we ask them whether any usability activities were conducted prior to launch, the answer is often “yes!” So why is the redesign performing dismally? The main reason is poor methodology.
Common methodology mistakes:
Asking the wrong people for feedback: Your stakeholders and colleagues are not real users. They are not representative of your target audience. Involve the correct users in the design process, rather than having internal colleagues be participants, as they tend to be too close to the design to provide honest and accurate data.
Leading the witness: It’s natural for inexperienced facilitators to bias the study with leading questions and draw erroneous conclusions. Usability testing produces much more valid information when conducted by someone who understands the science of behavioral research.
Applying the wrong research method: User testing can mean different things to different people. It could include methods such as surveys, focus groups, A/B testing, usability testing, and so on. The method you choose depends on the questions you have and the stage of development. A survey is often appropriate for collecting opinion-based data, but not for interaction. Applying the wrong method can yield erroneous conclusions.
When conducting research studies, make sure that you know how and when to apply the correct methods to avoid bias and to get the right answers.
The Added Benefit of User Testing: Team Building
You probably are already aware of the main benefit of user research—gaining user data to inform your design and product decisions. However, there is a strong side effect of this activity: building consensus.
Clients often ask me to perform an expert review of their designs or concepts. I am happy to do it. However, I sometimes suggest running a usability study to help dissolve internal disagreements and speed up development time. It’s often better to test different theories then go in circles with internal debate.
Providing usability and design guidance is only part of the job of a UX designer or UX researcher. The other is to help teams move forward in a productive manner. The best way to do this is with data instead of opinions. It’s easy to override design solutions seeped in conjecture, but very difficult to challenge an option that is supported by evidence.
Sometimes the best way to end a meeting where assumptions are being made is to state, “Let’s test it!”
Follow that statement up with a lightweight user-research study. Not only will you have quickly either proven or disproven the assumption, but you will also have shown clear value for user research.
For example, there was recently a great debate among the people who design higher-education websites about whether a certain university’s new site was good or bad. Instead of debating, one of our UX specialists spent a single day testing that site and easily discovered the true answer (it’s bad).
Sketch and Test (then Repeat, if Necessary)
The notion of iterative design is not new (see our article from 1993) and the Lean Startup movement which emphasizes running experiments to test your design vision has helped increase its awareness. However, the mantra “Design, build, launch, measure,” commonly chanted by Lean converts, can be misleading. It assumes that a product must be coded and launched for systems to be tested with users and measurements taken. This is a misconception.
For many years we’ve helped clients speed up development time by testing sketches and wireframes first, before a single line of code is written. Don’t waste resources building something functional only to find out too late that it is not what people want or that it contains major flaws. Consider the questions you have about your concept at every phase of development and create the simplest artifact to test your assumptions. Practice makes perfect. Don’t expect to get it right immediately out the door.
Conclusion
User experience cannot exist without users. Creating user interfaces involves intricate and complex decisions. User research is a tool that can help you achieve your goals.
Even the most well thought out designs are assumptions until they are tested by real users. Different types of research can answer different types of questions. Know the tools and apply them accordingly. Leaving the user out is not an option.
To summarize this article in one line:
UX – U = X
(where “X” now means “don’t do it”).
Focusing on user experience can differentiate you from your competitors. Learn how at our course on UX Basic Training.
"
63,2014-02-09,"Many people think that it takes special skill and fancy equipment to run a user-testing session on mobile. Not true. If you’ve ever run user-testing sessions with regular computers, setting up a usability study with mobile devices is a breeze. You need to take care of a few extra details: equipment, the right testing room, and the right users.
Equipment
One of the daunting problems in user testing with mobile devices is: how do you  record the screen? In studies run on a computer, you can use a a screen-recording software such as Morae, Camtasia, or Ovo Studios, but what should you use for mobile devices? Screen-recording apps for mobile are still in incipient stages; often they require a jailbroken phone or don’t allow the user to take full advantage of the phone capabilities (for instance, UX Recorder for iOS only records the screen if the user uses an in-app browser). Screen-recording software rarely provides finger footage: the researcher will have to use a separate camera to record the study-participant’s fingers, and then possibly combine the finger and the screen streams.
For all these reasons, recording with an external camera is still the preferred method when doing user testing for mobile. Almost any video camera can be used, provided that it will stay focused on the screen while the study participant is using the device. Although it’s possible to fix a regular camera on a tripod right above the mobile device, most mobile-usability researchers prefer either a webcam or a document camera because they usually come with software that allows real-time projection of the video footage on a computer screen. That allows the facilitator to witness what the study participant does on her phone without intruding in her personal space. In our mobile-usability studies, we have used 2 types of cameras: a document camera and a webcam. There are advantages and disadvantages to each of them.
Document Camera
This solution somewhat restricts the movement of the phone during the session. Since the camera is fixed, study participants must maintain their phones within the camera’s range. They can do that either by keeping the device flat on the table, or by holding the phone in their hands (although in this last case the resulting video quality is suboptimal). They can also easily (and naturally) switch the device orientation from landscape to portrait as needed.


The document camera records the phone screen; the capture is projected live on a testing laptop.


Document cameras are available in a wide range of cost and quality. We have used Elmo (at the higher end of the spectrum) and iPevo (at the lower end). Better document cameras have manual focus, a variety of image controls, and the ability to zoom in without substantially altering the quality of the resulting image. They also have an adjustable, preferably tall neck that allows a wide range of recording and is suited for both tablets and phones.
Webcams and Mobile-Device Cameras
Webcams are another class of devices that can be used for mobile- or tablet-screen recording. These are usually installed on a cradle that can be either self-fabricated or purchased. The user then places her phone on the cradle (sometimes using Velcro to attach it) and holds the entire contraption in their hands, in a position that is more naturalistic than when the phone rests on the table.
How big the cradle is will influence the ability to record tablet screens (small cradles do not work well with tablets). Another essential quality of the cradle is lightness: since the participant is supposed to hold the camera, the cradle, and the mobile device in her hands, the resulting apparatus should be as light as possible.
The combination cradle plus webcam is sometimes called mobile-device camera; the web is full with instructions about building your own cradle. You can also buy either the cradle only or the entire package¬: cradle plus camera.
The webcam needs to allow manual focus and zooming. (With autofocus, the camera tends to focus on the participant’s fingers rather than on the device screen.)
Although at first the mobile-device camera may seem more appealing than the document camera (both in terms of cost and flexibility of device positioning), in practice there are some big disadvantages. First, the mobile-device camera is often too heavy (at least 200 grams), and users tire quickly and place it (and their phone) on the table. Second, the image quality is often less good than with a document camera. Third, often this solution is not suited for tablets: due to the larger surface of the tablet, the cradle needs to be taller than for a mobile phone to support a wider recording angle, and the resulting contraption becomes too heavy. Last but not least, the camera can block the view in landscape mode. There are some solutions to deal with this last issue — for instance, if the phone is not attached with Velcro to the cradle, the user may be able to switch it to landscape without turning the entire cradle and blocking the view with the camera. But that means that she has to use extra force in her fingers to keep the phone stable on the cradle while she’s working with it.
Software
Whatever camera you use, you will need some way to project it on a computer screen to allow the facilitator to follow the participant as he is working on the device. Document cameras and webcams usually come with software that projects the camera capture live on a computer screen. Once you have that live capture on a big monitor, you can take advantage of techniques normally used in traditional usability testing to record and broadcast the computer screen — for instance, you can use your favorite screen-recording software (Morae, Camtasia, Ovo, or even GotoMeeting and Webex).
Up to 2 observers can watch the video on a laptop screen by crowding close to the facilitator; with more observers use the laptop's external-monitor port to project the video feed to an external monitor or projector.
Some cameras come with their own video-recording software; if that is the case, you can skip the screen recording and use the camera software instead. The disadvantage is that often these programs require more computing power than your regular screen-recording software.
Testing Environment
The room in which you set up your testing session needs to satisfy a few extra constraints to be suitable for mobile testing. A regular user-testing lab will be nice, but is not necessary unless you plan to have many live observers.
First and foremost, you need to be able to control the sources of light in the room. That means either a windowless testing room or one with darkening blinds. To avoid glare, there should be no source of light directly above the testing device. In most cases, turning off the lights and using side table lamp(s) works well.
The brightness of the camera, device screen, and the monitor on which the device screen is projected will all influence the quality of the live projection and of the resulting video. You should be able to easily adjust these at the beginning of the testing session and, if necessary, during testing (although it’s best to avoid interrupting the participant during the tasks with a request to adjust device-screen brightness).
You will also need to make sure that you have a good cellular signal in the testing room, as well as a high-speed wireless network available. We usually recommend that you test apps and sites on both cellular and WiFi networks, as these different tests often provide complementary information. Because connectivity is usually worse on cellular signal, you can see how your app fares on a poor connection. In contrast, with a good WiFi, you can focus on interface issues: Is that button salient enough? Can the user understand the content?
Unlike for traditional user testing, for (at least some) mobile studies it may be ok if the testing room has some nearby employee traffic and occasional noise. Often, mobile devices are used in interruptible environments, so sporadic, nondeterministic external sounds may create a slightly more naturalistic situation.
Users
Unless you plan to study the learnability of a new device, we usually recommend that you  recruit people who are familiar with their devices and have been using them for at least 3 months. New users often do not exhibit typical behavior — they may not know how to use their device yet, or they may not be familiar enough with conventions specific to the operating system.
Unless you're Apple or Google, it's less important to you how people fare the first few times they use a new phone or tablet. If you have a website or mobile app, almost all your users will already have learned how to use their device by the time they get around to using your site or app.
Some people own several phones or several tablets. When you screen participants, make sure that the questions specify the device that you want to study. For instance, if you want iPad users who shop on their device, you may want to ask “How often do you shop on your iPad?” rather than “How often do you shop on your tablet?”
If you want users to install apps from an app store on their devices, make sure that:

They know how to do it (this can be an issue especially for tablets, which are often shared devices).
They either know their App Store password or they will bring it for the testing session.
They have a credit card associated to their account (if you want them to download paid apps during the study). If they don’t have one, be prepared to give them a gift card that they could use for purchasing the apps —and, of course, reimburse them right away for any such purchase.

Other Parts of Testing
For anything else (screening, task creation, facilitation, and  reporting), you can use the skills learned from regular user-testing sessions. The most important point about a mobile usability study is that it's a usability study, not that it's mobile. We already have wide range of  articles on user testing, as well as  full-day courses  with in-depth hands-on practice in study facilitation.
Conclusion
In order to setup a mobile-usability study, invest in a good document or mobile-device camera and make sure that you have a testing room with proper lighting. Connect the camera to a testing computer and use existing screen-recording software to record and broadcast the sessions. Finally, take advantage of prior knowledge about running and analyzing usability studies.
One thing that we've learned from running hundreds of mobile usability studies — whether for clients or to get examples from our  courses on mobile and tablet design — is that the usability requirements increase as the platform shrinks. Smaller screens equal bigger needs to test your design with real users, because there are more ways for users to fail. Since it's easy enough to run these usability studies, it's even more important to test your design when you're targeting mobile users."
64,2014-01-26,"Introduction
""Echo. Boomerang. Columbo."" No this is not a bastardized version of the NATO phonetic alphabet, rather it’s a handy way of remembering 3 safe and productive approaches for interrupting or answering users during usability tests and other research studies.
Facilitation Techniques for Handling Test Users’ Questions During Usability Studies
How does an unprepared usability-test facilitator respond when the user asks her a question or offers an indecipherable comment? Usually, not well. Admittedly, facilitating a usability study is not a natural way to interact with other human beings. So it is totally understandable why most of us have trouble facilitating, making classic mistakes such as:

Commence nervous chatter or panic and fumble for words, worrying about the negative impact they may be making as they speak.
Treat the test session as a conversation rather than an observation.Talking too much, at inappropriate times, or leading the user can affect what he does and says, which can invalidate part or all of the research findings. Interviewing methods are different from observational methods. While interviews can reap fruitful information about people’s desires and impressions, direct observation of live user behavior is an invaluable research method when trying to learn what people actually see, act on, and how they interact.
Go the complete opposite direction and stay completely silent, succumbing to fears about interrogating the user. Sitting completely mute is probably better than saying too much, but it's not an advanced facilitation practice as it doesn't enable gathering the most possible information during a study. Probing at the right times, without making mistakes (such as leading the user or asking closed questions which beckon only yes or no answers) can reap rich responses and lead to insights about exactly why a design is or is not useful and usable.

There is no need to panic or blather on when a user asks a question. Instead try the echo, boomerang, or Columbo techniques described here.
Echo
With the echo technique, the facilitator repeats the last phrase or word the user said, while using a slight interrogatory tone. Using the exact word(s) that the participant used ensures that the facilitator does not bias the participant by making a suggestion or describing anything in the interface. Instead, they just parrot and probes in a benign way. Here are two examples of good echo technique:
User: This table is weird, well, hmmm, not sure what, uh…
Facilitator: Not sure what?
Or
Facilitator: Table is weird?
Say these few words while using a tone that makes it clear that the phrase is a question. This will naturally put the user in the mindset of answering the question by elaborating on what he meant by those same words.
Echo Audio Example (Note that I play both the user and the facilitator.)

 

Boomerang
With the boomerang technique, the facilitator formulates a generic, nonthreatening question that they can use to push the user’s question or comment back to him. Examples of such nonthreatening questions are “What do you think?” or “What would you normally do?” So if a user asks a question such as, ""Do I have to register to buy this?"" the facilitator should not say, ""Erm. I guess so,"" or whisper, ""No, that’s okay."" Rather, the facilitator should try the boomerang technique. For example:
User: Do I have to register to buy this?
Facilitator: What do you think?
Or
Facilitator: What would you do if you were at home now?
Or
Facilitator: What would you do if you were really doing this on your own?
The facilitator does not answer the user’s question; rather, they bounce the user's question back to them. The idea is to remind the user that they are to try and work out the issue as they might if they were not in a research environment.
Boomerang Audio Example (Note that I play both the user and the facilitator.)

 

Columbo
With the Columbo technique, be smart but don't act that way.
Peter Falk immortalized the character of Lieutenant Columbo in the late 1960’s television series in which he caught a myriad of criminals, mainly by enticing them to underestimate his investigative skills. Columbo seemed forgetful and inarticulate, but he was actually perceptive and astute.
While facilitators in a usability study aren’t trying to catch anyone as Columbo was, they are trying to craft tasks and questions in a way that coaxes people into saying what they think and into doing what comes naturally. Facilitators can take a page from Columbo’s handbook and act less like experts, and more like investigators. One way to do this is to ask just part of a question, and trail off, rather than asking a thorough question. This may sound strange, but it achieves the following:

Saying fewer words means the facilitator is less likely to teach or sway the user.
Forming less complete and perfect questions and pausing can cause the user respond more quickly, so they don't have to wait for the facilitator to finish they thought. And some users try to help the facilitator by answering before a question is fully formed.

So if a user asks a question such as, ""If I close here will I lose my work?” the facilitator may be tempted to, but should not say, ""I am not sure” or “Try it” or “I don’t think so.” Rather, the facilitator should try the Columbo technique. For example:
User: If I close here will I lose my work?
Facilitator: Uhm, you are wondering if [pause] you might [pause.]
User: I am just not really sure if I should pick ""close"" or ""cancel"" or ""ok."" I guess I don't know the difference between these buttons.
Columbo Audio Example (Note that I play both the user and the facilitator.)

 

A facilitator may also apply the Columbo technique effectively when they want to initiate asking a post-task question. For example, they may be tempted to but should not ask the user, ""Did you see the filters on the left?"" Instead they might try this:
Facilitator: I was wondering if you could look at the page [pause] and this section [pause and motion to an area in the interface.]
Deciding Whether to Speak to the User
Don’t mistake any of the tips mentioned in this article as license to interrupt the user any time he makes a sound. Instead, follow these guidelines before talking to the user:

Decide whether what the user said was a real question that you actually need to answer, or it was a rhetorical question, or just thinking aloud.
Determine whether the noise or comment that the user made was indecipherable, or whether it was actually enough to draw a fair conclusion from.
Consider whether you will truly benefit from probing the user further, or whether you have enough information from just observing what he is doing.

Key events which signal an appropriate moment to interrupt a person during a test session occur when the user has:

offered some comments
asked the facilitator a question or is otherwise seeking guidance from the facilitator
naturally interrupted their own flow in some way.

Conclusion
When in doubt about whether to talk to the usability test participant, count to 10 silently. Then decide whether you should say something. If yes, try using echo, boomerang, or Columbo techniques for speaking with the user. Remembering these 3 practices can help you learn more from your users, and improve your own test-facilitation skills.
More Facilitation Advice
Many more guidelines for improved test facilitation techniques in the full-day training course about Usability Testing."
65,2014-01-12,"The most effective way of understanding what works and what doesn’t in an interface is to watch people use it. This is the essence of usability testing. When the right participants attempt realistic activities, you gain qualitative insights into what is causing users to have trouble. These insights help you determine how to improve the design.
Also, you can measure the percentage of tasks that users complete correctly as a way to communicate a site’s overall usability.
What Users Need To Be Able To Do
In order to observe participants you need to give them something to do. These assignments are frequently referred to as tasks. (During testing I like to call them “activities” to avoid making the participants feel like they’re being tested).
Rather than simply ordering test users to ""do X"" with no explanation, it's better to situate the request within a short scenario that sets the stage for the action and provides a bit of explanation and context for why the user is ""doing X.""
Before you can write the task scenarios used in testing, you have to come up with a list of general user goals that visitors to your site (or application) may have. Ask yourself: What are the most important things that every user must be able to accomplish on the site?
For example, nngroup.com users must be able to accomplish 3 main goals:

Find articles on a specific topic
Sign up for UX Week seminars
Learn about our consulting services

Engage Users with Task Scenarios
Once you’ve figured out what the users' goals are, you need to formulate task scenarios that are appropriate for usability testing. A task scenario is the action that you ask the participant to take on the tested interface. For example, a task scenario could be:
You're planning a vacation to New York City, March 3 − March 14. You need to buy both airfare and hotel. Go to the American Airlines site and jetBlue Airlines site and see who has the best deals.
Task scenarios need to provide context so users engage with the interface and pretend to perform business or personal tasks as if they were at home or in the office.
Poorly written tasks often focus too much on forcing users to interact with a specific feature, rather than seeing if and how the user chooses to use the interface. A scenario puts the task into context and, thus, ideally motivates the participant.
The following 3 task-writing tips will improve the outcome of your usability studies.
1. Make the Task Realistic
User goal: Browse product offerings and purchase an item.
Poor task: Purchase a pair of orange Nike running shoes.
Better task: Buy a pair of shoes for less than $40.
Asking a participant to do something that he wouldn’t normally do will make him try to complete the task without really engaging with the interface. Poorly written tasks make it more difficult for participants to suspend disbelief about actually owning the task. In the example, the participant should have the freedom to compare products based on his own criteria.
Coming up with realistic tasks will depend on the participants that you recruit and on the features that you test. For example, if you test a hotel website, you need to make sure that the participants would be the ones in their family responsible for travel research and reservations.
Alternatively, you can decide to let the participants define their own tasks. For example, you could recruit users who are in the process of buying a car and let them continue their research during the session, instead of giving them a task scenario. (Field studies are ideal for observing users in their own environment as they perform their own tasks, but field studies are more expensive and time consuming.)
2. Make the Task Actionable
User goal: Find movie and show times.
Poor task: You want to see a movie Sunday afternoon. Go to www.fandango.com and tell me where you’d click next.
Better task: Use www.fandago.com to find a movie you’d be interested in seeing on Sunday afternoon.
It’s best to ask the users to do the action, rather than asking them how they would do it. If you ask “How would you find a way to do X?” or “Tell me how you would do Y” the participant is likely to answer in words, not actions. And unfortunately, people’s self-reported data is not as accurate as when they actually use a system. Additionally, having them talk through what they would do doesn’t allow you to observe the ease or frustration that comes with using the interface.
You can tell that the task isn’t actionable enough if the participant turns to the facilitator, takes her hand off the mouse, and says something like “I would first click here, and then there would be a link to where I want to go, and I’d click on that.”
3. Avoid Giving Clues and Describing the Steps
User goal: Look up grades.
Poor task: You want to see the results of your midterm exams. Go to the website, sign in, and tell me where you would click to get your transcript.
Better task: Look up the results of your midterm exams.
Step descriptions often contain hidden clues as to how to use the interface. For example, if you tell someone to click on Benefits in the main menu, you won’t learn if that menu label is meaningful to her. These tasks bias users’ behavior and give you less useful results.
Task scenarios that include terms used in the interface also bias the users. If you’re interested in learning if people can sign up for the newsletter and your site has a large button labeled Sign up for newsletter, you should not phrase the task as ""Sign up for this company's weekly newsletter."" It's better to use a task such as: “Find a way to get information on upcoming events sent to your email on a regular basis.” 
Avoiding words used in the interface is not always easy or natural and can even be confusing to users, especially if you try to derive roundabout ways to describe something that already has a standard, well-known name. In that case, you may want to use the established term. Avoiding clues does not mean being vague. For example, compare the following 2 tasks:
Poor task: Make an appointment with your dentist.
Better task: Make an appointment for next Tuesday at 10am with your dentist, Dr. Petersen.
You might think that this second task violates the guideline for tasks to be realistic if the user's dentist isn't really Dr. Petersen. However, this is one of those cases in which users are very good at suspending disbelief and proceeding to make the appointment just as they would with a differently-named dentist. You might need to have the user pretend to be seeing Dr. Petersen if you're testing a paper prototype or other early prototype design that includes only a few dentists.
Conclusion
If the task scenario is too vague, the participant will likely ask you for more information or will want to confirm that she is on the right path. Provide the participant with all the information that she needs to complete a task, without telling her where to click. During a usability test, mimic the real world as much as possible. Recruit representative users and ensure that each task scenario:

is realistic and typical for how people actually use the system when they are on their own time, doing their own activities
encourages users to interact with the interface
doesn’t give away the answer.
"
66,2013-12-15,"Defining Competitive Evaluations
Definition: Competitive usability evaluations are a method to determine how your site performs in relation to your competitors’ sites. The comparison can be holistic, ranking sites by some overall site-usability metrics, or it can be more focused, comparing features, content, or design elements across sites.
Evaluations can take the form of expert reviews, where an experienced usability practitioner reviews the designs based on her expertise and knowledge of usability, or competitive usability testing, where users complete a set of tasks using 2 or more competing sites.
Rather than simply looking at a competitor’s site to see what they’re doing and what you personally think is interesting or different, doing an evaluation allows the design team to understand what works and what doesn’t from a user’s perspective or an expert’s perspective.
Goals
Competitive evaluations let you assess if your design is better or worse than your competitors as well as discover the relative strengths and weaknesses of competing designs. They allow you to take an in-depth look at how others solve the same design problems. The goal of any competitive evaluation is to see what competitors are doing, how they’re doing it, what’s working and what’s not.
Competitive evaluations are often a good initial research activity for a project. They can help determine the direction of a design or the need for the development of a feature. The goals of the competitive evaluation should be clear before any work is started.  What design challenge are you trying to solve? What features of your competitors seem interesting or appealing? What feature on your site do you want to compare to others? 
Organizations with a substantial investment in strategically-managed user experience can also conduct longitudinal competitive evaluations to track relative status over time. This is expensive and not for companies at lower levels of organizational maturity. (Nielsen Norman Group typically charges clients $45,000 for a competitive usability test, so to do a lot of them requires a beefy budget.)
Benefits
A usability evaluation, whether a test or review, can help the team make decisions based on knowledge about what elements work well for users, rather than based on personal opinions. Competitive evaluations do this, and also have a few other benefits:

Risk reduction: Often you'll find that features that are pushed hard at industry tradeshows don't do much for your customers. Get this insight early, before investing in the feature, by testing some competitive sites that are early adopters.
Adding value: Other times you'll identify features or design approaches that dramatically benefit users. In these cases, you can proceed to add this value to your site.

Your team can gain these insights based on the usability work of your competitors. Testing others’ designs can show you the strengths and weaknesses of a design that may have already been through one or several rounds of usability testing or reviews. Your team benefits from the work that the competitors’ teams have already done. Competitive evaluations allow you to test several approaches to the same design – and can be done before doing any design work.
In addition, competitive evaluations reflect what your customers do already.  As they look for information on the web, they compare the content, functionality, and overall experience that your site offers to what others offer. People typically browse in parallel, moving between several sites before deciding which company to patronize. It is beneficial for your team to know what your users see and do as they look around at the competition. 
Defining the Competition
A typical review or test focuses on 2 to 4 competitors’ sites. Any more than that can be too expensive and too overwhelming to analyze. If you have a large number of competitors, do an initial review of several of them to determine which sites:

Offer similar content and functionality to your site, or to what your site strives to provide
Provide the best overall user experience
Use innovative designs that set them apart
Are your strongest or most important competitors
Are the competitors that your customers are most likely to compare you against 

Don't simply pit your site against those hated big competitors that are your worst enemies in the marketplace. Competitive evaluation is supposed to derive insights that can drive your user experience to the clichéd next level. Often the best insights come from comparing yourself to smaller or more innovative companies. You can also often learn much from companies that are only tangential competitors because they may do similar things in different ways based on traditions from different industries or market segments.
Competitive evaluations aren’t about picking competitors who have poorly designed sites and saying your site is the best. While you can learn a lot about what doesn’t work from looking at poorly designed sites, it is crucial to also learn what works by looking at sites with better usability than your own.
Competitive Reviews
In a competitive review, a usability expert takes an in-depth look at a series of related sites. The expert is looking for relative strengths and weaknesses, trends, patterns, and differences. Looking at similar content across several sites can help identify holes in your content or functionality; holes on others’ sites may inspire an addition to your site.
Competitive reviews can be as broad or narrow as desired. They can dive into a particular feature or area, or encompass an entire site.  You may look only at ordering processes, or you may review the overall site experience. In any broad evaluations, it is best if some key areas are identified to help focus the review and its results.
Competitive Testing 
In a competitive test, an expert runs a usability test on your design and on the designs of your competitors.   Each study participant typically completes a series of tasks on 2 or 3 different sites. Testing more than 3 sites with any given participant can be overwhelming to the study participants. Alternate which sites are paired together and which is tested first for each user.
Usability tests are focused on specific user tasks and can give an in-depth look at key areas and functions of the site. You want to see what users do, rather than ask them to look at the site and express their opinions. Focus on representative and key tasks that can be completed across sites, as well as on tasks where sites offer different approaches to information, functionality, or design.
At the end of each session, ask participants to compare the sites they used. People often naturally do this as they start to use the second site, with the experience of the first site still fresh in their minds. Comparing the two designs can help users verbalize what was clear or confusing in each design and can help you gain further insight into strengths and weaknesses.  
Competitive Tests on Design Variations
Competitive evaluations can also be completed with variations on a single design developed in a parallel design process. Not sure which of several designs is best? Evaluate them and find out. 
Rather than sitting around a conference table arguing about the best approach, do some usability testing or an expert review to determine which design works best for users. Competitive evaluations, like any usability evaluations, can be done in the early stages of design. Designs don’t have to be complete – or even interactive – to be tested. Depending on the design, paper-prototype testing can get the results you need before you spend any money on development.
Analyzing Results
By collecting metrics, a “winner” can emerge from competitive testing or competitive evaluations. This can be based on success scores, time on task, users’ subjective ratings, or a scorecard developed to analyze findings. However, the main goal of a competitive evaluation is not to declare a winner. The goal is to improve your design. While metrics can emphasize the point that your site performed, say, 30%  better – or worse – than your competitors’ sites, the important information lies in the details of what worked and what didn’t across designs.
Focus on what’s relevant to your design questions or challenges. What are the biggest strengths of competing designs? What trends arise across sites – and are they good or bad? Did any sites offer unique solutions to a common problem? How did they perform? Is there any opportunity to stand out from the competition? Were there any gaps in the information, content, or features that the sites provided? Don’t forget to look for opportunities beyond what other sites offer. You want to beat the competition, not copy them.
When writing – or reading – a competitive report, keep in mind that what matters is the information that can help you improve your site. Knowing what worked well and why it worked well can help direct design decisions. Knowing what failed and why can help teams avoid others’ mistakes or fix their own.  A good competitive evaluation not only assesses the competition’s designs, but also translates that assessment into recommendations for the site, whether written in the report or brainstormed with the team. The results need to help you design your own solution.
Summary
Competitive evaluations can give your team the benefit of reviewing several sites’ designs, to understand what the competition is doing and what they’re doing well. Learning from others’ designs can help your team create a better site.
"
67,2013-12-08,"Earlier this year I conducted a remote usability study on our own website, nngroup.com, to evaluate its content.
I could have conducted a traditional in-person usability study; however, I wanted to take advantage of the benefits of moderated remote usability testing: saving money and capturing users in their own environment in real time, as they were doing their tasks. I was even able to conduct several international usability sessions, without leaving my office in California.
I had 4 requirements for the study:

Participants had to be interested in the content on our site because they were considering attending a UX Conference seminar or purchasing a research report.
I had to be able to directly communicate with the participants and see their screen.
The sessions needed to be recorded.
Observers had to be able to watch from anywhere.

Traditionally, for a usability study, representative users are recruited and scheduled in advance. Recruiting representative users is difficult and expensive for a B2B site with a narrowly targeted audience. Also, since I was specifically interested in the content on the website, I wanted to ensure engagement. To do this, I opted for recruiting people who were using our website and running a test with them right then and there, during their visit to the site–that is, run a live-intercept study. This ensured that they were: 1) interested in the information on our website, and 2) actual users of our website.
Here's the quick 9-step summary of my test setup, described in more detail below:

Participants were recruited on nngroup.com through ethnio. 
If they qualified, I called the participant on the number they provided in the screener.
I started a GoToMeeting (GTM) session on my computer.
Over the phone I gave him the instructions to join my GTM session.
At this point I sent an email out to the observers (who were also remote) with the same GTM session details.
To get the audio connected without having to end the call in step 3, I started a 3-way call with the GTM phone line.
I made the participant the presenter, so I could see her screen.
Then I started the recording in GTM.
Immediately after the session, I emailed the participant an Amazon gift card and a thank-you note.

It's obviously possible to use other tools than the ones listed here, and we certainly don't endorse these tools as necessarily being the best. However, they were suitable for our project.
Recruiting Participants with ethnio
Ethnio is a customizable screener that can be placed as a pop-up on your site or accessed through a direct link to an ethnio URL.
Since I wanted to recruit users from nngroup.com, I opted for the intercept from our site. Ethnio provides a 2-line JavaScript code, which I placed in our CMS. The resulting pop-up looked like this:

The initial screener pop-up placed on course- and report-description pages on nngroup.com. This pop-up appeared 15 seconds after users first arrived on a page.

I placed the screener pop-up on pages for both seminar and research reports on our website. The screener appeared with a 15-second delay — that is 15 seconds after the page was loaded on the participant's computer. From our analytics, I knew users spent an average of 80 seconds on these pages. Ethnio offers the option to present the pop-up immediately on page load; however I wanted to give participants a chance to determine if the page was right for them and to start looking for the information they were interested in.
Thus, by introducing the pop-up with a delay I weeded out those visitors not interested in the content on that page. I also found that a 15-second delay worked better than a 30-second delay. This could be because in 30 seconds users may have determined that the seminar or report was not for them and may have left the page, or they may have been already deeply engaged in the content and may have dismissed the pop-up because it disrupted their reading.
It also took some trial and error to determine the right balance between session length and incentive. More site visitors responded to the screener when I asked for 20 minutes of their time instead of 30 minutes; however 20 minutes was too short to conduct a thorough test. The extra 10 minutes were necessary for setup and debriefing. In terms of incentive, as you can imagine, the more money I offered, the more users responded to the screener.
Participants were offered an Amazon gift card for their time. I customized the gift card with our logo and emailed them their gift card after the session. Amazon gift cards worked well for this study, because they can be purchased ahead of time, are familiar and trustworthy, and available in several countries.
In the ethnio pop-up, if the user clicked Continue, he was taken to the screener questions. Ethnio screeners allow for 10 customizable questions. Ethnio also provides the option to automate screening and will send participants directly to, for example, a usertesting.com session based on their answers.
However, I needed more than 10 questions and wanted to review their answers. So I used 7 questions from ethnio to ask for basic information and then completed the screening process over the phone.

Basic screening questions asked through the ethnio interface.


The final screener page, in which participants were told they’d be contacted by phone if they qualified.

I set up ethnio to email me whenever someone completed the screener. I made sure to have the ethnio screener live on our website only when I was available, so users wouldn’t complete it and not get a phone call.

An example of an email from ethnio notifying me of a new recruit.

Starting the GoToMeeting Session
If the participant qualified and agreed to the study, I walked him through the instructions provided by GoToMeeting when you click on Invite Others:

I provided this information to the participant over the phone so he could join the GTM session.

I also emailed these same instructions to observers (using Copy to Clipboard in the Invite Others window).
If you’re going to have observers dropping in and out of sessions, you might want to disable the beep that plays by default when people join or exit a meeting. You can do this by going to the Audio section in the Edit menu and un-checking Play Entry/Exit Chimes.
When the participant joined, he saw a white screen with some details about the meeting.

The initial screen seen by participants who join a GoToMeeting session.

Only 2 of the 15 participants had trouble installing GoToMeeting, due to firewall settings at their work.
Screen Sharing and Audio
To get the audio connected without having to end the existing call with the participant, I added the phone line that GoToMeeting provided to the call with the participant. I told the participant I'd be putting them on a brief hold; then I started a 3-way conference call. To GTM it looks like there’s only one audio line, but it actually captures both the facilitator and the participant. (When you conduct scheduled moderated remote usability sessions, you can provide the participant with the phone number to dial into).

The phone-number information provided by GTM, needed to capture audio in the recording.

Then I made the participant the presenter:

To see the participant’s screen you have to make him the presenter, by selecting his name in the Change Presenter dropdown.

I also made sure to disable most of the attendee features. Participants didn’t need to be able to chat, see attendees, or share webcam.

Unselect all three items in the Options menu to disable these features for attendees.

Recording Sessions
The final step was to start the recording, by clicking the Start Recording button. Make sure you have the same options selected in the Settings dialog as in the screenshot below:

Select the Settings link next to the recording button. Then make the indicated selections in the next window.

To end the session, you can either end the meeting in the File menu or stop the recording first. If you end the meeting, the recording automatically stops and is saved where you indicated.
Conclusion
Recruiting participants live from our website helped me locate engaged target users and use the tasks that they had in mind. By intercepting them when I was available to conduct a study, I could start the session immediately.
Keep in mind that intercept recruiting has a low response rate (that is, few users will complete the screener). In this study 2% of the users who viewed the screener completed it. (This matches ethnio’s reported average 2% response rate.) Of the users who completed the screener, 30% qualified for the study. Of those users, 60% participated (this number is 18% of the people who completed the screener and 0.3% of those who viewed the screener — in other words, we had one successful test for every 500 visitors to a page of interest).



 
Response rate 
Selection rate 
Success rate 


NNgroup.com 
2%
30%
60%


Ethnio average 
2%
25%
40%



The number of users who completed the screener (Response rate), qualified for the study (Selection rate), and actually participated (Success rate).
I found that GoToMeeting offered all the functionality needed to see the participants’ screens and record the sessions. The only work around was for the audio connection, for which I placed a 3-way call between me, the participant, and the GTM line. This was only necessary because I wanted to call the participant before the session to qualify them for the study, rather than emailing them the GoToMeeting details.
For more detailed advice on this type of user research (including hands-on practice), please come to our full-day training course Usability Testing."
68,2013-10-12,"Remote usability tests are like traditional usability tests with one key difference: the participant and facilitator are in two different physical locations. Rather than the usability expert going to a participant’s location or vice versa, the participant interacts with the design in his own home, office or other location, and the expert watches remotely.
Generally, we recommend in-person usability testing whenever possible. It is easier for usability facilitators to read users’ body language and to recognize an appropriate time for a probing or follow-up question when they are in the room with the user. It can also benefit product teams to see users interact with their designs in real life, rather than watching them on a remote feed. (For much more on how to conduct user studies, see the full-day course on Usability Testing.)
However, when in-person testing simply isn’t possible due to budget or time constraints, remote testing is preferable to the alternative: skipping the test altogether.
Remote usability sessions don’t require either the participant or the facilitator to travel. As such, remote testing is a great solution for teams with limited budget, or for testing products whose users are geographically dispersed. Scheduling a series of online studies can be preferable and far less costly than traveling around the country or the world.
Online testing is also a good solution in a tight timeframe — travel doesn’t have to be coordinated, and facilities for testing don’t have to be secured. Further, participants can be from any geographic area rather than concentrated in one location, which can make recruiting faster and easier.
Remote research allows participants to use their own computers for the study, letting you and your team see how they set up their desktop, navigate between programs, and use tabs, for instance. This insight into how people work with their own machines is valuable, but it also makes it more difficult to troubleshoot any problems participants have with the remote tools needed to conduct the study.
In moderated remote testing, users and facilitators are in the same “virtual” space at the same time — the facilitator is watching the usability test remotely as it happens, and communicating directly with the participant via the telephone, email, chat, or a combination of methods. In an unmoderated remote session, the participant completes the study on his or her own schedule, recording the session for later review by the usability expert.
Moderated Studies
Moderated sessions allow for back and forth between the participant and facilitator, because both are online simultaneously. Facilitators can ask questions for clarification or dive into issues through additional questions after tasks are completed.
It can be difficult to know when to ask a question in a remote study, however. Silence on the other end of the line may mean that the user is confused, immersed in content, looking around the page, or distracted. It can be difficult to find the balance between letting users know you are listening and interrupting them. Although the same is true in face-to-face studies, the problem can be magnified in remote studies.
Unmoderated Studies
Unmoderated usability sessions are completed alone by the participant. Although there is no real-time interaction with the participant, some tools for remote testing allow predefined follow-up questions to be built into the study, to be shown after each task or at the end of the session. Questions can also be emailed to be completed after the user has finished her session. In both cases, questions are the same across users. There is no opportunity to ask detailed questions specific to the user’s actions.
Users don’t have real-time support if they have a question, need clarification, or can’t get the technology to work, although you can provide them with an email address or phone number to contact someone for assistance. This disconnect also means you don’t know what the session was like until it’s finished. If a user did run into a problem, skipped tasks, or failed to complete what was asked, you don’t know until it’s over. Some sessions may end up being unusable or less valuable, depending on the issue.
Unmoderated tests can also be quieter than moderated tests. We typically use the think-aloud protocol in usability testing, asking users to talk us through what they’re doing as they’re doing it. In a moderated study, the facilitator can gently nudge a quiet participant to share more about what he’s doing. In an unmoderated study, you can ask a user to think aloud, but there is no one there to remind her if she doesn’t do it.
Because of the lack of detailed follow-up, it is preferable to use unmoderated remote usability tests when the main focus of the study is a few specific elements, rather than an overall review. Remote studies are great for gathering data on an element or widget or for seeing the impact of a relatively minor change.
Unmoderated studies can also be good for tight timeframes: users can complete sessions on their own schedules and even simultaneously, rather than trying to fit into scheduled time slots.
Tips for Remote Usability Testing
Practice the technology. Even if you’ve used your company’s tools a million times before, test them with someone you know outside the company, mocking up a real test situation. Make sure the instructions for signing in are clear. Practice sending URLs or tasks to your user and make sure you know how the technology works on your end — and theirs.
Rewrite everything. Write tasks far enough in advance to pilot-test them. In a moderated session, the facilitator can get a user back on track if a task is misunderstood. In an unmoderated session, there is no safety net. The written instructions need to stand on their own. Every instruction, task, and question needs to be fine-tuned to eliminate the potential for misunderstanding. As we know from every study ever done on instructional design, anything that can be misunderstood will be misunderstood.
Be available. Even for unmoderated studies, be available by email (if not by phone) as much as possible to help with any potential user questions. In moderated sessions, sign in to the testing tool early, in order to know when your participant arrives and to troubleshoot if needed.
Recruit more users. No-show rates for any remote study can be higher than for in-person studies. You also don’t know the quality of an unmoderated session until you’ve watched it. It’s better to add a few more users than you think you need in order to accommodate such problems.
Learn more about remote and in-person user testing in our full-day Usability Testing training courses, which include hands-on practice writing tasks, facilitating sessions, and more.
"
69,2013-09-14,"If product teams don’t know what to do with usability results, they’ll simply ignore them. It is up to usability practitioners to write usability findings in a clear, precise and descriptive way that helps the team identify the issue and work toward a solution. 
One of the keys to helping teams improve designs through usability is making results actionable. Running a test, analyzing results, and delivering a report is useless if the team doesn’t know what to do with those results. Usability practitioners — particularly those new to the field — sometimes complain that teams don’t act on their studies’ results.  While this may be due to a myriad of issues, it is often caused or exacerbated by problems with the findings themselves. 
Usability findings have to be usable themselves: we need meta-usability.  Below are 5 tips for practitioners, old and new, to improve the usability of their results. These tips are also handy for managers, clients, product teams and customers of usability reports to help better assess the value of the reports they’re receiving.
Be specific
Vague findings don’t give product teams much to work with. A lack of detail or explanation can leave teams wondering what the problem was or at a loss about how to fix it.
A finding of “Registration was hard” doesn’t identify the problem or hint at solutions. Why was registration hard? Could users find it? Were fields for existing users to log in confused with fields for new users to register?  Were there too many questions? Were field labels unclear? Did the form ask for information the user didn’t have, didn’t know, or didn’t want to share?  Were any steps unexpected? Were buttons poorly placed? 
Findings need to be specific: “The button to register for the site had poor color contrast and faded into the page background.” Whenever possible, identify the specific area of the design, flow, or interaction that caused the user to have a problem. 
Don’t blame the user
It’s easy to fall into the trap of writing results in relation to the user. “The user was able to do this.” “Three users could not find that.” This type of finding focuses on the user, rather than on the design. This is a problem because it seems to blame the user.  It’s very easy for a team member to read a finding and think, “OK, that user couldn’t find the link, but others can” and dismiss the issue altogether. 
This also makes it hard for teams to know how a user activity translates into a change in the design. Findings should explain the elements of the design that confused users or led them down the wrong path. Was the navigational structure of the site unclear? Was a label or link poorly named? Did the way the site was organized match the users’ expectations? 
When starting a usability session, we always tell the test user, “we’re not testing you, we’re testing the system.” This is not propaganda — it’s really the most fruitful way of thinking about the test results.
Look for the bigger picture
It is easy to become so focused on the details in a usability test that a huge issue goes without notice.  While the smaller issues are important to identify, it can’t be done at the expense of seeing larger issues with the design. For example, changing button designs and link names may improve steps in a process, but it can’t fix a process that doesn’t match user expectations or meet their needs. 
Focusing too tightly on the details in a report can cause teams to add many band-aids to a design that’s really suffering from a broken leg. If users had trouble at every step, perhaps it’s the overall flow or structure that’s to blame, rather than small details along the way.
The reality is many projects don’t have the time, resources, or budget to fix large issues with designs — at least not in the near future. So keep the smaller details in the report, but include those findings about larger, over-arching issues so they don’t get overlooked. 
Help identify solutions
On most teams, the usability professional’s job is to identify issues with the design; fixing the design is the job of design and development. However, usability experts often have unique expertise in thinking about design solutions. They have first-hand knowledge of what worked — and what didn’t — in usability testing and may have years of experience in understanding what does and doesn’t work in a design. They can offer insights about potential design solutions. 
However, usability reports suffer when usability practitioners overstep their roles. Findings should not take the form of elaborate wireframes reworking the whole design. Handing over a set of new wireframes rather than a list of findings can cause resentment on the team. A quick mock-up here and there to illustrate a point is acceptable, but a full redesign document can quickly distract the team from the value of the findings.
Work closely with the design and development team, rather than simply delivering a report and walking away from the project.  When results are presented as a discussion the usability expert, who witnessed the problems (and successes) users had with the design first-hand, can add expert insights into what design solutions may or may not address the problems seen in the design. Schedule and participate in meetings where the team decides how to address issues that came up in testing. Better yet, invite team members to usability sessions and debrief with the team between sessions. 
Adding redesign recommendations to usability reports is another option. Such recommendations can help the team understand the issue and start to think of potential solutions. Suggest moving a button or changing a label, combining navigational categories or writing more explicit link names. But present recommendations as recommendations. Label them as such and explain to the team in writing or in person that the recommendations are intended to jumpstart thinking about design solutions and to illustrate usability issues, and are not presented as the only or best solution. A creative designer may come up with something even better.
Organize and rank findings
Every issue that’s discovered through usability testing is not equally important.  Further, a usability report may have 5 or 100 findings, depending on the scale of the study, the design tested, and the usability practitioner.  Teams need a way to parse through the findings and discover 1) what’s relevant to the screens, designs or elements they have responsibility for and 2) which problems were the biggest issues from a usability perspective.
Findings should be grouped with similar findings, meaning that there may be a section about navigational issues and other sections about particular pages or task flows. Beyond this, findings should also be ranked in terms of severity. Was the problem a slight hiccup that caused one user to stumble? Was it a problem seen in only 2 sessions, but which derailed the users’ entire experience? Was it something so big that the product can’t possibly be successful unless the issue is addressed? Ranking findings as low, medium or high severity helps the team understand what critical issues the usability study exposed. Don’t forget to include positive findings as well, letting the team know what’s already working. 
Accrue future value from descriptive reports
The advice in this article provides immediate value by increasing the chance that usability insights are acted upon in the current design phase.  This again increases the ROI from the organization’s usability investment by making the product better, resulting in higher conversion rates, higher customer satisfaction, fewer errors, and other measurable benefits. (Findings that are not acted upon don’t generate profits.)
Better descriptions of the study findings also provide future value by enhancing the organization’s cumulative knowledge about its customers.  If you maintain an archive of usability findings people can draw upon these insights as they plan and execute future design projects. No reason to make the same design mistake a second time.
"
70,2013-08-31,"Many times, when a client wants to do a usability testing evaluation of a currently in-service environment/application, they are looking for a traditional summative test: A smallish set of rigid tasks geared toward measuring key performance indicators (success rate, task completion time, and errors) with the goals of 1) discovering issues and 2) providing evidence to support investment in fixing those issues.
However, as more organizations embrace agile and lean methods, decision-making has become decentralized and traditional metric-based evidence is not as crucial for some. Organizations still want to hire usability experts to plan and conduct usability testing (until it becomes an internal competence), but they may not be as interested in generating “evidence” to secure budget for changes – instead, they want actionable findings and recommendations covering the broadest scope of their environment/application. To meet this need, we’ve found there is tremendous value in being more flexible by:

Iterating tasks between and within sessions
Improvising and customizing tasks to better suit individual participants
Inviting real-time client participation

(Note that by ""clients"" we simply mean the people who have to take action on the usability finding. Your clients can be a traditional external consulting client if you're a consultant or work in an agency, or they can be internal stakeholders if you work inside the organization.)
For flexible testing you still create a test guide, but the tasks within are not treated as unchanging components. Instead, the tasks are treated as a starting point.
There are two benefits to this:

Decreased pressure on the moderator/planner to create the perfect tasks right out of the gate: Tasks that we agree will be tested and changed if necessary are a lot easier to create.
Decreased pressure on the client: Often times, members of the client team have never observed a real usability test, so to ask them to come up with or approve a definitive list of tasks and expecting it to be thorough is silly; they simply don’t know what they don’t know. Once they start observing testing, the task ideas start flowing. 

Sample Test Guide Contents:

Study logistics (times, locations, number of participants, session duration, incentives, moderator)
Target participant criteria
Study goals
Study questions
Tasks

Top 10 tips for Flexible testing
To get the most out of flexible testing, we have provided some advice focused on the most critical aspects of planning, facilitating and management.
1. Define testing goals and intentions first
You need to determine two things:

Why the client is sponsoring the study.
What they intend on doing with the results.

It is imperative that the client answers both these questions honestly and that they are engaged in a conversation about how these goals and intentions will inform the research method selection and output. Of all your discussions, this is the one that will bear the most significant impact on your ability to meet expectations.




Matching Client Goals/Intentions with Testing Methods




 


Summative Testing


Flexible Testing




Generate statistical evidence


x


 




Convince executives to make changes/allocate budget


x


x




Inform design strategy


 


x




Guide design specifications


 


x




Inform design changes


 


x




Identify/prioritize issues with existing design and content


x


x




Identify good characteristics of existing site


x


x




Acquire a “safe feeling” of signing off on an exact, unchanging test plan before the research starts


x


 




2. Generate very specific questions the study must answer
The easiest way to come up with tasks to test is to start with the questions the study should be answering. I used to focus on asking clients to tell me their “mission critical tasks”, but that can limit your ability to come up with tasks that actually touch and test attributes that clients believe are problematic.
After goals and intentions are defined, ask the client to give you a list of questions they want answered in this study. You can use some of these sample questions to get the ideas flowing:




Potential Questions to be answered in Usability Testing 
(Web site/Application)




Navigation




Do people recognize the global navigation as navigation?




Do people use local navigation in [location]?




Do people use the navigation system to understand where they are?




Product/Content Findability




Does site search yield good relevant results?




Do overview pages effectively route users down a path?




Do people use filters? Under what circumstances?




Product information




What information do people fixate on?




What information do they have trouble finding?




What information do they want to see?




What is information is confusing?




Do users use related links?




Do users seek reviews?




Checkout




Can people easily find a way to checkout?




Are they nervous about any required/requested information?




Are there any issues filling out forms?




3. Design tasks to answer questions
The best way to ensure clients get what they need is to create testing tasks that expose the qualities of the site they are interested in evaluating. This doesn’t mean that you have to create a task for each question. It’s likely that typical user task scenarios will address several questions each and may overlap as well – and that’s OK.
4. Get as many client observers as possible
Testing is always better when observed by as many stakeholders, creators, and controllers as possible. But this is even truer when the tasks are intended to change. More brains lead to more ideas, which lead to more movement.
5. Incorporate competitive offerings into the test plan
If possible, you want to include some tasks to be performed on competitive sites so there is context. Also, this tends to be one of the most controversial topics in any organization along the lines of “So-and-so does it this way and it’s better.” The first benefit of observing competitors in any usability study is that it can prove/disprove those statements. The second benefit, specific to flexible testing, is that observing people using environments beyond your own can generate all sorts of additional task ideas.
6. Use pre-task questions to inform task improvisation
Define a series of questions to ask at the beginning of each session to better understand your participants.
Questions can focus on:

Needs: What are they seeking?
Experience: How do they use particular information/content/products?
Intentions: Why do they use particular information/content/products?
Knowledge: What do they know about this particular topic?

7. Physically separate the client team from moderator/participant
You want your team to be reacting and discussing together, so they can suggest task changes and formulate questions for the moderator between sessions. If they are in the same room as the participant, team interaction is limited to facial expressions and maybe passing notes (which, by the way, is totally noticeable to the participant and can be unnerving). It’s best to keep your client team in a separate room from the facilitation. This applies to traditional “lab” sessions as well as remote-user sessions where, ideally, the facilitator should be alone in the room.
8. Define ground rules for real-time participation so clients know how to help instead of hurt
Inform your client team of what is happening and how they can optimize the process for their benefit:

Tell them this testing is flexible – it’s intended to change.
Tell them why: Because as you watch the testing, you may get additional task ideas or become curious about other features or content that may be suitable to a particular participant (you know your site best!).
Do not allow task suggestions/changes in the first session – ask the team to watch it without real-time feedback.
Reserve a generous amount of time (at least one hour) to debrief after the first session: Collect feedback and make test plan changes.
Have the team elect one representative who will be responsible for sending you real-time task suggestions starting with the second session.
Tell them what is useful and what is not:
	
Do not send requests to ask for user opinion (“Do you like this logo?”).
Do send ideas about products you have that this user may find useful (“This user said she likes to golf, we sell a line of golf shoes – maybe we can see if she realizes that and then see if she’s able to find a pair that meets her needs?”).
Do send ideas about features/content that might suit a user based on their pre-test answers or behavior (“This user seems really fixated on peer reviews; any way to see how they react to our Q&A content?”).



9. Make sure users are minimally disturbed by the conversation between moderator and client
If you are using conference calling technology to broadcast and/or record your sessions (e.g., GoTo Meeting, Webex), do not also use that channel for the client team to talk with you. Cell phone texting (on mute) is less disruptive, so the user does not lose their train of thought or see information intended only for the research and/or client team.
10. Debrief and prep between each session
Between each session, reserve enough time to chat with the client team, get feedback on the facilitation and information derived, and make changes to the tasks.

Review each task and ask for edits.
Ask if they want to retire any tasks: This can happen when a team has seen enough  people failing on the same task. If they saw it two or three times, they’d rather use that time to expand the scope.
Quickly outline the tasks for the next participant.
Make your task sheets (if facilitating in person): If you have access to a printer, then type these up, but it’s fine to use pen and paper when pressed for time.

Getting the Most from Flexible Testing
Using a flexible testing method can help you observe a greater range of site features because tasks are allowed to change as client questions are answered and as tasks are adjusted or created to better suit individual participants. This method works well for organizations who are less concerned with generating metrics from usability testing and more interested in immediate action items.
Ideally, use experienced usability facilitators that can accommodate this constantly changing game plan and welcome the challenge. Experience helps, because there's no time to pilot-test the changes, so you need to get them right the first time. However, inexperienced staff can use a modified version of flexible usability testing where they treat the entire study as an extended exercise in iterative pilot testing and gradually make their tasks and test methodology more and more appropriate. Not as good, of course, but then the only way anybody gets to be an experienced usability expert is to start off with no experience and gradually get better. Flexible testing allows you to get better faster."
71,2012-09-09,"It almost doesn't matter where you conduct user testing. If you're lucky, you have a fancy  usability laboratory  with multiple cameras and one-way mirrors. The key benefit of such labs is the ability to  stick observers in a separate observation room  and leave the user and facilitator alone to  concentrate on the test tasks.
The #1 method for evangelizing usability is to get as many  stakeholders as possible to personally observe real users. A good usability lab with a comfy observation room (well-stocked with snacks) encourages an invaluable side effect of user testing: designers, developers, and managers become emotionally invested in fixing usability problems they have seen themselves.
What should you do if you  don't have a lab? After all, dedicated lab space usually appears only after a company has reached stage 5 on the  1–8 scale of corporate UX maturity.
Luckily,  user testing can be done anywhere, as long as you can close the door and ensure the privacy and focus of your study participant. Some of the more common locations for user testing include:

Conference rooms
Private offices
Rented space in ""office hotels""
Hotel meeting rooms
Airport lounges (assuming you can book a boardroom-style space)
Tradeshow booths (ideally in a booth with a private back room, though you can do short 5-minute tests standing at a demo station in the back of a regular booth)
Company cafeterias
Coffee shops (not the Amsterdam kind :-)

The key elements in user testing are the facilitator's skill in drawing out  user behavior  without biasing the user, and the facilitator's analytical skills in determining  valid and useful design conclusions  from observations of this user behavior. These two crucial points don't depend on the specific location or on having lots of advanced equipment at hand.
Portable Lab Equipment
Nielsen Norman Group runs  many international studies and often tests at client locations. For the vast majority of our user research, the  equipment fits in a carry-on bag.
Here are photos of the set-up for our study in Sydney, Australia, last month:

 

The equipment:

a big laptop for the user;
a small laptop for the facilitator;
a webcam to record the user's  thinking aloud comments and facial expressions; and
a real mouse for the user (we don't want users to have to struggle with a touchpad to click links).

That's all. We ran this study at our conference hotel, and they provided the coffee cups.
The  user laptop should be hefty  for three reasons:

A good, full-sized keyboard encourages users to type as much as they want with minimal typos.
A large screen ensures that even middle-aged or older users can see the information.
There's sufficient computer power to run whatever websites or applications users encounter while still having leftover capacity to run Morae (the screen-recording software we use).

The big screen also allows an additional observer (besides the facilitator) to sit slightly behind the user and still see what's on the screen. You want the observer to be out of sight—and out of mind—as far as the user is concerned.
(Unfortunately, when testing  mobile devices, we need a bit more equipment. Although users typically bring their own phones and tablets, we need a fancier camera to capture the small screens. That camera requires a bag of its own, so we usually ship it ahead of time.)
Testing in Progress—Keep Out
Here's one more artifact from the Sydney study:

We stick a sign like this on the door when we test in locations that are unaccustomed to usability studies and their associated protocols. We really don't want the hotel staff to interrupt the study; nor do we want people wandering the hallways looking for other meetings to poke their heads into our room.
If you conduct user testing at your company, it might be nice to have the sign say something like, ""Usability Study in Progress."" This will give you a bit of internal PR, as colleagues pass the conference room and note that you're doing more testing.
But when you test in hotels or other outside locations, it's too much to assume that anybody will understand what ""usability"" is or why it's bad to disturb a test user during a study. We therefore go with ""Recording in Session,"" which everybody understands. People might even be quiet as they pass the room.
Accommodating More Observers
For international testing, you're lucky if the client will invest in flying out one observer. Still, it's sometimes nice to be able to accommodate multiple observers, who can't all squeeze in behind the user's chair.
In such cases, one option is to hook up a  slave monitor, as shown in this photo from a study we ran in Hong Kong:

Here, we simply used a mid-sized HD TV, which is available at most hotels (and thus you don't have to ship it around). Because the projected image of the user's screen is fairly large, a handful of observers can crowd around the TV.
As the photo shows, from the user's viewpoint, the observers are pretty much screened out by the bulk of the television. Again, you want observers to be out of sight and out of mind as far as the user is concerned.
If you have many observers, you can run the video cable to an adjourning conference room and use a projector to generate as large an image of the user's screen as you want. It's a bit trickier to pipe in sound, but you can do that as well—sometimes through the low-cost trick of using speakerphones and simply muting the one in the observation room.
No Excuses
No equipment? That's no excuse for not testing your design.
You can test a  paper prototype or you can use a laptop (which I'm sure you have anyway). Although we like to show video clips from our research in our training seminars, for in-house studies, you don't even need the webcam or recording software.
No usability lab? Not an excuse either.
As I've shown here, you can test almost anywhere, in any country (including your own!), with a small amount of equipment that you can either scavenge on site or bring in a carry-on bag. The one thing you do need, however, is  real users who represent the target audience.
For more info on how to set up a usability study, run it, and analyze the findings, see our full-day Usability Testing training course."
72,2012-06-03,"If you want a single number, the answer is simple:  test 5 users in a usability study. Testing with 5 people lets you find almost as many usability problems as you'd find using many more test participants.
This answer has been the same since I started promoting ""discount usability engineering"" in 1989. Doesn't matter whether you test websites, intranets, PC applications, or mobile apps. With 5 users, you almost always get close to user testing's maximum benefit-cost ratio.
As with any human factors issue, however, there are  exceptions:

Quantitative studies (aiming at statistics, not insights): Test at least 20 users to get statistically significant numbers; tight confidence intervals require even more users.
Card sorting: Test at least 15 users per user group.
Eyetracking: Test 39 users if you want stable heatmaps.

However, these exceptions shouldn't worry you much: the vast  majority of your user research should be qualitative — that is, aimed at collecting  insights to drive your design, not numbers to impress people in PowerPoint.
The main argument for small tests is simply  return on investment: testing costs increase with each additional study participant, yet the number of findings quickly reaches the point of diminishing returns. There's little additional benefit to running more than 5 people through the same study; ROI drops like a stone with a bigger  N.
And if you have a big budget? Yay! Spend it on additional studies, not more users in each study.
Sadly, most companies insist on running bigger tests. During the UX Conference, I surveyed 217 participants about the practices at their companies. The average response was that they used  11 test participants  per round of user testing — more than twice the recommended size. Clearly, I need to better explain the benefits of small-N  usability testing.
(Weak) Arguments for More Test Participants
""A big website has millions of users.""  Doesn't matter for the sample size, even if you were doing statistics. An opinion poll needs the same number of respondents to find out who will be elected mayor of Pittsburgh or president of France. The variance in statistical sampling is determined by the sample size, not the size of the full population from which the sample was drawn. In user testing, we focus on a website's functionality to see which design elements are easy or difficult to use. The evaluation of a design element's quality is independent of how many people use it. (Conversely, the decision about whether to fix a design flaw should certainly consider how much use it'll get: it might not be worth the effort to improve a feature that has few users; better to spend the effort recoding something with millions of users.)
""A big website has hundreds of features.""  This is an argument for running several  different tests  — each focusing on a smaller set of features — not for having more users in each test. You can't ask any individual to test more than a handful of tasks before the poor user is tired out. Yes, you'll need more users overall for a feature-rich design, but you need to spread these users across many studies, each focusing on a subset of your research agenda.
""We have several different target audiences.""  This can actually be a legitimate reason for testing a larger user set because you'll need representatives of each target group. However, this argument holds only if the different users are actually going to behave in completely different ways. Some examples from our projects include

a medical site targeting both doctors and patients, and
an auction site where you can either sell stuff or buy stuff.

When the users and their tasks are this different, you're essentially running a new test for each target audience, and you'll need close to 5 users per group. Typically, you can get away with 3–4 users per group because the user experience will overlap somewhat between the two groups. With, say, a financial site that targets novice, intermediate, and experienced investors, you might test 3 of each, for a total of 9 users — you won't need 15 users total to assess the site's usability.
""The site makes so much money that even the smallest usability problem is unacceptable.""  Rich companies certainly have an  ROI case to spend more on usability. Even if they spend ""too much"" on each quality improvement, they'll make even more back because of the vast amounts of money flowing through the user interface. However, even the highest-value design projects will still optimize their ROI by keeping each study small and conducting many more studies than a lower-value project could afford.
The basic point is that it's okay to leave usability problems behind in any one version of the design as long as you're employing an  iterative design process where you'll design and test additional versions. Anything not fixed now will be fixed next time. If you have many things to fix, simply plan for a lot of iterations. The end result will be higher quality (and thus higher business value) due to the additional iterations than from testing more users each time.
83 Case Studies
The following chart summarizes 83 of Nielsen Norman Group's recent  usability consulting projects. Each dot is one usability study and shows how many users we tested and how many usability findings we reported to the client. (The chart includes only normal qualitative studies; we also run competitive studies and benchmark measurements, and conduct other types of research not shown here.)


There's a small correlation, but it's really tiny. Across these many projects, testing more users didn't result in appreciably more insights.


Why did we run more users in the first place, given that I certainly believe my own research results showing the superiority of small-N  testing? Three reasons:

Some clients wanted bigger studies for internal credibility. When a study's sponsor presents findings to executives who don't understand usability, the recommendations are easier to swallow when more users were tested. (If management trusted its own employees, much money could be saved.)
Some design projects had multiple target audiences and the differences in expected (or at least  suspected) behaviors were large enough to justify the expense of sampling additional users.
Finally, the very fact that these were consulting projects justified including a few more users, which is why we often run studies with around 8 users. ROI is the ratio between benefits and expense. When hiring a consultant, the true expense is higher than just the fee because the client must also spend time finding the consultant and negotiating the project. With higher investment, you want a larger benefit.

The last point also explains why the true answer to ""how many users"" can sometimes be much smaller than 5. If you have an  Agile-style UX process with very low overhead, your investment in each study is so trivial that the cost–benefit ratio is optimized by a smaller benefit. (It might seem counterintuitive to get more return on investment by benefiting less from each study, but this savings occurs because the smaller overhead per study lets you run so many more studies that the sum of numerous small benefits becomes a big number.)
For really low-overhead projects, it's often optimal to test as few as 2 users per study. For some other projects, 8 users — or sometimes even more — might be better. For most projects, however, you should stay with the tried-and-true: 5 users per usability test."
73,2012-03-25,"Let's compare 3 different approaches to achieving better design:



 
A/B Testing
Usability
Radical Innovation


Cost 
Low
Low–medium
High (unless lucky)


Benefits 
1–10%
10–100%
100–1,000%


Risk 
None
Low
High


Who can do? 
Everybody
Everybody
Geniuses


How often? 
Weekly
Monthly
Every 10 years


GDP impact 
Medium
High
Medium



Definitions:

A/B testing splits live traffic into two (or more) parts: most users see the standard design (""A""), but a small percentage sees an alternative design (""B""). After collecting statistically significant numbers, the design with the best KPI (key performance indicator, such as conversion or  bounce rates) becomes the new standard. (One can test more than 2 design variations at a time through a related method called multivariant testing: for the sake of this article I'll lump these analytics methods together and talk about AB, regardless of the number of variables being measured.) As a form of web analytics — or intranet analytics — A/B testing has the huge advantage of testing live traffic to your website (or intranet).
Usability refers to the  full range of user-centered design  (UCD) activities:  user testing, field studies,  parallel and iterative design,  low-fidelity prototyping,  competitive studies, and many  other research methods. These types of studies usually require you to recruit test participants who should be representative of your target audience.
Radical Design Innovation creates a completely new design that deviates from past designs rather than emerging from the hill-climbing methods used in more standard redesign projects. Example innovations include fundamental breakthroughs (the steam engine),  new product categories (the locomotive), and major reconceptualizations of existing categories (the iPhone).

Cost
Incrementally, A/B testing is very cheap. You do need to pay a designer to create the ""B"" design, but most of the cost lies in the software to run and analyze the test, which is a one-time expense. Thus, if you're going to do it at all, you should run lots of A/B tests.
With usability methods, the costs range from  $200 for a few quick activities to  $38,000 to have a website analyzed by an independent expert. However, even the high end of usability costs pales in relation to the full cost of any enterprise-scale project. What's the budget to run a big-company website for a year? Easily a million when including staffing costs and overhead.
In contrast, radical innovation quickly runs into tens or hundreds of millions of dollars for fancy research labs and experimentation — and even then the vast majority of inventions go nowhere. Sure, a blinding insight occasionally creates a wonderful invention without the need for elaborate research. The invention of vulcanized rubber and penicillin are canonical examples of luck causing radical innovations. Still, luck favors the prepared mind, and Sir Alexander Fleming had already spent considerable time on fruitless bacteriological experiments when he stumbled across penicillin.
Benefits: From 1% to 1,000% Improvement
There's easily an order of magnitude difference in the effect size expected from each of the 3 design approaches:

A/B testing usually identifies small improvements that might increase sales or other KPIs by  a few percent. Sometimes you're lucky and get 10% or more. Website analytics' advantage is that it's the only way to reliably determine the best design approach when there's little difference between alternatives. Perhaps 1% doesn't sound like much, but if you could realize such gains every week for a year, you'd cash in more than 50%.
In contrast, the full usability process typically  doubles the benefit of the metrics you target (i.e., you get  100%  improvement). For example, an enterprise software project might cut training costs in half or double employee productivity. If the old design was particularly poor, we sometimes see gains of 1,000% or more, but that's rare. More narrow usability efforts, such as fixing a particular design element, might result in a 10% gain for the desired usage metric.
The sky is the limit for radical design innovation. Something sufficiently good could be  1,000%  better than what went before. When defining a completely new product category, you can claim infinite gains, going from  zero  sales to  some  sales. However, more realistically, the new thing should be measured against the opportunity cost of business as usual, which surely is better than zero in any company that can afford to invest in advanced development.

Risk
There's virtually no risk in A/B testing: assuming that the statistical analysis is done correctly, there's close to 100% probability that you'll choose the design variation that makes the most money. If the difference between the two designs is small, you might have to wait a long time to collect enough traffic for statistical significance. But when the alternative design is only marginally better (if it's better at all), you lose little by sticking with the old design during a drawn-out test.
Usability methods also carry very low risk. Indeed, subjecting all designs to usability studies before shipping is prudent  risk-management. Any terrible ideas that emerge from your fevered imagination will be shot down when confronted with real customers during user testing.
Radical innovation is extremely risky. Yes, you might invent the next iPhone. But you're more likely to invent the next Newton (Apple's early and doomed attempt at a personal digital assistant). In fact, almost all innovations fail. Even if it's a good idea, an innovation might be too early for the market. Pets.com, for example, is known mainly for its sock-puppet commercials during the dot-com bubble and for its  spectacular bankruptcy, but other companies that eschewed the purported first-mover ""advantage"" are now making money selling pet food on the Internet.
Who Can Do It?
A/B testing can be done by a monkey (if the monkey has a graduate degree in statistics or can use stats software correctly). You don't need to understand design principles or user behavior. Simply try something different than your current design. If it scores better, keep it on the site. Otherwise, try something else.
Advanced usability methods often require trained specialists, but simple usability activities can be  done by any member of a design team.
Returning to the theme of luck, it's certainly possible for below-genius personnel to be lucky and stumble upon a radical innovation. But typically, to systematically target radical innovation, you need to employ the best people in the world. Ask yourself whether you can realistically recruit, say, the top 1% of experts for your project. Even if you could, that's likely true only for 1% of projects. The remaining 99% of projects must make do with less exalted staff members, who can still be plenty talented and capable of more everyday advances — like the ones actually needed for most projects to succeed.
How Often Can Improvements Happen?
Websites with enough traffic can finish most A/B studies in a day or two. The main limiting factor is your ability to dream up new design variations. Also, designers need some time to realize each idea as an integrated user experience to give it a fair chance against the current site. But basically, there's no reason you shouldn't run a new A/B test  every week.
Usability certainly can be done weekly as well. Since 1989,  I've been evangelizing much faster and cheaper usability methods than most companies employ. (By comparison,  Agile UX design is an upstart.) However, despite my best attempts at increasing the pace of usability,  monthly  turnaround is more common, so that's what I put in the table at the beginning of this article.
Realistically, for something to be truly radical, it can't happen too often. You can find a few exceptions that prove the rule, but most companies are happy to realize a radical innovation once every decade. (Most don't achieve this, even when they try; either their innovations end up being more modest or they fail completely.)
Economic Impact
Given the hugely bigger profit potential from a winning radical innovation, you might expect this approach to have the highest impact on the overall world economy. But all experience shows that most value is realized not from original breakthroughs but rather from the thousands of tweaks and implementations that build on that original work over subsequent decades.
Radical innovations soon become commodities: everybody has electricity, locomotives, and computers. What matters is what companies do with them: the railroad that invents a better way to ship grain might make more money than the company that invented the locomotive. Similarly, while the iPhone was a great advance, Android and other competitors are gradually eating market share.
On the other hand, because A/B testing usually results in tiny improvements, you might expect its overall contribution to be modest. Not so, since it's a method you can systematically apply again and again. Many small streams become a mighty river.
You can determine an approach's value by multiplying 3 factors:
V   =   G   ×   F   ×   N,
where

V   = total  value, in terms of contribution to the economy's GDP.
G   = the  gain  from each improvement. Radical innovation totally wins this one.
F   = the  frequency  with which improvements can happen. A/B testing wins here, with usability as a close runner-up. Radical innovation is left several laps behind on this racetrack.
N   = the  number  of firms that participate in creating innovations. Again, the everyday methods win here because they can be employed by all companies, all the time. Working on payroll processing software? Usability will make it much better, but we'll probably wait 50 years for radical improvements in the product line. (Cloud-based processing might be somewhat better, but surely doesn't count as ""radical"" these days.)

Radical innovation is worth a lot when it happens, but it happens rarely and thus has only a medium-sized impact on the economy. A/B testing also probably has only a medium impact on the overall economy because many of its gains consist of moving market share among competing companies. Usability offers steady product improvements across a broad range, and its productivity gains are cumulative, resulting in a high potential for raising GDP.
The rough estimate in Tom Landauer's  The Trouble with Computers  is that GDP growth would increase by one percentage point if all companies employed sound usability methods. This may not sound like much, but it would add up to  $2 trillion more  in the United States alone over the next 10 years. (€1.5 trillion in the E.U.)
What to Choose?
Because usability makes the most money on average, it's the strategy that I recommend. (No surprise, if you've read my past articles.)
But, in truth, there's  no reason to limit yourself to a single strategy  because the 3 approaches complement each other well.
If you have the budget (or the luck), do try for radical innovation. But employ usability engineering to check whether your ""innovation"" is in fact any good before investing a fortune in bringing a failed product to market. And, once you have a product, refine it through both usability and A/B tests to ensure  continuous quality improvement  and stay ahead of the competition. In the long run, the cumulative effect of many small quality improvements is worth more than rare big breakthroughs."
74,2012-01-15,"""Thinking aloud may be the single most valuable usability engineering method.""  I wrote this in my 1993 book,   Usability Engineering, and I stand by this assessment today. The fact that the same method has remained #1 for 19 years is a good indication of the  longevity of usability methods.
Usability guidelines live for a long time; usability  methods  live even longer. Human behavior changes much more slowly than the technology we all find so fascinating, and the best approaches to studying this behavior hardly change at all.
Defining Thinking Aloud Testing
To define  thinking aloud, I'll paraphrase what I said 19 years ago:
Definition:   In a thinking aloud test, you ask test participants to use the system while continuously thinking out loud — that is, simply verbalizing their thoughts as they move through the user interface. 
(""Simply"" ought to be in quotes, because it's not that simple for most people to keep up a running monologue. The test facilitator typically has to prompt users to keep them talking.)
To run a basic thinking aloud usability study, you need to do only 3 things:

Recruit representative users.
Give them representative tasks to perform.
Shut up and let the users do the talking.

Think-Aloud Benefits
The method has a host of advantages. Most important, it serves as a  window on the soul, letting you discover what users really think about your design. In particular, you hear their misconceptions, which usually turn into actionable redesign recommendations: when users misinterpret design elements, you need to change them. Even better, you usually learn  why  users guess wrong about some parts of the UI and why they find others easy to use.
The thinking aloud method also offers the benefits of being:

Cheap.  No special equipment is needed; you simply sit next to a user and take notes as he or she talks. It takes about a day to collect data from a  handful of users, which is all that's needed for the most important insights.
Robust.  Most people are poor facilitators and don't run the study exactly according to the proper methodology. But, unless you blatantly bias users by putting words into their mouths, you'll still get reasonably good findings, even from a poorly run study. In contrast,  quantitative (statistical) usability studies are ripe with methodology problems and the smallest mistake can doom a study and make the findings directly misleading. Quant studies are also  much more expensive.
Flexible.  You can use the method at any stage in the development lifecycle, from early  paper prototypes to fully implemented, running systems. Thinking aloud is particularly suited for  Agile projects. You can use this method to evaluate any type of user interface with any form of technology (although it's a bit tricky to use thinking aloud with speech interfaces — see report on  How to Conduct Usability Evaluations for Accessibility for advice on testing with blind or low-vision users who rely on screen readers such as JAWS). Websites, software applications, intranets, consumer products, enterprise software, mobile design: doesn't matter — thinking aloud addresses them all, because we rely on the users doing the thinking.
Convincing.  The most hard-boiled developers, arrogant designers, and tight-fisted executives usually soften up when they get direct exposure to how customers think about their work. Getting the rest of your team (and management) to sit in on a few thinking-aloud sessions doesn't take a lot of their time and is the best way to motivate them to pay attention to usability. (For more on how to motivate teams to deliver superior user experiences, see the  UX Basic Training course.)
Easy to learn.  We teach the basics in a day and provide thorough team training in a 2-day ""Learning-by-Doing"" course. Of course, this doesn't cover all the twists and advanced modifications needed to hang out your shingle as a usability consultant, but the point is that  you don't need these extras  to run basic  tests for your own design team.

Think-Aloud Downsides
Being cheap and robust are huge upsides of qualitative methods such as thinking aloud. But the flip side is that the method  doesn't lend itself to detailed statistics, unless you run a huge, expensive study. You can certainly do this — I simply don't recommend it for the vast majority of projects. Better to conserve your budget and invest in  more design iterations.
Other problems:

Unnatural situation.  Unless they're a bit weird, most people don't sit and talk to themselves all day. This makes it hard for test participants to keep up the required monologue. Luckily, users are typically quite willing to try their best, and they quickly become so  engaged in the test tasks   that they all but forget that they're in a study. You can show users a short video demo of a think-aloud session to quickly and vividly explain what's expected of them.
Filtered statements (vs. brain dump).  Users are supposed to say things as soon as they come to mind rather than reflect on their experience and provide an edited commentary after the fact. However, most people want to appear smart, and thus there's a risk that they won't speak until they've thought through the situation in detail. Don't fall for this trap: it's essential to get the user's raw stream of thought. Typically, you have to prompt users to keep them talking.
Biasing user behavior.  Prompts and clarifying questions are usually necessary, but from an untrained facilitator, such interruptions can very easily change user behavior. In such cases, the resulting behavior doesn't represent real use, so you can't base design decisions on the outcome. At the very least, try to identify those cases where you've biased the user so you can discard that small part of the study. (It's worse when you don't know that you've done wrong — then you risk giving the design team bad advice.)
No panacea.  That this one method isn't the only usability tool you'll ever need is not a true downside, as long as you are willing to use other methods from time to time. Thinking aloud serves many purposes, but not all purposes. Once you get a few years' experience with usability, you'll want to use a  wider range of user research methods.

Don't let the downsides get you down. If you haven't tried it before, go run a quick thinking aloud study on your current design project right now. Because these simplified studies are so cheap,  weekly user testing is completely feasible — so if you make a few mistakes the first time, you can always correct them next week."
75,2012-01-03,"This is the article to give to your boss or anyone else who doesn't have much time, but needs to know the basic usability facts.
What — Definition of Usability
Usability is a quality attribute that assesses how easy user interfaces are to use. The word ""usability"" also refers to methods for improving ease-of-use during the design process.
Usability is defined by 5 quality components:

Learnability: How easy is it for users to accomplish basic tasks the first time they encounter the design?
Efficiency: Once users have learned the design, how quickly can they perform tasks?
Memorability: When users return to the design after a period of not using it, how easily can they reestablish proficiency?
Errors: How many errors do users make, how severe are these errors, and how easily can they recover from the errors?
Satisfaction: How pleasant is it to use the design?

There are many other important quality attributes. A key one is utility, which refers to the design's functionality: Does it do what users need?
Usability and utility are equally important and together determine whether something is useful: It matters little that something is easy if it's not what you want. It's also no good if the system can hypothetically do what you want, but you can't make it happen because the user interface is too difficult. To study a design's utility, you can use the same user research methods that improve usability.

Definition of Utility = whether it provides the features you need.
Definition of Usability = how easy & pleasant these features are to use.
Definition of Useful = usability + utility.

Why Usability Is Important
On the Web, usability is a necessary condition for survival. If a website is difficult to use, people  leave. If the  homepage fails to clearly state what a company offers and what users can do on the site, people leave. If users get lost on a website, they leave. If a website's information is hard to read or doesn't answer users' key questions, they leave. Note a pattern here? There's no such thing as a user reading a website manual or otherwise spending much time trying to figure out an interface. There are plenty of other websites available; leaving is the first line of defense when users encounter a difficulty.

The first law of  ecommerce is that if users cannot  find  the product, they cannot  buy  it either.

For intranets, usability is a matter of  employee productivity. Time users waste being lost on your intranet or pondering difficult instructions is money you waste by paying them to be at work without getting work done.
Current best practices call for spending about 10% of a design project's budget on usability. On average, this will more than  double a website's desired quality metrics (yielding an improvement score of 2.6) and slightly less than double an intranet's quality metrics. For software and physical products, the improvements are typically smaller — but still substantial — when you emphasize usability in the design process.
For internal design projects, think of doubling usability as cutting training budgets in half and doubling the number of transactions employees perform per hour. For external designs, think of doubling sales, doubling the number of registered users or customer leads, or doubling whatever other KPI (key performance indicator) motivated your design project.
How to Improve Usability
There are many methods for studying usability, but the most basic and useful is user testing, which has 3 components:

Get hold of some representative users, such as customers for an ecommerce site or employees for an intranet (in the latter case, they should work outside your department).
Ask the users to perform  representative tasks  with the design.
Observe  what the users do, where they succeed, and where they have difficulties with the user interface. Shut up and let the users do the talking.

It's important to test users individually and let them solve any problems on their own. If you help them or direct their attention to any particular part of the screen, you have contaminated the test results.
To identify a design's most important usability problems,  testing 5 users is typically enough. Rather than run a big, expensive study, it's a better use of resources to run many small tests and revise the design between each one so you can fix the usability flaws as you identify them.  Iterative design is the best way to increase the quality of user experience. The more versions and interface ideas you test with users, the better.
User testing is different from  focus groups, which are a poor way of evaluating design usability. Focus groups have a place in market research, but to evaluate interaction designs you must closely observe individual users as they perform tasks with the user interface.  Listening to what people say is misleading: you have to watch what they actually do.
When to Work on Usability
Usability plays a role in each stage of the design process. The resulting need for multiple studies is one reason I recommend making individual studies fast and cheap. Here are the main steps:

Before starting the new design,  test the old design  to identify the good parts that you should keep or emphasize, and the bad parts that give users trouble.
Unless you're working on an intranet,  test your competitors' designs  to get cheap data on a range of alternative interfaces that have similar features to your own. (If you work on an intranet, read the  intranet design annual to learn from other designs.)
Conduct a field study to see how users behave in their natural habitat.
Make  paper prototypes  of one or more new design ideas and test them. The less time you invest in these design ideas the better, because you'll need to change them all based on the test results.
Refine the design ideas that test best through multiple iterations, gradually moving from low-fidelity prototyping to high-fidelity representations that run on the computer. Test each iteration.
Inspect the design relative to  established usability guidelines whether from your own earlier studies or published research.
Once you decide on and implement the final design, test it again. Subtle usability problems always creep in during implementation.

Don't defer user testing until you have a fully implemented design. If you do, it will be impossible to fix the vast majority of the critical usability problems that the test uncovers. Many of these problems are likely to be structural, and fixing them would require major rearchitecting.
The only way to a high-quality user experience is to start user testing early in the design process and to keep testing every step of the way.
Where to Test
If you run at least  one user study per week, it's worth building a dedicated usability laboratory. For most companies, however, it's fine to conduct tests in a conference room or an office — as long as you can close the door to keep out distractions. What matters is that you get hold of real users and sit with them while they use the design. A notepad is the only equipment you need."
76,2011-11-20,"Last week, I made a slide for the new  User Experience (UX) Basic Training course with the  recommended number of test users  for different types of studies. I like teaching foundational courses because they afford me just this kind of opportunity — to distill 25 years of usability process research into a single table. Patterns crystallize when complex topics are condensed to the essence.
For example, why do we recommend  testing more users for card sorting than for  usability studies? Because the usual rule,  ""we're testing the system, not you,""  doesn't apply to card sorting. When eliciting  mental models, we're actually testing the individual users instead of a predefined artifact, and the variability is thus larger.
The thing that surprised me most about my own table: I recommend doing most  quantitative user testing with a sample size that typically entails a  19% margin of error.
19% sounds sloppy. How come a fairly low level of accuracy usually suffices in estimating usability metrics?
Two reasons:

A 19% confidence interval pretty much represents the worst-case outcome. Usually, the error is much smaller.
The  average usability difference between websites is  64%, so even in those few cases where we get a 19% measurement error, we'd usually pick the correct winner anyway.

These mathematical points suffice to defend the idea of saving budget and limiting quantitative studies to mid-sized samples.
But there are two deeper arguments that are even more important.
Focus on Big Problems
You  shouldn't care about small issues  in usability. At this stage, we still have bigger fish to fry. When redesigning a website for usability, the average  improvement in key performance indicators (KPI) is  83%. Clearly, most websites still contain horrible usability problems. Intranets and mobile sites/apps are often even worse.
Your focus should thus be on the really big design problems, where your user experience is failing to meet customer needs. Typically, there are only a few  issues with immense bottom-line impact. Better to invest heavily in those crucial improvements than mess around with changes that'll gain you only a percent or two.
Wasting your budget on overly precise measurements can easily sidetrack you from the important issues; for sure, you'd have less budget left over to work on them.
Maybe in 20 years, user interfaces will be good enough that our only remaining goal will be to fine-tune them for the last few percents' quality gain. That's definitely not the case today."
77,2011-08-14,"If you work in any form of user experience role — or even manage a company with a user interface, such as a website or intranet — you should be a test participant in a usability study now and then.
There are four reasons to do this:

You better  appreciate your average customers  when  you  have to struggle with a new and unfamiliar user interface.
You gain  empathy with the test users  in your regular usability studies. Discovering how stupid and embarrassed you feel when you fail a task helps you understand the importance of making test users comfortable in test situations.
You  sharpen your skills as a test facilitator  by observing the test from a different angle.
Finally, you offer a ""warm body"" to  fill a slot for pilot testing, freeing up the real users for the actual study and thus saving on your recruiting budget.

Pilot Testing = Relaxed Recruiting Criteria
I've harped endlessly in previous columns about the need to  recruit people who represent your target audience to serve as test participants in user testing. This requirement remains essential for valid test results. Testing the wrong people means that you get the wrong findings.
I've also repeatedly said that  designers are not users, and even that  vice presidents are not users. In fact, anyone who works at your organization knows too much and isn't qualified to represent a test user, even if he or she meets the profile of your target audience. (Exception: for intranet studies, you obviously do want to test your own employees. Stay away from members of the intranet team or the IT department, though.)
Given these rules, how can I recommend that you serve as a test participant?
Using insiders as test users  works in pilot testing, and pilot testing only.
The distinction between a pilot test and a regular test is that  pilot testing is concerned only with refining the test methodology. We're not looking for actual usability findings to improve the user interface. We want only to improve the test itself, so that once we bring in real users, we can squeeze out of them as many insights as possible in the limited time we have for the study. (For more on how to run studies, see our full-day Usability Testing course.)
Still, it's best to use representative users as pilot test participants, because it lets you more realistically assess the test plan. You might, for example, want to know whether you have the right number of test tasks for the available time, and internal users typically complete tasks much faster than external users.
So, if it's easy for you to recruit lots of members of the target audience, go ahead and ""burn"" some of them as pilot users. If you do this, you can use some of the pilot session's qualitative findings as real usability findings. However, you can't combine quantitative data between pilot sessions and normal test sessions, because you'll have refined the test script between the two tests. Which is, of course, the point.
Assume  that you'll  change the test plan after pilot testing. In fact, if you're a less-experienced usability specialist or if you're running a particularly high-risk or high-profile study, you should run many rounds of pilot testing and improve the test plan after each round. Just because you're testing a study design instead of a UI design doesn't change the basic value of  iterative design for driving increased quality.
Ethics of User Testing
To my knowledge, our  training course on user testing is one of the very few to cover the  ethics of working with human subjects.
One of the key ethical requirements is to protect participants from mental anguish. We don't want people to leave our study feeling depressed or worthless because they repeatedly failed at using an ""obvious"" computer system. It's very easy for users to blame themselves for the many errors and miscomprehensions they encounter in a typical usability study.
Of course, we always say at the outset that, ""we're testing the design, not you."" But people tend to forget this point when wrestling with a difficult user interface.
Thus, it's important that test facilitators take active steps to make participants feel comfortable. We want them to leave happy and feel energized about having helped improve an important design that they might actually use one day. Not only is this an ethical imperative, but it also helps us pragmatically in getting referrals from among the user's friends and colleagues who might be good candidates for future studies.
It's easy to emphasize the emotional aspect of user testing in our courses, but there's still nothing as powerful as personal humiliation to drive home the need to be gentle with users.
Clearly, there are benefits to periodically changing roles and being the user. Every 2 years or so is a good frequency for such an experience. The rest of the time, even for pilot testing, working with real users is best."
78,2011-01-17,"You can achieve a high-quality user interface by combining 3 design process models:

Competitive testing
Parallel design
Iterative design

Although you should apply the methods in the above list's sequence, I'll discuss them in the opposite order here.
All 3 methods share one basic idea: there's  no one perfect  user interface design, and you can't get good usability by simply shipping your one best idea. You have to  try (and test) multiple design ideas. Competitive, parallel, and iterative testing are simply 3 different ways to consider design alternatives. By combining them, you get wide diversity at a lower cost than simply sticking to a single approach.
Iterative Design
I start with iterative design here because it's the

simplest  process model (a linear progression);
oldest  foundation for user-centered design (UCD),
cheapest  (you often can iterate in a few hours); and
strongest, because you can keep going for as many iterations as your budget allows (competitive and parallel testing are usually one-shot components of a design project).

I hardly need to  define iterative design: I wrote a long paper about it 18 years ago, and it hasn't changed:

Iteration simply means to step through one design version after another. For each version, you conduct a usability evaluation (such as  user testing  or  heuristic evaluation) and revise the next version based on these usability findings.
How Many Iterations?
I recommend  at least 2 iterations. These 2 iterations correspond to 3 versions: the first draft design (which we know is never good enough), followed by 2 redesigns. But my preference is  5–10 iterations  or more, particularly when  iterating weekly (or even more often).
Of course, one iteration (that is, a single redesign, for a total of 2 design versions) is still better than  shipping your best guess without usability-derived improvements. But experience shows that the first redesign will have many remaining usability problems, which is why it's best to plan for at least 2 iterations.
More iterations are better: I've never seen anyone iterate so much that there were no usability improvements to be had from the last iterations. In my research 18 years ago,  measured usability improved by 38% per iteration. These metrics came from traditional application development; if we look at websites, improvements are typically bigger. In a newer case study, the targeted KPI improved by 233% across 6 iterations (7 design versions = 6 iterations between versions), corresponding to 22% per iteration. The key lesson from this latter case study is that it's best to keep iterating, because you can keep piling on the gains.
To get many iterations within a limited budget and timeline, you can use  discount usability methods: create  paper prototypes for the early design versions, planning about 1 day per iteration. In later versions, you can gradually  proceed to higher-fidelity  renderings of the user interface, but there's no reason to worry about fine details of the graphics in early stages, when you're likely to rip the entire workflow apart between versions.
Simple user testing (5 users or less) will suffice, because you'll conduct additional testing for later iterations.
Limitations of Iterative Design?
A classic critique of iterative design is that it might  encourage hill-climbing  toward a local maximum rather than discovering a superior solution in a completely different design space area.
My main comeback?  ""So what.""  The vast majority of user interface design projects are pedestrian attempts to make an  e-commerce site, an  employee directory, or some similar feature that has a massive number of  well-documented best practices.
Yes, some design problems are radically new — say, the original design of the iPad or the Kinect. And some are semi-radical, such as designing a first-generation  iPad app or  Kinect game (after the platform itself had already been designed, but before the corresponding usability guidelines had been documented). But most design problems belong to categories with numerous existing designs.
Of course, superior solutions that exceed current best practice are possible; after all, we haven't seen the perfect user interface yet. But most designers would be happy to nearly  double their business metrics. Simply polishing a design's usability through an iterative design has extremely high  ROI, and is often preferable to the larger investment needed for higher gains.
Parallel Design
Although I remain a strong fan of iterative design, it's true that it limits us to improving a single solution. If you start out in the wrong part of the design space, you might not end up where you'd really like to go.
To avoid this problem, I prefer to start with a parallel design step before proceeding with iterative design, as this diagram shows:

In a parallel design process, you create  multiple alternative designs  at the same time. You can do this either by encouraging a single designer to really push their creativity or by assigning different design directions to different designers, each of whom makes one draft design.
In any case, to stay within a reasonable budget, all parallel versions should be created quickly and cheaply. They don't need to embody a complete design of all features and pages. Instead, for a website or intranet, you can design maybe 10 key pages and, for an application, you can design just the top features. Ideally, you should spend just a few days designing each version and refine them only to the level of rough wireframes.
Although you should create a minimum of  3 different design alternatives, it's not worth the effort to design many more. 5 is probably the maximum.
Once you have all the parallel versions, subject them to user testing. Each test participant can test 2 or 3 versions. Any more and users get jaded and can't articulate the differences. Of course, you should alternate which version they test first, because users are only fresh on their first attempt. When they try the second or third UI that solves the same problem, people inevitably transfer their experience from using the previous version(s). Still, it's worth having users try a few versions so that they can do a compare-and-contrast at the end of the session.
After user testing,  create a single merged design, taking the best ideas from each of the parallel versions. The usability study is not a competition to identify ""a winner"" from the parallel designs. Each design always has some good parts and some that don't hold up to the harsh light of user testing.
Finally,  proceed with iterative design  (as above) to further refine the merged design.
15 years ago I conducted a  research study of parallel design, in which we tried and evaluated 3 different alternatives:

Out of 4 parallel versions, simply  pick the best one  and iterate on it. This approach resulted in measured usability  56% higher  than the average of the original 4 designs.
Follow the recommended process and  use a merged design, instead of picking a winner. Here, measured usability was  70% higher, giving us an additional 14% gain from including the best ideas of the ""losing"" designs.
Continue iterating  from the merged design. After one iteration, measured usability was  152% higher  than the average of the original designs. (So, an  extra iteration added 48%  usability to the merged design — calculated as 2.52/1.70. This is within the expected range of gains from iterative design.)

Of course, there was no reason to stop with one iteration after the merged design; I simply ran out of budget. As always, I'd recommend at least 2–3 iterations.
My study was in the domain of traditional application development. In a  recent study, Steven P. Dow and his colleagues from Stanford University took this approach to the domain of Internet advertising. For the Stanford study, a group of designers created banner advertisements for a social media site, aiming to optimize the click-through rate (CTR). Ads created through a parallel design process achieved 0.055% CTR, whereas ads created without parallel design achieved 0.033% CTR. So,  parallel design performed 67% better. They recorded these scores over the  first 5 days  of the advertising campaign.
Over the full  15-day campaign, parallel-design ads scored 0.045% CTR compared with 0.040% CTR for nonparallel-design ads. Over this longer campaign, parallel design was only  12% better.
We've long known that people tend to  screen out Web ads. This might imply that it's best to constantly launch new ads and run very short campaigns with each, though I'd like to see more data before forming a firm conclusion on this point.
So, even though the conclusions are less strong for ads than for apps, the bottom line is the same: parallel design generates better outcomes.
(Learn more about parallel design in our full-day Effective Ideation Techniques for UX Design course.)
Competitive Testing
In a competitive usability study, you  test your own design and 3–4 other companies' designs. The process model  looks the same as for parallel design, except that the original design alternatives are pre-existing sites or apps as opposed to wireframes you create specifically for the study.
The benefit of competitive testing is also the same as for parallel design: you gain insight into user behaviors with a broad range of design options before you commit to a design that you'll refine through iterative design.
Competitive testing is also advantageous in that you don't spend resources creating early design alternatives: you simply pick from among the ones available on the Web (assuming you're doing a website; competitive testing doesn't work for intranets and other domains where you can't easily get your hands on other companies' designs.)
Just as with parallel design, a competitive test shouldn't simply be a benchmark to anoint a ""winner."" Sure, it can get the competitive juices stewing in most companies to learn that a hated competitor scores, say, 45% higher on key usability metrics. Such numbers can spur executive action. But as always,  quantitative measurements provide weaker insights than qualitative research. A more profitable goal for competitive studies is to understand  why and how  users  behave  in certain ways; learn what features they  like  or find confusing across a range of currently popular designs; and discover opportunities to serve  unmet needs.
Many design teams skip competitive testing because of the added expense of testing several sites. (For example, Nielsen Norman Group  currently charges $45,000 for most competitive testing and only $22,000 to test a single website. Of course, you can get cheaper tests from Tier-2 or Tier-3 usability firms, but they'll still charge more for bigger studies.) But this step is well worth the cost because it's the best way to  gain deep insights into users' needs  before you attempt to design something to address these needs.
Competitive testing is particularly important if you're using an  Agile development methodology because you often won't have time for deeper explorations during individual sprints. You can do the competitive study before starting your development project because you're testing existing sites instead of new designs. You can later reach back to the insights when you need to make fast decisions during a sprint. Insights from pre-project competitive testing thus serve as money in the bank that you can withdraw when you're in a pinch.
Exploring Design Diversity Produces Better UX
All 3 methods — iterative, parallel, and competitive — work for the same reason: Instead of being limited to your one best idea, you  try a range of designs  and see which ones actually  work with your customers  in user testing.
The methods have different ways of helping you explore diverse designs and push you to move into different directions. This is important because there are so many dimensions in interaction design that the resulting design space is humongously vast.
In the ideal process, you'd first conduct competitive testing to get deep insights into user needs and behaviors with the class of functionality you're designing. Next, you'd proceed to parallel design to explore a wide range of solutions to this design problem. Finally, you'd go through many rounds of iterative design to polish your chosen solution to a high level of user experience quality. And, at each step, you should be sure to  judge the designs based on empirical observations  of real  user behavior instead of your own preferences. (Repeat after me:  ""I am not the Audience.""  )
Combining these 3 methods prevents you from being stuck with your best idea and maximizes your chances of hitting on something better."
79,2010-05-23,"For decades, I've recommended  inviting all design team members  and several levels of management above them to attend usability sessions. In fact, one of the main reasons to invest in a  usability lab  as a company progresses up the UX process maturity scale is the ability to host more observers without intimidating test participants. (But with only a few observers, any room where you can close the door can serve as your test facility.)
In our course on how to  do your own user testing, we spend a fair amount of time discussing how to manage these observers, given that they don't know much about usability. That's indeed an important question. At one of my recent conferences, however, an audience member asked an even more important question:  Why invite these ""outsiders"" to the test in the first place?  Wouldn't it be more efficient to just run the test yourself without having to worry about developers, executives, and the like?
Most of the time, it probably  would  be more efficient to leave a single person in charge of all usability activities and let everybody else stay in their office.
The main exception is the common case of testing buggy  pre-release software. In such cases, it's handy to have a few developers around to help you overcome the inevitable crashes. (Remember, it's important to test the early builds: the more primitive the better, because early usability feedback has a much higher impact on the final product than findings that arrive too late to be implemented.)
Building Team Awareness
In real-world organizations, usability process efficiency is not measured in terms of staff-hours spent per amount of data collected. It's measured by staff-hours spent relative to the degree of product improvement.
Collecting buckets of user data will do no good if the resulting recommendations are not followed.
The main reason to invite team members to observe usability sessions is that it  vastly increases the acceptance  of the usability findings.  Seeing is believing.  And seeing it live is even more powerful.
True, we can  record videos  of customers having problems with a website and show clips from these videos in meetings. Doing so does have an effect, and it's definitely something we do in our seminars to increase the impact and memorability of our findings.
However, it's vastly more powerful when team members have seen those same usability problems for themselves during  live test sessions  with users. It's not really that they suspect us of faking the test videos. (Which you should  never do, of course; once caught with phony data, your future ""findings"" will lack all credibility.) Actually being there while something happens has a bigger impact than seeing a recording after the fact, which itself has higher impact than simply reading about it.
Having team members present during usability sessions has many benefits:

Credibility.  Because they've seen how you derive insights, they'll believe your usability findings and reports (vs. thinking you made them up or are just offering your personal opinions or preferences).
Buy-in.  In addition to inviting team members to observe, you should also invite them to a  debriefing  to discuss what happened in the test sessions and to help draw the early conclusions. When people participate in the  analysis, they're more likely to accept and act on the  recommendations.
	
Note:  This is not just a gimmick to enforce your design advice. The actual findings will be better when a group with broader expertise helps you analyze the observations. Plus, each additional pair of eyes will observe something extra.


Memorability.  It's hard to remember findings that you've only seen presented in bullet points or read in a long report. It's easier to remember findings when you can relate them to your personal experience of observing some of the user sessions that generated the findings.
Empathy.  Seeing nice people suffer under your design is a powerful motivator to make it right. Also, the excuse that ""only  stupid users would get this wrong"" isn't used (even subconsciously) by team members who've heard those users make articulate and perfectly reasonable requests for a design that suits their needs.
Fewer design mistakes.  When designers and developers have seen their actual customers, they're less likely to go overboard with design ideas that aren't going to work for users. The better the raw UI, the fewer fixes will be needed after the next round of user testing.

Inviting executives to attend user testing has many of the same benefits. They're more likely to prioritize user experience after experiencing users. And they're less likely to believe bogus ad-agency claims that souped-up designs ""promote the brand"" when they've heard how harshly their paying customers curse such designs. Finally, when it comes time to allocate next year's budget, you can't help but get more if management understands what you do — which they will because live user sessions lodge themselves so firmly in the mind.
Risks of Partial Observations
It's  your  job to do the user testing. Other members of the design team have plenty of other things to do, and executives are even busier.
You can often count on the actual UI designers to attend the entire usability study when testing their own designs, but even they might come for only a session or two during competitive tests. On the other hand, the marketing manager might come for all of the competitive sessions, but bow out before the fifth iterative test of a feature that's proving hard to nail.
Most team members won't have time to attend all of the user sessions. That's okay. Be sure to note in the invitation that people are welcome to come and observe for one or two sessions if that's all they have time for. However, when people  haven't attended the complete study  , there are some problems to look out for:

Premature conclusions from a partial sample.  It's  enough to test 5 users to get a good idea of the main usability insights. But it's  not enough to test 1 or 2 users  if you want to identify true trends and patterns. Anyone who sees only a few users might misidentify the main usability issues and won't have enough insight to analyze the solutions correctly.
Memory biases.  As discussed further in our seminar on  The Human Mind and Usability, our memory is subject to many biases and is pretty poor at remembering abstractions. The beauty of observing user sessions is that usability issues become more memorable because you see them happen first-hand. The danger is that you'll remember the usability problems you actually  saw yourself  much better than those you only read about in the report from the full study.
Entertainment bias: a variant of memory bias, where it's easier to remember particularly striking or outspoken users than the more quiet ones. To enhance the business value of your site or product,  test representative customers, not entertaining ones. And emphasize to observers that user testing is not show business, so ""boring"" users may be just as important.
Empathy biases.  Similarly, designers, developers, and managers will subconsciously feel that it's more important to cater to those users they've  seen  than to meet the needs of other — equally important — user segments that they've only  heard  about.

These problems are inevitable, but you can alleviate them once you're aware of them, as well as warn the other stakeholders about them. In any case, the downsides are far outweighed by the benefits of inviting colleagues and executives to observe as many user tests as they can. For most of them, it's the only chance they ever get to see a live customer in the flesh, so they'll be grateful to you, which is one more reason to do it."
80,2010-01-25,"Alice Davey sent me this question:
I've been to a couple of the courses including ""Application design"", which were very good. I am currently designing an XXX application which users will quickly become ""expert"" in as they will use it frequently. 
I want to do usability testing from an early stage, but what's stumped me is how to test whether the application will serve the ""expert"" user well. By definition, all my test participants will be novice….
This issue is becoming increasingly important as the balance between novice and expert users tilts in the direction of the experts.
However, before we discuss usability for expert users, let's remember that nobody becomes an expert without having been a novice first. You can't forget about usability for new users, except in those rare cases where your user population is fixed, and you don't expect to sell any more copies of your product or hire any new staff to replace or supplement your existing people.
On the Web, the initial user experience is especially important: people — with ten years' experience using other sites — will be novices with respect to your site the first time they click through from a search engine. Unless your site meets their expectations and can be understood immediately, they'll beat a fast retreat back to the sites they already know.
Expert User Testing: The Basics
The basics of testing experts are the same as any other user testing:

Recruit  representative users 
Give them  realistic tasks 
Ask them to  think out loud  (while you shut up and avoid biasing their behavior with untimely hints)

Also, it's usually best to first test with a handful of users and then iterate the design before the next round of testing. You should conduct these small studies as early as possible in the design process, using low-fidelity design prototypes. Paper prototyping might work even better with expert users than with novices, because the experts are used to performing the test tasks. They can thus focus even more on the problem at hand, as opposed to, say, whether a dialog box is presented on an index card or as a rectangle on the screen.
One difference from testing with novices is that the ""realistic tasks"" are obviously more advanced for expert users. You can have them dig deeper and solve larger and more difficult problems.
A second difference might not be as obvious: We almost always ask users to  verbalize their thoughts  in a running monologue as they use a design. This think-aloud process tells you how people interpret the design elements, whether any are confusing, and which ones are compelling or repelling. Although the test situation is a bit artificial, a good facilitator and engaging tasks can make users suspend disbelief and thus tell you the unvarnished truth.
Expert users, however, cause difficulties for think-aloud studies:

Skilled behavior is often automated behavior (as discussed further in our seminar on the human mind and usability). When people are unaware of how they think about a certain behavior, they can't verbalize the reasoning behind their actions. For example, consider how you drive a car: If a facilitator asked you to think aloud during your drive to work, you typically wouldn't describe the steps you take to make the car go faster or slower. But, if you usually drive an automatic and were given a stick shift for the study, you'd probably verbalize your thoughts about using the clutch. In contrast, if you drive a car with manual transmission every day, odds are that you wouldn't mention the clutch during the think-aloud session. To get around this problem with expert users, you can use more elaborate usability methods to slowly (and tediously) analyze slow-motion replays of the interaction and thereby deduce what was going in those cases where users didn't tell you.
Expert users can turn into design critics and bend your ear with their opinions on the product (since they know it so well), as opposed to staying in the user role and engaging with the actual features. Gracefully accept their comments, while remembering that what people say and what they do can be very different. The reason for usability studies is to collect behavioral data, so guide participants back to the role of using the design as fast as possible.

Recruiting Experts
If you're redesigning an existing product, you're in luck: there will already be expert users in the wild. All you need is to recruit them to come visit for an hour or two. You can follow traditional methods for recruiting test participants, with a few twists. Often, a list of registered customers can short-circuit the tedious process of cold-calling study prospects. Sometimes, you can also work with managers of key accounts to contact some of their users of your product. (If so, emphasize that you want average users — not the ""star performers"" that managers often like to show off.)
For a website, you might be able to recruit existing users by posting a request on the site or by asking for volunteers in your email newsletter (which has the added benefit of reaching your most loyal users).
Other options for recruiting expert users include industry tradeshows (particularly if your company has a booth), user group meetings, and social media (especially your own site's social features).
Growing Experts Quickly
The question that prompted this column concerned a new product. In this case, expert users don't exist because the product as yet has no users.
You can't recruit people who don't exist, so you'll have to create your own experienced users. First, you can violate the rule against  testing internal staff. Usually, we don't want to test anyone involved with a design project or the company itself (except for intranet studies, of course). These people know too much, so we won't discover usability problems that stem from the misconceptions of outside users.
Knowing too much becomes a partial virtue in expert studies. Internal users are still not ideal test participants because their mental model of the system will have a much better structure than anything grown organically through mere exposure to the surface manifestation of the UI.
As an example, consider a website's structure: outside users must deduce the structure from the navigation design. Good navigation does give users a clue about the IA, but that's not their primary concern. Users are on a site to get things done, so they don't tend to pay close attention to the site structure, nor do they retain much of this knowledge from one visit to the next. In contrast, internal users' existing knowledge about the product line and the company's way of doing business forms a conceptual model that helps them grow a better mental model of the site structure.
Thus, when you test internal experts, remember that they'll know more and behave differently than external experts.
A second, and better, approach is to grow fresh experts by  fast-tracked training  of new users. You can assign a personal tutor to take test participants through the design and answer all of their questions. (Usually, we don't answer users' questions, because we don't want to bias their behavior, but if we're testing expert use and not initial use, we care less about how people overcome early difficulties. Thus, coaching is allowed.)
You can give test participants plenty of  time to practice  with your design before you start the study. Doing so can be cost-effective because you don't have to monitor them closely during practice sessions. Of course, if users need a week's practice to gain expertise, you have to pay for a week of their time, but at least you don't have to sit next to them all week. Just give them a cubicle and check in on them periodically (and give them a hotline number so they can call the tutor if they have questions).
Training and Manuals
Providing extra-supportive training is one way to quickly produce new experts. You can also use the regular training courses or instructional material if you're testing a product that offers such user support. For a new product, you can even make this part of the test plan and get empirical data on the usability of the training materials themselves.
The master guideline for all user research is to approximate the real world as closely as possible. So, if you have a manual or offer a training course, it's fine to let your test participants access these resources.
Unfortunately, in the real world, not all users will take the training course, even if you offer it upon initial application rollout. New hires who come onboard the following year are lucky if their colleagues take the time to fill them in on whatever they remember from the training sessions. (Sadly, most people remember very little from a course they attended a year ago.)
Similarly, you might offer a manual or users' guide at time of purchase, but this documentation might be long lost by the time new employees are asked to operate the machinery or application.
Thus, it's usually best to run some usability sessions with the docs and training, and some without this info.
Expect Smaller Improvements
The rule of thumb for usability's return-on-investment is that you can  double the desired business metric  (such as conversion rate) the first time you conduct user testing. Particularly on websites, you'll usually find at least one horrendous user-repelling design element that insiders never find problematic, but that's costing the company a fortune in lost business.
For expert users, the improvements from usability research are typically lower. The good news is that human beings are incredibly flexible and adaptive creatures. We live from Greenland to Equatorial Guinea and we can use Linux if we try hard enough. People who've used a software product for a decade will have invented workarounds and tricks to overcome its design flaws. And they will have internalized many of the arbitrary rules behind the UI. Many people are gluttons for punishment and grow to like bad design so much that they resist the change to something better.
After all, they already know how to use the difficult UI, so why change to an easy one that will require some amount of learning? (Users have a strong bias in favor of doing instead of ""wasting"" time learning.)
Because experienced users will have adapted to the old design, the potential for enhancing their performance is usually less than the 100% we often get for new website visitors. On average, a 1/3 improvement is more realistic.
Should You Cater to Experts?
It's more expensive to study usability for expert users, and the expected improvement is smaller. So why do it? Several reasons:

Users are novices for a short time but experts for a long time — at least for any product that they continue to use. Thus, the (smaller) benefit of better expert usability continues to accrue for more years and will eventually sum to much more than the one-time gain from novice improvements.
Some products have a large installed base with a large existing pool of users that's not expected to grow much. To compute the true gain from any usability advances, you multiply the per-user gain by the number of users, so this scenario also favors a focus on expert users.
Some products see heavy, repeated use. An app for call center reps is a classic example. Other products might see only intermittent use, but the impact of good vs. poor user performance is immense. Error handling in industrial control rooms is a good example here. In these cases, users might be highly trained and novice usability less of a concern, but it's crucial to nail expert usability.
Finally, heavy users might account for a disproportionally large share of profits, particularly on those e-commerce sites that cultivate loyal customers. On such sites, you want to make frequent, big purchases particularly easy.

Whatever the reason, it's often worth investing the extra usability resources to improve the user experience for experts."
81,2009-12-20,"One of the  discount usability movement's basic tenets is that we need a drastic  expansion in the amount of usability work  done in the world, and to make this happen we need  more people to take on usability assignments.
This goal is perfectly feasible for several reasons:

Designers and developers are capable of performing basic usability activities such as user testing. Small projects can cope without the benefit of dedicated usability experts.
Simplified usability (e.g.,  testing 5 users) can be  cheap  and easily  integrates with Agile development and other fast-moving projects.
The most important usability methods are  easy to learn. For example, we teach design teams how to do user testing in a  3-day learning-by-doing workshop, where we take them through a fast test of their own design with a few of their own customers.

Honestly, it takes only 3 days to complete a small usability project:

Day 1:  Plan the study and write the test tasks.
Day 2:  Test 5 users for about 1 hour each (cleaning up between sessions).
Day 3:  Analyze the findings and write up the top recommended design improvements.

Even the fastest-moving project should be able to set aside 3 days to improve the user experience. There's no excuse for unleashing your design on the unsuspecting public without at least one round of user testing.
Usability's ROI is huge: if you've never done any testing, you can typically at least  double your conversion rate  or other key business metrics.
When Usability Experts Can Help
I run a company that makes 1/3 of its money from usability consulting (the other 2/3 comes from publishing independent  user research reports and hosting regular  usability conferences.) How can I say that  anybody  can do usability? Have I just written myself out of a job? No.
Usability is like cooking dinner: 

Everybody needs the outcome: As with your need to eat, your company needs to meet its business goals, which it can do much better if the design has been improved through usability.
Anybody can perform the most basic activities: Most anyone can fry a chicken, cook potatoes, run a quick test with 5 customers, or score a design for compliance with a checklist of usability guidelines.
Anyone can learn these basics pretty quickly: They're not all that difficult.
There's a level of excellence beyond the basics: Going to a fancy restaurant and eating a meal cooked by a master chef is vastly different than eating something you throw together yourself in 20 minutes. Similarly, a usability expert will give you insights into your users' needs and your possible design directions that are much deeper than advice you'd get from someone whose main job is in a different field.
Skill levels form a continuum  from beginner to expert; it's not a dichotomy. Every time you learn something, your performance improves. Usability and cooking are particularly suited for continuing education, because anything you learn will remain  useful for many years to come. This is why I place so much emphasis on  usability training: you get better results for every extra bit you learn.

The cooking analogy stretches even further:

Although multi-star gourmet restaurants are wonderful, there's also a place in the world for  modest neighborhood restaurants. Similarly, you should sometimes hire a second-tier usability firm or even a third-tier local consultant instead of bringing in a world-class usability firm. Most design projects include many workman-like, everyday usability activities that the lower-end guys can do well enough.
Even if you can afford it, you  shouldn't eat out every day. Your waistline benefits from getting more modest meals most of the week. Similarly, it's good for your project if many day-to-day usability activities are performed by the designers and developers themselves. The more usability guidelines these folks know, the fewer design mistakes they'll make, and the less rework you'll have to do after discovering how people really use your product.
Variety  is the spice of life. Mexican, Indian, Chinese, Japanese, Italian, French. All great cuisines. Why pick only one? Similarly, combining many usability methods — such as user testing, guideline reviews, analysis by independent experts, analytics or  A/B testing, and field studies — offers the best insights into optimal design. Experienced usability professionals have a very  rich toolbox  that goes beyond the simpler methods that anyone can use after a few days' training.
Sometimes it's nice to  have others do the work. Much as I love Indian food, I never cook it. Too much work to mix and roast the spices. And I don't have a tandoor oven. The people who specialize in these things can do it faster and already have the right tool for the job.
There's  value to being an outsider  who's not restrained by corporate politics or ""the way things are usually done."" In cooking, sometimes you want the chicken done differently than your grandmother's sacred recipe demands. In usability, the person who takes a fresh view can see new things and can say things that insiders would be fired for even contemplating. (Maybe the VP's big, beloved animation shouldn't really be on the homepage. Feel free to blame me if you want to get rid of it.)

It's really a matter of  balance. Yes, experts add value in usability, as in other walks of life. Yes, an  expert can go beyond most peoples' accomplishments. But no, that doesn't mean that usability should be the responsibility of the experts alone. Everybody on the team needs to take responsibility for improving the user experience. And anybody can do usability; the basic methods are simple enough."
82,2009-09-13,"In September 1989, I presented a paper entitled ""Usability Engineering at a Discount"" at the  3rd International Conference on Human-Computer Interaction  in Boston. In so doing, I officially launched the discount usability movement, though I'd been working out the ideas during the 2 years prior to that talk.
The idea, like many, was born of necessity. As a university professor in the 1980s I had a much smaller budget than, say, the IBM User Interface Institute at the T.J. Watson Research Center, where I'd worked before rejoining academia. Clearly, I couldn't do the same kind of high-budget usability projects that the fancy labs did, so I turned this to my advantage and started developing a methodology for cheap usability.
My 1989 paper advocated 3 main components of discount usability:

Simplified user testing, which includes a handful of participants, a focus on qualitative studies, and use of the thinking-aloud method. Although thinking aloud had been around for years before I turned it into a discount method, the idea that  testing 5 users was ""good enough"" went against human factors orthodoxy at the time.
Narrowed-down prototypes  — usually  paper prototypes — that support a single path through the user interface. It's much faster to design paper prototypes than something that embodies the full user experience. You can thus test very early and iterate through many rounds of design.
Heuristic evaluation  in which you evaluate user interface designs by  inspecting them relative to established usability guidelines.

It might be hard to appreciate today when many people talk about ""lean UX,"" but these ideas were  heresy  20 years ago. The gold standard then was  elaborate (and expensive) studies with quantitative metrics. Even now, I appreciate the place of this older approach, and we sometimes run benchmark studies for both our independent research and for bigger clients who want to track metrics despite the expense. But quant should be the exception and qual the rule.
That I advocated simplified methods was bad enough; I also had the gall to  celebrate  them. People were supposed to be ashamed when forced to run a cheap study, but in my 1989 paper, I boasted about applying discount usability in two financial-sector case studies: one that redesigned bank account information and the other that redesigned information about individual retirement accounts (IRAs). For the bank account project, I tested  8 different versions  of the design; in the IRA project, I tested  11 different versions. These extensive iterations were completed in 90 hours in the first case and 60 hours in the second. Both projects had great results and were possible only with discount methods.
Discount usability often gives  better results  than deluxe usability because its methods drive an  emphasis on early and rapid iteration  with frequent usability input.
So, from today's standpoint, can I claim victory? Although my 20 years of campaigning for discount usability have certainly not been in vain, I can't yet declare a win:

Most companies still waste their money testing more than 5 users per usability testing round. My 1989 paper actually advocated testing 3 users, which usually gives the highest  ROI. I've since backtracked from this radical position and now advocate 5 users as a compromise that will work in most organizations, even if they're not as fast-moving as I envisioned designers to be in 1989.
People still pay far more attention to  questionable quantitative studies than they do to simpler qualitative studies that have much greater validity.
Most design teams still don't believe in paper prototyping, preferring instead to spend considerable time creating elaborate design representations before they start collecting user feedback.
Many people still reject the value of usability guidelines and of heuristic evaluation, even though it's become the second-most popular usability method.
While discount usability methods are a perfect match for Agile development projects, many companies adopted Agile without taking on the accompanying discount usability. (Result: confused UX.)

Robustness of Usability Methods
Another, greater controversy surrounded my claim that usability methods are robust enough to offer  decent results even when you don't use perfect research methodology. Obviously, the best methods deliver the best results, and I've spent much of the last  26 years teaching people the correct usability techniques.
But even people who aren't dedicated usability specialists and don't use the refined approach I demand of people I hire for my team can still conduct user studies. For example,  designers can do their own user testing. I've seen countless examples of the value of less-than-perfect methodology. In the spirit of celebrating discount usability's anniversary, here's data from a 1988 study I did while I was evolving the thinking behind the 1989 paper:

In this chart, each of the 20 dots represents a team that ran a usability study of  MacPaint 1.7  (an early drawing program). Each team tested 3 users, and I judged how well they ran the study on a 0–100 scale, with 100 representing my model of the ideal test. In total, the software package had 8 major usability problems (along with more smaller issues that I didn't analyze).
The chart clearly shows that teams using better methodology tended to find more usability problems. In fact, the  study-quality measure accounted for 58%  of the variability in the number of identified usability problems. (The other 42% was determined by the talent of the team members — and probably a bit of pure luck.)
So yes, it does pay to take a course on proper usability testing :-)
Better usability methodology does lead to better results, at least on average. But the very best performance was recorded for a team that only scored 56% on compliance with best-practice usability methodology. And even teams with a 20–30% methodology rating (i.e., people who ran lousy studies) still found 1/4 of the product's serious usability problems.
Finding two serious usability problems in your design is well worth doing, particularly if you can do so after testing only three users — basically, an afternoon's work. Of course, I'd rather that you get a usability expert to dig even deeper, but even so, two is infinitely more than zero (the amount of usability knowledge you'd accrue without  any  testing).
Bad user testing beats no user testing.
This last point is still not widely accepted, and discount usability doesn't yet rule the world of user interface design. But it has come a long way since I launched it to a lecture room of maybe 100 or so people in 1989. On balance, I'm happy that I started this campaign and will continue the fight for simpler usability, more broadly applied."
83,2009-06-07,"Should you offer users help to adjust font sizes or can you simply rely on the built-in browser commands? This question was recently posted to an interaction designers' discussion group (which will remain unnamed to preserve the anonymity of the individuals dissected below).
12 people responded to this question. Most simply offered a personal opinion as to what they would prefer. Fair enough: All people are experts on their own preferences. But there were 6 postings that commented on what would be best for  other  people.
2/3 of these postings were pure guesses, whereas 1/3 was based on some form of data in the form of empirical user observations.
Guesses:

""In this day and age, [...] most people who need to increase their font sizes in their web browser already know how to do it.""  WRONG 
""People who do need to resize type will do so via the browser; it's not hard to do so.""  WRONG 
""It's not 1995; not all 50+ people are such newbies that they don't know, or wouldn't want to know, how to resize text in a browser.""  WRONG 
""The people who most need to increase font size are people 65+, which is the group least-likely to be skilled enough to have adjusted settings.""  RIGHT 

Data:

""I had to set it manually for my parents, and while the percentage of people over 65 becoming more and more savvy is increasing at an amazing rate — hidden functions like adjusting text size is something that escapes them.""  RIGHT 
""I've observed usability studies on sites that included text resize widgets [...] most, if not all, the participants [...] had no idea what it was.""  RIGHT 

Data Beats Guesses
The general guideline is to use relative font sizes that  let users resize (if they know how), but to display big and legible text as the default. This conclusion is based on numerous observations that show that many  older users don't have the skills to resize fonts.
In our discussion group example,

100%  of the designers who provided  external data  were right, whereas
25%  of the designers who relied on their  personal opinion  were right.

Most strikingly,  75% of guessers were wrong. You'd be  better off tossing a coin  than asking advice of these people.
In this simple example, basing design advice on the  smallest amount of empirical observation  of real users quadrupled the probability of being right.
A word of caution: Although data from your parents is better than no data, I don't recommend that you base design decisions on your family members because they're likely to be smarter than average users. (Because  you're  smarter, being somebody who understands usability.) We know from our studies of  children and  teenagers that average kids and teens have much greater difficulties using websites than one would think after listening to Internet executives proudly tell stories about their offspring's online skills.
Testing 2 Users Beats Guessing
While striking, our text-size example is based only on a small set of responses. Another example provides a similar conclusion with a bigger sample.
We tested two different ways of displaying bank account information with 76 users each, for a total of 152 test participants in a between-subjects benchmark test. We asked users to perform tasks such as checking their account balances and finding out what interest rate the bank was currently offering. The results were as follows:



Usability Metric
Design A
Design B


Success Rate (across four tasks)
56%
76%


Time to Complete Four Tasks (min:secs.)
5:15
5:03


Subjective Satisfaction (1–5 scale, 5 best)
2.8
3.0



On all three  usability attributes, version B scored better, though only the difference in success rates was big enough to be statistically significant. Overall, there is no doubt that  B was better.
(In contrast to this study, sometimes both designs win on different usability attributes. For example, one design might make people more successful, while the other helps them accomplish the task faster. In such cases, you might have to make tradeoffs or, when possible, create a third design that combines the best aspects of both alternatives.)
In this case, I showed designs A and B to 21 people who were taking an interaction design course and asked them which one they would recommend to the bank. Going purely on their personal  guesses  as to which design was best, the probability of getting the best design recommended was  50%. That is, no better than flipping a coin. (Asking your trusty coin is an easy way to save on consulting fees.)
I then asked another group of 38 people taking the same course to test the two designs with 2 users for each design. Now, going on empirical  observations  of 2 users' behavior for each alternative, the probability of recommending the best design was  76%  .
Another way of looking at this outcome is that testing just 2 users per design reduced the probability of being wrong from 50% to 24% — cutting it in half. Of course, a 24% probability of picking the wrong design is not good enough if you're talking about a  high-ROI design decision, so we'd obviously want to test more than 2 users per design in such cases. (I usually recommend  5 users.)
Still, even though it's an extremely scaled-back study, testing 2 users per design hugely improved the recommendation over the flipping-a-coin performance from guessing.
(In this study, the two versions looked equally good, which is important for measurement studies. If you compare a rough-looking prototype with a fully refined graphic design, you will bias the scores.)
When Guesses Go Horribly Wrong
Comparing our two case studies, the guessing camp from the text-size example had by far the worst performance. A person who based a design decision on these guesses would be wrong 3/4 of the time. In the bank example, they'd be wrong only 1/2 the time.
So, why the miserable discussion-group guesses? The answer lies in the following two statements:

""In this day and age...""
""It's not 1995...""

Sadly, too many Web designers refuse to believe in the  durability of usability findings. Thinking that  ""things that were difficult in the past must surely be easy now""  has led many websites to their doom.
When we actually study real users, we see how  slowly they learn  about technology and how little their ability to use fancy websites has improved. And, most important, we see how little users  care  about learning fancy Web techniques. People just want to get in, get their stuff done, and get out. They don't want to learn.
Guesses go wrong because many designers desperately want to believe in the potential of advanced design. They simply can't fathom  how little most people know about their pet technologies.
(Yes, in recent testing, we did find a  few small advances in users' skills, but it's slow progress; you'd better believe that simplicity will continue to win the day for decades to come.)
A Little Data Goes a Long Way
In my two examples, the probability of making the right design decision was vastly improved when given the tiniest amount of empirical data: observing your own parents, or testing 2 users per design.
Of course, a bigger study would be better, but  any  data is better than no data. How many design decisions do you make without any empirical observation of your customers' behavior?"
84,2008-07-27,"I've always recommended  fast and cheap user testing, with as many iterations as you can squeeze into your schedule. My true preference is weekly testing; make, say,  every Wednesday your User Day  and schedule 4 customers to test.
Unfortunately, I know of few projects that have sustained this pace, and many of those were my own. Recently, I came across another project that employed weekly testing: the redesign of tivo.com.
I've long been a fan of TiVo's usability. Indeed, 4 years ago,   The New York Times  quoted me calling the oversized yellow pause button in the middle of the TiVo remote ''the most beautiful pause button I've ever seen.''
As it happens, Nielsen Norman Group's own research confirms the new TiVo website's usability. Before I knew about the design team's methodology, we had selected the site to be part of one of our independent research studies, in which we compare a bunch of sites to derive  general lessons for Web usability. We included Tivo.com as an example of a ""fancy"" design, and it  scored in the top 20%  in our study. It's definitely possible to have a good-looking, media-rich site, as long as you keep usability as a goal throughout the design process.
After hearing that TiVo employed my favorite usability approach, I decided to find out more about how they did it. Here's what we learned from talking with three key members of the TiVo team.
TiVo Case Study
Although the usability of TiVo's DVR product interface has been celebrated, it was only in a recent redesign that the company applied the same rigorous user research to its website. As a company known for including user testing in its product development, TiVo realized that this situation was incongruous.
""I'm selling you a product where the key differentiator is ease of use,"" says Margret Schmidt, the company's vice president of user experience and design, ""but if the website isn't easy to use, how will you believe that the product is? We tried to bring that to the site.""
12 Weeks, 12 Tests
The project team had a short timeline for the comprehensive site redesign. To make the best use of that time, team members devised a research plan to conduct 12 consecutive weeks of user testing leading up to the site's relaunch. ""We had a certain timeline for the project,"" says Deborah Torres, a TiVo user researcher. ""We were working backwards from a launch date.""
For 8 of the 12 sessions, Torres and her team employed remote testing methods, engaging users with collaborative online tools such as Go-To Meeting or WebEx. ""We tested users across the country,"" she says. ""Midwest [and] East Coast tested designs and content remotely — we did remote usability testing where we shared the mouse.""
For the last sessions, the team brought users into the company's usability lab and tested 1-on-1. ""We got stakeholders in the room,"" says Torres, adding that it was good for the team to ""see frustrations in people's faces"" rather than simply listen over the phone.
Key company stakeholders supported the testing effort, believing strongly that the investments at the project's beginning would pay off in fewer site changes after launch. And indeed, as Schmidt notes, the early testing revealed things that they otherwise wouldn't have found until after implementation — when the problems would have been both more expensive and more difficult to fix. ""It would be hard to get those [problems] fixed later,"" says Schmidt. ""The more info we get now, the better.""
Actionable Results
One of the major findings of the user testing was the need for a  new approach to content, both for words and presentation. Despite being well intentioned, the website's content didn't resonate with users.
""When the agency first wrote the content, it had a voice and personality to bring out the attributes of the brand,"" says Schmidt. ""But when you see 8 people who say, 'I don't read fluff — I read bullets,' it changes things.""
Based on the testing results, the design team changed the content. In some cases, they reduced chunks of text down to 1 or 2 sentences, followed by bullets. No fluff. In other cases, they changed the content's presentation, wording, or placement, all to good effect.
""The biggest learning was that when people are shopping for a DVR, they need a lot of information and very detailed information, but also want a clean, sleek design,"" says Torres.
The team balanced these two requirements by offering  deeper links, with more detailed information in inline links  that users can access without leaving the page. ""This way we are providing the details without muddying it up for those who want high-level information,"" she says. By applying this simple interface solution, they were able to present the same information to purchasers at different phases of the purchase cycle.
Another problem they discovered was that users weren't finding a critical link because it wasn't well labeled or well placed on the page. ""Below the fold we had a link for:  'Already have a TiVo DVR?'  "" says Torres. ""It links to how-to articles. Users were completely ignoring this. They thought it looked like an ad. And this information is one of the factors in purchase decisions.""
Although team members saw (and heard) users express keen interested in such content, the  users couldn't find it. By renaming the link  ""want step-by-step instructions?""  and moving it up the page, the team helped users find the link.
Such content changes are, for the most part, small ones. But their impact on task success was huge.
""Words and phrases matter to the users,"" says Schmidt. ""With content, you think you've got it nailed down, then you see users get stuck on something really simple. Without hearing [the feedback] on the way, we wouldn't know why the site was not working later.""
This is perhaps one of the most compelling reasons for undertaking this type of rigorous testing prior to launch. Once a site goes live, you don't know what's behind the stats that you see reflecting user behavior. That is, you don't know the reasons why people act the way they do.
""It's hard to drill down into why people are doing or not doing something,"" says Schmidt. ""It's hard to do with just analytics. When you hear and see people react, you can get into their heads and make decisions based on this.""
As Schmidt rightly notes, such information is invaluable. ""This is  huge!"" she says. ""It's easy to iterate now, and hard to do after it is built.""
Design Team Benefits
At TiVo, iterative testing was a good business decision that effectively addressed quick fixes and identified longer-term projects for the site. The testing was also  a great tool for the designers  — that is, it was both useful  and  encouraging.
""It was amazing: They could do tests, and turn around and get us feedback the next day,"" says user interface designer Alex Logan. ""It's informal feedback, but useful. Timelines are tight, and it would allow me to get my work rolling right away.""
Another reason this design approach was so successful for the designers was that  the research built on itself. ""I never felt like the research contradicted itself from week to week,"" says Logan. ""We got a general validation of a design. In my mind, you never really know about a design, whether it will work, until you put it in front of users.""
Logan says that the learning came in ""tiny pieces"" that let the team immediately make defined changes. ""With each test you get closer to the ideal design,"" he says.
""Even minimal testing can [offer] something substantial,"" says Logan, adding that sometimes they had two or three options, but didn't know which users would prefer. Instead of just picking one, he says, ""  We let the users decide. No matter how brilliant I think I am, I'm not secure until [my design] is in front of people.""
5 Reasons to Take This Approach

Costs the company less.  TiVo's testing approach included many remote user sessions, so the company investment was mostly in staff time, with a limited amount devoted to incentives. And the  ROI was great. As Schmidt put it, ""one to two thousand dollars is not that much money for what you learn.""
Offers motivation.  Rapid testing cycles give the design team reassurance that they're on the right path. ""Test after test, you see that you're going in the right direction,"" says Schmidt. ""It can be very motivating to the team.""
Helps drive business decisions.  Because the feedback is weekly and available in real time, the team can react in real time. Schmidt used the feedback cycles to push back on business owners and ask, ""Do we really have to keep pushing forward, or fix the issues? The users are telling you how they will react in the field.""
Creates a testing culture.  Admittedly, TiVo's culture is already user-focused on the product side; until this exercise, however, the company wasn't particularly user-focused on the Web side. After seeing the testing cycle results, the company committed to user experience across the board. ""Now, user experience is key,"" says Schmidt. ""It has changed how we structure all Web projects moving forward. Each new [Web] project will have the same level of UE testing as products.""
Builds internal knowledge.  ""Build internal knowledge,"" says Schmidt. ""Use internal resources, internal research groups that can move quickly.""
	""Agency timelines can be long and it takes a while for new people to get up to speed on issues,"" she says. ""We present a new design to a researcher who can learn what's [right and wrong] about it in the context of other projects. Designers build a design gut,"" she says. ""Every time you listen — you become a better designer.""

8 Keys to Successful Outcomes

Just do it.  ""Do it and listen to it,"" says Schmidt. ""You have to take the ego out of the design. The design is great when the  users  say it is.""
Get support from stakeholders and find a project champion.  ""It is only valuable if you have stakeholders who are willing to listen and act on the feedback,"" says Torres.
Encourage participation — from across the organization.  ""Invite everyone who has an interest in it,"" says Torres. ""Every usability test session, the observation room was packed with designers, product managers, and content people. Even remote sessions, there were managers watching and logging in."" That kind of participation ensures widespread buy-in across the organization.
Determine priorities so you get clear results.  Ask pointed questions and set clear research objectives for each screen/task so you can turn them into actionable results immediately after the test sessions.
Circulate the results — quickly.  The morning after each test session, Torres released a top-line summary of teaser bullets telling people the test results from the night before. People across the organization appreciated these summaries.
Match the research method to what you're trying to uncover.  Different test methods suit different goals. The TiVo team used a variety of methods, from focus groups and concept testing to remote user testing and onsite 1-on-1 task-based testing. By matching the method to the testing goal, the team obtained a range of valuable feedback.
Ask for buy-in on the results.  According to Schmidt, the most effective way to get buy-in — especially in a rapid testing situation — is to present the findings along with a solution. ""When you share the raw research without proposing a solution,"" she says, ""they worry about it. You don't need 20 people debating how to address the findings.""
Keep a sense of humor and a good attitude.  Have a flexible staff that's willing to roll with the pace and unknown nature of the work. ""You don't know what you're going to test next,"" says Torres. ""It was really taxing on researchers. At times, I didn't know what we were testing until the night before, and I had to build a protocol and be ready to moderate the next day.""

You Can Do It
TiVo may have more of a user-centered design tradition, but there's no reason you can't institute weekly testing at your company as well. By making user exposure an  expected  part of the week, you keep everybody focused on customer needs."
85,2008-06-22,"99% of readers can stop here; this column is for people who have a great website or intranet.
More typically, your  site sucks  and it's incredibly easy to discover your main usability problems.  Test with 5 users and you'll likely get enough insight to  double your site's business value. This step is easy; we can  teach design teams to run simple usability studies by walking them through a complete test of their designs in just 3 days.
But what if you've already run many rounds of qualitative user testing and improved your site through  iterative design? Say you doubled your conversion rate. And then you doubled it again. You're now at a stage where you have a  really  nice design that satisfies all the obvious user needs and lacks obvious impediments to use. (Like I said, I'm writing for only 1% of readers here. But eventually, you'll be in this situation if you keep polishing your website or intranet through continuous quality improvement.)
Once you've picked all the proverbial low-hanging fruit, it's no longer so easy to improve usability. Quick simple studies are not enough; your design's remaining issues won't be of the gobsmacking kind that are glaringly obvious from testing a handful of users.
To make a good user experience even better, try the following four ideas:

Identify where your design  exceeds expectations, and apply that success even more broadly.
Look at things that are  close to going wrong  and ensure that they never will.
Go beyond user experience for individual customers to consider  enterprise usability.
Discover  unmet needs.

When Things Go Well
Taking a metaphor from the airline industry, it's now time for you to  study ""safe flights.""  Historically, the airline industry improved safety by studying airline accidents. Modern airplanes now crash so rarely that the industry studies safe flights to achieve further improvements, investigating why flights are safe and whether they had any close calls.
Similarly, you can now review usability study segments that you often skip — that is, those sections where users immediately click the right thing, have no difficulties, and basically like your site.  Why  was the correct link the most apparent?  Why  was it so easy for those users to accomplish their goal? And  what  did they particularly like about the design?
You likely have site areas that have yet to implement all such best practices. Hunt them down and elevate them to the standard established by pages that users love.
To exceed the average levels of user success,  identify  ""lucky"" users  — the ones who are outliers in terms of  uncommonly successful, high-performance use  of your design. Do they exhibit any special behaviors that you might strengthen and make even more fruitful? Are there ways to encourage average users to exhibit those same ""lucky"" behaviors so that they can benefit as well?
Tiny Symptoms Pinpoint Incremental Opportunities
We often observe users who  hesitate  before clicking or  initially misinterpret  information, but then correct themselves before they get into trouble. In such cases, it's tempting for the design team to simply utter a collective  ""whew""  — we almost had a usability problem, but our design won in the end.
Certainly, if your design ultimately works for your test users, you don't have high-priority usability issues. But  miscues  are still symptoms of something you ought to correct (once you've fixed the design disasters, of course). Correcting miscues is important for two reasons:

A miscue wastes time. A hesitation might last only a second, and a misinterpretation might be cleared up in five seconds. But that's still lost time. For a website, slower user performance translates into less-than-smooth user experience; the site just doesn't quite feel right and thus reduces customer satisfaction. For an intranet, every wasted second lowers employee productivity. And, because miscues happen hundreds of times per day, they can add up to a full workday over the course of a year.
Some users are less lucky. Even if all your test users overcome a miscue without getting into real trouble, some users will in fact click the wrong link or act on their misinterpretation. We can't test millions of users; anything that's error-prone in a test will, in real life, trigger an actual error in a small percentage of cases. If your design is only  slightly  error-prone in user testing, you'll observe miscues as symptoms rather than seeing actual mistakes. Real errors will still happen, just not very frequently.

If you conduct  eyetracking studies of your content, you'll sometimes identify  regressions  : cases in which the eye moves back instead of forward because the user needs to read something twice.
A regression is another symptom of a small problem: the text was difficult to understand. Regressions can result from several different readability problems:

A  word  that the user didn't know
An overly complex  sentence structure 
A  muddled argument  or seemingly dubious information

The vast majority of websites and intranets don't need to worry about regressions because they've yet to rewrite their content to comply with the existing  guidelines for writing for the Web. In such cases, doing eyetracking studies is a waste of money. But once you've achieved good online writing, watching for regressions can help you polish content usability even further.
Enterprise Usability
Most usability methods focus on improving the user experience for individual users. However, there is a bigger scope of experience:  enterprise usability, which considers a design's implications across an entire company.
Improving experience beyond the individual level is particularly important for  B2B sites. A classic business example is the need for content that can help users  convince an ultimate decision-maker about the value of a product or service. In B2B, it's often not enough for users to think your product is good — the person charged with researching purchases on the Web might need to make a presentation or write a report to a committee or a boss who will make the final decision.
To study these organizational issues, you need broader research methods, which are more expensive than single-user testing.
Discovering New Needs
No matter how good you are at meeting customers' current needs, there are probably opportunities to do new things that they don't even realize they want. If you discover something truly new, you can even patent it and thus ensure a sustainable advantage over your competitors.
How do you identify needs that customers don't know they have? By running  field studies in which you observe user behaviors. As I've said a million times,  don't listen to what they say; look at what they do.
Sadly, field research is much more expensive than testing a handful of users in the lab. But, once you've perfected your current UI, that's what you have to do to advance to the next level.
Are Small Improvements Worth the Effort?
Most websites and intranets offer such a miserable user experience that it takes only a microscopic usability effort to create astounding improvements. The return on investment is usually several thousand percent for a company's first usability project. ROI continues to be sky-high for many years, because it takes a long time for a company to progress in  usability maturity to the stage where it routinely releases designs with superb user experience.
But once you're good, then what?
ROI will be smaller  when  more  usability work is required to identify  fewer  design improvements. Still, I have two arguments for why you should nonetheless continue usability work:

Small ROI doesn't mean negative ROI. Obviously, if you lose money on usability, you shouldn't do it. But I don't think any website is so perfect that further improvements would be worthless. As your site improves, usability might become less of an urgent priority, but it shouldn't be abandoned.
Competitive pressure  means that this year's leading design will be an average design in a few years — and a miserable design a few years after that. User expectations continue to increase. Thus, even if you could afford to halt your usability efforts today, you'd have to resume them in a year or two as the competition passed you by. But, since it  takes several years to build high-level usability competency  in an organization, you can't simply resume usability at will after it's been discontinued. If you really think it will be 2 years until you find any further design improvements worth doing, spend the interim doing fundamental user research into your customers' deep needs.
"
86,2008-04-20,"I became a full-time usability professional in 1983, and celebrated my 25th  anniversary in the field a few months ago. Seems like a good time to take stock...
Evolution: How Things Have Changed
The field's main difference today compared to when I started is size: it is  much  larger now. In 1983, usability was a  narrow discipline  pursued by a few people, largely confined to academia, phone companies (mainly Bell Labs), and a few pockets of enlightenment in the biggest computer companies.
When we met at conferences, we all knew each other. Although new people did join the field (as I did in '83), the new membership rate was about a handful per year. All told, there were maybe 1,000 usability people in the world (primarily in the U.S. and the U.K.).
Today, by my estimates, there might be as many as 50,000 full-time usability professionals in the world, supplemented by about half a million people with part-time usability responsibilities or interest.
Usability now exists in a vast range of companies in any industry you might imagine — far beyond its origins in the high-tech sector. For an example, just  look at the 1,345 companies that sent 2,187 people to my usability conference in 2007. Over the past two years, 67 countries — from Estonia to Peru — have been represented; usability has truly become an international field.
Yes, some big organizations like Amnesty International, the California Franchise Tax Board, Cathay Pacific Airways, HP, PayPal, Verizon, Virgin, Wells Fargo, and the World Bank sent large teams last year (often 10 or more people). But the average of only 1.6 participants per company shows how widely dispersed usability interest is. Most companies still only have one or two usability people.
Highlights of 1983
Usability basics haven't changed  in 25 years. Methods for user testing were already well established by 1983 — the year that John Gould and Clayton Lewis presented a paper outlining 3 main principles for successful design:

Establish an  early focus on users  and run field studies before starting any design work.
Conduct  empirical usability studies  throughout development.
Use an  iterative design process.

These are the  same 3 things we  teach today as the most important usability steps. The main difference now is that Gould and Lewis talked about collecting quantitative measurements during their tests, whereas I've emphasized faster, qualitative studies for most projects since I started evangelizing ""discount usability"" in 1989.
In 1983, character-based user interfaces dominated, and GUIs were still new. In a February 1983 BYTE magazine interview, Larry Tesler — now Yahoo's head of user experience — discussed the role of user testing for Apple's Lisa (the precursor to Macintosh). Later that year, a Xerox team presented a study on the best way to design a mouse: 1, 2, or 3 buttons? The winner was 2 buttons, but that didn't prevent the launch of the 1-button Mac the following year. It took several more years before we got a widely used 2-button mouse.
The prevalence of mainframe systems during the first years of my career came to good use many years later: the first generation of Web-based applications was similar to the good old IBM 3270 screens. More recently, when we tested Flash-based applications, we reprised many findings from the studies I did of Macintosh software during the second half of the 1980s. (Our application usability seminar is based on 25 years' of experience — the examples are recent because the audience dislikes old screenshots, but they often illustrate UI principles I first saw in testing PC, Mac, or mainframe apps many years ago.)
In general, it's good for usability professionals to have  experience with many generations of user interface technologies  — this allows you to:

Generalize the underlying issues in interaction design: when you see something every year for 25 years, you know there's some truth to it.
Avoid being swayed by the surface appearance of the latest gizmo.

Personal Retrospective
I've been happy with my career choice. I've had a great time in every single one of these 25 years, with so many interesting studies and exciting findings. Usability allows us to make  everyday life more satisfying  by empowering people to control their destiny and their technology rather than be subjugated by computers like many observers feared when I was in graduate school (remember  ""I'm a human being, don't fold, spindle, or mutilate me""?).
At the same time it helps humanity, usability also strengthens business by making companies more profitable through increased sales and higher productivity. Very few jobs allow you to simultaneously do good for both human rights and profitability.
Usability as a Career
If a young person today asked me whether usability is still a good career choice, I wouldn't hesitate to say  yes. If anything, user experience is a  better career now  than when I started.
In 1983, usability was an oppressed discipline. We few pioneers had to struggle against the prevailing attitude that computing is about power and features — not ease of use and a pleasurable user experience.
Today, usability is widely recognized as one of the key drivers of website profitability. Not a day passes without a big-shot CEO declaring support for better user experience. Nonetheless, much remains to be done, and most companies are still at a  low maturity level in terms of embracing the full user-centered design lifecycle.
It's exactly  because  most companies are only beginning to progress toward full-scale usability that I feel confident in declaring it a great career choice: We know that usability works — it adds vastly  more value to design projects than it costs, and companies tend to add more and more usability over time as they experience this payoff in their own projects (as opposed to just reading about it in my articles).
Still, we're unlikely to reprise the 5,000% growth of the past 25 years. Over the next quarter century, the field is more likely to  grow by 1,000%. But that's still pretty good. We have  job security as long as there's stupid design  in the world, and that's forever: every new technology that comes along will be abused.
Come join us. You'll have a great time. I certainly am, and will enjoy continuing to keep pace with this ever-growing field.
See Also
Other retrospectives:

30 years with computers 
Alertbox 10 years
Nielsen Norman Group 10 years
"
87,2008-03-16,"One of usability's main laws is that ""designers are not users."" This insight is about as important as ""vice presidents are not users"" and ""users are not designers"" (so  don't listen to them; watch them).
The usability discipline exists because  systematic methods can overcome the gap  between the design team and its target audience. You  don't have to rely on your best guesses: you can  find out  how actual customers behave and what you can do to make them buy more.
There are  3 different degrees of difference  between designers and their users, from a small fissure to a gaping gulf. The strategy for bridging the gap depends on how severe it is.
Level 1: The Designer  Is  the User
Sometimes this is literally true: you're designing something that  only you will use. If you're making a spreadsheet to track your butterfly collection, feel free to use any obscure abbreviations you please. If no one else needs to understand the design, you can safely toss the usability book out the window.
More commonly, designers at this level are core members of the larger target audience.  Open software  often falls into this category: designed  by  geeks,  for  geeks. That's why Linux, Apache, Perl, and many similar products have been so successful — at least as long as the audience remains a group of technology-obsessed users. Of course, these same products don't stand a chance of growing their user base to include ordinary humans.
This level is our best-case scenario. But, even when designers are representative of the target audience, a crucial difference remains: designers  know far more  about the product simply because  they built it.
So, even here, you need usability studies both to find out how your target users think about the product and to optimize it accordingly.
Level 2: The Designer Understands the Product
Say you're designing a mobile phone. You use one yourself. You use voice mail a bit. And, from talking with your mother over dinner this last holiday, you even have some insights into how non-geeks use their phones. Bravo! You have your core persona nailed (""Mom""), you know what features are the most useful, and you know what buttons are annoying to push. You're ready to design the next killer phone — no doubt about it.
Uh, not so fast. It's true, you do use a phone. But you don't necessarily know the features people need to do their jobs (which are likely completely different than yours). And you don't know what UI most people will find easy or difficult to use.
As with the cell phone, so, too, with a whole range of other designs, such as workhorse consumer websites, news sites, photo sharing sites,  intranet employee directories, or project management applications. Simply because you use these yourself, you assume you know what other users need.
In fact, there's a big gap between designers and the majority of users. Last month, for example, we tested Banana Republic's website and several male participants had trouble buying a suit there. This wasn't because the site designers didn't know about suits. It was because the navigation and product pages confused people who didn't understand how the company thinks about its products:

Click  Men  in the top navigation, followed by  Suits  in the left nav.
Get a category page with thumbnails of guys in various suits.
Click a photo of a suit you like.
Get a product page with a bigger photo of a guy wearing the suit.
Click  Add to Bag  if you like the suit, and complete the checkout process.
A few days later, you receive a package from Banana Republic. Surprise: You got only the jacket, not the complete suit.

Turns out that, to buy a suit, you have to buy  both  the jacket  and  the pants as separate SKUs. Given the interaction sequence I just outlined, how would you know? Well, if you read everything closely, you would realize that the product page under step 4 was the page for the jacket, not the suit. But the product page  shows the guy wearing the suit, and we know that  users don't read every last word on every Web page.

A Banana Republic product page reached by clicking through from the ""suits"" category page.
The initial view (left); the view after users selected an alternate photo (right). 
Once you have a strong conceptual model of this process (and remember: users don't — their mental model starts out spotty), it's clear from the initial photo that it only represents the jacket. However, if you  expect  to see a suit based on your interaction history, you might easily believe that this photo shows a suit, even though it's showing only the suit's upper part. Furthermore, the thumbnails under the hero shot give users alternate views. This is a correct implementation of usability  guidelines for product pages. Unfortunately, the last alternate photo shows the guy  wearing the full suit  .
So, you click  Suits  in the navigation, you click a product from the list of suits, and you click the  Buy  button on a page showing a guy wearing a full suit. Any wonder that you expect to get a suit?
I wonder how many angry customer-support calls Banana Republic has to field, and how many costly returns they have to process — all because the designers knew how the site worked and didn't expect users to have trouble with something as simple as buying a suit.
Design Team ≠ Typical Users
Generally, if you're a member of a design team, you are  not representative  of the target audience. I don't care if you're the interaction designer, the graphics artist, the information architect, the writer, the programmer, or the marketer. All of these people:

know too much  about the product (be it a website, intranet, application, phone, whatever);
are too skilled  in using computers and the Web in general; and
care too much  about their own baby (so they can't imagine visitors bouncing after scanning the homepage for 30 seconds —  but that's what outside users do).

Finally, you're almost certainly the  wrong age  compared to large segments of the target audience. True, you might remember being a teenager, but that doesn't mean that you know  how teens use the Web or what problems they'll have navigating your design. For sure, you have no idea about the  troubles senior citizens will have using your site.
Getting to Google is Hard
How difficult is it to perform a search on Google?
I'm not talking about the challenge of formulating a good query, interpreting the results, or revising your search strategy to reap better results. Those are all very complicated research skills, and few people excel at them.
I'm talking only about the very first step in searching the Web:  Getting to your favorite search engine  so that you can run a search there.
Would you say this is easy or difficult? Think a bit before reading on.
**** pause ****
If you thought it's easy to get to Google, think again. In our current round of usability research, only  76%  of users who expressed a desire to run a Google search were successful. In other words,  1/4 of users who wanted to use Google couldn't do so. (Instead, they either completely failed to get to any search engine or ended up running their query on a different search engine — usually whatever type-in field happened to be at hand.)
On the one hand, 76% is a high  success rate. On the other hand, getting to Google is a very simple task. It's not even a true task — that is, it's not something users want to accomplish for its own sake or something we'd pose as an assignment in user testing. Getting a Google search box is the first step in searching the Web, which is only the first step in doing something real (such as, in one of our test tasks, to find  ""a strong vacuum cleaner that is easy to use, can pick up pet hair, and costs under $300""  ).
Also, for this round of research we're deliberately  recruiting above-average users, so the success rate across all Internet users is probably lower than our finding. (Our goal is to discover usability guidelines for the sites people visit  after  they click through from the SERP, not to document search engine market shares. As a result, we're not concerned with measuring this precisely.)
I doubt that  any  Web designer would be incapable of running a Google search. So, the fact that 1/4 of users can't do it is a striking demonstration that you can't rely on your own experience if you want to reach a broader audience.
Level 3: Designing for a Foreign Domain
The vast majority of high-value interaction design projects fall into this last category: mission-critical applications or  B2B websites targeted at highly  specialized users  who perform  narrow tasks  that depend on expert  domain knowledge  within a  context  you can't even envision.
A few examples:

Oil company geophysicists integrating drilling data and seismological data to determine the best place to drill for oil.
Telephone company planners deciding when to order additional switches for a central office (buy too early and you waste millions; buy too late and you run out of lines and can't provision new subscribers). I did some studies of these applications back when I worked in the Bell System:  very  complicated screens.
Gastroenterologists analyzing esophageal motility recordings for their patients.
Insurance company claims adjusters working on customer cases, from car crashes to earthquake damage.

A website example: to beef up our forthcoming course on  Fundamental Guidelines of Web Usability, we tested a number of specialized sites targeting skilled users. One set of studies involved dentists using B2B sites that promoted diagnostic  dental clinic equipment. All I know about dentistry comes from being on the wrong end of the drill, so I have no clue about what dentists look for when they're deciding whether to acquire an innovative product. But I don't need to get a DDS degree to find out. We  recruit real dentists and learn from them what the site should do.
If you're working on domain-specific user interfaces you have no choice: you need up-front user research (such as field studies) to discover how the experts do their work. You should also collect additional usability data at each step in the design process, beginning with testing simple  paper prototypes of early feature ideas.
Bridging Gaps = Get the Data
The  wider the gap  between your situation and the  users, their  tasks, and their  context, the more you need a systematic usability process to inform and adjust your design.
In most design projects, the gap is wide indeed and you usually need more usability activities than you suspect. Even when you're a member of the target audience, the design should reach wider than just your corner of the group. To achieve that, you still need usability. Just not as much."
88,2007-11-04,"I typically advocate ""discount usability engineering"" — that is, cheap and fast methods to immediately improve your user interface. But in some cases, it makes sense to  invest more to get more.
When assessing your usability options, you'll often find two alternatives with drastically  different costs, such as:

Testing solely with domestic users vs. including users from one or more foreign countries to assess  international usability issues
Lab-based  user testing vs.  field studies at customer locations
Testing only your own design vs. including 2–3 competitors' designs ($28K vs. $45K on  our price list)
Paper prototyping vs. a more fully implemented prototype
Hiring usability professionals at different skill levels (and thus different  salaries: $69K vs. $100K for entry-level vs. somewhat experienced staff, respectively)

To decide which option to choose, you have to  combine 4 parameters:

The project's expected value
The cost of the usability alternatives
The estimated difference in outcome for the alternatives
The rate at which you discount future money streams to account for uncertainty

Example Calculation
As an example, we'll plug some sample values into the spreadsheet. First, let's say our project has an  expected value of $1M. It might be an e-commerce site with expected sales of $5M per year and a 20% profit margin, or an intranet redesign that's expected to  increase the productivity of 10,000 employees by $100 per employee per year.
Note: I'm only considering the value realized during the first year after launch. This might be unfair, since projects often have a longer lifetime. For example, the average  time between intranet redesigns is 3 years. If you expect your project to live for more than a year, you should obviously add the subsequent years' value (after reducing those values by your chosen discount rate to account for the added uncertainty of future time periods). Here, for simplicity's sake, I'll only look at the first year.
Next, let's estimate the difference in cost and outcome for the two usability alternatives. Let's say that the cheap approach costs  $10K  and that the expensive approach costs  $40K.
Conservatively, we'll then assume that we expect the expensive approach to generate only  20% better results  than the cheap approach. Because  usability tends to double a design project's value, we'll estimate that the cheap approach will increase use by 90% and the expensive approach will increase use by 110% to bracket the expected outcome.
A 20% difference in outcome is typical when you go from domestic to international user testing, for example. Even though you learn more by testing in more countries, most findings are the same everywhere. So, if a 20% gain is going to cost you 300% more, you might think that it's a no-brainer to go with the cheap approach — but you should read on.
Finally, let's pick our  discount rate: 100%.  That is, whatever value we gain during the first year after project launching is only worth half as much in today's money: a $2 gain next year is worth $1 today. This huge discount rate makes sense because  design projects are hugely uncertain:

First, the project might be  cancelled  or fail for reasons beyond our control. Even the best design has zero value if it never ships.
Second, all of the numbers we plug into the spreadsheet are  estimates. We don't know for a fact how successful our project will be, and we don't know what user research will yield until after we've spent the money. True, usability consultants might give you firm quotes for the cost of the studies, but if you're doing it yourself or paying by the billable hour, you don't even know for sure how much the two alternatives will cost.

So, we basically have 3 options: no usability, cheap usability, or expensive usability.

No usability  is the easiest to compute, since we don't have to consider the cost of the usability alternatives. We assumed a value of $1M and a discount rate of 100%, so we can easily compute the project's present value: $500K before deducting project costs. So, if design and development (other than usability) cost, say, $200K, the project has an NPV (net present value) of  $300K, and should go ahead.
Cheap usability  adds 90% to the project's value, resulting in $1.9M, or $950K after applying the discount rate. We then deduct the $10K usability cost, resulting in a present value of $940K. In other words, usability has added $440K to our project's expected present value. After deducting the $200K development costs, the project now has an overall NPV of  $740K.
Expensive usability  adds 110% to the value, resulting in $2.1M, or $1,050,000 after applying the discount rate. We now have to deduct $40K, resulting in a present value of $1,010,000. Thus, usability has added $510K to the expected present value. After deducting the development costs, overall NPV is  $810K.

Under our assumptions, using expensive rather than cheap usability  increased NPV by $70K, so we should go ahead and use the expensive usability approach.
(These calculations assume that development costs are the same with and without usability. In other words, I assume we chose appropriate usability methods and set the design direction early in the project, and thus didn't have to rework any already-implemented features. If you conduct user testing late in a project, this assumption doesn't hold, and you'll have to add some cost for fixing the flaws that testing uncovered. Let's say that the cost of fixes is $50K for cheap usability and $60K for expensive usability because there's 20% more to fix. Deducting these costs doesn't change the conclusion; it simply reduces expensive usability's advantage by $10K.)
Heuristics for Choosing: Cheap vs. Expensive
If you'd rather avoid the fancy calculations, following are five general rules for choosing between cheap and expensive usability.

The  higher the project's expected value, the more you should invest in usability. Usability payback comes from multiplying the project's value due to increased use (or more efficient use, in the case of an intranet). If your project's value is low, you won't earn much by doubling it.

	
For example, if your project has an expected value of $5K, you shouldn't even do cheap usability, because gaining an additional $5K isn't enough to justify spending $10K. (However, there are still some  ultra-cheap usability methods you can use for tiny projects.)


The  higher your project uncertainty, the higher your discount rate. In such cases, cheap usability options are typically the way to go. Yes, the more expensive usability option would offer higher gains if you were ultimately successful, but your probability of a total loss would be too great to justify the investment.
The  earlier in your project  you employ usability, the more you should invest because you can implement the added findings without much rework. Late studies should be cheap; you won't be able to make many fundamental changes anyway.
	
Note the  tension between project stage and uncertainty: early in the project, your uncertainty is higher (which counts against big expenses), but your potential for implementing the gains is bigger (which counts for thorough studies). This is why you should save some of your usability budget for later studies, when you have more polished designs to test.


The  bigger the expected difference in outcomes, the more you should go with expensive methods. When the expected outcome difference is small, save your money and go with the cheap methods.  Quantitative studies are a great example: they're very expensive and usually only worth the cost for huge projects because they otherwise add relatively little value. A similar analysis can be made for international studies: if you have few overseas customers, the added cost of testing them won't be worth the expense.
Finally, obviously,  the more expensive  the high-cost methods are, the less you should use them. In our example, the NPVs will be identical if the expensive method costs $110K. Thus, if the expensive method is 20% better than the cheap method, you should use it as long as it's no more than 10 times as expensive as the cheap method. If it's 11 times as expensive, you won't get enough added benefits to justify the added cost.

When it comes to selecting usability methods, there are many parameters to consider, and many different scenarios. That's why both expensive and cheap usability methods make sense under the appropriate circumstances."
89,2007-10-14,"Sometimes you need to  test a large number of users. One option, of course, is to apply the standard user-testing methodology, and just do more of it. Keep testing until you're blue in the face. Unfortunately, this often gets you into serious trouble with project deadlines.
Alternatively, you can use multiple-user simultaneous testing, or MUST (a term I have from Dennis Wixon). As the name indicates, with MUST, you test multiple users at the same time so you get done sooner. In most MUST studies, we test 5–10 users at once (but, as I describe below, you can also set up labs with many more test stations). Theoretically, there's no upper limit to the number of users you can test in each session.
When to Use MUST
Most usability studies should be  simple and small-scale, but in some scenarios, it's useful to conduct MUST:

For  quantitative studies  and  benchmarking, you typically need to  test at least 20 users per condition in order to get statistical significance.
For  long-duration tasks, you need to  test each user for days or weeks  to observe valid behaviors. Examples here include:
	
Developer tools. You can't test a system to support professional programmers by having users develop and debug a 20-line ""hello world"" program; users must work through an industrial-scale problem. The same is true for other high-end problem-solving applications, such as CAD.
E-learning. You can't test lesson 39 unless the learners have first made it through lessons 1–38. For a sufficiently advanced e-course, each test could take a week or more.


Usability focus groups. To alleviate the  problems with traditional focus groups, each participant should start with a one-on-one session testing the live user interface. Following the test sessions, participants can then congregate to discuss the experience and how it relates to their everyday needs. This method definitely requires MUST because all participants should test the interface just before the focus group meets.
Games design. I describe this case in detail below.

How to Run Many Users Simultaneously
When testing many users, you usually need  many test facilitators. The main exception is for the long-duration tests, where a few facilitators can circulate among users and/or review video recordings of critical incidents.
If you're one of the lucky few companies with many usability specialists, they can facilitate MUST sessions. This is expensive, but efficient: All you need to do is turn the experts loose, since they already know how to run a study.
Most companies, however, don't have enough usability pros to assign one to each test user. Happily,  non-usability staff can run user test sessions, especially if a seasoned usability expert has prepared the test plan and written the tasks.
For our latest MUST study, we hired cognitive science students from Don Norman's former department at the University of California, San Diego; they were excellent facilitators. In other studies, we've used developers and marketers from the project team. Being responsible for a MUST session is a great way for such team members to get intense exposure to customers.
Training Facilitators
Ideally, your new facilitators would go through a  full usability training workshop, but this is rarely possible in practice. Still, it's best if you spend at least a few hours training facilitators before giving them a go with real users:

First, of course, you should explain the  theory and best practices of user testing, including steps such as ""keep quiet and let users do the talking,"" which I've discussed many times before.
Second, newbie facilitators should  watch an experienced usability expert  run a sample session with a pilot user. Doing so
	
shows newbies how to facilitate a study, and
better concretizes the test plan and test tasks than simply discussing them or going through them on paper.


Third, conduct a  role-playing exercise  in which the usability expert plays the user and simulates difficult situations that facilitators can encounter, such as users who don't talk or users who ask if they can use certain features. (In the latter case, we typically say:  ""You can do anything you would normally do at home/in the office."")

Preparing the Users
There's  very little special preparation  to do for MUST study participants. Just follow standard procedure for  recruiting test users, welcome them to the session, give them consent forms and instructions, and so on.
However, the actual MUST sessions differ from traditional sessions in two key ways:

Thinking aloud  doesn't work when people are tightly packed into small cubicles, so you shouldn't include the usual instruction for users to vocalize their thoughts as they move through your design.
You can  minimize the distraction  of having multiple users by telling participants that they'll likely be working on different tasks. This reduces their natural inclination to look at other users' screens and also prevents people from feeling stupid if other people finish before them and leave the room.

Why Not Use Automated Testing?
Why is MUST worth the trouble when you can just outsource your testing to one of several services that promise to run a  panel of users  through your site and give you elaborate charts of the outcome? Because usability is strategically vital to the success of any website or interactive product — and outsourced panels just aren't up to the job.
The #1 rule of all user testing is to test with  representative customers. Panels rarely meet this requirement; they're composed of  people who get paid a pittance to sit like drones  and complete online tests. If you're targeting a very low-level audience, then this might be worth a gamble. But not if you're a  B2B site selling to construction engineers or hospital pharmacists. Not even if you're a normal B2C e-commerce site.
For fun, some of my colleagues once signed up with a panel operator. Despite being fully truthful in their responses to the initial questionnaire (which many people aren't when they register for panels), they were assigned to several studies for which they were not even remotely in the target audience. These ""studies"" are often a form of  voodoo usability that generate misleading results.
Even if a panel operator could get you representative customers, automated studies are still a shadow of real usability research because you can't sit next to the user.  Direct observation  is invaluable, both for  seeing the details  that would never get reported in a chart, and for gaining the  deep understanding  of each user's individual behavior.
Based on user responses to questions, a screener assigns users to various segments. It's common to find that such assignments are a bad fit, however. Using typical  persona names, for example, you might say,  ""no way is this user a Susan, but he's a pretty close Patrick,""  and then reclassify the user. Other times, you might have to throw out the data because someone is simply not in your target audience; still other times, you can use that person's insights qualitatively — as a representative of a corner case — even though you won't include them in the core sample. When you're in the room with users, you can identify these situations and act accordingly. When all you get is a chart, you won't even know that some of the test participants were border cases or outside your audience entirely.
Finally, having members of your project team serve as facilitators and get live exposure to real customers is immensely motivating. A chart from 500 anonymous panel members doesn't have a fraction of the  emotional impact  of sitting next to a few fellow human beings as they suffer through your design.
High-End MUST Lab: Microsoft Games Studios
There are many different ways to run MUST. The most impressive setup I know of is at Microsoft Games Studios. Here's one of its playtest labs:
  Playtest lab at Microsoft Games Studios.
Headsets are helpful when many people in the same room are playing audio-intensive games. 
  Floor plan for three labs at Microsoft Games Studios. 
According to Dennis Wixon, Microsoft Games Studios' User Research Manager, his group runs 8,000 gamers through its labs every year. This is a huge amount of user testing — far beyond the amount of research conducted by the average company. No wonder Microsoft needs a high-end lab devoted exclusively to gameplay testing.
(Microsoft tests its software and websites in other usability labs. These labs don't have Xbox 360s at each seat as they might distract users from making their pie charts in Excel. :-)
Why does Microsoft do so much usability testing for its games? A  huge  amount of money is at stake. A game like  Halo 3  is considered the ""drive title"" for the Xbox 360. If  Halo 3  is great, gamers will buy the 360. If  Halo 3  sucks, they'll stay with their old Xbox or get a PlayStation.
I don't know the budget for Wixon's group, but I'd bet it's a tiny fraction of the $300M  Halo 3  sold in its first week alone — and an even tinier fraction of those Xbox sales that are dependent on the title's success. Obviously, the usability group works on many other games besides the Halo series, but for the latest version of that game alone, they  analyzed more than 3,000 hours of gameplay across 600 test participants. This detailed user research made  Halo 3  much more fun to play and also more approachable for new players, who are essential to boosting sales beyond those of  Halo 2  .
The second reason to test multiple users for computer games is that it's a more  difficult field of user interface design  than the domains we usually test. Every game is a new world, whereas all websites follow much the same rules.
When we test websites, we can rely on  thousands of documented guidelines that explain user behavior with this style of interaction design. So, when we observe a behavior, we can typically conclude:  ""oh, this is an instance of guideline #728,""  which we have already seen hundreds of times with other users on other sites. Recognizing a documented behavior means that we don't need to test that many users, because a few observations suffice to build confidence that we're on the right track. Also, data analysis is simplified because we can build on published research.
Games require a more delicately balanced user experience  than functional interfaces. Say that you're designing a weapon's targeting system. If you work for the Army, you want to make the system's user interface as fast and accurate as possible. Shooting the bad guys before they get you is obviously the way to go. But, if you're doing the equivalent design for  Halo 3, the answer isn't as obvious. Make targeting  too  fast and easy, and the game soon stops presenting a challenge. Sure, you could blast all the bad guys without breaking a sweat, but the game's  purpose  is to make you feel like you're on the edge and living dangerously. (A danger real soldiers typically want to avoid.)
It's easy to test a UI to eliminate difficulties. It's hard to determine whether one presents just the right amount of difficulty. That's why Microsoft Games Studios runs so many users through its playtest labs.
Simpler MUST Labs
You can run MUST studies without the fancy lab. The following photo shows a study we ran recently, where we tested 5 users in each session.
  Impromptu cubicles installed in a meeting room for multiple-user simultaneous testing.
(The photo shows 3 of the lab's 5 test stations.) 
In our lab, we simulated cubicles by taping  cardboard partitions  to each desk. Of course, it's better to use real cubes, but our discount cubes worked swimmingly. We used  slave monitors in each cubicle  so facilitators could get a good view of the action on their user's screen without having to lean in. However, for most studies, it's perfectly fine to use a single monitor.
In addition to cubicles, we've used three other setups for previous MUST studies:

Single-person offices  for each user. (For an intranet study, we used participants' actual offices.) Individual offices can be in different parts of the building (or even in different buildings) so long as each user is assigned a facilitator who stays for the session's duration.
Large-scale usability labs  with a row of test labs. In such cases, we tested each user in a separate lab. This setup is particularly suited for studies in which users work on large-scale problems for days at a time, and a few facilitators move around among users. Because the facilitators can enter and leave each lab's observation room without the user's knowledge, it's possible to watch many users in a day without disrupting their concentration. Also, you can leave users who are working on a part of the task you don't care about without communicating this fact and thereby biasing their behavior.
Offices turned into a networked lab. This case mixes the previous two setups: many single-user offices are wired to a single observation room. You can do all this over the local-area network, which can carry the streaming screendumps for a remote slave monitor as well as webcam views of the users.

Ultimately, the vast  majority of usability studies should be qualitative  and  test 5 users. Still, there are situations in which you need more, and that's when it's nice to have MUST in your toolkit so that you can get the study done before your deadline.
Learn the many different variants of user testing (and when to do what) in our full-day Usability Testing course."
90,2007-06-24,"At my seminars, I'm often asked whether  designers and developers can perform usability activities  or whether those activities should be left to  dedicated usability specialists. The answer depends on your circumstances; there are several important pros and cons to having designers and developers branch out into usability.
Con: Specialization Drives Performance
We've known since  Adam Smith that specialized workers are more productive than people who try to do everything. That's true in the user experience disciplines as well. You can't even talk about ""designers"" as a single group. There are graphic artists, interaction designers, information architects, writers, and many other professionals, each of whom specializes in designing some aspect of the total user experience. Sure, a single person can do both visuals and information architecture, but such efforts will rarely match the quality of work done by dedicated specialists.
Indeed, even usability professionals often specialize in usability subfields, such as quick qualitative studies, formal measurement studies, field studies, competitive studies, site analytics, surveys, guidelines and standards, and so on.
The more different activities you have to do, the less time you'll devote to learning each one's intricacies — and the less experience you'll build up with each one as well. Lack of experience is especially problematic for usability, because the ability to correctly analyze user behavior is extraordinarily dependent on the experience of having  previously observed a wide range of behaviors.
The argument for specialization is particularly compelling for design vs. usability because different personality types tend to excel at each discipline. Design obviously appeals to people with a drive to put things together, whereas usability requires analytic thinking and conceptualization skills.
There are many other fields in which people might branch out into different job categories. In filmmaking, for example, a lead actor  could  write the script, but it's usually better to have a skilled screenwriter do the job.
Pro: Less Staff Required
It would be great if every project team had ten designers who were each experts in different aspects of user experience design. And it would be great to have a bunch of usability professionals support that design by working on multiple forms of user research and other usability activities.
A big company typically has a user experience department with dozens or even hundreds of such dedicated specialists — once it's moved sufficiently up the  maturity scale to invest appropriately in user experience. But a small company (or an immature big company) won't have such a large team.
At many companies, the user experience teams are too small to justify a dedicated usability professional. Indeed, many project ""teams"" consist of a single person. Luckily, usability basics are easy enough to learn: we take team members through a simple  user test of their design in a 3-day workshop.
Not having usability people is no reason not to have usability. Your team will benefit from doing some of the simpler usability activities itself, especially if following an  Agile development process. Discount user testing can be done with  minimal resources.
Con: Lack of Objectivity
If you test your own design, however, you might be less willing to admit its deficiencies. Designers can be too willing to dismiss user complaints or problems as minor or unrepresentative, when in fact the test indicates the need for a deep redesign. Also, designers can get so caught up in their own theories about how users  ought  to behave that they forget to test for cases in which people behave differently.
One of the key things we teach in our user-testing courses is how to  write good test tasks, because most novice test facilitators use the wrong tasks and get poor data as a result. Designers are susceptible to employing tasks that focus almost exclusively on their own pet features rather than on goals that users really want to accomplish.
Of course, if you  know  you might lack objectivity, you can proactively work to overcome this deficiency. For example, you can force yourself to include test tasks that go beyond the things you personally care about.
Designers can also increase their testing objectivity by asking colleagues to review their test plans or to listen in on a session or two.
Pro: Higher Credibility, Easier Communication
When the same person does both design and usability, you don't have to worry about the designer dismissing the usability person's findings. People tend to believe in their own work!
When different people focus on different project aspects, they have to communicate, which takes time in meetings and time for report writing. In contrast, when a designer runs a test, he or she knows what happened and can immediately start redesigning to fix the problems that the test identified. No meeting, no report, no communications overhead required, so long as the info is lodged within a single brain.
The downside to the lack of meetings and reports is that the usability findings don't get refined and discussed, which again leads to fewer deep insights and major user-interface reconceptualizations. Even though usability reports are overhead, they're also a good way to build  institutional memory that will help new designers and future projects.
Any Test Is Better Than No Test
If you can afford it, it's better to have dedicated usability specialists perform your project's usability activities. But, the choice is not between this ideal and doing nothing. For many projects, there's a middle road: let the designers or the developers do double duty and take on some usability work. This is much better than having no usability at all.
Full-Day Course
More details in the course on The One-Person UX Team Tool Box."
91,2007-04-29,"As long as you're testing within a single country, there's no reason to expend resources traveling to multiple cities and conducting the same usability study again and again. You'll simply  observe the same behaviors repeatedly, and learn nothing new  . Better to save your budget and spend that money on new tests of either additional design ideas or your competitors' designs.
This conclusion -- that the test location doesn't matter -- is different than the usual lesson from market research, where you find different results in different regions of the country. It's therefore common to conduct focus groups in 4 to 5 cities, or more if the budget allows.
Because traditional wisdom recommends conducting research in multiple locations, we've done so for many projects over the years. But, except for the few special cases discussed below, we've always identified the same usability findings, no matter where we tested. By now, we can clearly conclude that it's a waste of money to do user testing in more than one city within a country.
Behavior vs. Opinion
Why does usability differ from market research when it comes to the number of required study locations? Because with usability, we  test behavior, not opinion  . Further, we test that  behavior with a defined artifact  (i.e., a specific user interface).
People obviously have different attitudes in different regions, including differences in what they'll pay for a given product and in how many people will want the product in the first place.
But when it comes to reacting to a set of interaction design options, people usually interpret the screen elements the same, no matter where they live. What's easy in one city is just as easy in another city. For example,  breadcrumbs  facilitate navigation of hierarchical websites equally well in Los Angeles, New York, or Boise, Idaho. You don't need to test your breadcrumb design everywhere. Similarly, many Web users rely on search, not because they live in a rural or an urban environment, but because search is an inherently useful way for users to gain control of a vast and diffuse information space.
Why One Location is Enough
To show why a single testing location is sufficient, we'll use a parking meter as an example.
In some cities, paying for parking would be considered outrageous: like charging for the air you breathe. In other cities, 25 cents per hour would be acceptable, while in yet other cities, several dollars per hour might be the norm. So, doing market research on parking fees would definitely have to be done as a separate project for each city.
But, let's say that we're testing parking meter  usability  : Can users determine where they're supposed to insert their money? Can they understand the instructions about the cost per hour? Do they notice the display showing the time they've paid for? Do they understand that the number changes to indicate remaining time?
The answers to these questions are crucial for the meter's interaction design. The answers would also be the same everywhere, which is why the design team would only need to  test in one location  .
In some cases, multiple tests might be beneficial. First, if a parking meter is intended for a region that had never before charged for parking, it might be a good idea to test with users who were completely new to the parking meter concept. Novices would doubtlessly encounter more usability problems than more experienced users. Second, cities with very high parking fees might need a feature that lets users pay with a credit card or dollar bills instead of coins; this different user interface would require a different test. Note: you can meet both of these additional testing needs without traveling to a new city -- it's a matter of which users you test, and which features you ask them to try.
In most cases, differences are due to  diversity in the users' circumstances  , not to geographical variation. It's therefore best to  recruit a diverse set of users  : some experienced users, some novices; some young, some old; some doctors, some nurses; some loyal customers, some who swear by your competition. Usually, you can cover the spectrum of user profiles in one location. What matters is the differences between people and their behaviors, not the differences between cities.
Another reason to limit your tests to your preferred city? It's usually where you're based. When you test in your own city, it's much easier for other  members of the design and development team to observe the test  . Yes, in theory they could watch a video recording. In practice, however, nothing beats watching a customer live as they use your design. Having the direct, personal experience of observing user test sessions is a powerful motivator that gets team members to buy into the usability findings. Thus, even if there might be a theoretical benefit from testing in another city, you'll usually get more usability findings implemented if you test on your home turf.
When to Test Elsewhere
As often in usability, there are exceptions to the general rule. In a few cases, test location does matter.
Sometimes,  a single industry dominates an area  to the extent that it's considered a company town. Such dominance might be due to a single company or to several similar companies. Example  company towns  include:

Detroit, if you're testing a car site
Washington, DC, if you're testing a government site
Hollywood, if you're testing a movie site
Downtown Manhattan, if you're testing an investment site
Brussels, if you're testing a European Union site
Silicon Valley, if you're testing a technology site

As an example of the latter, I remember a test I ran about 10 years ago. In  recruiting our test users  , we followed usual procedures and screened out anybody working in usability, Internet marketing, interface design, graphic design, or programming. Unless they're your target customers, you don't want such people in a test because they can't stay within their  role as  users   . They always have to be  critics  as well, and comment on how your design compares with their pet theories on what makes a good website. The entire idea of usability testing is to observe how customers use your interface, not to hear them speculate on how other people might use it.
In this case, despite our best screening efforts, one test participant turned out to be just such a critic. Although she met all of our qualification criteria, toward the session's end it emerged that  her roommate was a marketing manager at Yahoo!  She had become a Web insider by osmosis.
In general, the problem with company towns is that the locals  know too much about the industry and its products  . They are also too interested in industry gossip, and might actually read arcane PR announcements that average customers would never notice.
Because of these differences, locals have a much easier time using your website than people in other cities. To get a realistic impression of the user experience for 95% of your customers, you have to  test in a location that's not dominated by your industry  .
Another case where test location matters is for  international use  , which can obviously be studied only by testing in multiple countries.
For  intranets  , it's often best to test the design at both the corporate headquarters and at a field or branch office. Employees at non-HQ locations frequently have different levels of knowledge about company events, and they might also use applications differently or emphasize different features. Furthermore, including people outside HQ is a sound political move for the intranet team's usability effort because it makes the intranet seem less like something imposed from above.
Finally, some  products are simply not used  (or are used very differently) in some locations. For example, you shouldn't test a website to sell a particularly powerful home heating system in Florida, where most people don't have experience with high-end heaters. Similarly, you shouldn't test a tourist site in the city it's promoting, because it's targeted at travelers coming from elsewhere.
Despite such exceptions, the general rule remains clear: Usability findings are typically the same, no matter where you test. So, save your travel budget and conduct your studies in a single city. If you don't believe me, you can always test your next design version in a different location and see if the change in venue generates any new findings. Chances are, it won't."
92,2006-09-10,"I'm seeing a disturbing trend in which user studies are geared to entertain rather than to reveal knowledge about the target designs. The people running such studies have the best of intentions: they want to  show their clients well-known usability problems in a captivating way, and thereby gain support for design improvements.
Their philosophy holds that most Web managers are clueless about users, so the real goal of usability testing is simply to educate management.
However, if you stack your test to demonstrate something you already know, you  degrade your study  in two ways:

It's intellectually dishonest to run a study in order to generate specific findings. It's very easy to  bias  the study so that the findings are not even real.
When you know what you want to find, it's easy to  overlook other issues  that might turn out to be the design's biggest problems. In my experience, when clients ask you to study one aspect of their website (say, the navigation), the real problem is often something different (say, the content). It's therefore essential to keep an open mind during user testing and  expect the unexpected.

Typical Errors
Some common mistakes of entertainment-focused user testing include:

Screening for outgoing, articulate test participants  so that you can get more elaborate quotes and comments. While it's certainly more engaging for observers to watch users who talk a mile a minute, it's also potentially biasing. Out of the 233  guidelines for recruiting test participants, the number one guideline is to get a  representative sample of customers  . If you're trying to sell, say, networked storage to enterprise customers, you need to test a broad range of system administrators. You can't just screen out introverted nerds, since they could be the key influencers and might well behave differently than more easygoing sysadmins.
Focusing the test on the site's sexy parts, while skipping parts that require users to read detailed descriptions of products and services. But such boring behaviors often close the sale, particularly for  B2B sites where detailed product specs and whitepapers are a key part of the user experience.
Prompting users to talk  in great detail about the problem your design team is currently debating. Yes, it's tempting to get input for your next decision. Such feedback isn't reliable, however, if you've turned users' attention away from their tasks to grill them about multiple, alternative design ideas. You have to watch users  do  things --  not listen to them talk about hypothetical designs they aren't using. It's fine to test two or three alternative  paper prototypes that you've quickly mocked up. It's not fine to ask users to speculate on ""how would you like it if the site did this...,"" because what people say has no relation to what they'd do if they actually used such a user interface.

If you find yourself running tests with an eye on how they might play in the observation room, you're probably doing something wrong. You're definitely not getting the data that your team depends on you for (or that your client is paying you for).
Usability As Showbiz
All that said, there are aspects of usability that require a more popular approach. Irving Berlin was wrong in saying that  ""There's No Business Like Show Business,""  because all business is show business. In a business environment suffocated by  information pollution, you must wow and persuade people to keep their attention on your message.
When you're selecting  highlights from usability study videos, you shouldn't include a 10-minute clip of a user visiting 20 erroneous pages. Show the first one or two wrong clicks, then throw up a transition title that says ""nine minutes later...,"" and conclude with the clip where the user says, ""This is a horrible website. I can't find anything. I'm never coming back.""
Two entertaining minutes beat ten boring ones in a usability presentation. Not only will your team members and executives pay more attention to your findings, they'll also be more likely to attend your next usability briefing.
To heighten drama, cut out the boring parts. Yes, these many accumulated mistakes are what really teach you  why  the user couldn't find anything, but there are key differences in how you discover, present, and document problems.

To present findings, it's okay to emphasize material that engages and motivates your audience. For example, don't show a bunch of clips of your most introverted participants.
To discover findings, however, you can't bias your study and skip directly to the good parts for three reasons.
	
First, if you don't let users go through those 20 wrong page views, you won't learn as much about the flaws in navigation design.
Second, if you guide users past ""boring"" pages, you won't learn how users interpret those parts of the site.
Third, if you take users directly to the site's ""interesting"" parts, they'll arrive at those areas with a substantially different  mental model and attitude than people who've had to struggle through page after irrelevant page to get there. Users are more inclined to quickly leave a section if they've struggled to reach it. In contrast, users easily guided there will have a more positive attitude and will dig harder; the section will (falsely) appear to be more successful as a result.


To  document findings, your report should focus on the importance of the usability issues for the overall business, not on which issues generated the most colorful quotes.

Ultimately, if you run your research as an entertainment franchise, you'll weaken your findings and compromise product improvement. All the usability problems you missed due to poor testing methodology will remain in the interface, and your executives will conclude that usability doesn't have as  high an ROI as Ive been promising them.
In the long run, money talks, and you optimize ROI by emphasizing unbiased research and by  watching behavior. Usability's role in a design project is to be the  source of truth  about what really works in the world, as opposed to the team's hopes for what might work. The less thoroughly you uncover the truth, the less your company will need you in the long run."
93,2006-06-25,"We can  define usability in terms of quality metrics, such as learning time, efficiency of use, memorability, user errors, and subjective satisfaction. Sadly, few projects collect such metrics because doing so is expensive:  it requires 4 times as many users  as simple user testing.
Many users are required because of the  substantial individual differences in user performance. When you measure people, you'll always get some who are really fast and some who are really slow. Given this, you need to average these measures across a fairly large number of observations to smooth over the variability.
Standard Deviation for Web Usability Data
We know from previous analysis that  user performance on websites follows a normal distribution. This is happy, because normal distributions are fairly easy to deal with statistically. By knowing just two numbers — the mean and the standard deviation — you can draw the bell curve that represents your data.
I analyzed 1,520 measures of user time-on-task performance for 70 different tasks from a broad spectrum of websites and intranets. Across these many studies, the  standard deviation was 52% of the mean  values. For example, if it took an average of 10 minutes to complete a certain task, then the standard deviation for that metric would be 5.2 minutes.
Removing Outliers
To compute the standard deviation, I first removed the outliers representing excessively slow users. Is this reasonable to do? In some ways, no: slow users are real, and you should consider them when assessing a design's quality. Thus, even though I recommend removing outliers from the statistical analyses, you shouldn't forget about them. Do a qualitative analysis of outliers' test sessions and find out what ""bad luck"" (i.e., bad design) conspired to drag down their performance.
For most statistical analyses, however, you should eliminate the outliers. Because they occur randomly, you might have more outliers in one study than in another, and these few extreme values can seriously skew your averages and other conclusions.
The only reason to compute statistics is to  compare them with other statistics.  That my hypothetical task took an average of 10 minutes means little on its own. Is 10 minutes good or bad? You can't tell from putting that one number on a slide and admiring it in splendid isolation.
If you asked users to subscribe to an email newsletter, a 10-minute average task time would be extremely bad. We know from  studies of many newsletter subscription processes that the average task time across other websites is 1 minute, and users are only really satisfied if it takes less than 2 minutes. On the other hand, 10 minutes would indicate very high usability for more complex tasks, such as applying for a mortgage.
The point is that you collect usability metrics to compare them with other usability metrics, such as comparing your site with your competitors' sites or your new design with your old.
When you eliminate outliers from both statistics, you still have a valid comparison. True, average task time in both cases will be a bit higher if you keep the outliers. But, without the outliers, you're more likely to reach correct conclusions, because you're less likely to overestimate an average that happened to have a greater number of outliers.
Estimating Margin of Error
When you average together several observations from a normal distribution, the standard deviation (SD) of your average is the SD of the individual values divided by the square root of the number of observations. For example, if you have 10 observations, then the SD of the average is 1/sqrt(10) = 0.316 times the original SD.
We know that for user testing of websites and intranets, the SD is 52% of the mean. In other words, if we tested 10 users, then the SD of the average would be 16% of the mean, because .316 x .52 = .16.
Let's say we're testing a task that takes five minutes to perform. So, the SD of the average is 16% of 300 seconds = 48 seconds. For a normal distribution, 2/3 of the cases fall within +/− 1 SD from the mean. Thus, our average would be within 48 seconds of the 5-minute mean 2/3 of the time.
The following chart shows the margin of error for testing various numbers of users, assuming that you want a  90% confidence interval  (blue curve). This means that 90% of the time, you hit within the interval, 5% of the time you hit too low, and 5% of the time you hit too high. For practical Web projects, you really don't need more accurate interval than this.
The red curve shows what happens if we relax our requirements to being right half of the time. (Meaning that we'd hit too low 1/4 of the time and too high 1/4 of the time.)

Determining the Number of Users to Test With
In the chart, the margin of error is expressed as a percent of the mean value of your usability metric. For example, if you test with 10 users, the margin of error is +/ 27% of the mean. This means that if the mean task time is 300 seconds (five minutes), then your margin of error is +/− 81 seconds. Your confidence interval thus goes from 219 seconds to 381 seconds: 90% of the time you're inside this interval; 5% of the time you're below 219, and 5% of the time you're above 381.
This is a rather wide confidence interval, which is why I usually recommend  testing with 20 users  when collecting quantitative usability metrics. With 20 users, you'll probably have 1 outlier (since  6% of users are outliers), so you'll include data from 19 users in your average. This makes your confidence interval go from 243 to 357 seconds, since the margin of error is +/− 19% for testing 19 users.
You might say that this is still a wide confidence interval, but the truth is that it's extremely expensive to tighten it up further. To get a margin of error of +/− 10%, you need data from 71 users, so you'd have to test 76 to account for the five likely outliers.
Testing 76 users is a complete waste of money  for almost all practical development projects. You can get good-enough data on 4 different designs by testing each of them with 20 users, rather than blow your budget on only slightly better metrics for a single design.
In practice, a  confidence interval of +/− 19% is ample  for most goals. Mainly, you're going to compare two designs to see which one measures best. And the  average difference between websites is 68% — much more than the margin of error.
Also, remember that the +/− 19% is pretty much a worst-case scenario; you'll do better 90% of the time. The red curve shows that  half of the time you'll be within +/− 8% of the mean  if you test with 20 users and analyze data from 19. In other words, half the time you get great accuracy and the other half you get good accuracy. That's all you need for non-academic projects.
Quantitative vs. Qualitative
Based on the above analysis, my recommendation is to test with 20 users in quantitative studies. This is  very expensive,  because test users are hard to come by and require  systematic recruiting to actually represent your target audience.
Luckily,  you don't have to measure usability to improve it.  Usually, it's enough to  test with a handful of users and revise the design in the direction indicated by a qualitative analysis of their behavior. When you see several people being stumped by the same design element, you don't really need to know  how much  the users are being delayed. If it's hurting users, change it or get rid of it.
You can usually run a qualitative study with 5 users, so quantitative studies are about 4 times as expensive. Furthermore, it's easy to  get a quantitative study wrong and end up with misleading data. When you collect numbers instead of insights, everything must be exactly right, or you might as well not do the study.
Because they're expensive and difficult to get right, I usually warn against quantitative studies.  The first several usability studies you perform should be qualitative.  Only after your organization has  progressed in maturity with respect to integrating user research into the design lifecycle and you're routinely performing qual studies should you start including a few quant studies in the mix."
94,2006-03-05,"Generally, I advocate  qualitative  user testing: a handful of users is enough to discover most design flaws. Quantitative testing does have its place, however, and we've recently been running large tests for two different reasons:

We're testing hundreds of users to generate in-depth findings for our eyetracking research. To determine whether men and women look at Web pages differently, for example, we have to test many people using the same websites. This is because users wander all over sites, and we need to test numerous men and women to get a sufficient number of samples for each page.
We're running large-scale usability benchmarks for several clients so they can track their design improvements over time. These studies are really expensive and not recommended for small projects. For big projects, however, they're a good long-term management tool.

With all this fresh data at my disposal, I couldn't resist analyzing 1,520 measurements of user time-on-task performance for websites and intranets.
Does Usability Follow a Normal Distribution?
Almost all statistical analyses assume that data follows a normal distribution (the famous bell curve). Most people take this on faith, because it's true for so many phenomena. But let's check.
One way of assessing a dataset's distribution is to draw a quantile-quantile scatterplot. In a QQ plot, we plot each observation's empirical value on the  x  -axis and its hypothetical value on the  y  -axis, under the assumption that the entire set is normally distributed. We draw a straight line to represent a case with identical empirical and hypothetical values.
If our plotted datapoints are very close to the straight line, we conclude that the empirical values are very close to the hypothetical values. In other words, the observed data are the same as what the theory predicted, so the dataset follows a normal distribution.
Any datapoints that are far from the straight line represent cases in which the real and theoretical worlds differ substantially — in other words, the data doesn't follow the normal distribution.
I've plotted seventy QQ plots from our recent quantitative usability studies, and they all look the same, whether they come from website or intranet studies. Here are two typical examples:

QQ plots of two user studies: a test of a content-based magazine site (New York  Magazine, on the left) and a test of a transaction-based e-commerce site (Kiehl's, on the right).
Each dot represents the task time of one user. The x-axis indicates measured performance and the y-axis indicates the theoretically matching normal distribution. 
(Note: Because my analysis didn't include users who failed at tasks, the diagrams show only people who used the sites successfully. All seventy studies measured time on task — see earlier article on the definition of usability for other main quality attributes.)
Although the dots aren't exactly on the straight line, they're pretty close. There are a few outliers, but it seems safe to conclude that most  users do follow a normal distribution. Close enough for government work — or more to the point, close enough for any analysis you need in a practical development project.
Outliers for Fast Performance
Outliers in the lower left corner of each QQ plot are shown as solid  blue dots. These are users who  were fast, but not as fast as the theory predicted. In fact, in the left-hand QQ plot, two dots are below the  x-axis, indicating negative  y  values. The theory predicts that these two users would have finished their task before they started, which is obviously impossible.
In usability testing, there's a clear floor effect for measured task times: people simply can't be faster than a certain minimum, no matter how efficiently they use a site. Downloading pages and moving your hand between mouse and keyboard require a certain amount of time. Even the fastest typists still need time to type in search engine queries; the fastest readers still need time to read, regardless of how quickly they can find the salient information on a page.
All the studies I've analyzed included a few fast outliers. These fast (but not quite fast enough) users are easy to explain, however, and I don't think they should impact our thinking about Web usability.
Outliers for Slow Performance
Outliers in the upper right corner are shown as solid  red dots. These are users who were  dramatically slower than the slowest predicted users.
Of 1,520 cases, eighty-seven were outliers with exceedingly slow task times. This means that  6% of users are slow outliers. This is too many people to ignore. Of course, you should first and foremost improve the user experience for the 94% of users who are not outliers, but it's worth allocating some usability resources to that slow 6% as well.
The most seemingly obvious explanation for these outliers is simply that a few people are almost incompetent at using the Web, and they'll show up as slow outliers every time they test something.  But this hypothesis is false. Once we recruit people for a study, we ask them to do multiple things, so we know how the slow outliers perform on several other tasks. In general, the same users who were extremely slow on some tasks were fast on other tasks.
Sixty different users were responsible for the eighty-seven slow outliers, for an average of 1.5 outliers each. Given that users were tested on an average of 6.7 tasks across the analyzed studies, each of these users had an average of 5.2 ""normal"" tasks — 3.5 times as many as their outlying tasks.
This topic clearly needs more research, and would make for several good graduate theses. For now, my best conclusion is that  slow outliers are caused by bad luck  rather than by a persistent property of the users in question.
Good Luck in User Performance
Before turning to bad luck, let's acknowledge that good luck also happens on the Web. People are sometimes ""undeservedly"" lucky on a website and get exactly what they want in fewer clicks than expected. Maybe, for example, they're looking to buy something that happens to be the homepage's featured promotion that day. In other cases, some users happily skirt gross usability mistakes that cause other people grave difficulties and much frustration.
Here's an example of good luck from a test with disabled users trying to use the website of the IRS (the U.S. tax authorities). One blind user wanted to find out whether she could deduct money donated to a high school band.
Because the IRS page was long and overwhelming, the user decided to have her screen reader device read out the list of links on the page. Further, because the user was looking for tax rules about ""donations"" she commanded the screen reader to read links that started with a ""D."" As it turns out, the IRS uses the term ""deduction"" rather than ""donation"" — something the user would never discover from a simple page or site search using the word ""donation."" However, because both words start with ""D"" and the person was using a screen reader, she easily happened upon ""deduction"" as the correct link. A joyful outcome, but one that's purely due to good luck.
(There are a few additional usability notes here. First, by using the term ""deduction"" rather than ""donation,"" the site opts for system-oriented language over a term describing the user's action, which the site is presumably supposed to support. Second, using the screen reader shortcut is an expert behavior; you shouldn't use it as an excuse for long pages, which hurt less-experienced screen reader users. Finally, the ""read links"" feature is one of the reasons it's a guideline to avoid links with labels such as ""click here"" or ""more,"" which don't make sense out of context.)
Bad Luck in User Performance
Most website and intranet users are all too familiar with bad luck. Typical examples include:

Clicking the wrong link  and being lost forever in the wrong part of the site.
Using the wrong words. In contrast to the ""good luck"" example above, users can waste significant time scouring a site for a term that the site doesn't use and doesn't cross-reference to its own preferred term.
Companies with  multiple websites  often bounce users off to the wrong site, but users don't realize the error.
The link or information the user needs is  scrolled just off the screen  and so the user never sees it. (See also newer data on users' scrolling behavior.)
A  pop-up distracts users  just as they were about to get it right.
Registration hiccups take users on detours long enough that their attempts to buy stuff are doomed, even when they've successfully found what they wanted and placed it safely in the shopping cart.
Multiple small problems  — any one of which could be easily handled in isolation — occur in a row and thus derail users.

Of course, none of these issues are really ""luck"" in the superstitious sense of something ""unnatural"" happening. In fact, they're all small, but real defects in the design's usability. What qualifies these flaws as bad luck is that, under rare circumstances, they condemn users to terrible misfortune. If things had gone a tiny bit differently — say, a user had scrolled down one line further — he or she might have had good luck and a very pleasant user experience.
Given that slow outliers account for 6% of Web usage, it's unacceptable to simply write them off. Although the data shows that most users will avoid bad luck in their next online task, you can't just say ""better luck next time""; if you do, their next user experience will likely be on somebody else's website.
People leave websites that hurt them — they don't know that it's just bad luck, and that next time will be better. It's therefore incumbent on you to hunt down the root causes of bad luck and eradicate them from your site."
95,2005-09-11,"Most design teams spend appallingly little time observing their users' actual behavior. Of course, most companies never perform user testing at all, but even those that do log very few hours of behavioral observation.
The ideal is to designate one day per week — say, Wednesdays — as ""user day"" and bring in four users. New users, of course. It's one of the  guidelines for recruiting test users to use fresh users for each test (actually, it's seven guidelines, because the general rule has a few exceptions where it's OK to reuse participants).
With a steady stream of new users coming through, you can observe people using all of your design features, have them test out wild ideas in  paper prototype form, and see how they use your competitors' designs. With weekly testing and careful observation, you'll make very few design decisions that aren't grounded in users' real needs and behavior.
Although weekly testing is the best, it's almost never done. I know of only a handful of projects that have kept up this pace, and some of those are my own :-(
In most companies, it's a rare and wonderful experience to have actual customers show up to use the design. You should obviously make the most of this opportunity, but companies often waste too much of their precious time with users.
Top Time Wasters
The typical user test is 60–90 minutes. After that, users get tired, and it's difficult to run usability sessions that last more than two hours. (It's possible to run multi-day studies, but doing so requires a different technique and happens rarely in my experience.)
So, what should you do with the 60–90 minutes of user time? Focus on  having the users behave naturally  while interacting with your interface. That's the unique contribution of user testing: we get to see what people actually  do, as opposed to what they  say.
Sadly, many companies waste minute after minute trying to get test users to voice opinions rather than perform tasks. There are many great methods besides user testing for collecting commentary and opinions —  focus groups and  surveys  are two of the more common ones. Most likely, your company already collects lots of opinion data from many more customers than you'll ever have in one-on-one user testing situations. Use each method for what it's good at.
Common time wasters include:

Starting the session with an  extensive demographic survey.  It's better to collect this data using an online website survey. For test participants, make do with the data collected during the up-front screening process and refrain from asking additional questions during the test.
Asking users for  subjective satisfaction ratings after each task.  Subjective ratings are weak data in the first place. Research projects aside, overly fine-grained ratings are rarely worth the time required to collect them.
Using a  satisfaction questionnaire with dozens of questions  instead of a single overall satisfaction score. It's stupid to ask users to rate, for example, how much they like the graphics. If people have strong feelings about how something looks — whether it be pleasing, ugly, or inappropriate — they'll voice those feelings during the task. The one thing a questionnaire should ask users about is overall satisfaction. Detailed issues are much more valid if you assess them based on users' behavior while performing tasks, rather than asking for a retrospective rating.
Ending the session with a long discussion  about how users might feel about potential new product developments. Again, focus groups are better for this. Also, users' reactions to prototype designs while performing tasks are much more valid than people's hypothetical speculations about what they might like. Spend your time collecting valid data rather than speculative data, even if doing so requires you to mock up a few more pages.

Each of these wasteful steps takes only a few minutes. Together, however, they can amount to users spending as much as 30 minutes filling in questionnaires rather than performing tasks.
Maximizing Insights
Some overhead is inevitable in any test situation: you've got to welcome users, have them read and sign a consent form (ideally, a short one), and then debrief them and hand out incentives when the test is over. Try to keep such overhead to five minutes, then budget maybe another five minutes for collecting the most necessary subjective satisfaction ratings. By doing so, you'll  waste no more than 11%  of a 90-minute session.
If you spend 30 minutes on focus-group-style data and 10 minutes on general overhead (courtesy of endless bureaucratic forms or the like), you'll  waste 44%  of a 90-minute session.
Make a budget for your user testing time and ensure that you devote the bulk of it to observing users as they perform interactive tasks with your design. This is your project's best use of testing resources: the most valid and actionable usability insights come from observing behavior. It's also better for your career development. To  increase expertise in the usability field, you must observe the widest possible range of user behavior.
How many hours per year do you spend observing actual users performing tasks with your designs? If it's less than 20 hours, you need to do more user testing. You should probably also reprioritize the time budget for the test sessions you do run so that they'll generate more behavioral data."
96,2005-06-12,"I recently asked 245 usability practitioners what they do with their old user testing reports. Their answers were as follows:

12% keep reports in a  knowledge base  ;
27% keep reports  in a single online document collection  ;
29% keep reports  online in various locations  , so they have to track down reports when they need them; and
33%  don't keep old usability reports online  at all.

In other words, most organizations have a  disorganized archiving policy  . This is a shame, because there are many benefits to having easy access to past user research results.
Archive Strategies
I don't recommend establishing a formal knowledge base of usability reports. The current state of knowledge management (KM) is primitive, and you're unlikely to see benefits that are worth the cost and hassle. The exception here is if your company has already invested in KM for other documents, and you can piggyback on the existing system.
At a minimum, you should  establish a centralized intranet usability site  and populate it with a permanent repository of all your usability reports. When people have to search the intranet for usability reports in individual project subsites, they'll often fail -- or they won't even know what to look for because there's no single place that lists all available usability results. Even worse, if past project owners are the only source of results, you risk losing valuable institutional memory when these individuals leave the company or get reassigned.
Whenever you go to the trouble of creating an in-depth formal report with detailed analysis, you need to maximize your return on investment by encouraging future use of the insights. You should also archive  informal usability reports  . These ""quick findings"" and emailed summaries are important during usability projects, and while not as useful in the future as more detailed reports, they will have value then as well.
Archive Benefits
Having good archives of past usability findings offers four main benefits, ranging from the tactical to the strategic:

When  new people join the project  , past findings give them an easy way to review what's already known about users. This gets them up to speed quickly and will prevent them from repeating mistakes that you've already documented in past testing.
Projects can often stretch over many years, especially when there are multiple releases and modifications. As the years pass, different people get involved, especially if you outsource designs to an ever-changing set of agencies. In this situation, usability reports often provide the only  design rationale  for why certain approaches were chosen and others abandoned because they didn't work for users. If you don't know what was tested in the past, you might repeat old mistakes. If you don't know what has been learned about user needs, you'll be less capable of supporting those needs in the next version. Also, implementing usability insights can take a long time: an old consulting client recently called us up and said,  ""We're almost done implementing the recommendations you gave us two years ago.""  If they hadn't saved the old report and referred to it during years of design work, they wouldn't have gotten their full money's worth.
Individual findings can be  generalized to usability guidelines  once you observe them repeatedly. One of the most powerful ways to increase your usability group's productivity is to develop customized guidelines for your specific type of user interface.
After accumulating a large number of reports, you can  track trends over time  to assess your usability work's long-term impact: are you getting better or worse? You can also perform  meta-analysis on cross-project data  to gain insights that transcend individual projects. In my presentation on user satisfaction, for example, I use data from our tests of 209 websites to show the distribution of satisfaction ratings and calculate the score a site needs to have above-average customer satisfaction relative to the rest of the Web. Similarly, within a company, you can aggregate scores across multiple studies to discover the success rates, task times, or satisfaction scores a new project needs to be considered acceptable.

On a personal level, archival usability reports are a great  learning resource  for improving your usability skills. Although the best way to  become a great usability professional  is to conduct numerous user tests, reading the findings from other studies is a close runner-up. Thus, when you keep your reports to yourself or otherwise make them difficult to access, you shortchange your colleagues and your organization as a whole.
 "
97,2005-04-24,"I recently asked 258 usability practitioners which methods they use to communicate findings from their studies:

42%  produce a  formal written test report  with full details on the methodology
36%  write a ""  quick findings  "" report
24%  circulate an  email  that lists the study's top findings
15%  disseminate a  spreadsheet  of the findings
14%  enter usability findings into a  bug-tracking  database
21%  conduct a meeting in which they offer a  formal presentation  of the findings
27%  conduct an  informal meeting or debriefing  to discuss the findings
1%  show  full-length video  s from the test sessions
4%  show  highlights videos  from the test
3%  create and display  posters  or other physical exhibits

There's no one best approach to reporting usability study findings. Most people use more than one method, depending on their corporate culture and usability lifecycle approach.
That said, the survey clearly found that formal and brief reports are the two dominant means of disseminating usability findings. Both approaches have their place.
When to Use Quick Findings Reports
You can maximize user interface quality by conducting many rounds of testing as part of an iterative design process. To  move rapidly  and conduct the most tests within a given time frame and budget, informal reports are the best option.
Preparing a formal slide-based presentation will simply slow you down, as will using videos or statistics. Instead, simply hold a quick debriefing immediately after the test, structured around test observers' informal notes on user behavior. Follow this meeting with a short email to the entire team (the shorter the email, the greater the  probability that it will be read  ).
Some organizations thrive on formal presentations and slide-deck circulation. In my view, this is a poor method for documenting usability findings. Bullet points don't capture the subtleties of user behavior, and it's almost impossible to interpret a slide presentation even a few months after it was created.
Extremely brief write-ups work well for studies aimed at finding an interface's main flaws to drive design iterations. Such studies are largely throwaway; once you've created the design's next version, the study findings are rarely useful. As long as you're not looking to create long-term learning materials, you won't lose much by using this highly informal reporting.
Also, an old usability issue will sometimes rear its ugly head at the project's last stage. If it does, having a short description of the problem from an old study is much more useful than relying on a bullet point that mentions the issue without offering any details of user behavior.
Video clips and posters are relatively rare, but they do have a big propaganda value and thus help  evangelize usability  in the organization. I encourage you to try such non-document reporting formats.
When to Use Formal Reports
The formal report remains the most common format, but I think it's overused and prefer more rapid reporting and more frequent testing. The formal report definitely has its place, however, as in cases like these:

Benchmark studies  or other quantitative tests. Unless you document the measurement methods in detail, you can't judge the numbers. Also, one of the main reasons to measure a benchmark is to measure it  again  later and compare. To do so, you need to know everything about the original study.
Competitive studies.  When you test a broad sample of alternative designs, the resulting lessons are usually so fundamental and interesting that they warrant a complete report, with many screenshots and in-depth analysis.
Field studies.  Most organizations rarely conduct studies at customer locations; when they do, the findings deserve an archival report that can be used for years. Also, field studies usually produce important insights that are too complex to be explained in a quick write-up.
Consulting projects.  When you hire an expensive consultant to evaluate your user experience, you should milk the resulting insights for a long time to come. The outside perspective is only valuable, however, if it remains inside once the consultant has gone. To ensure this, you need a report that's both insightful and comprehensive.

All these cases require an  archival version of the findings  that can stand the test of time. Deeper and more comprehensive studies don't just produce a list of fixes to a design's current iteration. These studies generate insights into users' behavior and needs that are useful for years. When new people join the team, they should read several of these conceptual reports to gain background on current design directions. Understanding the big picture with respect to usability will prevent new people from making a lot of mistakes.
The best usability reports are learning tools that help form a shared understanding across the team. It's worth investing the effort to produce a few formal reports each year. One way to free-up resources and make some reports extra good is to scale down your ambitions for most of your everyday reports, keeping them quick and informal."
98,2005-02-13,"It's a miracle that user testing works: You bring people into a room where they're surrounded by strangers, monitored by cameras, and asked to perform difficult tasks with miserable websites. Often, there's even an intimidating one-way mirror dominating one wall. Under these conditions, how can anybody accomplish anything?
In fact, all experience shows that people can and do use websites and other user interfaces during test sessions, and that many valuable usability findings emerge from such studies. Why?
Two main reasons: the power of engagement and the suspension of disbelief.
User Engagement
When test participants are asked to perform tasks, they usually get so  engaged in using the interface  that the usability lab's distractions recede. Users know that there's a camera, and maybe even a one-way mirror hiding additional observers, but their attention is focused on the screen.
It's a basic human desire to want to perform well on a test. We can say,  ""we're not testing you, we're testing the system""  all we want. People still feel like they're taking a test, and they want to pass. They don't want to be defeated by a computer.
Because they want to be successful, users allocate their mental resources to what's happening on the screen, not what's happening in the room. Of course, this concentration can easily be broken, which is why it's a  cardinal rule of user testing to have  observers remain absolutely quiet  if they're in the room with users. Also, in fancy usability labs, I've seen users distracted by the noise of cameras moving along ceiling tracks, so that type of thing is best avoided. But, generally, as long as observers stay quiet and out of view (behind the user or behind the mirror), participants will remain engaged in their tasks.
One downside of users' tendency to engage strongly is that they sometimes work harder on tasks in a test session than they would at home. If the user says, ""I would stop here,"" you can bet that they'd probably stop a few screens earlier if they weren't being tested.
Suspension of Disbelief
In user testing, we pull people away from their offices or homes and ask them to pretend to perform business or personal tasks with our design. Obviously, an artificial scenario.
As part of the full usability life cycle, there are good reasons to conduct  field studies and observe users' behavior in their natural habitats. Unfortunately, field studies are much more expensive than lab studies, and it's typically difficult to get permission to conduct research inside other companies. During a design process, most usability sessions involve user testing; we're therefore lucky that users can typically overcome a lab's artificial nature and pretend to be at home.
The tendency to suspend disbelief is deeply rooted in the human condition, and may have developed to help prehistoric humans bond around the camp fire in support of storytelling and magic ceremonies. In the modern world, TV shows like  Star Trek  only work because of our propensity to suspend disbelief. Consider the number of untruths involved in watching  Star Trek:

You're not looking at people, you're looking at  pictures  of people, in the form of glowing dots on a video screen.
You're not looking at pictures of real people, you're looking at pictures of actors  pretending  to be characters, like Mr. Spock and Captain Picard, that don't exist.
You're not looking at pictures of actors using transporters, shooting phasers, and flying faster-than-light starships. All such activities are  simulated  with special effects.

You know all of this, and yet you engage in the story when watching the show.
Similarly, in usability studies, participants easily pretend that the scenario is real and that they're really using the design. For this to happen, you obviously need realistic test tasks and to have  recruited representative users who might actually perform such tasks in the real world. Assuming both, most usability participants will suspend disbelief and simply attempt the task at hand.
Suspension of disbelief goes so far that users engage strongly with  paper prototypes where the user interface is purely a piece of paper. As long as you can move through screens in pursuit of your goal, you will behave as if the system were real and not simulated.
In fact, sometimes users go too far in suspending disbelief and try to  role play  other  users' potential performance. You must put a stop to this immediately, as soon as you hear users speculate about what ""some people"" might do or might not know. Politely tell such users that you're testing many other people as well, but that you invited them to the test because their personal experiences are very important to the project. You can say to such users, ""be yourself"" and that ""you know what you know,"" then ask them to use their own job situation as the usage scenario's background legend.
When Engagement and Suspension of Disbelief Fail
User testing typically works, but there are exceptions. Occasionally, test participants are so lazy and difficult to engage that they never suspend disbelief and work on the tasks for real. For example, if you ask such users to select a product to solve a problem, they'll typically stop at the first remotely related product, even if it's basically unsuitable and wouldn't be bought by anybody who really had the target problem.
In rare cases, such nonrealistic usage is serious enough that you must simply excuse the participant and discard the session's data. If users haven't suspended disbelief and performed conscientiously, you can't trust that anything they've done represents real use.
More commonly, you can rescue the situation by employing a few facilitation tricks to encourage authentic behavior.
The easiest and most common approach is to  ask the user whether this is what he or she would do at the office  (or at home, for a consumer project). This small reminder is often enough to get users engaged. Variants of this technique include:

Ask users if they have enough information to make a decision (when they stop after finding the first nugget of information about the problem).
Ask users if they are sure that they've selected the best product (if they stop after finding the first likely purchase, without evaluating alternative options).

If users fail to take the tasks seriously in your first few test sessions, you can usually rescue the study by  modifying the test instructions  or task descriptions. For example, it's often enough to add an introductory remark such as, ""Pretend that you must not only find the best solution, but justify the choice to your boss.""
Including an absent boss in the test scenario encourages suspension of disbelief and usually works wonders. I've also found it effective to ask users to  ""write five bullet points for your boss, explaining the main pros and cons of this product.""  (You can also ask for three bullets of pros and three bullets of cons.)
In our  tests of the investor relations areas of corporate websites, we used a variant of this technique, simply asking the financial analysts and individual investors to decide whether they thought the company would do better or worse than the stock market and state the main reasons why.
Other times, you can induce constraints to better engage users with the task. For example, we once tested an e-commerce site that sold office supplies. In pilot testing, a test task for administrative assistants was to stock the office for a newly hired employee. Unfortunately, the pilot participant wanted to be really nice to this hypothetical new colleague, and bought all the most expensive pens, office chairs, and so on. Although in this case, we could have asked users to pretend that they had to answer to the boss, we chose instead to give them a specific budget. This small change was enough to make users consider each item carefully, which let us discover how they assessed product quality and value.
Despite the artificial nature of user testing, we typically see enough authentic behavior to easily identify design flaws that would diminish an interface's value for real customers. Getting to actually use something is so powerful an experience that most users provide good data. When they don't, you can typically prod users lightly with verbal reminders, or alter the task a bit to get them fully engaged."
99,2004-11-07,"The good news in user research is that we're building up a  massive body of knowledge about user behavior in online systems. The days are long gone when companies had to guess about website and intranet designs. The bad news is that the sheer amount of accumulated research findings can be overwhelming. Even worse? User research won't generate an additional penny of profit unless you understand it and act upon it.
Here's one way of quantifying the amount of current usability knowledge: We've published  3,326 pages of usability research reports. Those reports contain 1,217 general design guidelines and 1,961 screenshots, all of which have information about how specific designs helped or hindered real users. Other researchers publish reports as well, and you may also have an internal usability group or have commissioned consultants to study issues of interest to you.
In total, a huge mass of user research. How should you deal with all these findings?
First Steps
User research is a reality check. It tells you what really happens when people use computers. You can  speculate  on what customers want, or you can  find out. The latter is the more fruitful approach.
Research offers an understanding of how users behave online, and is a solid foundation on which to build a design. I still recommend that you user test your own design: any time you have a new idea, build a  paper prototype and test it so that you don't waste money implementing ideas that don't work. But, if you start with design ideas that are based on the actual behavior of real human beings, you'll have considerably fewer usability problems than if you target a design at a hypothetical or idealized user.
It can be overwhelming at first to see a long list of new research findings. Try to  process them in small bites. For example, look at your homepage or a typical content page in light of the new findings. Print out a copy and circle each design element that might violate a design guideline or cause users problems.
Make general issues concrete  by applying them to a familiar example. This is always a good way to build up understanding that can help you in future design projects.
You can also  use research findings as a checklist: go through your own design one guideline at a time and see whether you comply. Whenever you're in violation of an established usability finding, you can dig deeper into that finding's underlying user research and learn more about it. With your new knowledge, you might decide to fix your design to make it compliant with users' typical behavior. Or, of course, you might disagree with the research findings.
Handling Disagreements
There are two main reasons people disagree with a research study and its conclusions: their own research shows something different, or their personal opinions and preferences differ from the research recommendations.
If your research findings disagree with published results, you have two options. First, it's possible that your study had a methodology flaw, so it's worth reviewing the study in light of the new results. There are numerous  issues to consider to run a valid user test. Second, it could be that you're dealing with a special case, and your findings actually are different. Such cases do exist, although exceptions are more rare than people would like to think.
If your own intuition disagrees with published findings, view it as a learning opportunity that can improve your future insights.  Design is not religion. You don't have to defend the beliefs of your forefathers to the bitter end. Design is a business decision. You should follow the data and do what generates the biggest profits for your company, not what wins design awards.
If you disagree strongly, you can always run a study of your design to determine whether you are one of those rare exceptions. General usability guidelines typically hold true in about 90% of cases. There are many special circumstances that make the remaining 10% sufficiently atypical such that the best solution will be something other than the normal recommendation.
If you run an online business, you're in the user experience business: all the value flows through a user interface. It's essential to develop the expertise to interpret user research and an understanding of when to run usability studies. This is true even if you're not a usability specialist yourself and never want to personally run a study. You still have to know how to deal with the reports and make the research findings relevant to your business."
100,2004-02-29,"There are 2 main types of user research: quantitative (statistics) and qualitative (insights). Quant has quaint advantages, but qualitative delivers the best results for the least money. Furthermore, quantitative studies are often too narrow to be useful and are sometimes directly misleading.
The key benefit of quantitative studies is simple: they boil a complex situation down to a single number that's easy to grasp and discuss. I exploit this communicative clarity myself, for example, in reporting that using websites is  206% more difficult for users with disabilities and  122% more difficult for senior citizens than for mainstream users.
Of course, using bottom-line scores to summarize elaborate usability study outcomes neglects details that take 273 pages to explain:  Why  are websites more difficult for these groups?  What  should you do about it?
Numbers, however, have their own stories to tell:

They tell us that the situation is much worse for users with disabilities than for seniors. Because there are many more seniors and they constitute a particularly affluent audience, websites might nonetheless choose to spend more resources catering to seniors than to the disabled. Knowing the score lets organizations make conscious decisions in how they allocate scarce resources.
They tell us that the problems are not small. If the web were 5% more difficult for users with disabilities than for other users, most people would say ""whatever; deal with it."" But discriminating by 206% is too much for many of us to stomach.

Numbers also allow  comparisons between designs and tracking over time. 10 years from now (2014), if websites are only 50% harder to use for seniors than for younger users, we'll know that we've made substantial progress. (In fact, websites have become somewhat better for seniors in the 11 years between 2002 and 2013.)
Beware Number Fetishism
When I read reports from other people's research, I usually find that their qualitative study results are more credible and trustworthy than their quantitative results. It's a dangerous mistake to believe that statistical research is somehow more scientific or credible than insight-based observational research. In fact, most statistical research is  less  credible than qualitative studies. Design research is not like medical science: ethnography is its closest analogy in traditional fields of science.
User interfaces and usability are highly contextual, and their effectiveness depends on a broad understanding of human behavior. Typically, designers must combine and trade-off design guidelines, which requires some understanding of the  rationale and principles  behind the recommendations. Issues that are so specific that a formula can pinpoint them are usually irrelevant for practical design projects.
Fixating on numbers rather than qualitative insights has driven many usability studies astray. As the following points illustrate, quantitative approaches are inherently risky in a host of ways.
Random Results
Researchers often perform statistical analysis to determine whether numeric results are ""statistically significant."" By convention, they deem an outcome significant if there is less than  5% probability that it could have occurred randomly  rather than signifying a true phenomenon.
This sounds reasonable, but it implies that 1 out of 20 ""significant"" results might be random if researchers rely purely on quantitative methods.
Luckily, most good researchers — especially those in the user-interface field — use more than a simple quantitative analysis. Thus, they typically have insights beyond simple statistics when they publish a paper, which drives down, but doesn't eliminate, bogus findings.
There's a reverse phenomenon as well: Sometimes a true finding is statistically insignificant because of the experiment's design. Perhaps the study didn't include enough participants to observe a major — but rare — finding in sufficient numbers. It would therefore be wrong to dismiss issues as irrelevant just because they don't show up in quantitative study results.
The  ""butterfly ballot"" in the 2000 election in Florida is a good example: a study of 100 voters would not have included a statistically significant number of people who intended to vote for Al Gore but instead punched the hole for Patrick Buchanan, because less than 1% of voters made this mistake. A qualitative study, on the other hand, would likely have revealed some voters saying something like,  ""Okay, I want to vote for Gore, so I'm punching the second hole ... oh, wait, it looks like Buchanan's arrow points to that hole. I have to go down one for Gore's hole.""   Hesitations and almost-errors are gold  to the observant study facilitator, but to translate them into design recommendations requires a qualitative analysis that pairs observations with interpretive knowledge of usability principles.
Pulling Correlations Out of a Hat
If you measure enough variables, you will inevitably discover that some seem to correlate. Run all your stats through the software and a few ""significant"" correlations will surely pop out. (Remember: 1 out of 20 analyses are ""significant,"" even if there is no underlying true phenomenon.)
Studies that measure 7 metrics will generate 21 possible correlations between the variables. Thus, on average, such studies will have one bogus correlation that the statistics program deems ""significant,"" even if the issues being measured have no real connection.
In my Web Usability 2004 project, we collected metrics on 53 different aspects of user behavior on websites. There are thus 1,378 possible correlations that I could throw into the hopper. Even if we didn't discover anything at all in the study, about 69 correlations would emerge as ""statistically significant.""
Obviously, I'm not going to stoop to correlation hunting; I'll only report statistics that relate to reasonable hypotheses founded on an understanding of the underlying phenomena. (In fact, statistics program analyses assume that researchers have specified the hypotheses in advance; if you  hunt for ""significance"" in the output after the fact, you're abusing the software.)
Overlooking Covariants
Even when a correlation represents a true phenomenon, it can be misleading if the real action concerns a  third variable that is related to the two you're studying.
For example, studies show that intelligence declines by birth order. In other words, a person who was a first-born child will on average have a higher IQ than someone who was born second. Third-, fourth-, fifth-born children and so on have progressively lower average IQs. This data seems to present a clear warning to prospective parents: Don't have too many kids, or they'll come out increasingly stupid.  Not so. 
There's a  hidden third variable at play: smarter parents tend to have fewer children. When you want to measure the average IQ of first-born children, you sample the offspring of all parents, regardless of how many kids they have. But when you measure the average IQ of fifth-born children, you're obviously sampling only the offspring of parents who have 5 or more kids. There will thus be a bigger percentage of low-IQ children in the latter sample, giving us the true — but misleading — conclusion that fifth-born children have lower average IQs than first-born children. Any given couple can have as many children as they want, and their younger children are unlikely to be significantly less intelligent than their older ones. When you measure intelligence based on a random sample from the available pool of children, however, you're ignoring the parents, who are the true cause of the observed data.
(Update added 2007: The newest research suggests that there may actually be a tiny advantage in IQ for first-born children after correcting for family size and the parents' economic and educational status. But the point remains that you have to correct for these covariants, and when you do so, the IQ difference is much less than plain averages may lead you to believe.)
As a web example, you might observe that longer link texts are positively correlated with user success. This doesn't mean that you should write long links. Website designers are the hidden covariant here: clueless designers tend to use short text links like ""more,"" ""click here,"" and made-up words. Conversely, usability-conscious designers tend to explain the available options in user-centered language, emphasizing text and other content-rich design elements over more vaporous elements such as ""smiling ladies."" Many of these designers' links might indeed have a higher word count, but that's not why the designs work. Adding words won't make a bad design better; it'll simply make it more verbose.
Over-Simplified Analysis
To get good statistics, you must tightly control the experimental conditions — often so tightly that the findings don't generalize to real problems in the real world.
This is a common problem for  university research, where the test subjects tend to be undergraduate students rather than mainstream users. Also, instead of testing real websites with their myriad contextual complexities, many academic studies test scaled-back designs with a small page count and simplified content.
For example, it's easy to run a study that shows  breadcrumbs  are useless: just give users directed tasks that require them to go in a straight line to the desired destination and stop there. Such users will (rightly) ignore any breadcrumb trail.  Breadcrumbs are still recommended for many sites, of course. Not only are they lightweight, and thus unlikely to interfere with direct-movement users, but they're helpful to users who arrive deep within a site via search engines and direct links. Breadcrumbs give these users context and help users who are doing  comparisons by offering direct access to higher levels of the information architecture.
Usability-in-the-large  is often neglected by narrow research that doesn't consider, for example, revisitation behavior, search engine visibility, and multi-user decision-making. Many such issues are essential for the success of some of the highest-value designs, such as  B2B websites and  enterprise applications on intranets.
Distorted Measurements
It's easy to prejudice a usability study by helping the users at the wrong time or by using the wrong tasks. In fact, you can prove virtually anything you want if you design the study accordingly. This is often a factor behind ""sponsored"" studies that purport to show that one vendor's products are easier to use than a competitor's products.
Even if the experimenters aren't fraudulent, it's easy to get hoodwinked by methodological weaknesses, such as directing the users' attention to specific details on the screen. The very fact that you're asking about some design elements rather than others makes users notice them more and thus changes their behavior.
One study of online advertising attempted to avoid this mistake, but simply made another one instead. The experimenters didn't overtly ask users to comment on the ads. Instead, they asked users to simply comment on the overall design of a bunch of web pages. After the test session, the experimenters measured users' awareness of various brands, resulting in high scores for companies that ran banners on the web pages in the study.
Does this study prove that banner ads work for branding, even though they don't work for getting qualified sales leads? No. Remember that users were directed to comment on the page designs. These instructions obviously made users look around the page much more thoroughly than they would have during normal web use. In particular, someone who's judging a design typically inspects all individual design elements on the page, including the ads.
Many web advertising studies are misleading, possibly because most such studies come from advertising agencies. The most common distortion is the novelty effect: whenever a new advertising format is introduced, it's always accompanied by a study showing that the new type of ad generates more user clicks. Sure, that's because the new format enjoys a  temporary  advantage: it gathers user attention simply because it's new and users have yet to train themselves to ignore it. The study might be genuine as far as it goes, but it says nothing about the new advertising format's long-term advantages once the novelty effect wears off.
Publication Bias
Editors follow the ""man bites dog"" principle to highlight new and interesting stories. This is true for both scientific journals and popular magazines. While understandable, this preference for new and different findings imposes a significant bias in the results that get exposure.
Usability is a very stable field. User behavior is pretty much the same year after year. I keep finding the same results in study after study, as do many others. Every now and then, a bogus result emerges and publication bias ensures that it gets much more attention than it deserves.
Consider the question of web page download time. Everyone knows that faster is better. Interaction design theory has documented the importance of response times since 1968, and this importance has been seen empirically in countless Web studies since 1995. Ecommerce sites that speed up response times sell more. The day your server is slow, you lose traffic. (This happened to me: on January 14, 2004, Tog got ""slashdotted""; because we share a server, my site lost 10% of its normal pageviews for a Wednesday when AskTog's increased traffic slowed useit.com down.)
If 20 people study download times, 19 will conclude that faster is better. But again: 1 of every 20 statistical analyses will give the wrong result, and this 1 study might be widely discussed simply because it's new. The 19 correct studies, in contrast, might easily escape mention.
Judging Bizarre Results
Bizarre results are sometimes supported by seemingly convincing numbers. You can use the issues I've raised here as a  sanity check: Did the study pull correlations out of a hat? Was it biased or overly narrow? Was it promoted purely because it's different? Or was it just a fluke?
Typically, you'll discover that deviant findings should be ignored. The broad concepts of human behavior in interactive systems are stable and easy to understand.
The exceptions usually turn out to be exactly that: exceptions.
Of course, sometimes a strange finding turns out to be revolutionary rather than illusory. This is rare, but it does happen. The key differentiators are whether the finding is repeatable and whether others can see it now that they know where to look.
In 1989, for example, I published a paper on discount usability engineering, stating that small, fast user studies are superior to larger studies, and that  testing with about 5 users is typically sufficient per design iteration. This was quite contrary to the prevailing wisdom at the time, which was dominated by big-budget testing. During the 15 years since my original claim, several other researchers reached similar conclusions, and we developed a mathematical model to substantiate the theory behind my empirical observation. Today, almost everyone who does user testing has concluded that they learn most of what they'll ever learn with about 5 users.
As another example, my conclusion that  PDF documents are bad for online information access was supported by 4 different studies. We're finding the same problems in our newest study, so the conclusion holds across several years as well. I was initially hesitant to come out against online PDF, because it works so well in other contexts (most notably, downloading documents for printing, which is what it was designed for). As the evidence kept mounting, however, it became clear that the conclusion for online PDF was very different than for print PDF.
You might dismiss one study that concluded that the otherwise good PDF was actually bad online. But 4 or 5 studies constitute a  trend, which much enhances the finding's credibility as a general phenomenon.
Quantitative Studies: Intrinsic Risks
All the reasons I've listed for quantitative studies being misleading  indicate bad research; it's possible to do good quantitative research and derive valid insights from measurements. But doing so is expensive and difficult.
Quantitative studies must be done exactly right in every detail or the numbers will be deceptive. There are so many pitfalls that you're likely to land in one of them and get into trouble.
If you rely on numbers without insights, you don't have backup when things go wrong. You'll stumble down the wrong path, because that's where the numbers will lead.
Qualitative studies are less brittle and thus less likely to break under the strain of a few methodological weaknesses. Even if your study isn't perfect in every last detail, you'll still get mostly good results from a qualitative method that relies on understanding users and their observed behavior.
Yes, experts get better results than beginners from qualitative studies. But for quantitative studies, only the best experts get any valid results at all, and only then if they're extremely careful."
101,2003-06-01,"Can small companies with small websites employ usability methods to improve the quality of their designs? Yes. Should they? Definitely. Even a tiny budget will substantially improve a site's business value.
In May 2002,   .Net  magazine asked a range of Web design shops to bid on the  design of a seven-page website  for R. Thomas and Sons Butchers, a fictitious small business. (Unfortunately the article is not available online.) As you might expect, the quoted fees ranged widely, but many good proposals seemed to come in at around  $2,000.
Given that current best practices call for spending about  10% of design budgets on usability, this project would have about  $200 available for usability. What can you get for that money?
Plenty. But first an aside: it's obviously wrong to go in assuming a set number of pages for a site. The starting point for any Web project should be a task analysis of the information and features that users need. The second step is to construct an information architecture for this content. Only after you know the content and structure should you design navigation and pages. Still, it seems reasonable to assume that a small company's website would be around seven pages, so I'll go with that number here.
Four Steps to Small-Company Usability
I recommend splitting the $200 budget into four parts, with $50 for each of the following activities:

One hour to question customers about their information needs
One hour to review an early version of the design
One hour for a quick, in-store test of the design
One hour to enhance the site's search engine visibility

Of course, I'm assuming that we can get an hour's work for $50. This obviously wouldn't be possible if we were dealing with one of the big glamour design agencies with the hugely higher billable rates. But then, a big agency wouldn't even answer the phone for a $2,000 project. It's much more realistic to expect a butcher to use a small local design firm with lower rates.
Step 1: Determine the Customers' Questions
Take advantage of the fact that the client is a butcher and  spend an hour in the store  interviewing customers as they come in. Ask them what they want to know about the store. Ask them for details about what happened as they got ready to go shopping that day.
People are bad at remembering generalities and predicting what they might need, but they're reasonably good at remembering what they've just done. This recall method is not nearly as good as a real field study, where you can actually  watch what people actually do, as opposed to what they remember doing. But there's no way to do a proper field study for $50.
The main downside of this approach is that you're not researching the information  needs of prospective customers; you're only talking to people who already shop with the client. You could spend some of the interview time outside the store, intercepting people in the neighborhood, but it would be harder to get them to talk to you and might require a bigger budget for incentives.
In-store shoppers who help you should get a  very small incentive  -- perhaps a discount coupon or a bottle of barbecue sauce. (I cheated here, and didn't include the cost of such incentives in the $200 budget, but shopkeepers should be able to find something they can give away without much true cost to the business.)
While you're in the store,  talk to the person who answers the phone  and find out what callers' most common questions are. Answers to such questions are prime candidates for site content.
Step 2: Review Initial Design According to Usability Guidelines
An hour doesn't go very far in reviewing a website, but an experienced professional should be able to check whether the draft design complies with the  113 usability guidelines for homepage design and general Web usability principles.
With only an hour to assess the design, the reviewer obviously needs previous experience with the usability principles. There is no time to learn new guidelines or study up on how to judge when which principles apply. That's why it's so important to  select a design agency that regularly performs user testing on projects so that the staff is already well versed in how humans behave with interactive systems.
This hour's $50 budget doesn't include the $40 cost of the homepage guidelines. I assume the butcher is wise enough to pick a design agency with sufficient commitment to usability, and a corresponding library of usability books and research reports that can supply checklists when the going gets tough.
Step 3: Test a Paper Prototype In-Store
Next, you should quickly modify the design to fix the worst problems identified in your review. Then it's time to  test the design with real users.
To do this, you can print copies of the site's seven pages and run an in-store  paper prototype test. Although you could bring a laptop and test on-screen versions of the design, paper prototypes are more suited to a store environment: Rather than ask customers to sit at a computer, you can simply hold up pages one at a time, showing them to users wherever they're standing. Printing enlarged pages increases the likelihood that shoppers will be able to decipher screen content without their reading glasses, which might not be easily available.
User screening at this level is impossible — simply test shoppers who are willing to give you 10 minutes of their time. This is obviously a  convenience sample. You're dispensing with the  234 guidelines for recruiting user-testing participants and just testing whoever happens to enter the store. That's the only way to get user data for $50. That said, you should ask shoppers whether they use the Web, and only test Web users. Otherwise, your only finding will be that computers are too difficult for novice users.
Each session should last less than 10 minutes so that you can test the  recommended five users in an hour.
Step 4: Improve Search Engine Visibility
Most people will arrive at the website in one of two ways:

From the butcher's own promotions — say, printing the URL on shopping bags and wrapping paper
From search engines

There's no way you can achieve full-fledged professional search engine optimization for $50. Still, an hour's work by someone who knows the basics of search engine visibility should dramatically improve the likelihood that searchers will find the site. It's at least possible to ensure that the homepage title well represents the site in results listings, and that the page includes the most salient query terms.
The site might not get the coveted #1 spot in a search for ""Butcher East Falmouth"" or for ""where to buy meat in East Falmouth,"" but unless there are more than ten butchers in town, improving search engine visibility ought to get the site a nice listing on the first results page.
If the Butcher Can Do It, So Can You
Even a seemingly insignificant $200 budget can substantially improve a small company's site usability. These four simple steps can dramatically increase a site's contribution to the company's bottom line, and thus offer a higher return on investment for the $2,000 spent on the companys modest Web presence.
Based on this exercise, I have five recommendations:

Be creative.  Even a tiny budget is no excuse for eliminating usability from a design project. Although you might have to  create uncommonly quick-and-dirty variations  on established methods, you can still include usability.
Be diverse.  Employing various user-centered methods in  several small activities  is usually better than going with a single activity that's limited to a single method.
Be trusting.  Moving at the speed described in this article leaves no time for data analysis or for any type of report-writing. You will have to trust the main findings from each activity and act on them without debate.
Be prepared.  Executing four methods for $200 assumes that you  have an experienced staff  that is well versed in improving usability and search engine visibility, and is familiar with important guideline checklists.
Be opportunistic.  If the client has a store, for example, save on recruiting and incentive costs by running user testing onsite. With each project, you'll  find unique opportunities for saving money  if you look for them rather than always relying on the same methodology.

Luckily, most projects have usability budgets that are much bigger than $200, so they don't have to cut as many corners. The most important point to remember, though, is that  you can do it. No matter what your project, and no matter how big or small your budget, usability is there to help you succeed.
See Also
Case study of conducting  4 usability studies in 2 weeks for the design of an intranet (substantially bigger budget, but still very fast).
Hands-On Training
Full-day  tutorial on user testing at the annual  UX Conference."
102,2003-05-18,"During my  usability conference series, I was asked the same two questions in both New York and London:

How can a design agency convince clients to pay for usability testing?
What should you say when clients complain that there's no reason to test the design since they hired you because you supposedly know how to create good websites in the first place?

When people ask the same question repeatedly, I usually assume that many others will be interested in the answer as well. So here goes.
At first, it might indeed seem that subjecting a design to user testing challenges the design firm's professionalism. Are the designers so insecure about their own work that they need to test it? Shouldn't they just know how to produce something that works? In reality,  using sound methodology is the true sign of professionalism, as is knowing how to manage the project by planning for the necessary steps in advance.
Analogy: Software Development
Consider software programming as an analogy: If you hired developers to code a piece of custom software and they claimed that there was  no reason to debug the code, you would think they were crazy. In software development, we know from 50 years of experience that  all code has bugs  . It's impossible to write perfect software the first time you try; the only way to deliver high-quality programs is by using a sound development process with explicit steps for several types of testing.
Modern user interfaces are just as complex as software  in terms of the number of different variables we combine. More importantly, 20 years of usability engineering experience have shown that  it's impossible to design the perfect user interface on the first try. Even the world's best designer cannot immediately produce an interface that is perfectly simple, meets all users' needs, and never induces a user error. It cannot be done. It is reckless to bet that your project will be the first in the history of the world to create perfection without iteration.
There are plenty of other analogies:

Even the best  architects  don't just build the first building they envision -- they check to ensure that the proposed structure conforms to the building code, and they run many tests on its strength and integrity before starting construction. Hire architects that don't bother with these steps because they assume that whatever they draw will be great? Only if you don't mind having your building torn apart by the first storm.
Even the best  writers  ask editors to improve the correctness and readability of their writing before publishing it. Using an editor is not the sign of an unskilled writer; it's the sign of a writer who knows what it takes to write well.

In design,  usability serves the same role as debugging, building codes, or editing serve in other fields. We know that a design will have weaknesses, and usability provides principles and testing methodologies to find those weaknesses and improve the user interface.
Client Advice: Select Agencies That Use Sound Methodology
I advise clients to avoid design agencies that are too arrogant to include user testing in their project plans. If you hire them, you'll waste money on something that probably won't work well and that you'll have to redesign shortly after release -- when your customers are forced to serve as the design's real-world user testers.
In fact, before the project starts,  the only way a client can assess a design agency's professionalism is by reviewing its test methodology and other project management plans. Is the agency a fly-by-night operation that designs by the seat of its pants, or a professional organization with a mature development process? You can't know in advance whether the designers assigned to your account will create something as nice as the agency's other designs, but you can assess whether they know how to run a project and whether they understand user-centered -- rather than ego-centered -- design.
Selecting an agency that plans to user test your design has two major benefits:

Your design will be of higher quality and will be more likely to work for your customers, and thus to serve your business needs.
Assuming that the agency routinely tests projects, the designers will have experience with user behavior and will thus avoid many mistakes in your project. In contrast, agencies that don't test encourage bad habits among their designers, whose only reality check is what the client likes (not what the client's  customers  actually do).

Cheap Usability
One answer to the question of how to get clients to pay for usability is to include it in the overall price rather than charge extra. After all, you don't charge clients for the number of Photoshop licenses you need for their project nor for the lead designer's latest computer memory upgrade.
Usability is just a tool, and it need not be expensive. You can conduct simple user tests in 2-3 days and still improve a design considerably. If the client doesn't insist on a fancy usability report, you can run several rounds of tests in a week and view it as a standard part of your work.
Of course,  good usability reports should cost extra as they require considerable effort to produce. A report with insightful analysis of customer behavior provides long-term strategic value, and you should charge the client for it as a separate deliverable. But if all your report contains is a tactical list of design deficiencies to fix, then you probably shouldn't let it escape your agency.
Ultimately, the real answer to getting clients to pay for user testing and other user-centered design methods is to point out  usability's astounding return on investment. Simple usability can be  cheap, take only a few days, and still more than  double a website's effectiveness. Suggesting to your clients that they take advantage of this powerful tool is simply good advice, and will hopefully be appreciated as such by more and more people as they learn about interactive media."
103,2003-04-13,"In my experience,  designers almost never use paper prototyping in real design projects  despite its potentially immense contribution to creating a high-quality user experience.
Too Simple to Be True?
Why don't design teams use paper prototyping? Is it because it's so expensive and time consuming that project managers regrettably decide to allocate resources elsewhere so they can ship on time? No. Paper prototyping is one of the fastest and cheapest techniques you can employ in a design process.
Paper prototyping isn't used because people don't think they will get enough information from a method that is so simple and so cheap.  It feels like you're cheating  if you attempt to improve your project without investing more of the sweat of your brow. ""It's too easy; it can't work"" goes the reasoning, and ""we should wait until we have a better user interface before we show it to customers."" Wrong. If you wait, it will be too late to translate your usability findings into needed changes in your design's direction.
I'm here to tell you that paper prototyping works. There are many different grades of paper prototypes, and they all offer immense value relative to the time it takes to create and test them. I have run studies with nothing more than  three different homepage mock-ups  for a website, and still we learned a lot about how people would use the service and how our design concepts communicated to users.
Why Paper Prototypes Save Money
Twenty years of usability engineering experience uniformly indicates that the  biggest improvements in user experience come from gathering usability data as early as possible  in a design project. Measured usability can increase by an order of magnitude when you can change the project's basic approach to the problem, change the feature set, and change the user interface architecture. Usability insights also help later in the project, and there is value in fine-tuning user interface details, but late-stage changes impact the final user experience less than fundamental changes early in the design. It's a rough estimate, but I would say that the benefits from early usability data are at least ten times greater than those from late usability data. Late usability studies often add about  100%  to the final design's desired metrics, but early usability can add  1,000%  or more.
Forty years of software engineering experience uniformly indicates that it's much  cheaper to change a product  early in the development process than it is to make changes later in the process. The most common estimate is that it's  100 times cheaper  to make a change before any code has been written than it is to wait until after the implementation is complete.
So: Ten times the impact if you discover a needed design change early, and 100 times cheaper to make the change. The experience from both fields is clear: early is much better than late.
The benefits from early user research are so vastly superior that you should definitely use paper prototyping, even if you don't think the prototype will be as good as testing a fully developed design. If you simply try it, you'll be surprised at the amount of insight a ""primitive"" prototype can offer. Even if you don't believe me, believe the collective experience of usability and software engineers:  Early beats late  by so much that it outweighs the differences in prototype quality. (It may be ""Lean UX"" but it's also ""mean UX"" in terms of the ability to bite off large parts of bad design and replace with good design.)
A Usability Method That's Here to Stay
Paper prototyping has a second benefit, besides its impact on your current design project's quality. It will also  benefit your career. Consider all the other books you've read about computers, Web design, and similar topics. How much of what you learned will still be useful in ten years? In twenty years? In the immortal words of my old boss, Scott McNealy, technology has the shelf life of a banana.
In contrast, the paper prototyping technique has a shelf life closer to that of, say, paper. Once you've learned paper prototyping, you can use it in every project you do for the rest of your career. (See  photo of a paper prototype test from 1995: it looks exactly the same  today.) I have no idea what user interface technologies will be popular in twenty years, but I do know that I'll have to subject those designs to usability evaluation, and that paper prototyping will be a valuable technique for running early studies.
Training Video
Paper Prototyping: A How-To Video, 40 minute film available for download that demonstrates practical techniques for building the prototypes and testing them with users.
I made this training video because most companies don't believe in a technique that is this simple without actually  seeing  how it works. Video does this job well.
Hands-On Training
Full-day course on Agile wireframing and paper prototyping at the  UX Conference. Hands-on is even better for learning how to do it (but then bring home the video to show to the rest of your team).
Related Articles
Test Paper Prototypes to Save Time and Money: The Mozilla Case Study
How Iterative Testing Decreased Support Calls By 70% on Mozilla's Support Website
Book
Carolyn Snyder: Paper Prototyping: The Fast and Easy Way to Design and Refine User Interfaces. Even though it is from 2003, the methods don't change much, so I still recommend this book. See  Amazon.com's book page or  Amazon.co.uk's book page for more info."
104,2003-01-19,"By now, most companies accept the need to improve the usability of their websites, intranets, software and hardware designs, and other projects that have a user interface. Many companies also know that user testing is the fastest and easiest method in the usability engineering toolbox. (Unfortunately, many don't know about the other methods in the toolbox or how to effectively combine multiple usability methods throughout the project lifecycle, but that's a story for another day.)
Many people believe in user testing, but  in real design projects, not much testing takes place. Why the discrepancy between belief and action? Mainly, it is the company's  inability to fire off a quick, small test  when faced with a design decision. Few organizations can run such tests within the deadlines required by fast-moving development projects. This lack of test-readiness means that testing becomes a rare and precious event that -- at best -- happens once per project.
Single-test projects invariably defer usability testing until the complete design is available. This practice still occurs despite twenty years of experience uniformly showing that most projects require  multiple rounds of testing  and redesign to achieve acceptable user-experience quality, and the equally strong finding that it is a  hundred times cheaper to fix usability problems discovered early in the project  rather than at its end.
Simplifying User Testing
To increase the number of companies that apply usability methods correctly, we must make it  easier and cheaper to do the right thing  .
The three main rules for  simplified user testing are:

Get representative  users 
Ask them to perform representative  tasks  with the design
Shut up  and let the users do the talking

The third rule is surprisingly difficult, while rule #2 requires some experience to execute well. Still, the main obstacle to quick and frequent user testing is the difficulty of finding warm bodies that satisfy rule #1. Most companies have no  procedures for getting five customers to show up at specified times next Wednesday, and yet that's what is required for a successful usability study.
Participant recruiting is the unglamorous foundation for all user testing.  Without recruiting, you won't have users. Having a systematic recruiting program in place will make a huge difference in how much usability testing your organization conducts, and the quality of your recruiting will immediately increase the quality of your test results.
Recruiting: The State of the Art
To assess the current state of recruiting usability study participants, Nielsen Norman Group  surveyed 201 usability professionals. Because we wanted to report how recruiting actually occurs in today's design projects, we deliberately surveyed a biased sample of respondents who were actively involved in usability testing and recruiting. Of course, because most companies don't currently conduct user testing, they also don't recruit test participants. The findings reported here relate solely to the practices of those companies that run usability tests.
Of the survey respondents, 54% were based in the US, 8% in the UK, 7% in Canada, and 5% in Australia. Continental Europe accounted for 14% of respondents, and Brazil, China, Ecuador, India, Israel, Mexico, New Zealand, Singapore, and South Africa were represented as well. Clearly, usability testing, and thus participant recruiting, is a worldwide phenomenon.
Specialized Recruiting Agencies
Most companies recruited their own test participants, possibly because of the cost of engaging a specialized recruiting agency. Only  36% of our respondents used an outside recruiting agency. These companies typically handled some of their own recruiting as well; only 9% of respondents used outside agencies to find all of their test participants.
Recruiting agency costs can be substantial: the average  agency fee was $107  per participant. Fees varied significantly based on geography, with the highest fees in the world on the West Coast of the US, where the average was $125 per participant. As I can attest from painful experience, Silicon Valley is not only an expensive place to do business, it's also a place where you must work extra hard to recruit people who have not already been studied to death.
Companies that did their own recruiting reported spending an average of  1.15 hours of staff time for each participant recruited. Still, 24% of respondents reported spending more than two hours per participant. If you don't have a streamlined recruiting process in place, with a skilled recruiting specialist, it might not pay off to handle recruiting in-house.
Recruiting fees also varied dramatically by user profile. It's no big surprise that agencies charge twice as much to recruit high-end professionals ($161 per person) as to recruit average consumers or students (about $84 per person).
Incentives for Test Participants
Companies offered monetary incentives to only 10% of participants in internal studies, such as intranet or MIS system tests. This finding corresponds well with my recommendation that companies not pay their own employees extra money to participate in usability testing, because they're already being paid for their time.
About a third of companies (35%) did offer non-monetary incentives to internal test participants. Typically, this was a small gift, such as a coupon for a free book or lunch in the company cafeteria.
In contrast,  companies typically offered cash to participants recruited from outside  as incentive to participate in a test: 63% of external users received monetary compensation, 41% received non-monetary incentives, and 9% received nothing. (The numbers total more than 100% because a lucky 13% of external participants were given both monetary and non-monetary incentives.)
The average incentive paid to external users was  $64 per hour  of test time. Again, the US West Coast was the most expensive, with an average incentive of $81 per hour.
Incentives varied even more by user profile:  High-level professionals received almost four times as much as nonprofessional users  ($118 vs. $32 per hour, on average).
No-Show Rates
Study participants reported an  average no-show rate of 11%, which translates into one in nine users failing to show up as promised. However, because of uncontrollable events like weather, traffic, and random personal events, no-show rates are highly variable from one study to the next. Thus, if you're running a ""standard"" simple test with five users, you might easily be hit by one or two no-shows.
There are many tricks for minimizing no-shows and alleviating their impact when they do occur, but unfortunately we cannot completely eliminate the problem. No-shows are highly annoying and, therefore, one of the main reasons I recommend paying fairly generous incentives to test participants, even when their normal hourly salary is relatively low.
Initiating Systematic Recruiting
I strongly recommend that you treat participant recruiting as an important component of your user experience process. The more you establish and systematize your recruiting approach, the easier it will be to run studies when you need usability data.
If you're a small company, or if you haven't done much user testing in the past, I recommend that you consider hiring a professional recruiting agency. If you can't afford the agency fee or if you conduct too few studies to employ an in-house recruiting specialist, you can certainly recruit your own test participants, and it will become easier to do so as you gain experience.
In either case, following best practices for recruiting will reduce your usability program's overall cost and increase the validity of your test findings. If you get the wrong users, or if you don't get enough users, your usability studies will not generate the results you deserve, and your credibility could suffer as a result.
Full Report (Free)
The full report on recruiting test users for usability studies is available for free download.
(Update added 2013: Recruiting agency fees and recommended user incentives are now around 20% higher than the numbers reported here, but the relative size of the expenses remains the same.)
"
105,2002-07-21,"To reach the goal of making technology truly suited for humans, the world will need about half a million new usability professionals over the next 20 years. The sooner their training begins, the better off we'll all be.
People frequently ask me what it takes to become a usability professional and get a job in the field. The answer lies in  three characteristics that all great usability professionals share:

Knowledge of  interaction theory and user-research  methodologies, especially the principles of user testing
High  brain power 
10 years' experience  running user tests and other usability activities, such as field studies

Unfortunately, only the first of these characteristics can be taught (yes, we offer UX training). Usability expertise is mainly an issue of talent and experience rather than theory.  Much of usability work requires pattern matching, which is why it's so dependent on brain power and past experience: Once you observe slight traces of a usability issue in users' behavior, you must deduce the underlying implications for design.
A bad usability specialist will report,  ""User 1 liked this, but User 2 did not.""  Not much help for the design team. A good usability specialist  combines the observations  across multiple users,  distills the patterns, and arrives at a  conceptual insight  that can drive the design.
This is not to say that theory is irrelevant. After all, I've written several books on usability myself, and I believe in transferring experience: There is no reason for you to make the same mistakes I made twenty years ago. If you want to enter the user experience field, it's certainly a good idea to read some  basic UX books first.
In addition to the basic theory, there are also many  practical tips and tricks for user testing that have evolved over the years. If you run a study with bad methodology, you won't learn much. Every methodological insight makes your own studies a little better, and leveraging others people's experience offers a great return on investment
Learn by Doing
It's a bit circular, but to  really  learn about usability you have to run usability studies. Reaching usability nirvana requires many years' experience observing diverse users in varying contexts:

Study a  wide range of people: the young and old, utter novices, experts, Unix geeks, sales staff, physicians, repair technicians, administrative assistants, executives, users of different nationalities.
Watch those people perform a  wide range of tasks: shopping, searching, planning vacations, researching school projects, managing an erupting oil well.
Observe them using a  wide range of interface designs and styles. Ideally, the interfaces should feature different ways of solving the same design problem so that you can compare and contrast how different design details affect usability.
Experiment with a  wide range of interaction platforms, from wall-sized ""virtual windows"" to pocket-sized PDAs. It can also help to watch people use text-only designs like a mainframe or classic Unix, or futuristic technologies like VR that might be currently useless, but can serve as a source of ideas.

Note the frequent use of the term  ""wide range""  in this list. Breadth of experience is the foundation that lets you confidently say things like, ""Users typically have problems when faced with this kind of situation, but here are some alternative designs that often work."" There is no single answer in usability. The more you've seen, the better equipped you are to handle new situations.
Getting Started
There are only two barriers to running a usability study:

the fear that you can't do it, and
the desire for perfection.

The first barrier is hogwash. Of course you can do it. As you set out, try to avoid the classic mistake of novice usability engineers: interfering with users and thus biasing how they use the interface. Keep quiet and let the user do the talking.
The second barrier is quite real. Your first study will not be perfect. But if you wait until you are perfect, you will never get anything done, because the only way to get really good at usability is to do it.
I recommend that you  start with a small project  that's easy to manage. Ideally, you should know the project designers and developers well, or perhaps be the primary designer or developer yourself. Usability testing is such a powerful method that even if you mess it up, you'll get useful results that will significantly improve the design. Still, it's better to mess up on a small project than a big one.
After a few rounds of testing and design improvements on this project, two things will happen:

You will become better at usability.
Management will notice big quality improvements in the tested designs.

That's when it's time to move to a bigger, high-profile project. If you work on an intranet,  pick something that all employees use frequently. Making a widely used tool easier to use will not only increase productivity (and thus make the company lots of money), but it will also evangelize usability to all the design and development people — who, after all, will be using the improved tool themselves.
Learning the basics of usability testing isn't difficult. We teach it in a day. What is hard is getting all the subtleties right and achieving deep conceptual understanding. It takes about ten years to build up the sufficiently big mental database of previous observations required to be a truly great usability professional. People with a few years' experience perform reasonably well, but the first year or so, some insights will escape you. Still, you  must  go through this first year, and the sooner you start running your own tests, the sooner you'll become a master."
106,2001-04-14,"Traditionally,  critical incident analysis  has been a great tool for collecting user feedback about existing user interfaces. To do it, you basically ask the user to recall a prominent case where the interface was uncommonly helpful or particularly disappointing. I usually ask users for both positive and negative examples, and the responses always help me understand how they're using the system and how I can improve it by making certain aspects more or less prominent.
Unfortunately, critical incident analysis is less useful for many Web projects for two main reasons:

The site or new feature may not exist yet, so users have no real-life experience using it.
Websites often fail in ways that are critical to the  company  but not to users, who simply leave and go to another site.  Users rarely recall why they left a site  after a minute or two. Abandoning a shopping cart because you can't find the shipping costs is not an incident that burns itself into your memory cells. This is why there is little value to those surveys on ""why users abandoned their shopping carts"" that some analysts publish.

Xerox PARC Study of Critical Incidents on the Web
Researchers from Xerox PARC recently presented the mother of all critical incident studies. The big question: What are the important things people do on the Web as a whole? Although individual websites may not generate good critical incidents, the totality of users' online experience surely does.
Julie Morrison, Peter Pirolli, and Stuart Card collected responses to the following statement from 2,188 people:
Please try to recall a recent instance where you found important information on the World Wide Web, information that led to a significant action or decision. 
The obvious weakness of this request (and the entire critical incidence method) is that it  does not address  average  Web use; it looks only at  important  use.  For example, only 2% of the respondents referred to reading news when describing a critical incidence, whereas a separate survey of what these same users do on the Web found that 24% of them read news regularly.
However, we can turn this bug into a feature. Looking at what users find important on the Web provides several advantages:

Critical tasks are more likely than average tasks to lead to  value-added services that users will pay for.
If you support important tasks, users are likely to turn to you for everyday tasks.
By understanding what's critical to users, you might gain insight into what's different and exciting about the Web; this can inspire you to innovate.

Main Method: Goal-Driven Collection
The PARC researchers analyzed the methods users described for arriving at the information they needed for their critical tasks.

Collect: 71%.  Users searching for multiple pieces of information. They are driven by a specific goal, but are not looking for one particular answer.
Find: 25%.  Users searching for something specific.
Explore: 2%.  Users looking around without a specific goal.
Monitor: 2%.  Users repeatedly visiting the same website to update information. Visits are triggered by routine behavior rather than a particular goal.

The most obvious conclusion is that, when it comes to critical Web use,  users are almost always goal-driven: 96% of the time in the PARC study. Although this has been common knowledge for some time, the magnitude of the percentage surprised even me.
It's also interesting that it is almost three times as important for users to find multiple pieces of information as it is to locate a single specific piece. The entire browsing paradigm is optimized for accessing individual locations. Users are typically on their own when they want to collect more than one answer.
Main Task: Compare and Choose
In the study, the primary reasons for the respondents' important use of the Web was classified as:

Compare/Choose: 51%.  Evaluate multiple products or answers to make a decision.
Acquire: 25%.  Get a fact, get a document, find out about a product, download something. (Note: Morrison et al. use the term ""find"" to refer to these tasks, but I prefer the term ""acquire"" to differentiate the goal from the method, as discussed above.)
Understand: 24%.  Gain understanding of some topic; this generally includes locating facts or documents.

The important tasks are thus divided almost equally between cases where the user is trying to decide between multiple options and cases where the user is pursuing a single option.
Implication for Usability: 3C Testing
The three Cs, collect, compare, and choose, describe most of the Web's critical use. As a result, we should make sure to include test tasks that address these issues when we plan usability studies of websites.
Of course, usability studies should also test simpler tasks. We should not overlook the less-critical aspects of using the Web, since they account for more of users' time. But, considering how poorly the Web currently supports the 3 Cs, we do need to give them more focus so we help users better succeed with their most important tasks.
Reference
Morrison, J.B., Pirolli, P., and Card, S.K. (2001): ""A Taxonomic Analysis of What World Wide Web Activities Significantly Impact People's Decisions and Actions."" Interactive poster, presented at the Association for Computing Machinery's  Conference on Human Factors in Computing Systems, Seattle, March 31 - April 5, 2001. (Warning: link leads to a PDF file.)
 
"
107,2021-07-20,"Numbers are powerful (even though they are often misused in user experience). They offer a simple way to communicate usability findings to a general audience. Saying, for example, that ""Amazon.com complies with 72% of the e-commerce usability guidelines"" is a much more specific statement than ""Amazon.com has great usability, but it doesn't do everything right.""
Metrics are great for assessing long-term progress on a project and for setting goals. They are an integral part of a benchmarking program and can be used to assess if the money you invested in your redesign project was well spent.
Unfortunately, there is a conflict between the need for numbers and the need for insight. Although numbers can help you communicate usability status and the need for improvements, the true purpose of a user experience practice is to set the design direction, not to generate numbers for reports and presentations. Thus, some of the best research methods for usability (and, in particular, qualitative usability testing) conflict with the demands of metrics collection.
The best usability tests involve frequent small tests, rather than a few big ones. You gain maximum insight by working with 4–5 users and asking them to think out loud during the test. As soon as users identify a problem, you fix it immediately (rather than continue testing to see how bad it is). You then test again to see if the ""fix"" solved the problem.
Although small tests give you ample insight into how to improve design, such tests do not generate the sufficiently tight confidence intervals that traditional metrics require. Think-aloud protocols are the best way to understand users' thinking and thus how to design for them, but the extra time it takes for users to verbalize their thoughts contaminates task time measures. Plus, qualitative tests often involve small tweaks from one session to the next, and, because of that metrics, collected in such tests are rarely measuring the same thing.
Thus, the best usability methodology is the one least suited for generating detailed numbers.
Measuring  Success
One of the more common metrics used in user experience is task success or completion. This is a very simple binary metric.  When we run a study with multiple users, we usually report the success (or task-completion) rate: the percentage of users who were able to complete a task in a study.  
Like most metrics, it is fairly coarse — it says nothing about why users fail or how well they perform the tasks they did complete.
Nonetheless, success rates are easy to collect and a very telling statistic. After all, if users can't accomplish their target task, all else is irrelevant. User success is the bottom line of usability.
Levels of Success
Success rates are easy to measure, with one major exception: How do we account for cases of partial success? If users can accomplish part of a task, but fail other parts, how should we score them?
Let's say, for example, that the users' task is to order twelve yellow roses to be delivered to their mothers on their birthday. True task success would mean just that: Mom receives a dozen roses on her birthday. If a test user leaves the site in a state where this event will occur, we can certainly score the task as a success. If the user fails to place any order, we can just as easily determine the task a failure.
But there are other possibilities as well. For example, a user might:

order twelve yellow tulips, twenty-four yellow roses, or some other deviant bouquet
fail to specify a shipping address, and thus have the flowers delivered to their own billing address
specify the correct address, but the wrong date
do everything perfectly except forget to specify a gift message to enclose with the shipment, so that mom gets the flowers but has no idea who they are from

Each of these cases constitutes some degree of failure.
If a user does not perform a task as specified, you could be strict and score it as a failure. It's certainly a simple model: Users either do everything correctly or they fail. No middle ground. Success is success, without qualification.
However, we sometimes grant partial credit for a partially successful task. It can seem unreasonable to give the same score (zero) to both users who did nothing and those who successfully completed much of the task. How to score partial success depends on the magnitude of user error.
In the flower example, we might define several levels of success:

complete success: the user places the order with no error, exactly as specified
success with one minor issue: the user places the order but omits the gift message or orders the wrong flowers
success with a major issue: the user places the order but enters the wrong date or delivery address  
failure: the user is not able to place the order

Of course, the precise levels of success would depend on the task and your and your users’ particular needs. (For example, if you did a survey and determined that most mothers would consider it a major offense to get tulips instead of roses, you may change the rating accordingly).
Reporting Levels of Success
To report levels of success, you simply report the percentage of users who were at a given level. So, for example, if out of 100 users, 35 completed the task with a minor issue, you would say that 35% of your users were able to complete the task with a minor issue.  Like for any metric, you would have to report the confidence interval for that number.




Level of success


Number of users (out of 100)


How you report it




Complete success


20


20% of our participants were able to complete the task successfully with no error. Based on this result, we expect that between 13% and 29% (*) of our general user population will complete the task with no error.




Success with a minor issue


35


35% of our participants placed an order but had a minor issue. Based on this result, we expect that between 26% and 45% (*) of our general user population will complete the task with a minor error.




Success with a major issue


30


30% of our participants placed an order but encountered a major issue. Based on this result, we expect that between 22% and 40% (*) of our general population will complete the task with a major error.




Failure


15


15% of our participants were not able to place the order. Based on this task, we expect that between 9% and 23% (*) of our general population will not be able to place an order.




(*) In this table, the ranges represent 95% confidence intervals calculated using the Adjusted Wald method.

Levels-of-success data can be shown as separate columns with corresponding confidence intervals. In this graph the error bars represent 95% confidence intervals.

Note that this method simply amounts to using multiple metrics for success instead of just one — each level of success is a separate metric.
You can also use other metrics such as number of errors; for example, you could define different error types (e.g., wrong flowers, wrong shipping address) and track the number of people who made each of these errors. Doing so may actually give you a more nuanced picture than using levels of success because you might be able to say precisely which of the different errors is more common and, thus, focus on fixing that one.
Do Not Use Numbers for Success Levels
A common error that people make when working with success levels is to assign numbers to them; for example, they may say:

complete success = 1
success with one minor issue = 0.66
success with a major issue = 0.33
failure = 0

And then, instead of reporting success, they simply average these success levels for their participants. In our example, they might say that the success rate is:
(20*1+35*0.66+ 30*0.33+0*15)/100 = 0.53 = 53%
This approach is wrong! The numbers that we assigned to the different levels of success are simply labels and they form an ordinal scale, not an interval or ratio scale. That means that, even though there is an order established across these levels of success (e.g., failure is worse than success with major issue), there is no mathematical meaning to these numbers and we cannot average them because we cannot truly guarantee that these numbers are evenly spaced on a 0 to 1 scale (or whatever other scale we’re using between complete success and complete failure). In other words, we don’t know and have no reason to assume if the difference between complete success and success with minor issue is the same as the difference between failure and success with major issue.
Since the temptation of averaging numbers is so big in real life, we strongly recommend that you assign word labels to levels of success instead of numbering them."
108,2001-01-20,"Usability can be measured, but it rarely is. The reason? Metrics are expensive and are a poor use of typically scarce usability resources.
Most companies still under-invest in usability. With a small budget, you're far better off passing on quantitative measures and reaching for the low-hanging fruit of qualitative methods, which provide a much better return on investment. Generally, to improve a design, insight is better than numbers.
However, the tide might be turning on usability funding. I've recently worked on several projects to establish formal usability metrics in different companies.  As organizations increase their usability investments, collecting actual measurements is a natural next step and does provide benefits. In general, usability metrics let you:

Track progress between releases.  You cannot fine-tune your methodology unless you know how well you're doing.
Assess your competitive position.  Are you better or worse than other companies?  Where  are you better or worse?
Make a Stop/Go decision before launch.  Is the design good enough to release to an unsuspecting world?
Create bonus plans for design managers and higher-level executives.  For example, you can determine bonus amounts for development project leaders based on how many customer-support calls or emails their products generated during the year.

How to Measure
It is easy to specify usability metrics, but hard to collect them. Typically, usability is measured relative to users' performance on a given set of test tasks. The most basic measures are based on the  definition of usability as a quality metric:

success rate (whether users can perform the task at all),
the time a task requires,
the error rate, and
users' subjective satisfaction.

It is also possible to collect more specific metrics, such as the percentage of time that users follow an optimal navigation path or the number of times they need to backtrack.
You can collect usability metrics for both  novice users and experienced users. Few websites have truly expert users, since people rarely spend enough time on any given site to learn it in great detail. Given this, most websites benefit most from studying novice users. Exceptions are sites like Yahoo and Amazon, which have highly committed and loyal users and can benefit from studying expert users.
Intranets, extranets, and weblications are similar to traditional software design and will hopefully have skilled users; studying experienced users is thus more important than working with the novice users who typically dominate public websites.
With qualitative user testing, it is  enough to test 3–5 users. After the fifth user tests, you have all the insight you are likely to get and your best bet is to go back to the drawing board and improve the design so that you can test it again. Testing more than five users wastes resources, reducing the number of design iterations and compromising the final design quality.
Unfortunately, when you're collecting usability metrics, you must test with more than five users. In order to get a reasonably tight confidence interval on the results, I usually recommend testing 20 users for each design. Thus, conducting quantitative usability studies is approximately four times as expensive as conducting qualitative ones. Considering that you can learn more from the simpler studies, I usually recommend against metrics unless the project is very well funded.
Comparing Two Designs
To illustrate quantitative results, we can look at those recently posted by Macromedia from its usability study of a Flash site, aimed at showing that Flash is not  necessarily  bad. Basically, Macromedia took a design, redesigned it according to a set of usability guidelines, and tested both versions with a group of users. Here are the results:



 
Original Design
Redesign


Task 1
12 sec.
6 sec.


Task 2
75 sec.
15 sec.


Task 3
9 sec.
8 sec.


Task 4
140 sec.
40 sec.


Satisfaction score*
44.75
74.50


*Measured on a scale ranging from
			12 (unsatisfactory on all counts) to 84 (excellent on all counts).



It is very rare for usability studies to employ tasks that are so simple that users can perform them in a few seconds. Usually, it is better to have the users  perform more goal-directed tasks that will take several minutes. In a project I'm working on now, the tasks often take more than half an hour (admittedly, it's a site that needs  much  improvement).
Given that the redesign scored better than the original design on all five measures, there is no doubt that the new design is better than the old one. The only sensible move is to go with the new design and launch it as quickly as possible. However, in many cases, results will not be so clear cut. In those cases, it's important to look in more detail at  how much  the design has improved.
Measuring Success
There are two ways of looking at the time-to-task measures in our example case:

Adding the time for all four tasks produces a single number that indicates ""how long it takes users to do stuff"" with each design. You can then easily compute the improvement. With the original design, the set of tasks took 236 seconds. With the new design, the set of tasks took 69 seconds. The improvement is thus  242%. This approach is reasonable if site visitors typically perform all four tasks in sequence. In other words, when the test tasks are really subtasks of a single, bigger task that is the unit of interest to users.
Even though it is simpler to add up the task times, doing so can be misleading if the tasks are not performed equally often. If, for example, users commonly perform Task 3 but rarely perform the other tasks, the new design would be only slightly better than the old one; task throughput would be nowhere near 242% higher. When tasks are unevenly performed, you should compute the improvement separately for each of the tasks:
	
Task 1: relative score 200% (improvement of 100%).
Task 2: relative score 500% (improvement of 400%).
Task 3: relative score 113% (improvement of 13%).
Task 4: relative score 350% (improvement of 250%).

	You can then take the geometric mean of these four scores, which leads to an overall improvement in task time of  150%.

Why do I recommend using the  geometric mean  rather than the more common arithmetic mean? Two reasons: First, you don't want a single big number to skew the result. Second, the geometric mean accounts fairly for cases in which some of the metrics are negative (i.e., the second design scores less than 100% of the first design).
Consider a simple example containing two metrics: one in which the new design doubles usability and one in which the new design has half the usability of the old. If you take the arithmetic average of the two scores (200% and 50%), you would conclude that the new design scored 125%. In other words, the new design would be 25% better than the old design. Obviously, this is not a reasonable conclusion.
The geometric mean provides a better answer. In general, the geometric mean of  N  numbers is the  N  'th root of the product of the numbers. In our sample case, you would multiply 2.0 with 0.5, take the square root, and arrive at 1.0 (or 100%), indicating that the new design has the same usability as the baseline.
Although it is possible to assign different weights to the different tasks when computing the geometric mean, absent any knowledge as to the relative frequency or importance of the tasks, I've assumed equal weights here.
Summarizing Results
Once you've gathered the metrics, you can use the numbers to formulate an overall conclusion about your design's usability. However, you should first examine the relative importance of performance versus satisfaction. In the Macromedia example, users' subjective satisfaction with the new design was 66% higher than the old design. For a business-oriented website or a website that is intended for frequent use (say, stock quotes), performance might be weighted higher than preference. For an entertainment site or a site that will only be used once, preference may get the higher weight. Before making a general conclusion, I would also prefer to have error rates and perhaps a few additional usability attributes, but, all else being equal, I typically give the same weight to all the usability metrics. Thus, in the Macromedia example, the geometric mean averages the set of scores as:  sqrt  (2.50*1.66)=2.04. In other words, the new design scores 204% compared with the baseline score of 100% for the control condition (the old design).
The new design thus has 104% higher usability than the old one. 
This result does not surprise me: It is  common for usability to double as a result of a redesign. In fact, whenever you redesign a website that was created without a systematic usability process, you can often improve measured usability even more. However, the first numbers you should focus on are those in your budget. Only when those figures are sufficiently large should you make metrics a part of your usability improvement strategy.
More Case Studies
See full report on the return-on-investment (ROI) from usability for many more examples of before–after usability metrics. For even more depth, see the full-day course on Measuring User Experience,"
109,2000-03-18,"Some people think that usability is very costly and complex and that user tests should be reserved for the rare web design project with a huge budget and a lavish time schedule. Not true. Elaborate usability tests are a waste of resources. The best results come from testing no more than 5 users and running as many small tests as you can afford.
In earlier research, Tom Landauer and I showed that the number of usability problems found in a usability test with  n  users is:

where  N  is the total number of usability problems in the design and  L  is the proportion of usability problems discovered while testing a single user. The typical value of  L  is 31%, averaged across a large number of projects we studied. Plotting the curve for  L  =31% gives the following result:

The most striking truth of the curve is that  zero users give zero insights.
As soon as you collect data from a  single test user, your insights shoot up and you have already learned almost a third of all there is to know about the usability of the design. The difference between zero and even a little bit of data is astounding.
When you test the  second user, you will discover that this person does some of the same things as the first user, so there is some overlap in what you learn. People are definitely different, so there will also be something new that the second user does that you did not observe with the first user. So the second user adds some amount of new insight, but not nearly as much as the first user did.
The  third user  will do many things that you already observed with the first user or with the second user and even some things that you have already seen twice. Plus, of course, the third user will generate a small amount of new data, even if not as much as the first and the second user did.
As you  add more and more users, you learn less and less  because you will keep seeing the same things again and again. There is no real need to keep observing the same thing multiple times, and you will be very motivated to go back to the drawing board and redesign the site to eliminate the usability problems.
After the fifth user, you are wasting your time by observing the same findings repeatedly but not learning much new.
Iterative Design
The curve clearly shows that you need to  test with at least 15 users to discover all the usability problems  in the design. So why do I recommend testing with a much smaller number of users?
The main reason is that it is better to distribute your budget for  user testing across many small tests instead of blowing everything on a single, elaborate study. Let us say that you do have the funding to recruit 15 representative customers and have them test your design. Great.  Spend this budget on 3 studies with 5 users each!
You want to run multiple tests because the real goal of usability engineering is to improve the design and not just to document its weaknesses. After the first study with five participants has found 85% of the usability problems, you will want to fix these problems in a redesign.
After creating the new design, you need to  test again. Even though I said that the redesign should ""fix"" the problems found in the first study, the truth is that you  think  that the new design overcomes the problems. But since nobody can design the perfect user interface, there is no guarantee that the new design does in fact fix the problems. A second test will discover whether the fixes worked or whether they didn't. Also, in introducing a new design, there is always the risk of introducing a new usability problem, even if the old one did get fixed.
Also, the second study with 5 users will discover most of the remaining 15% of the original usability problems that were not found in the first round of testing. (There will still be 2% of the original problems left — they will have to wait until the third study to be identified.)
Finally, the second study will be able to  probe deeper into the usability of the fundamental structure  of the site, assessing issues like information architecture, task flow, and match with user needs. These important issues are often obscured in initial studies where the users are stumped by stupid surface-level usability problems that prevent them from really digging into the site.
So the second study will both serve as quality assurance of the outcome of the first study and help provide deeper insights as well. The second study will always lead to a new (but smaller) list of usability problems to fix in a redesign. And the same insight applies to this redesign: not all the fixes will work; some deeper issues will be uncovered after cleaning up the interface. Thus, a third study is needed as well.
The ultimate user experience is improved much more by 3 studies with 5 users each than by a single monster study with 15 users.
Why Not Test With a Single User?
You might think that 15 studies with a single user would be even better than 3 studies with 5 users. The curve does show that we learn much more from the first user than from any subsequent users, so why keep going? Two reasons:

There is always a risk of being misled by the spurious behavior of a single person who may perform certain actions by accident or in an unrepresentative manner. Even 3 users are enough to get an idea of the diversity in user behavior and insight into what's unique and what can be generalized.
The  cost-benefit analysis of user testing provides the optimal ratio around 3 or 5 users, depending on the style of testing. There is always a fixed initial cost associated with planning and running a study: it is better to depreciate this start-up cost across the findings from multiple users.

When To Test More Users
You need to test additional users when a website has  several highly distinct groups of users. The formula only holds for comparable users who will be using the site in fairly similar ways.
If, for example, you have a site that will be used by both children and parents, then the two groups of users will have sufficiently different behavior that it becomes necessary to test with people from both groups. The same would be true for a system aimed at connecting purchasing agents with sales staff.
Even when the groups of users are very different, there will still be great similarities between the observations from the two groups. All the users are human, after all. Also, many of the usability problems are related to the fundamental way people interact with the Web and the influence from other sites on user behavior.
In testing multiple groups of disparate users, you don't need to include as many members of each group as you would in a single test of a single group of users. The overlap between observations will ensure a better outcome from testing a smaller number of people in each group. I recommend:

3–4 users from each category if testing two groups of users
3 users from each category if testing three or more groups of users (you always want at least 3 users to ensure that you have covered the diversity of behavior within the group)

Reference
Nielsen, Jakob, and Landauer, Thomas K.: ""A mathematical model of the finding of usability problems,""  Proceedings of ACM INTERCHI'93 Conference  (Amsterdam, The Netherlands, 24-29 April 1993), pp. 206-213.
Follow-Up Articles

Newer analysis of the problem discussed in this article:  How Many Test Users in a Usability Study?
Quantitative Studies (usability metrics): Test  20  Users
Card Sorting: Test  15  Users
"
110,1999-02-20,"
 A recent
 
  study
 
 by
 
  IPSOS-ASI
 

  purported to show that Web advertising was as good for branding as television advertising
 
 : users remember 40% of Web banners and 41% of TV commercials. Because of this striking finding, the study was widely
 
  reported in the press
 
 .


 The finding was indeed so striking as to conflict with most other evidence:



  qualitative observations of Web users show that they are very goal-driven and
  
   ignore the ads while focusing completely on their task
  


  eye-tracking studies by
  
   Will Schroeder
  
  and others quantitatively confirm the existence of
  
   banner blindness
  
  where the user's gaze never rests in the region of the screen occupied by advertising
 


   click-through rates are dropping
  
  like a stone and recently reached 0.5% (half of the 1% rate a year ago), causing
  
   savvy advertisers like P&G to pay no more than 1/7 the normal cost
  
  of banners
 


 When new data contradicts existing evidence and established interaction theory, the first reaction should be to
 
  suspect the new data
 
 ; not to overthrow the
 
  old insights
 
 . Sure,
 
  sometimes
 
 an Einstein will have discovered a Newtonian paradigm that doesn't hold, but most paradox cases are cold fusion.


 In this particular case, a close inspection of the research methodology shows a small, but important aspect of the way users were treated that makes the results
 
  unrealistic as a prediction of real-world Web usage
 
 .


 The television viewers were watching a show that they had been
 
  asked
 
 to view rather than one they had selected themselves. This is a slightly unnatural situation, but probably one that doesn't much impact the extent to which they pay attention to the commercials.


 In an attempt to use a similar approach to studying the online users, they were asked to go to a section of America Online ""to evaluate the content of that area.""
 

   Evaluating
  
  something is a very different user experience than
  
   using
  
  something for a self-imposed goal
 
 (the normal way people use the Web). When asked to evaluate some pages that they have no reason to use, people are likely to look around each page and check out every design element on the page. In contrast, when people actually
 
  use
 
 the Web, they go straight for the most likely solution to their problem and ignore every other part of the page: the second users see a link that leads to their goal, they click it and are off the page. (As an aside, this phenomenon also explains why it is invalid to measure usability by asking a survey panel to check out a site and rate it on a questionnaire.)


 A smaller difference between the measures of TV and online advertising is that the TV viewers were responding to verbal questions read over the telephone whereas the computer users were responding to a visual questionnaire on their screen. It is possible that the visual representation of the questions was better at triggering the respondents' memory than the auditory questions. I am not sure how much this latter issue influences the outcome of the study, but the first issue is sufficient to make the conclusions irrelevant for anybody interested in real Web users.


 It is admittedly hard to design a perfect study to compare TV and online because
 
  user behavior is so different in the two media
 
 . In the study I am discussing here, the goal was to treat people identically, but that's exactly why the outcome is unrealistic for the Web. As an analogy, think of comparing bicycles with cars by asking riders and drivers to move with the same speed. You would find that car owners do not use their vehicles to travel very far.


 A better study would be more naturalistic to allow for the differences in user behavior: have people watch TV for an hour (selecting their own preferred show) and see how many commercials they remember, and have users browse the Web for an hour (while performing real tasks like booking airline tickets, researching what scanner to buy, or tracking down the address of a long-lost friend) and see what banners they remember. Even this study is not fully realistic because people often use the Web for very quick in-and-out access to specific data.


 This case study shows the importance of meticulous attention to detail when planning quantitative studies. The smallest problem in the methodology can significantly impact the outcome and give you results that are irrelevant for the real-world problem you are trying to solve.


 Luckily most Web usability studies are more robust since they don't involve numeric comparisons between different concepts. The most common Web study looks at the way your customers use your site and where they have difficulties. As long as you have representative users and don't bias their actions, you
 
  will
 
 discover the major usability problems in your design. So don't despair: most real projects will have considerably less weaknesses than the study I criticized here.


  


 See
 
  comments by Marianne Foley
 
 (from the organization that ran the study)

"
111,1998-05-16,"It is easy to test the usability of individual Web pages: simply have users interact with the pages and see whether they understand them or have trouble. Page templates are another story. In a traditional test two issues overlap and are hard to separate:

usability of the  layout  specified by the template design
usability of the specific  content  that has been poured into this template on the individual pages

Before you unleash a page template on a large number of content creators and direct them to design thousands of pages to its specifications, it would be nice to have more data on the usability of the template itself. By ""usability of template"" I mean  whether a page gets more or less usable because of the layout  specified by the template. It would also be interesting to study whether it is easy for page authors to design with the template, but that is a different matter.
Thomas S. Tullis from Fidelity Investments recently conducted a study to assess the usability of five alternative template designs for their intranets. To test whether the layout itself helped users find the page elements,  all text on the page was ""greeked""  — that is, all words were replaced with unintelligible nonsense (sometimes called  mumble text). When they can't read the text, users have to rely on the  inherent communicative aspects of the layout  to perform the test task. If a layout performs well when users can't understand any of the content, then there is hope that the template will survive substantial abuse from authors who fill it with content of varying quality.
Each test user was given printouts of the greeked pages (in random sequence) and asked to draw labeled blocks around the parts of a page that corresponded to each of nine standard elements. Blocks could not overlap and could not be nested. If a user thought that an element was missing from a page, it was to be marked as ""not there."" The study included five templates of which two are shown here (reproduced with permission).

Page Template 1 
 

Page Template 3 
Each page template had the following nine standard elements which you should try to identify in the figures:

Main content selections for this page
Page title
Person responsible for this page
Intranet-wide navigation (e.g., intranet home, search)
Last updated date
Intranet identifier/logo
Site navigation (e.g, major sections of this section of the intranet)
Confidentiality/security (e.g, Public, Confidential, etc.)
Site news items

The  correct solutions are shown in a sidebar, but please try to  solve the problem yourself before peeking. You only get one chance to approach a problem as a novice user.
As shown in the table, users performed best with Template 3, where they were able to correctly identify 67% of the standard page elements. Users were also asked to subjectively rate how well they liked the templates on a -3 to +3 scale. As shown in the table, Template 1 was the best liked, even though users performed significantly worse with this template. This outcome demonstrates the danger of simply showing designs to people and asking them which one they like the best: users may not know what's good for them.



 
Correctly Identified
			Page Elements
Subjective
			Appeal


Template 1
52%
+1.3


Template 3
67%
+0.9


Final Design
72%
+2.1



Since different designs scored best on the two usability metrics (performance and satisfaction), Fidelity decided to combine the best parts of the designs into a new, final design. When this final design was tested with the same methodology, it scored better than any of the original designs, proving that it was indeed an improvement.
The greeking technique is a very fast way of gathering usability data about proposed page templates before a large number of pages have been constructed. Since users can't actually  use  nonsense pages, there is no need to implement anything: instead, each design can be printed out and marked up on paper.
Reference
Tullis, T.S. ""A method for evaluating Web page design concepts."" In ACM Conference on Computer-Human Interaction  CHI 98 Summary  (Los Angeles, CA, 18-23 April 1998), pp. 323-324.
Unless you were at the conference, you will have a hard time getting hold of the conference summary book: I am including the reference for completeness and to acknowledge Tullis' paper, but I cannot supply reprints."
112,1998-05-02,"It takes  39 hours to usability test a website  the first time you try. This time estimate includes planning the test, defining test tasks, recruiting test users, conducting a test with five users, analyzing the results, and writing the report.  With experience, Web user tests can be completed in two work days  .
In a recent project at the Technical University of Denmark, Rolf Molich and Christian Gram collected data from 50 teams of students who conducted usability tests of commercial websites as part of a user interface design class. The average time spent by each team was 39 hours. Furthermore, the students had sat through 15 hours of lectures on user test methodology. These numbers are an upper estimate of the time investment the first time you decide to user test your site.
With experience it is possible to conduct much more rapid user tests of a site. Good test tasks can be written in one or two hours,  recruiting can be outsourced   to a recruiting firm (at a cost of less than $1,000 for five users), the actual test can be done in a day, and the results can be analyzed in a few hours. If you are a member of the design team, then there is no reason to write an extensive report which nobody will read, so reporting can be done in a one-hour meeting supplemented by a summary that takes 2-3 hours to write. In total, a discount usability study takes only two work days once you know what you are doing.
Even though experts can do the work more efficiently (and usually with better results), it is encouraging that utter beginners could complete a full Web usability project in less than one week. This truly proves that ""limited budget"" and ""lack of time"" are not valid excuses for inflicting difficult sites on your users.
The students tested nine large Danish sites: seven sites from major Danish corporations as well as the University's own site and the university library's site.
After  rating the usability problems for severity  , the study found that each site had an average of

11 usability catastrophes
20 serious usability problems
29 cosmetic problems

In this study, a ""catastrophe"" was defined as a usability problem that prevented the user from completing a task. A ""serious"" problem was one that slowed down users significantly but did allow them to complete their task. A ""cosmetic"" problem delayed users slightly or annoyed the users as indicated by their verbal comments.
 
I am not at all surprised by that major commercial sites contain this many usability problems. In fact, Molich and Gram note that the sites probably had even  more  problems that were not found in the study since the students only tested a small part of each site (though presumably they focused on the most important parts).
Most of the usability problems found by the Danish students were similar to problems we have seen in studies of English-language sites over the last five years. This similarity is due to the fact that the study addressed domestic usability (Danish users accessing Danish sites) and not international usability (Danish users accessing, say, American sites). A few problems related to international usability were observed since some sites contained a combination of Danish and English content. Some sites switched language without warning when an unsuspecting user followed certain links.  Multilingual search  also caused problems since it was not clear to users whether an English-language search page would include the Danish pages in its search scope. Some other issues in multilingual search were discussed in my  August 1996 Alertbox on international usability   but it is striking how little is known about this topic.
In a  study of 15 large commercial sites   in the U.S., Jared Spool and colleagues found that users were only  successful 42% of the time  when asked to find specific information. When asked to rate ""overall ease of use"", people scored these sites 4.9 on a 1-7 scale (7 best); somewhat better than the neutral rating. This latter result highlights why it is not sufficient to simply ask people whether they like your site: people tend to be polite and give relatively high ratings even when the site is unusable.
These statistics form an interesting  baseline for your own usability studies  :

the average site has 11 usability catastrophes (design elements that prevent users from completing test tasks)
on average, users are only able to complete 42% of the test tasks
users' average subjective rating of websites is 4.9 on a 1-7 scale

If you did not use any systematic usability engineering methods in the development of your site, your score will probably be along the lines in this list; often worse.
Test Coverage
Web usability problems fall into two categories:

Site-level usability  : home page; information architecture, navigation, and search; linking strategy; internally vs. externally focused design; overall writing style; page templates, layout, and site-wide design standards; graphical language and commonly used icons
Page-level usability  : specific issues related to the individual pages: understandability of headlines, links, and explanations; intuitiveness of forms and error messages; inclusion or exclusion of specific information; individual graphics and icons

A usability test with  5 users will typically uncover 80% of the site-level usability problems  plus about half of the page-level usability problems on those pages that users happen to visit during the test. The reason for the lower coverage of page-level problems is that different users will visit different pages, so most pages will be tested by less than the 5 users it takes to find 80% of the problems in a design. A test with 2 users typically finds half of the usability problems in a design, so that is my estimate of the proportion of page-level problems found.
Of course, on a large site, most individual pages will not be visited by  any  of the test users. Thus, the main goal of user testing a site should be to find the site-level problems. You should obviously take note of the page-level problems so that they can be fixed on those pages that the users happened to visit. The most important outcome of finding page-level usability problems is a better understanding of the extent of page-level problems on your site. Also, the specific set of problems can serve as a starting point for developing a  list of typical page-level design pitfalls  that need special attention in the design of future pages.
It is possible to run  specialized user tests of particularly important pages  to increase coverage of their page-level usability problems. For example, it is often a good idea to test registration pages and the pages where users actually buy products or download software. It is possible to substantially increase the number of users who are able to complete their transactions.
For most other pages it is necessary to increase page-level usability through other methods like  heuristic evaluation   and by training content developers in principles of good Web usability. One good way of increasing the page-level staff's understanding of Web usability is to invite them to observe a few user tests. Even when other people's pages are being tested, one can still learn a lot about how users interact with Web pages from simply observing one or two tests.
The estimates of usability problems found with different numbers of test users comes from a project I did with Tom Landauer. For more information, see my book  Usability Engineering   and Landauer's book  The Trouble With Computers: Usefulness, Usability, and Productivity  .
"
113,1997-01-01,"The introduction of the spreadsheet turned millions of people into programmers without the benefit of a computer science degree. Because of the resulting lack of knowledge about even the simplest debugging techniques, spreadsheet formulae and macros are riddled with bugs and million-dollar business decisions are sometimes based on calculation errors. It has been estimated that at least 40 percent of spreadsheets have bugs.
The introduction of the Web is causing a similar phenomenon in user interface design. My current estimate is that there will be about 10 billion Web pages on the Internet by the Year 2001. Intranets and extranets will probably hold at least 10 times that many pages. We already have two million pages on SunWeb (the intranet at Sun Microsystems).
Each Web page is a user interface design problem equivalent to that of a dialogue box: you must design a task flow that brings the most important items to users' attention and design alternative options for them to click on -- all the while keeping the meaning of these options clear for novice users. Considering that the world will design more than a 100 billion of these dialog-box equivalents in the next three or four years, extremely simple and inexpensive usability methods are crucial if we are to avoid a usability meltdown on the Web.
Amateur Designers
Most Web pages are designed by amateurs. Even at companies that believe in usable Web design, only the top few pages ever receive the attention of user interface professionals or the benefit of traditional user testing in the  laboratory  . The vast majority of pages are designed by marketing staff or others with little experience in interaction design or usability methods. Clearly, it is not an option to require that every Web page be designed by professionals or go through full-fledged traditional usability studies. Doing so would be equivalent to demanding that only people who can recite the collected works of Dijkstra in their sleep be allowed to build a spreadsheet. In the real world, such demands go nowhere: people want to develop their own spreadsheet analyses  now  , not wait months for the MIS pros to get around to helping them. Similarly, people want to build their own Web pages in their own departments without having to clear the results with the ""design police"" at corporate headquarters.
Inadequate use of usability engineering methods in software development projects have been estimated to cost the US economy about $30 billion per year in lost productivity (see Tom Landauer's excellent book  The Trouble with Computers  ). By my estimates,  bad intranet Web design will cost $50-100 billion per year in lost employee productivity  in 2001 ($50B is the conservative estimate; $100B is the median estimate; you don't want to hear the worst-case estimate!). Bad design on the open Internet will cost a few billion more, though much of this loss may not show up in gross national products, since it will happen during users' time away from the office.
A usability loss of $100 billion may sound like a lot, but considering that in 2001 there will probably be about 200 million people designing intranet pages, each designer's work will contribute only $500 of that usability loss -- not nearly enough to justify the costs of hiring professional designers or paying for advanced usability work.  Discount usability engineering   is our only hope. We must evangelize methods  simple  enough that departments can do their own usability work,  fast  enough that people will take the time, and  cheap  enough that it's still worth doing. The methods that can accomplish this are simplified user testing with one or two users per design and  heuristic evaluation  .
Teach Usability in Schools
User testing and heuristic evaluation should be taught as part of the standard elementary school curriculum The proper role of Internet technology in schools is not the completely naive ideal that politicians have proposed: kids sending e-mail to the world's best scientists asking for help with their homework. No one would stay a leading expert long if they spent their time answering a flood of messages: personally, I can't even keep up with unsolicited mail from PhD-level students. Much more productive is to have kids use the Internet to build their own hypertext information spaces as part of their course work. It is much more inspiring to write for others than for the teacher's red pencil. They should usability-test their pages with one or two other students and conduct critical usability inspections of different designs for each problem.
The only way we can hope to teach usability engineering from the third grade up is to teach discount usability methods. Advanced and sophisticated methods that require, say, an understanding of statistics are clearly impractical.
Some critics will no doubt say that it is unacceptable to teach kids less than perfect methodology or to test interface designs with only two users or to do any of the other things I recommend here. I agree that these methods are imperfect. But the only realistic alternative is to do nothing. Given the amount of usability work we'll need in the coming years, it is quite simply not possible to do it all with deluxe methodology.
Just Do It
The true choice is not between discount and deluxe usability engineering. If that were the choice, I would agree that the deluxe approach would bring better results. The true choice, however, is between doing something and doing nothing. Perfection is not an option. My choice is to do something!
"
114,1996-07-31,"
	They don't call it the  World  Wide Web for nothing. A single click can take you to a site on another continent and a business can attract customers from hundreds of countries without ever going to a Frankfurt trade show where they book you into a hotel two hours down the autobahn.

	The unprecedented international exposure afforded by the Web increases the designer's responsibility for ensuring international usability. International use is not a new phenomenon: most computer companies have half their sales overseas, and several books have been published with general advice for making user interfaces more international. Let me just mention the latest: Elisa del Galdo and I had our new book,   International User Interfaces, published by John Wiley & Sons last month.

	Most of the guidelines remain the same: don't use icons that give your users the finger (or the foot, or other gestures that are offensive in their culture), don't use visual puns (e.g., a picture of a dining table as the icon for a table of numbers), don't use baseball metaphors (except, obviously, at baseball sites), translate fully if you do translate, and so on. This column looks at some of the issues that are specific to the Web.

	A major problem is the fact that Web sites attract many international customers.  We should all have this problem,  you may say, but some companies are not interested in overseas business. Make it clear up front if you are only interested in serving a local market to avoid wasting both parties' time. Also, many companies have significantly different product offerings across countries, and it can be quite confusing for a customer to access, for example,  Mercedes-Benz' main site only to discover that some of the models are not for sale outside Germany. Always make it clear if different models, prices, or procedures apply in different countries.

	The Web and the Internet allow real-time interactions, with celebrity chat sessions and with Olympic or World Cup results posted as the events happen. In announcing any real time event, you cannot simply say that it will happen from 2:30-4:00. First, is it 2:30 in the morning or the afternoon, and, second, what does that translate to in my own time zone anyway? It may be obvious to you that nobody would put on an event at 2:30 in the morning, but if that happens to correspond to 11:30 AM in my country, I might not think so. Any times listed on a web page should always at a minimum make it clear whether they are given in the AM/PM system or the 24-hour system (and if AM/PM, then these suffixes should be given) and which time zone they refer to. Time zone abbreviations (e.g, EDT) are not universally understood, so supplement them with an indication of the difference to GMT. Many users don't understand GMT either, so optimal usability would involve translating the time into local times in a few major locations (e.g., ""the press conference starts 1:00 PM in New York (GMT -5), corresponding to 19:00 in Paris and 3:00 the next day in Tokyo"").

	International Usability Testing

	Because of the myriad of issues in international usability, I recommend doing  international usability testing with users from a few countries in different parts of the world. No guidelines yet published are sufficiently complete to guarantee perfect international usability, so an empirical reality check is always preferred. Luckily, the Web makes international usability testing relatively easy.

	It is possible to test Web designs internationally without ever leaving home. Since users can access the Web from everywhere, they can access your site without you having to go to their country to set up the test. One option is to place telephone calls to the users and ask them to think aloud as they navigate the site. Assuming that you can identify users in other countries who speak your language well enough for a telephone interview, this is a very easy way to conduct international testing. The two downsides are the need to get up  early  in the morning and the difficulty in following the user's navigation from a purely verbal description.

	A good alternative is to have staff from your local offices in various countries conduct the test themselves, even though they are not user interface specialists. We have done so at Sun with some success, and you can see the  instructions I gave to the local offices for further information about how this can be done.


Draft of ""user cafe"" for the Internet Access PlusPack subsite.
		Shown at 80 percent of full size. 








	The best choice, though, is to travel to the foreign country yourself. Of course, this is expensive, but again the nature of the Web comes to the rescue. It is possible to conduct informal tests during trips that are planned for other purposes since you can pull up a Web page any place you can get to a computer. For example, I was at a meeting in Sweden recently while the rest of the team continued working on a new subsite design back in California. Not only could I email back comments on the design as it progressed, but I also took the opportunity to run it by a few locals. The figure shows the draft design I tested. One of the results was that people didn't understand the difference between the Information button and the Documentation button. As shown by the example, international usability testing often reveals problems that could well exist for domestic users also. Other problems related to recognizing the espresso machine, though most people understood the general cafe concept which had been one of my main worries.

	Language Choice

	In many ways, the ideal international user interface is one that is available in the user's preferred language. Eventually, language choice will be handled by content negotiation between the user's client and your server so that the user will only need to specify a list of preferred languages once and for all as a client setting. At the moment, content negotiation is not sufficiently widely used to be a reliable solution, so many websites use manual options for language selection.

	To choose between a small number of languages, I recommend listing the name of each language as a word, using each language's own name for itself. For example: English – Français. Lists of more than 7 foreign words are hard to scan, so for lists of between 8 and 21 languages, I recommend using visual symbols to supplement the names. For lists of 22 languages or more, scanning is hopeless, and the only solution is a long alphabetical list (in which case non-Latin languages should be listed twice: once in Latin characters in the proper alphabetical order and once in the true character set at the end of the list). The best visual symbol for a language is probably a  flag. Icons playing on national stereotypes are possible and can be fun, but risk being offensive (not all Americans wear cowboy hats).

	If language choice is supported by a site, I recommend providing a link to the choice on every single page since users often go directly to pages from search services or bookmarks without passing through the home page. Some sites put up a language choice page before the user can reach the home page, but I recommend against this if it is possible to determine a default language that will be used by a very large proportion of the users (the  Louvre Museum in Paris is a good example: fair enough to start in French). Clicks and download time can be saved by going straight to a page for the main language as long as the home page has a very prominent (and internationally understood) entry for language change. Also, the pages for the various languages should have their own URLs so that users can bookmark the proper entry point and bypass language choice if they visit again.

	Multi-Lingual Search

	A special problem is the search of multi-lingual information spaces. If all of the information has been replicated in every language, then there is no need to search more than one language. In this case, the search interface should know of the user's preferred language and only display hits in that language.

	Unfortunately, it is often not possible to translate all documents, so many sites require searches of several languages if the user needs complete coverage of the available information. Currently, multi-lingual search requires the user to manually enter synonyms of the desired search terms in all the requested languages. This is obviously an unpleasant task, and users often forget to search for translated terms, even if they understand several languages. It would be better to have the computer automatically perform multi-lingual searches by understanding the meaning of the search terms in several languages. Doing so is easier than the general problem of natural language translation (for example, the term ""rock"" would not normally refer to music if used on a geology site) and there are some research systems that have performed reasonably well on multi-lingual searches.

	Printing

	For rich or large hyperspaces, I recommend providing a special version that can be downloaded and  printed as a single document. Any file that is intended for printing must be able to accommodate the two most common page formats: A4 and 8.5x11 (U.S. Letter). To do so, the width of the page must fit on an A4 sheet and the height of the page must fit on an 8.5x11 sheet, since A4 is the narrowest format and 8.5x11 is the shortest format. It is recommended to leave a margin of at least half an inch (13 mm) for all four sides of the page to ensure that it will print on all printers and to facilitate photocopying. With half-inch margins, the printable area is 7 1/4 inches (18.5 cm) wide by 10 inches (25.4 cm) tall; with one-inch margins (preferred), the printable area would be 6 1/4 inches (15.9 cm) by 9 inches (22.9 cm).

	Design Guidelines

	See also my 61 design guidelines for supporting international users and the  230 tips and tricks for a better usability test (of which 14 tips relate specifically to international testing — but the remaining tips will help improve such tests as well).

	See Also: Use of (^_^) instead of :-) as a  Japanese emoticon.

	 
"
115,1996-01-01,"In Sweden, the Automatic Teller Machines have very large buttons. I hadn't noticed this particular design element on previous visits, which have usually been in warmer months. In 1996 I was in Stockholm in February and immediately realized why the ATM buttons are so big: you can press them wearing thick gloves.
Clearly, the ATM vendor had manufactured a localized version of the product with Swedish users in mind. Unfortunately, although products are commonly used in countries other than the one they were designed for, designers often forget to consider different usage circumstances.  International use of the Web is particularly common since users can access pages from all over the world with a single click.
Levels of Globalization Concerns
International user-interface concerns come at three levels.

A computer must be capable of displaying the user's native language, character set, and notations (such as currency symbols).
The user interface and documentation must be translated into the user's native language in a way that is understandable and usable.
A system must match the user's cultural characteristics. This goes beyond simply avoiding offensive icons; it must accommodate the way business is conducted and the way people communicate in various countries.

Most major computer vendors consider the first level fairly routine, though it has certainly not been fully solved. Most systems either support sets beyond ASCII already or have aggressive plans to do so. Also, most vendors have special manuals that show developers how to code for an international market and the use of, for example, 16-bit character sets.

The figure shows a problem in the culture category from a game called  Give the Dog a Bone  found on the otherwise excellent  JumpStart Toddlers  CD-ROM: when the dog asks for its ball, most European kids would probably point to the biscuit rather than the football since they have never experienced balls that are not round. As computers transmute into communication tools, the need to match the users' local language and culture becomes paramount.






Unfortunately, the second- and third-level concerns are much harder to address than the character set issues. There is not yet a standardized way of conforming to language and culture. Indeed, very few people have yet given much thought to systematic ways of addressing these deeper issues in internationalization.
Testing Abroad
Even though some results and guidelines are available, when confronted with a product intended for use abroad your best bet is to conduct international usability testing. As with all user testing, the two fundamentals of international user testing are to involve real users and have them do real tasks without your help. You can usually recruit users through your local branch office in the country in question, but it is important to emphasize that you need users who sit at the keyboard on a day-to-day basis and not necessarily the branch's immediate customer contacts. If you are not explicit about these needs, you may find that unrepresentative test participants have been recruited when you show up for the test. By then it will be too late.
There are four main ways to conduct international user testing:

go to the foreign country yourself
run the test remotely
hire a local usability consultant to run the test for you
have staff from your local branch office run the test, even though they are not trained in usability

A fifth possibility is open only to the largest companies: build additional usability groups in your major markets. This last option may be the best, but it is usually beyond the available budget. Also, even if you do have local usability groups in major foreign markets, you may still want to do additional testing in some of the smaller markets -- in which case, you face the same problem over again.
In many ways, the ideal approach is to go to the country and conduct the test yourself. Visiting local customers in various countries will give you a much stronger impression than simply reading even the most well-written report. There are always many small but important details you will observe if you go there yourself.
Because observing the customers in their own environment is beneficial, I recommend trying to set up the test at the customer's premises, but this is not always possible. You often need special equipment, hard-to-install software, and access to data that is only available on your internal corporate network (which only works inside your branch office). If you do visit customer locations, try to get permission to bring a still camera and take pictures of their installation and working conditions. Pasting several such photos on a wall in your development lab will often be a good way to remind everybody on the project team about the different needs in different countries.
If you do travel yourself, you should account for jet lag. Conducting users tests is a very intense experience: you have to pay attention to the user, the user interface, the test tasks, your notes, any additional observers, and any video equipment you may be using -- all at the same time. Also, the test may be conducted in a foreign language, which can make it even harder to concentrate. I recommend that you spend the first two days after your arrival visiting your branch office and checking equipment and software to make sure that the tests will run smoothly.
You can eliminate many travel costs by using remote user testing. To do this, you have the user run prototype software (or a screen-by-screen mock-up) on his or her own computer while you observe over the Internet (or some other network connection). There are many utilities that let one user see what is on another user's screen; these utilities are perfect for remote user testing. You can communicate with the user over an audio link via telephone or the Internet. The downsides to remote testing are that you have little visual feedback as to what the user is doing and the user must install and operate possibly quite unfamiliar utilities. You also have to be in your office at horrible hours to accommodate the time difference.
Overcoming the Language Gap
Whether you travel yourself or conduct the test remotely, you will normally have a language problem. One solution is to recruit users who speak your language, even if they don't speak it fluently. This solution is not perfect, but pragmatically, it is often the easiest to implement. The key issue is to make sure that you don't get unrepresentative users who, for example, have spent years at college in your country and thus have been acclimated to the possible linguistic and cultural peculiarities you hope to smoke out during the test. Another option is to conduct the test in the local language. This may work if you speak the language reasonably well, but you will often have to rely on an interpreter. Although you can usually understand what is happening on the screen (since you know the product well), the user's comments will normally lose a lot in translation. You should also meet interpreters beforehand and remind them that they should not help users during the test.
The final option is to either have a local usability consultant or staff from your branch office conduct the tests. You will definitely get the highest quality report from a usability professional, but there are benefits to involving your local staff: not only is it cheaper, but it brings them into the product-development process and they will likely learn a great deal from the test itself. As always, additional information is gained from actually carrying out a usability study as opposed to simply reading the report, and this added information might as well benefit people within your company.
How Many Countries?
You must determine how many foreign countries to cover in international usability engineering. The optimal solution is to cover all countries in which you have non-negligible sales (or where you hope to expand). However, doing so is normally unrealistic unless you are a rather small company that only exports to one or two countries. The typical solution is to evaluate international usability in a few countries with at least one country in each of the main areas of the world. If you only have the resources to cover a single foreign country, then you should do it. There is much to be gained in taking that first step, and the knowledge extends well-beyond the benefits you gain in the test country.
Start Small
A key point is:  Don't despair!  Don't over plan. Don't give up because you cannot implement the ideal international usability study the first time. You may have to start with a single country. The important thing is to do it. Typically, management is so encouaged by the results of even a small-scale pilot project that more resources are freed up for future projects.
(Obviously, you can hire us to do international testing for you, including having us manage testing through our foreign network of subcontractors. However, a cheaper option is have us teach your team — and your international teams — to do their own testing.)"
116,1995-05-25,"Icon usability testing was conducted with two different methods during the design process for Sun Microsystem's early website:

Icon intuitiveness  test in which an icon was shown to a small number of users (typically five) without its label. The users were asked to state their best guess as to what the icon was supposed to represent. This test assessed the degree to which the graphic chosen for the icon represented the intended concept.
Standard  usability test  in which the icons were shown to users as part of the full user interface and where the users were asked to ""think aloud"" as they used the system to perform set tasks. This test accessed the degree to which the icon would work well in context of the interface as a whole (where it would typically be displayed with a label).

Our initial studies were done with simple sketched icons in black-and-white ink on paper. For each icon, several alternative concepts were tested and we chose the most promising one for further development into a color icon rendered as an imagemap on the computer. The color icons were further developed through several rounds of iterative design.
Here are examples of the iterations of three of our icons.

 



This icon represents the concept of  Technology and Developers. The first two icons with a chip and a CD-ROM were somewhat too hard to understand and seemed to represent the finished products more than the development phase. The construction worker was good at representing development but was rejected due to the strongly negative connotations this symbol has on the WWW where it is often used to represent pages that are ""under construction"" (something that was much hated by our test users).
The second row shows our next round of attempts where a person was used to represent the developers. As you can see, we couldn't restrain ourselves from having a propellerhead despite the fact that we had rejected this aspect from the  first iteration of the home page. The first developer icon was liked the most, though some commented that it represented hardware and not software development. Also, some people liked that the second icon represented ""harnessing the power"". So for our first color icon, we chose mainly the first developer icon but with the wrench replaced by the lightning bolt.
The first color icon met with poor results in usability testing, though. Comments included:

thunder and lightning
electric - looks painful
person being killed by technology
dance machine
why do Sun developers look bug-eyed?

We clearly had to get rid of the human in this icon, and for the second color icon, we retained only the lightning and the cogwheels. Users still complained that the thunderbolt looked too much like lightning striking the machinery and destroying it. We finally decided to get rid of the references to electricity and the final design represented the concept of development by a CD-ROM. Throughout the testing, the cogwheels worked fine as a way of communicating engineering and technology even though computers obviously don't have gears. Sometimes elements of obsolete technology can work well as a stereotype to communicate concepts in an icon.




This icon represents the concept of  Products and Solutions. I personally liked the machine coming out of a box the best, but it had to be rejected because it only represented hardware and not software (nor, of course, the idea of solving problems for the customers and not just selling them products).
Some users liked the man holding up a computer because ""it says strength and power - I can do it for you."" However, the icon was very busy with the large number of computers behind the person (of course, we hope to sell that much, but even so, icons should be simple!). Nobody liked the guru and most of the people we asked liked the light bulb. One problem that was mentioned by a few users was that the man holding up the computer was indeed a man and not a woman, We briefly considered that we could use some mix of men and women if we had an icon family with people in most of the icons, but in the end the desire for simplicity resulted in icons with  no  humans in them.
As you can see, all the versions of the color icon were essentially the same since the computer with a light bulb going on was easily recognized by all users as representing some combination of computers and bright ideas (the Sun solutions!). The light bulb did double duty as representing software (something on the screen) and solutions. The only changes to this icon were those necessitated by the changes in home page layout: first the icons were made slightly smaller and then they were made to look more like (slightly three-dimensional) buttons.


This icon represents the concept of  Sun on the Net. We initially had two different ideas: a talking server (telling you about itself) and world-wide communication. Most users thought that the world icons represented footballs (after a few failed attempts at color icons we finally recognized the simple truth that a world had to be round!).
We rejected the overly anthropomorphic servers and chose a metaphoric style for our first color icon: a literal server serving information on a silver platter. Unfortunately, the users thought it was a pilgrim's hat. Also, of course, this literal interpretation of the word ""server"" would have failed international usability testing.
We then combined the idea of a world with the idea of a speech bubble, but users thought it was a punctured balloon. The next two icons were attempts at a more literal globe, but users interpreted them as an astronaut in a space suit, an olive, and ""a golfer trying to hack his way out of the rough.""
The final icon is simplified to the utmost — and it works.
(For more information on improving individual web page design elements, see our full-day training course Web Page UX Design: Optimizing Pages to Deliver on Business Goals.)"
117,1995-05-25,"
Usability lab setup for paper prototyping, card sorting, and traditional usability testing. 
Paper Prototyping
We used several rounds of usability testing to improve the user interface for Sun's new WWW pages. The initial tests focused on the home page and were done using paper prototyping. We also conducted competitive usability tests where users were observed while browsing WWW pages from other companies.

Usability testing with paper prototypes involves printing out screen designs and having users perform tasks using the printed screens.  
For the Initial usability studies done using paper prototyping, we printed out early homepage designs on a color printer. The printouts were magnified to compensate for the poorer quality of the color pixels in the printout and to make it easier for the observers to see what the user was pointing at.
The test was conducted by showing the page to users and asking them to first comment on their general impression of the page and then to point to any element on the page that they thought they could click on and tell us what they expected would happen. This simple method provided us with early feedback indicating the importance of a prominent placement of the month name (since we wanted users to know that the page would change monthly) and the need to make the ""What's Happening"" bar look very clickable.
The photo shows  version B of the homepage ""on the operating table,"" but we ran paper prototype tests of many more versions. It's the beauty of this method that it allows for very rapid iteration and tests of many alternative user interface designs.
A nice trick for paper prototyping (which Meghan Ede suggested to us) is to tape up an area of the desk with masking tape. Marking up an area usually ensures that the user keeps the printed page within the area without a need for the experimenter to try to stop the user from moving the printout around too much. We preferred having the printed page stay within a set area to facilitate video taping as well as observation by other team members in the control room.
Card Sorting to Discover the Users' Model of the Information Space
The category grouping used for the main icons on the home page was derived based on a card sorting study: several users were given a series of index cards with various concepts from the server written on them. The users were then asked to sort cards into piles, placing cards that seemed similar to them into the same pile. The users were also allowed to group piles to indicate looser levels of similarity, and we finally asked the users to name their piles. These names provided us with additional insights into the users' mental model of the information space and served as inspiration for the names we finally chose. Concepts that were placed in the same pile by many users were deemed sufficiently similar that we should place them in the same category in our new design. 
See my article about how we  designed Sun's intranet, SunWeb, in 1994 for another example of card sorting and a photo of a variant of the method, where users are asked to sort cards onto existing categories. (Often called  closed card sorting  to differentiate it from the more common open sorting where users build their own categories without any constraints.)  

For card sorting, different topics are written on cards to be grouped by participants.  
Traditional Usability Test of Running System
Of course, we also used traditional user testing, where users were asked to use a running prototype of the new pages. This is how most usability tests are set up: the user interface that is being tested is running on a computer in the usability lab and the user sits down at the computer and starts working. Normally, we give users a series of set tasks (e.g., find information about the Spring distributed operating system project) and we certainly did so for the web studies. Sometimes, rather primitive HTML mock-ups were used, with many dangling links or links that pointed to pages on the old server that used an obsolete user interface. Even so, we were able to learn a lot about how people use WWW pages. For this particular type of user interfaces we also found it important to let the user spend some time exploring the information space freely: we wanted to see what interface elements the users naturally found interesting without any prompting. We did keep some set tasks since we also wanted to know how usable the design was for people who needed to find specific information.
Most tests were conducted in the usability lab in Mountain View, CA as shown in the photo, though we also conducted three tests in Sun offices in Europe and Asia to assess international usability. The usability lab has a large one-way mirror that allows team members in the control room to observe the test and discuss its outcome without interrupting the test user. Some times the experimenter sits in the control room but my preference is to sit with the user in the lab itself. We have two cameras in the ceiling (not shown) and one camera on a tripod. These cameras are used to record the user's reactions to the interface as well as to film the screen. We do have a scan converter that is used to record a complete screen image, but we often find that there is some extra value from having a camera pointed at the screen because it can record cases where the user points to something on the screen.
It is hard to see on this photo, but to the right of the computer is the lavaliere microphone which we ask users to wear during the test. We have found that the only way to get good sound is to use these clip-on models since table mikes capture too much noise to make it clear for people in the observation room what the user is saying.
A small trick: we have a small table clock to the left of the computer. This clock makes it easy for the experimenter to time the test and to manage the available session time much more unobtrusively than when having to look at a wristwatch.

Traditional usability testing consists of users interacting with the live Web site. 
Usability Test of the Old Web Site
In addition to studying our various new design ideas, we also conducted a usability study of Sun's old WWW design. Of course, we were well aware that it contained many usability problems (e.g., inconsistent headerbars and several very strange imagemaps) but we still wanted to learn what worked well and what worked less well in the old design. One interesting finding was that the top button-bar did not look enough as buttons: the design did not have a clickability affordance but was seen as mere decoration by most users. We redesigned this specific aspect of the old design immediately without waiting for the full redesign to come online. The following figure shows before and after versions of the top row of buttons (""What's New"", etc.) in the old design:


Changing the buttons to make them look more clickable as illustrated above resulted in  416% increased use  over a two-month period (January-March 1995). Considering that the use of the server in general increased by ""only"" 48% in the same period, there can be no doubt that usability engineering worked and resulted in a significantly improved button design.
"
118,1994-12-18,"In this study, users were observed as they browsed the Web sites of Hewlett-Packard, IBM, Microsoft, Sun Microsystems, and Time Warner. The report has only been very lightly edited and thus represents my thinking about Web usability in 1994. In fact, the report was originally written for distribution to the rest of the Web team on paper since we were not heavy intranet users in 1994, despite having designed SunWeb a few months before this study.
The report is of some historical interest, both because it includes screen captures of several famous early websites and because it is one of the first formal usability studies of the Web. In fact, it is remarkable how well the findings and conclusions in this report hold up in the light of the current state of the Web. People often talk about how the Web changes on ""Internet time"", but usability issues seem to change much more slowly since they stem from human capabilities and interests.
Method
Three external participants were tested: an MIS director, a programmer, and a systems administrator. All were employed with technically oriented companies in the Silicon Valley area, all had extensive Unix experience, and all were highly technically competent. One participant was a current Sun user, one used another version of Unix on the Intel platform, and one was currently a Windows user but had been a Sun user for seven years. All participants had used the WWW extensively before the test. Thus, the participants were very advanced users and one would expect that less sophisticated users would have many more problems in using the WWW than described here. In other words, this test investigated a best-case situation.
Each participant was tested for 60-90 minutes. During the test, the participants visited 2-4 WWW sites from a list of sites prepared by the experimenter. For each site, the participants were first asked to give their initial impression of the home page, after which they were allowed to explore the site freely. After about ten minutes of exploratory browsing, the participants were given a directed task that asked them to find some specific information that was available on the site. The sites and the directed tasks were:



Site
Directed Task


Hewlett-Packard
You want to see the photo of the garage in which the company was founded


IBM
You want to know how many hard drives you can put into a PC Server 50


Microsoft
You are interested in knowing how Windows'95 will support access to the Internet


Sun Microsystems
You are interested in knowing more about the Spring Distributed Operating System Project (a project trying out alternatives to Unix)


Time-Warner
You want to read the profile of the co-creator of Mosaic. His first name is Marc and his last name starts with an A, but you can't remember the exact spelling



The users were tested using NCSA Mosaic version 2.4 for X Windows running on a SPARC 10 workstation with a megapixel 256-color display. The workstation had an indirect connection to the Internet through a firewall with transfer rates of approximately 10 kilobytes per second. This is about five times faster than a modem and about the same as ISDN and thus represents best-case performance for home users, though on the slow side of what one would experience from an office computer that was directly connected to the Internet. When using the Sun WWW pages, the users accessed an internal mirror server to which the workstation had a direct network connection, and transfer rates were much faster for the Sun pages (around 65 kB/s) than for the pages retrieved from the Internet through the firewall. The users commented several times that access to the Sun pages was faster than their normal experience and that access to the non-Sun pages seemed a little slow. Thus, we can assume that the network speeds used in this study were approximately (though not exactly) the same as those normally experienced by the users.
The Sites
We obviously wanted to test Sun's own existing web pages since the previous release of a product is the first prototype of its own replacement. Four additional sites were selected for the study based on discussions in the www.sun.com redesign project. Project participants were asked to recommend good web sites that had interesting design with an emphasis on sites with a magazine or news-oriented approach to WWW information. We believed that frequent information updates are an emerging trend and would be important for attracting repeat traffic to our WWW site. IBM was selected because it used a magazine format for its home page with a monthly cover story. Time Warner was selected because of its highly news-oriented design. HP was chosen because of its clean and highly structured design, and Microsoft was chosen due to its status in the computer industry.


Many other sites were considered and were rejected either because they were similar to the ones chosen or because we were sure that we did not want to emulate their designs. For example, the home page for HotWired used icons and labels that were essentially impossible to understand. Such a design may be appropriate if ""hipness"" is the essential quality to be communicated, but I felt that our design should involve qualities like ease of use and powerful computers. One possibility would be to follow the example set by eWorld and design the corporate Web page to mirror the graphical user interface of our desktop software, but we were not aware of the eWorld Web site when the comparative usability test was planned, so this option was not tested, except for the observation that users liked a similar design feature at Microsoft's site.

Findings
The study was performed using the  discount usability engineering approach where a small number of users are tested. Due to this method, statistical data analysis is not appropriate and the findings reported here are qualitative in nature.
Page Complexity

Users were often overwhelmed by the amount of information on the pages. All users found the Time Warner page too complex. One user commented that the Sun home page ""contains to much stuff that I will just ignore anything that is not clear."" As an example of something that was not clear and would be ignored, this user pointed to the SunSITE entry.

On the Sun home page, all users clicked on the Santa Claus icon first due to its prominent position at the top left corner of the page. Also, several users expressed a liking for the untraditional icon design with the button graphically protruding from the image. All users overlooked the row of flat buttons on top of the Santa button. Several users were actively looking for a ""new at Sun"" button and a search feature but did not find them. The design used for the Sun home page makes the button bar look too much like captions or headers for the top row of much more prominent icons.

All users complained when they retrieved pages with a screen or more of unstructured text. They plainly did not want to read much, and at best scanned the text for important or relevant paragraphs. The users liked information that was presented in lists that were easily scannable, especially when icons were used to indicate the different parts of the list (see the figure from Hewlett-Packard's site). Also, users liked the use of a ""New"" indicator to highlight new information in a list and the use of horizontal rules (<HR>) to partition a page. Long pages (more than a screen) were only deemed acceptable when users could quickly decide to ignore most of them and focus on the relevant parts (e.g., a list of Microsoft networking products organized by operating system).
In one case, a user retrieved a page that was very similar to one he had seen before and wondered whether the two pages were in fact identical. This incident indicates a potential problem with representing the same information multiple times with slight variations.
Search
Observations regarding search in this study confirmed the results from more quantitative studies conducted as part of the AnswerBook redesign project: users typed very short search strings (normally one or two words) and sometimes overlooked the item they were trying to find even when it was included on a list of search results. The main conclusion from these observations is that one cannot rely on search as a mechanism for leading users to the information they need.
Users also sometimes scoped their search incorrectly: in one case, a user was looking for information on the Sun server and came across the Catalyst project page, which included a search form for finding information in these pages. The user happily used the Catalyst search in the belief that it searched the entire Sun server and not just the Catalyst database. The conclusion from this observation is not to offer search features that are limited to a subset of the information on a sever. If scoped search is needed anyway for some reason, it should be made very clear to the users what information is being searched and what it not being searched and there should be a direct hyperlink to the global search page. Also, users sometimes identified the need for search after they had traversed several links and were deep in the hierarchy. They expressed a desire to be able to start a search from their current location and not have to navigate back to the home page first.
Navigation and Metaphors
During this study, users were mostly very good at knowing what site they were currently visiting, so they did not seem to get lost in hyperspace at the macro level of knowing what company's information they were browsing. One user complained about the inclusion of information about third party products on Sun's server. He felt that he was browsing Sun's web to get information about Sun's products and that he would access those other companies' sites if he wanted information about them. A possible solution to this problem would be to make third-party pages more visibly different from Sun's own pages or to eliminate the information itself from Sun's server and only provide links to the other companies servers.
On the Time Warner page, one user was surprised when the ""Virtual Garden"" button pointed to information for home gardeners. The user had expected it to be a list of links to other interesting Web sites to visit. Users may have become so accustomed to the heavy use of metaphors in many WWW designs (and systems like Bob and Magic Link) that they expect design elements to be metaphorical rather than literal. The lesson for user interface design is to consider not just the first-level interpretation of proposed design elements but also whether they could be misconstrued as inappropriate metaphors.
In a few cases, users retrieved information that caused a PostScript viewer to be launched without warning. The users complained that the WWW page had not warned them that the link would download a PostScript document rather than jumping to another hypertext screen and they also did not want to read the PostScript document (as mentioned above, the users disliked long texts). Even though the users disliked links to PostScript files that were displayed in a separate viewer, they did like the possibility of using FTP to retrieve additional information, executables, patches, or other non-hypertext files.
Overview Diagrams

Even though users understood what sites they were visiting, they often got lost within sites, and they several times expressed a wish for overview diagrams. Hewlett-Packard's design was particularly good at informing users about their current location in the information space.

Users consistently praised screens that provided overviews of large information spaces. For example, they liked the HP navigation screens, Microsoft's home page with its extensive listing of server content, and the overview page for Windows'95 (see figure at the beginning of this report). Some users did complain that the text in the Microsoft home page overview was too small.
Several users who were getting lost coped by analyzing the file names with full directory paths given for HTML files to understand the server hierarchy and their current location. Unix experts may be capable of doing so, but general users would need a simpler mechanism for understanding the information space.
Thumbnails

Users several times complained about small thumbnail pictures where too much photographic detail was shown in too little space to be clearly visible. The figure shows a page with several overly complex thumbnails from IBM. Another example from the IBM server, below, shows how a small picture can be perfectly acceptable provided it shows a fairly simple object without much detail. In general, users appreciated when the system was giving them advance notice before they decided to retrieve a large file (e.g., a big image).

Broadly speaking, it seemed that users disliked 1x1 inch thumbnails but liked 1x2 or 2x2 inch thumbnails. To communicate graphical information in a one-square-inch space one should thus use icons and not thumbnails. The general conclusion regarding thumbnails, though, is that they should communicate some information to the user and have a clean and uncluttered appearance. Thus, the exact size of a thumbnail should be a function of the complexity of the original image. If it is impossible to represent an image clearly with a thumbnail then it will be better to use a textual description.
Incomplete Sites
Users distinctly disliked seeing ""under construction"" markers. As one user put it, ""either the information is there or it is not; don't waste my time with information you are not giving me."" Users were particularly aggravated when they had linked to a page only to find that it was under construction. One user said ""At least give me something for going to the page; don't put it out there if it is not working."" Users were less upset when currently inactive hypertext markers had construction signs that could be seen before activating the link.
Users had little patience for server error messages. One user said that he might try retrieving the page once more, knowing that servers sometimes got overloaded, but if he got a second error message from the same server then he would never visit it again: he would assume that it was too unreliable and/or too poorly maintained to be worth his time.

Users were also very disappointed when they came across evidence that a server was not being kept up to date. For example, several sites listed talks or conferences that had already taken place as if they were still upcoming. In related comments, users praised the way some information on the Sun and Microsoft servers was labeled with dates to indicate how current it was. Users also liked the use of the month name to indicate the currency of the information on IBM's home page.
The Human Touch

All users liked Microsoft's picture of their Webmaster in front of the server. They felt that it was nice to know that they were communicating with a service supported by an actual person and not a faceless entity. The users also liked being able to read the technical specifications for Microsoft's server, again from the perspective that the information was not just coming in over the wire, but was coming from somewhere. Of course, server specs may be less interesting to less technically oriented users than the ones tested in this study.
Two-Way Communication
The users liked the ability to request further information about products by filling in a form. One user mentioned that he would often use a Web site to get a general idea about a company's products but that he would then fill in a form asking to get contacted with more information about the specific products he was interested in buying. He was less motivated for having to search the Web site himself to find this detailed information. This user was particularly interested in finding lists of contact telephone numbers and email addresses for the various products.
The users also liked the forms that were provided at several sites to allow them to give feedback to the Webmaster. This observation may also be related to the ""human touch"" phenomenon mentioned above since having the form makes the users feel more in contact with the actual people who maintain the server as opposed to being just consumers of a flow of information.
Users did not mind online surveys and questionnaires as long as they were at most one page long. Users frequently complained when they came across pages with questionnaire forms that did not fit on the screen. Users expressed a wish to have as much information as possible filled in for them. For example, a user who had already filled in his name and address on one questionnaire wanted the computer to automatically insert that information in another questionnaire he retrieved from the same server a few minutes later. One user wanted to be able to enter his email address to subscribe to notices about updates for a certain page that he found particularly interesting.
Mirror Servers
One problem related to mirror servers was encountered during the test: one user had initially accessed Sun's WWW site through the internal mirror. From there, he had linked to an outside site which happened to have a link back to Sun. The user followed this link, but since the outside link pointed to the outside version of Sun's Web site (the ""original"" copy of the site), all further cross-reference links to other Sun pages now pointed to the outside copies. This change in addressing caused a usability problem because the Web viewer did not display appropriate breadcrumbs on the link anchors. Normally, Mosaic shows anchors for links that the user has already followed in a different color, but since even those links now pointed to new URLs, they were displayed in the color reserved for unexplored links. This confused the user who remembered having followed some of the links earlier in the session.
This usability problem cannot be solved in the design of any individual server but must await a change to URNs or some other addressing scheme that takes mirror servers into account.
Parallel Sessions
All users opened two Mosaic windows and kept separate sessions going in parallel. Most of the time, these two sessions visited the same Web site, but explored different parts of its information space. The users would typically start a retrieval operation in one window and, while waiting, turn their attention to the other to study the information it contained.
Two conclusions can be drawn from the users' extensive user of parallel sessions: WWW users are very impatient and do not like waiting for information to be retrieved over the Internet. As far as possible, one should avoid having them wait (since their attention will wander).
One cannot assume a one-to-one correspondence between the stream of WWW requests received from a user and that user's hypertext browsing, since two or more browsing sessions may be interleaved.
Integration with Rest of World
One user who had read a page about the speed of PhotoShop on Sun computers, wanted to get a demo of PhotoShop from Sun's WWW server (or have a link to Adobe's server with a demo). Another user mentioned that he normally requested demo CD-ROMs for products before buying them.
One user had read in the newspaper that Sun had information about the Rolling Stones online and searched for that information during the study. Knowing about certain information from paper-based sources is one way that users can get prompted to look for it, and the URL should be given in those printed sources as far as possible. Some users mentioned that they would not want to spend time during the day to read extensive information but that they might print it out or email it to themselves to read later. With current technology, most printouts will be restricted to grayscale graphics and much email will be restricted to ASCII text, so the information should be designed to be useful in those formats.
Color Graphics

Users sometimes had problems with flashing colormaps, leading to a recommendation to minimize the number of colors used in bitmaps. Also, several users expressed an appreciation for graphics with a small number of colors (e.g., the HP home page) because they looked clean and felt like they did not waste a lot of bandwidth (and user time) when they were transmitted. It is possible that the move to screens with larger color spaces and the use of faster networks will change this preference for less-colorful images, but for now, it seemed prevalent.
Conclusions
Users had low tolerance for anything that did not work, was too complicated, or that they did not like. They often made comments like ""if this was not a test I would be out of here"" or stated that they would not want to visit a site again after a quite small number of problems. With non-WWW user interfaces, the technically oriented users in this study would normally persist for some time in trying to figure out how to use the system, but with the WWW, there are so many sites out there that users have zero patience. Thus, the demands for good usability are probably higher for WWW user interfaces than for normal user interfaces, even though the designers' options are fewer. ""Under construction"" signs should be avoided and the server should always provide a response within a few seconds. If the requested information cannot be provided, a meaningful error message should be given instead.
I found that users wanted search and that global search mechanisms should be globally available. Even so, users were poor at specifying search strings and they often overlooked relevant hits. Thus, we cannot rely on search as the main navigation feature. Navigational structure and overviews are necessary to avoid user confusion and should be provided both in the large (server structure and location) and in the small (structure for the individual pages with iconic markers for the various types of information). Users liked the feeling of being part of a two-way communication with a site staffed by real humans and not just the recipients of a stream of bytes coming in over the net. Care should be taken to provide a ""high-touch"" feeling in addition to the ""high-tech"" image of a WWW server."
119,1994-01-31,"Orignially published as: Nielsen, J. (1994). Usability laboratories. Behaviour & Information Technology 13, 1&2, 3–8.
Comment added December 1996:  This article was written in 1994 to summarize a special issue on usability laboratories I edited for the journal Behaviour & Information Technology. The special issue itself is well worth reading if you can get hold of it (many large technical libraries subscribe to the journal and should have the issue). The special issue on usability labs was published as a double issue: Behaviour & Information Technology, vol.  13, nos. 1-2, January-April 1994.
Usability is playing a steadily more important role in software development. This can be seen in many ways, including the growing budgets for usability engineering. In 1971 Shackel estimated that a reasonable share for usability budgets for non-military systems was about 3% (Shackel 1971). Later, in 1989, a study by Wasserman (1989) of several corporations found that ""many leading companies"" allocated about 4-6% of their research and development staff to interface design and usability work. Finally, in January 1993, I surveyed 31 development projects and found that the median share of their budgets allocated to usability engineering was 6% (Nielsen 1993). Thus, usability budgets have been steadily growing. Other indications of the added emphasis on usability is the increasing number of personal computer trade press magazines that include usability measures in their reviews and the overwhelming response to the call for papers for this special issue of  Behaviour & Information Technology  on usability laboratories
Assuming that a company has decided to improve the usability of its products, what should it do? Even though this is a special issue on usability laboratories, I am not sure that the answer is to build a usability lab right away. Even though usability laboratories are great resources for usability engineering groups, it is possible to get started with simpler usability methods that can be used immediately on current projects without having to wait for the lab to be built. See Nielsen (1993) for an overview of these methods, which are often referred to as ""discount usability engineering."" Almost always, the result of applying simple usability methods to current projects is a blinding insight that usability methods improve products substantially, making it hard to believe how anybody could ever develop user interfaces without usability engineering. Unfortunately, many people still do just that, and we need cheap and simple methods to enable them to overcome the initial hurdle of starting to use usability engineering.
Once a company has recognized the benefits of the systematic use of usability methods to improve its products, management often decides to make usability a permanent part of the development process and to establish resources to facilitate the use of usability methods by the various project teams. Two such typical usability resources are the usability group and the usability laboratory.
The staffing, management, and organizational placement of usability groups are important issues that must unfortunately be left unresolved here since virtually no research is available to resolve them. Suffice it to say that usability laboratories as a physical resource can be used by the usability groups almost no matter how they are organized. For example, one of the main schisms in the organizational placement of usability specialists is whether to centralize them in a single human factors department or to distribute them as specialized team members of the individual development projects. Some of the arguments in favor of centralized usability departments are that they are better at attracting talented usability staff because of their higher visibility; that they can nurture the special skills of usability specialists by providing an environment focused on usability issues where new techniques are discussed and developed; that they provide a clear management chain (with the ensuing career paths) for usability staff; that they can maintain corporate interface standards and serve as ""interface police"" to ensure consistency; and that they can take an objective view of the user interfaces they are evaluating since the interfaces come from outside departments. Some of the arguments in favor of distributed usability staff is that it is more satisfying for usability specialists to work for an extended time on a single product than to consult briefly on multiple products; that usability specialists on a product team are more likely to contribute to the design efforts throughout the lifecycle rather than just clean up the GUI; that some domains are so complicated that one needs to ""live"" with the product team for an extended period of time to be able to contribute; that usability specialists will only be taken seriously by developers if both groups are part of the same team; and that the communication channels will be shorter (leading to greater productivity) the less organizational distance there is between the produ cers and the consumers of usability knowledge. In spite of the clear distinction between the two types of organizations and the great need to know more about their relative advantages and disadvantages, no research results are currently available to assess what circumstances should lead one to prefer one organization of usability groups over the other.
Interestingly, even though a centralized usability group is the prime candidate to manage a usability lab, the existence of a corporate usability lab is not necessarily an argument in favor of a centralized usability department. It is possible to have a lab supported by a small number of dedicated support staff even while it is being used by usability specialists from a large number of distributed groups. As noted by Dayton, Tudor, and Root in their paper on Bellcore's user-centered-design support center in this issue, a shared usability lab can even serve as a gathering point to provide some of the cross-fertilization and educational benefits to distributed usability specialists that others get from a centralized department.
Usability laboratories are typically used for user testing as discussed in most papers and summarized in the next paper by Salzman and Rivers, ""Smoke and mirrors: Setting the stage for a successful usability test."" There is no doubt that user testing is the main justification for most usability laboratories and that user testing is one of the foundations of usability engineering. Once a usability lab is in place, however, it becomes a convenient resource for many other kinds of usability activities such as focus groups or task analysis. Palmiter, Lynch, Lewis, and Stempski discuss several such non-test forms of data collection in their paper on ""Breaking away from the conventional usability lab,"" and Zirkler and Ballman describe ways of combining focus groups and traditional testing in their paper on ""Usability testing in a competitive market."" Usability laboratories are sometimes also used to record design sessions, though this is mostly done as part of research projects and not as part of practical development projects. One exception is the use of participatory design sessions using methods like PICTIVE (Muller, Wildman, and White 1993) where several users and designers sketch out interface designs using bits of colored paper that is moved around on a desk. With this design technique, much of the design information is never written down, so capturing the dynamics of the design session on video can provide a valuable record for later reference.
Usability laboratories can also be used for certain variants of heuristic evaluation (Nielsen 1994). Heuristic evaluation is based on having usability specialists or other evaluators inspect an interface to find usability problems that violate a list of established usability principles (the ""heuristics"") and does not normally require a special laboratory since the evaluators can work anywhere and are normally supposed to do so individually. Sometimes, however, one wants to have an observer present to log the usability problems discovered by the evaluators so that they do not need to spend time and effort on writing up a report. It may be valuable for this observer to have access to a video record of the evaluation session, and it may also sometimes be advantageous to have developers or other project representatives observe a heuristic evaluation session from behind the one-way mirror in a usability lab. In general, though, only a small minority of heuristic evaluation sessions take place in usability laboratories. 




Company Name
Main Product
Other Labs in Company?
Date of First Usability Lab in Company
Floor Space of Typical Subject Room in Sq. Meters
Floor Space of Total Lab Area in Sq. Meters
Number of Rooms (Subject Rooms, Control Rooms, etc.)
Number of Cameras in Typical Subject Room
Scan Converter Used to Directly Tape Screen Image?
One-Way Mirror?
Usability Staff Supporting vs. Utilizing Lab


Ameritech
Communications service
No
1989
12.5
237
7
2
Yes
Yes
1 / 10


Bellcore
Telco software
Yes
1985
12.3
121
7
2
No
Yes
0.3 / 30


BT (British Telecom)
Telephone service
No
1988
40
96
3
3
Yes
Yes
0.5 / 70


IBM
Computer systems
Yes
1981
11.7
165
14
2
No
Yes
0.1 / 4


MAYA Design Group
Design consultants
No
1990
8.8
42.9
3
2
No
Yes
3 / 12


Microsoft Corp.
PC software
No
1989
10.8
181.3
19
2
Yes
Yes
4 / 22


NCR
Computer systems
Yes
1966
13.4
31.2
3
2
Yes
Yes
2 / 15


Philips, Corp. Design
Consumer electronics
Yes
1990
30
40
2
3
No
Yes
0.25 / 10


Philips, IPO
Consumer electronics
Yes
1990
9
35
3
2
No
No
1 / 25


SAP
Enterprise business apps
No
1992
37
63
2
1
No
Yes
2 / 12


SunSoft
Workstation software
No
1988
25.1
202.3
8
3
Yes
Yes
3.5 / 8


Symantec
PC software
No
1992
23.8
47.6
2
2
Yes
Yes
1 / 1


Taligent
PC operating systems
No
1992
13.4
26.8
2
2
No
Yes
1 / 2


Mean 
38%
1987
19.1
99.2
5.8
2.2
46%
92%
1.5 / 17.0


Median 
No
1989
13.4
63.8
3
2
No
Yes
1 / 12





Table 1. 
Overview of the usability laboratories discussed in this special issue. For each company, the usability laboratory described in this table is the one discussed in the paper in this special issue by authors from that company. For IBM, the lab is the one described by Fath et al., and for SunSoft, the lab is the one described by Rohn.
Notes: The first number in the entry for ""usability staff supporting vs. using the lab indicates"" the headcount of the staff that is dedicated to keeping the lab up and running. The second number in this entry indicates the number of usability specialists who share the lab for their work, even though they may not all be using it full-time. One square meter is 10.76 square feet. Scroll the table to the right to see more data.

The papers in this special issue describe thirteen usability laboratories that are summarized in Table 1. As noted in the table, 38% of the companies have other usability laboratories that are not represented in the table, so it should only be seen as representing a survey of usability labs and not as a complete listing of labs. It can be seen from the table that the defining characteristics of a usability laboratory seem to be video cameras (used in all the labs) and the one-way mirror (installed in 92% of the labs). The table also shows that usability laboratories are a fairly recent phenomenon, with the median year of the first usability lab in these companies being 1989. Of course, some companies have had usability laboratories for a long time. Also, we should note that companies in industries like aerospace and control room design have employed usability laboratories for many years even though they are not represented in the table. The papers in this issue are mostly from the computer and telecommunications industries, though de Vries, van Gelderen, and Brigham write about usability labs for consumer products at Philips. In general, the lessons described in the papers in this issue may apply most directly to the usability of information technology, but there is no reason to believe that most of the same methods could not be used for other types of products and services.
The table shows that the median usability laboratory has three rooms. Normally, the room distribution is a room for the test subject(s), a control room for the experimenter and other usability specialists involved in running the experiment and operating the recording and logging tools, and an ""executive observation lounge"" where additional staff can observe the test without interfering with either the subject or the experimenters. Sometimes additional rooms are used for waiting areas for subjects, and the larger labs also often have special video editing rooms to avoid occupying an entire test suite by using the control room facilities for editing rather than testing. The observers in the executive observation lounge may sometimes in fact be executives, but more commonly they are members of the development team who take the opportunity of the user test to get exposure to the users. Even though the video tape equipment in the labs are often used to produce very communicative highlight tapes of the most notable usability problems and colorful user utterances, there is still no substitute for observing a test live.
It can be seen from the table that some usability labs have a very large number of rooms. Often, this only means that multiple tests can be conducted in parallel, but sometimes larger facilities are used for experiments in computer-supported cooperative work, video conferencing, and other cases where multiple users have to be combined for a single test. Lund's paper on Ameritech's usability laboratory discusses this issue in further depth.
At this time of writing, only slightly less than half of the laboratories surveyed in the table used scan converters to make it possible to tape the computer screen image directly without having to point a camera at it. Scan converters have been somewhat expensive, but since they are dropping in price, their use can be expected to be more widespread in the future.
One of the major advantages of having a usability laboratory is that the incremental hurdle for user testing of a new product becomes fairly small since all the equipment is already in place in a dedicated room that is available for testing. This effect is important because of the compressed development schedules that often leave little time for delay. Thus, if usability testing can be done now and with minimal overhead, it will get done. Similarly, usability may get left out of a project if there is too much delay or effort involved before results become available. Because of this phenomenon, the support staff form a very important part of a usability laboratory in terms of keeping it up and running, stocked with supplies, and taking care of the practical details of recruiting and scheduling test users. In my opinion, the ratio of one support person to twelve usability specialists running tests that is shown as the median in Table 1 is actually too small. I believe that a higher number of support staff are well worth their cost in terms of more efficient usability work (which again leads to a larger amount of usability work being done).
The building of a usability laboratory involves a myriad of decisions and trade-offs. Most of the papers in this special issue touch upon several such issues, and the papers by Sazagari, Rohn, Uyeda, and Neugebauer and Spielmann are particularly detailed. The papers by Lucas and Fisher; Dayton, Tudor, and Root; Lund; and Blatt, Jacobson, and Miller take the detailed needs analysis one step further and describe how they redesigned their second-generation usability laboratories based on experience with older labs and using trusted user-centered design principles to find out what usability specialists really need in their lab.
A usability laboratory does not necessarily need to be a fixed facility in a given set of rooms constructed for the purpose. Szczur describes how she used a regular conference room at NASA as a low-budget usability lab by using part of the room for the subjects and part of the room for the observers. Several papers describe ""portable usability labs"" with video kits and other data logging equipment that can be brought to the field to allow testing, and Zirkler and Ballman emphasize the need to visit customer sites when assessing the usability of systems like their specialized databases for users in the legal and financial communities. Smilowitz, Darnell, and Benson compare standard usability testing in the lab with Beta testing done by having customers test the software at their own sites and report usability problems on their own without supervision by a usability specialist. Even though the Beta tests did have some limitations (notably, that they were restricted to the very last stages of the development lifecycle), they did provide a cheap source of additional usability data that should be considered as a supplement to the lab-based sources.
In addition to the building and equipment of the usability laboratory, an important issue is obviously the actual methods used in running experiments in the lab. Many papers discuss methodology issues, and Fath, Mann, and Holzman provide detailed coverage of five main phases used in evaluation sessions in IBM's Atlanta usability laboratory: designing the evaluation, preparing for it, conducting it, analyzing the data, and reporting the results. One of their conclusions is that as much of the data reduction process as possible should be automated. Usability testing generates huge masses of raw data, and any meaningful conclusions have to be based on analyses that summarize the test events in a comprehensible manner. Hoiem and Sullivan provide detailed information about the integrated set of usability data collection and analysis tools built for Microsoft's lab. In general, it is characteristic for the state of the art that almost all CAUSE tools are homemade for the individual labs. There are probably two reasons for this: First, we still do not know enough about what computerized tools usability professionals actually need, and we know even less about how to make such tools sufficiently usable and efficient. Second, the market is still fairly small, though it is constantly growing and may one day support a wider variety of commercial offerings.
A classic form of CAUSE tool used in many usability labs is the event logger, which is used by an observer to record occurrences of events of interest as the user progresses through the experiment. Typically, events are automatically timestamped (and often linked to a videotape record of the event), and the logger also records the type of event as classified by the human observer in real time -- possibly with a supplementary natural language annotation. In addition to human-coded event logs, usability labs often produce keystroke logs of all user input and activity logs of higher-level dialogue actions (like menu selections, error messages, etc.). These logs can be subjected to many different kinds of analysis, including the sequential data analysis techniques discussed in the paper by Coumo.
Much user testing is simply aimed at generating qualitative insights that are communicated through lists of usability problems and highlights videos showing striking cases of user frustration. Such insights are sufficient for most practical usability engineering applications where the goal is the improvement of a user interface through iterative design. Often, there is no real reason to expend resources of gathering statistically significant data on a user interface that is known to contain major usability problems and has to be changed anyway. By using a probabilistic model of the finding of usability problems and an economic model of project management, Nielsen and Landauer (1993) found that one often gets the optimal cost-benefit ratio by  using between three and five test users for each round of user testing.
User testing with the goal of learning about the design to improve its next iteration falls in the category of formative evaluation. Sometimes, one wants to do summative evaluation that is quantitative in nature and results in numbers that can be compared across products. One case where summative evaluation is desired is the comparative product reviews produced by personal computer magazines like the ones discussed in Bawa's paper. Another application of competitive testing is the selection of recommended (or prescribed) software for a big company where the benefits in terms of reduced training and support and increased productivity are sufficiently large to warrant the investment of considerable resources on choosing the most usable product from the available offerings on the market. To my knowledge, not many user organizations currently perform such competitive usability testing before major software purchases, though there are a few that do. Also, it is becoming fairly common for software vendors to commission third-party usability consultants to perform comparative studies and then advertise the results if their own product wins. Finally, summative evaluation is sometimes used for regular software development projects when one wants to investigate whether a revised product has achieved a sufficient improvement in usability over the previous version. The paper by Bevan and Macleod describes a comprehensive set of measurement tools and principles developed as part of the ESPRIT MUSiC project on usability metrics.
Even though usability metrics can be very elaborate (for example, Bevan and Macleod describe the measurement of heart rate variability as a way of assessing the user's mental effort over time), it is also possible to have a ""discount usability engineering"" approach to usability metrics. As an example, Molich's paper presents a technique for keeping track of the number of ""user interface disasters"" (certain kinds of serious usability problems experienced by at least two users) as a simple, yet quantifiable measure of interface quality.
As shown by the contrast between the full-blown usability metrics and the simplistic count of interface disasters, there are many different possible approaches to usability engineering (Nielsen 1993). It is important to realize that it is possible to achieve major improvements in usability even if one does not utilize all the most advanced techniques and even if one has a fairly primitive usability lab (or sets up a temporary lab in a conference room as described by Szczur). The single-most important decision in usability engineering is simply to do it! The best intentions of some day building the perfect lab will result in exactly zero improvement in current products, and if the choice is between perfection or doing nothing, nothing will win every time. Luckily, it is possible to start small and then grow a usability effort over time as management discovers the huge benefits one normally gets (Ehrlich and Rohn 1994). It is in the nature of things that this special issue mostly has papers on leading usability laboratories from companies that have a better-than-average approach to usability engineering. This does not mean that people in less fortunate companies should abandon usability, it only means that they have something to strive for as they start small and gradually expand their usability groups and usability labs.
Acknowledgments
The main review burden in selecting the papers was shouldered by the following reviewers who had been especially selected for this special issue: Tomas Berns (Nomos Management AB), Linda I. Borghesani (The MITRE Corporation), Joseph S. Dumas (American Institutes for Research), Tom G. Gough (University of Leeds), Lovie Ann Melkus (IBM Consulting Group), James R. Miller (Apple Computer, Inc.), Michele Morris (BNR Europe Ltd.), Kenneth Nemire (Interface Technologies), Markku I. Nurminen (University of Turku), Diane J. Schiano (Interval Research Corporation), Sylvia Sheppard (NASA Goddard Space Flight Center), Nancy Storch (Lawrence Livermore National Laboratory), Paulus-Hubert Vossen (Fraunhofer-Institut IAO), and David Ziedman (Philips Interactive Media of America). Additional ad-hoc reviews were provided by: Rita M. Bush (Bellcore), Tom Dayton (Bellcore), Arye R. Ephrath (Bellcore), Richard D. Herring (Bellcore), Arnold M. Lund (Ameritech), Michael J. Muller (U S WEST Advanced Technologies), and Estela M. Tice (Bellcore).
References

Bias, R. G. and Mayhew, D. J. (Eds.) 1994,  Cost-Justifying Usability (Academic Press, Boston, MA).
Ehrlich, K. and Rohn, J. 1994, Cost-justification of usability engineering: A vendor's perspective, in Bias, R.G., and Mayhew, D.J. (Eds.), Cost-Justifying Usability (Academic Press, Boston, MA).
Muller, M. J., Wildman, D. M., and White, E. A. 1993, ""Equal opportunity"" PD using PICTIVE.  Communications of the ACM   36, 4, 64-66.
Nielsen, J. 1993, Usability Engineering (Academic Press, Boston, MA).
Nielsen, J. 1994, Heuristic evaluation, in Nielsen, J., and Mack, R. L. (Eds.),   Usability Inspection Methods. (John Wiley & Sons, New York, NY), 25-64.
Nielsen, J. and Landauer, T. K. 1993, A mathematical model of the finding of usability problems,  Proceedings of the ACM INTERCHI'93 Conference  (Amsterdam, the Netherlands, April 24-29), 206-213.
Shackel, B. 1971, Human factors in the P.L.A. meat handling automation scheme. A case study and some conclusions.  International Journal of Production Research   9, 1, 95-121.
Wasserman, A. S. 1989, Redesigning Xerox: A design strategy based on operability. In Klemmer, E. T. (Ed.),  Ergonomics: Harness the Power of Human Factors in Your Business, Ablex, Norwood, NJ. 7-44.
"
120,1994-01-01,"One of the oldest jokes in computer science goes as follows:
Q:   How many programmers does it take to change a light bulb? 
A:   None; it is a hardware problem! 
When asking how many usability specialists it takes to change a light bulb, the answer might well be four: Two to conduct a field study and task analysis to determine whether people really need light, one to observe the user who actually screws in the light bulb, and one to control the video camera filming the event. It is certainly true that one should study user needs before implementing supposed solutions to those problems. Even so, the perception that anybody touching usability will come down with a bad case of budget overruns is keeping many software projects from achieving the level of usability their users deserve.
1 The Intimidation Barrier
It is well known that people rarely use the recommended usability engineering methods [Nielsen 1993; Whiteside et al. 1988] on software development projects in real life. This includes even such basic usability engineering techniques as early focus on the user, empirical measurement, and iterative design which are used by very few companies. Gould and Lewis [1985] found that only 16% of developers mentioned all three principles when asked what one should do when developing and evaluating a new computer system for end users. Twenty-six percent of developers did not mention a single of these extremely basic principles. A more recent study found that only 21% of Danish software developers knew about the thinking aloud method and that only 6% actually used it [Milsted et al. 1989]. More advanced usability methods were not used at all.
One important reason usability engineering is not used in practice is the cost of using the techniques. Or rather, the reason is the perceived cost of using these techniques, as this chapter will show that many usability techniques can be used quite cheaply. It should be no surprise, however, that practitioners view usability methods as expensive considering, for example, that a paper in the widely read and very respected journal Communications of the ACM estimated that the ""costs required to add human factors elements to the development of software"" was $128,330 [Mantei and Teorey 1988]. This sum is several times the total budget for usability in most smaller companies, and one interface evangelist has actually found it necessary to warn such small companies against believing the CACM estimate [Tognazzini 1990]. Otherwise, the result could easily be that a project manager would discard any attempt at usability engineering in the belief that the project's budget could not bear the cost. Table 1 shows the result of adjusting a usability budget according to the discount usability engineering method discussed below. The numbers in Table 1 are for a medium scale software project (about 32,000 lines of code). For small projects, even cheaper methods can be used, while really large projects might consider additional funds to usability and the full-blown traditional methodology, though even large projects can benefit considerably from using discount usability engineering.

Table 1 
	Cost savings in a medium scale software project by using the discount usability engineering method instead of the more thorough usability methods sometimes recommended.


Original usability cost estimate by [Mantei and Teorey 1988]
$128,330


Scenario developed as paper mockup instead of on videotape
- $2,160


Prototyping done with free hypertext package
- $16,000


All user testing done with 3 subjects instead of 5
- $11,520


Thinking aloud studies analyzed by taking notes instead of by video taping
- $5,520


Special video laboratory not needed
- $17,600


Only 2 focus groups instead of 3 for market research
- $2,000


Only 1 focus group instead of 3 for accept analysis
- $4,000


Questionnaires only used in feedback phase, not after prototype testing
- $7,200


Usability expert brought in for heuristic evaluation
+ $3,000


Cost for ""discount usability engineering"" project
$65,330



British studies [Bellotti 1988] indicate that many developers don't use usability engineering because HCI (human-computer interaction) methods are seen as too time consuming and expensive and because the techniques are often intimidating in their complexity. The ""discount usability engineering"" approach is intended to address these two issues. Further reasons given by Bellotti were that there were sometimes no perceived need for HCI and a lack of awareness about appropriate techniques. These two other problems must be addressed by education [Perlman 1988, 1990; Nielsen and Molich 1989] and propaganda [Nielsen 1990a], but even for that purpose, simpler usability methods should help. Also, time itself is on the side of increasing the perceived need for HCI since the software market seems to be shifting away from the ""features war"" of earlier years [Telles 1990]. Now, most software products have more features than users will ever need or learn, and Telles [1990] states that the ""interface has become an important element in garnering good reviews"" of software in the trade press.
As an example of ""intimidating complexity,"" consider the paper by Karwowski et al. [1989] on extending the GOMS model [Card et al. 1983] with fuzzy logic. Note that I am not complaining that doing so is bad research. On the contrary, I find it very exciting to develop methods to extend models like GOMS to deal better with real-world circumstances like uncertainty and user errors. Unfortunately, the fuzzy logic GOMS and similar work can easily lead to intimidation when software people without in-depth knowledge of the HCI field read the papers. These readers may well believe that such methods represent ""the way"" to do usability engineering even though usability specialists would know that the research represents exploratory probes to extend the field and should only serve as, say, the fifth or so method one would use on a project. There are many simpler methods one should use first [Nielsen 1992a, 1993].
I certainly can be guilty of intimidating behavior too. For example, together with Marco Bergman, I recently completed a research project on iterative design where we employed a total of 99 subjects to test various versions of a user interface at a total estimated cost of $62,786. People reading papers reporting on this and similar studies might be excused if they think that iterative design and user testing are expensive and overly elaborate procedures. In fact, of course, it is possible to use considerably fewer subjects and get by with much cheaper methods, and we took care to say so explicitly in our paper. A basic problem is that with a few exceptions, published descriptions of usability work normally describe cases where considerable extra efforts were expended on deriving publication-quality results, even though most development needs can be met in much simpler ways.
As one example, consider the issue of statistical significance. I recently had a meeting to discuss usability engineering with the head of computer science for one of the world's most famous laboratories, and when discussing the needed number of subjects for various tests, he immediately referred to the need for test results to be statistically significant to be worth collecting. Certainly, for much research, you need to have a high degree of confidence that your claimed findings are not just due to chance. For the development of usable interfaces, however, one can often be satisfied by less rigorous tests.
Statistical significance is basically an indication of the probability that one is not making the wrong conclusion (e.g., a claim that a certain result is significant at the p<.05 level indicates that there is a 5% probability that it is false). Consider the problem of choosing between two alternative interface designs [Landauer 1988]. If no information is available, you might as well choose by tossing a coin, and you will have a 50% probability of choosing the best interface. If a small amount of user testing has been done, you may find that interface A is better than interface B at the 20% level of significance. Even though 20% is considered ""not significant,"" your tests have actually improved your chance of choosing the best interface from 50/50 to 4-to-1, meaning that you would be foolish not to take the data into account when choosing. Furthermore, even though there remains a 20% probability that interface A is not better than interface B, it is very unlikely that it would be much worse than interface B. Most of the 20% accounts for cases where the two interfaces are equal or where B is slightly better than A, meaning that it would almost never be a really bad decision to choose interface A. In other words, even tests that are not statistically significant are well worth doing since they will improve the quality of decisions substantially.
2 The Discount Usability Engineering Approach
Usability specialists will often propose using the best possible methodology. Indeed, this is what they have been trained to do in most universities. Unfortunately, it seems that ""le mieux est l'ennemi du bien"" (the best is the enemy of the good) [Voltaire 1764] to the extent that insisting on using only the best methods may result in having no methods used at all. Therefore, I will focus on achieving ""the good"" with respect to having some usability engineering work performed, even though the methods needed to achieve this result are definitely not ""the best"" method and will not give perfect results.
It will be easy for the knowledgable reader to put down the methods proposed here with various well-known counter-examples showing important usability aspects that will be missed under certain circumstances. Some of these counter-examples are no doubt true and I do agree that better results can be achieved by applying more careful methodologies. But remember that such more careful methods are also more expensive -- often in terms of money, and always in terms of required expertise (leading to the intimidation factor discussed above). Therefore, the simpler methods stand a much better chance of actually being used in practical design situations and they should therefore be viewed as a way of serving the user community.
The ""discount usability engineering"" [Nielsen 1989b, 1990a, 1993] method is based on the use of the following three techniques:

Scenarios
Simplified thinking aloud
Heuristic evaluation

Additionally, the basic principle of early focus on users should of course be followed. It can be achieved in various ways, including simple visits to customer locations.
2.1 Scenarios
Scenarios are a special kind of prototyping as shown in Figure 1. The entire idea behind prototyping is to cut down on the complexity of implementation by eliminating parts of the full system. Horizontal prototypes reduce the level of functionality and result in a user interface surface layer, while vertical prototypes reduce the number of features and implement the full functionality of those chosen (i.e. we get a part of the system to play with).

Figure 1 
	The concept of a  scenario  compared to vertical and horizontal prototypes as ways to make rapid prototyping simpler.






Scenarios take prototyping to the extreme by reducing both the level of functionality and the number of features. By reducing the part of interface being considered to the minimum, a scenario can be very cheap to design and implement, but it is only able to simulate the user interface as long as a test user follows a previously planned path.
Since the scenario is small, we can afford to change it frequently, and if we use cheap, small thinking aloud studies, we can also afford to test each of the versions. Therefore scenarios are a way of getting quick and frequent feedback from users.
Scenarios can be implemented as  paper mock-ups [Nielsen 1990b] or in simple prototyping environments [Nielsen 1989a] that may be easier to learn than more advanced programming environments [Nielsen et al. 1991]. This is an additional savings compared to more complex prototypes requiring the use of advanced software tools.
2.2 Simplified Thinking Aloud
Traditionally, thinking aloud studies are conducted with psychologists or user interface experts as experimenters who videotape the subjects and perform detailed protocol analysis. This kind of method certainly may seem intimidating for ordinary developers. However, it is possible to run user tests without sophisticated labs, simply by bringing in some real users, giving them some typical test tasks, and asking them to think out loud while they perform the tasks. Those developers who have used the thinking aloud method are happy about it [Jørgensen 1989, Monk et al. 1993], and my studies [Nielsen 1992b] show that computer scientists are indeed able to apply the thinking aloud method effectively to evaluate user interfaces with a minimum of training, and that even fairly methodologically primitive experiments will succeed in finding many usability problems.
I have long claimed that one learns the most from the first few test users, based on several case studies. In earlier papers, I have usually recommended using between three and five test users per test as a way of simplifying user testing while gaining almost the same benefits as one would get from more elaborate tests with large numbers of subjects. Recently, Tom Landauer and I developed a mathematical model of the number of usability problems [Nielsen and Landauer 1993], and when plugging in typical budget figures from different kinds of user testing, we derived curves like the ones shown in Figure 2 for the ratio between the benefits of user testing and the cost of the test for medium-sized development projects. The curves basically show that the benefits from user testing are much larger than the costs, no matter how many subjects are used. The maximum benefit-cost ratio is achieved when using between three and five subjects, confirming my earlier experience.

Figure 2 
	Cost-benefit trade-off curve for a ""typical"" project, varying the number of test users, using the model and average parameters described by Nielsen and Landauer [1993]. The curve shows the ratio of benefits to costs, that is, how many times the benefits are larger than the costs. For example, a benefit-to-cost ratio of 50 might correspond to costs of $10,000 and benefits of $500,000.






Besides reducing the number of subjects, another major difference between simplified and traditional thinking aloud is that data analysis can be done on the basis of the notes taken by the experimenter instead of by videotapes. Recording, watching, and analyzing the videotapes is expensive and takes a lot of time which is better spent on running more subjects and on testing more iterations of redesigned user interfaces. Video taping should only be done in those cases (such as research studies) where absolute certainty is needed. In discount usability engineering we don't aim at perfection anyway, we just want to find most of the usability problems, and a survey of 11 software engineers [Perlman 1988] found that they rated simple tests of prototypes as almost twice as useful as video protocols.
2.3 Heuristic Evaluation
Current user interface standards and collections of usability guidelines typically have on the order of one thousand rules to follow and are therefore seen as intimidating by developers. For the discount method I advocate cutting the complexity by two orders of magnitudes and instead rely on a small set of heuristics such as the  ten basic usability principles (listed on a separate page).
These principles can be presented in a single lecture and can be used to explain a very large proportion of the problems one observes in user interface designs. Unfortunately it does require some experience with the principles to apply them sufficiently thoroughly [Nielsen 1992c], so it might be necessary to spend some money on getting outside usability consultants to help with a heuristic evaluation. On the other hand, even non-experts can find many usability problems by heuristic evaluation and many of the remaining problems would be revealed by the simplified thinking aloud test. It can also be recommended to let several different people  perform a heuristic evaluation as different people locate different usability problems [Nielsen and Molich 1990]. This is another reason why even discount usability engineers might consider setting aside a part of their budget for outside usability consultants.
3 Validating Discount Usability Engineering
In one case, I used the discount usability engineering method to redesign a set of account statements [Nielsen 1989b]. I tested eight different versions (the original design plus seven redesigns) before I was satisfied. Even so, the entire project required only about 90 hours, including designing seven versions of twelve different kinds of statements (not all the forms were changed in each iteration, however) and testing them in simplified thinking aloud experiments. Most versions were tested with just a single user. To validate the redesign, a further experiment was done using traditional statistical measurement methods. It should be stressed that this validation was a research exercise and not part of the discount usability engineering method itself: The usability engineering work ended with the development of the improved account statements, but as a check of the usability engineering methods used, it was decided to conduct a usability measurement of one of the new designs compared with the original design.
3.1 Experiment 1: Double Blind Test Taking Usability Measurements
The validation was done using a double blind test: 38 experimenters each ran four subjects (for a total of 152 subjects) in a between-subjects design. Neither the experimenters nor the subjects knew which was the original account statement and which was the new. The results which are reported in Table 3 show clear and highly statistically significant improvements in measurement values for the new statement with respect to the understandability of the information in the statement as measured by the average number of correct answers to four questions concerning the contents of the statement. The value had indeed been the usability parameter which had been monitored as a goal during the iterative design. Two other usability parameters which had not been considered goals in the iterative design process (efficiency of use and subjective satisfaction) were also measured in the final test, and the two versions of the statement got practically identical scores on those.

Table 3 
	Result of Experiment 1: a double blind test (N=152) comparing the original and the revised version of a bank account statement. The values measured are: How many of the subjects could correctly answer each of four questions about the contents of the statement (and the combined average for those four questions), the average time needed by subjects to review the statement and answer the questions, and the subjects' average subjective rating (scale: 1 [bad] to 5 [good]).
	The rightmost column indicates whether the difference between the two account statements is statistically significant according to a  t  -test.


 
Original design
Revised design
Significance of Difference


""Size of deposit""
79%
95%
p  <.01


""Commission""
34%
53%
p  <.05


""Interest rates""
20%
58%
p  <.01


""Credit limit""
93%
99%
p  <.05


Average correct
56%
76%
p  <.01


Task time (sec.)
315
303
n.s. (  p  =.58)


Subjective satisfaction [1-5 scale]
2.8
3.0
n.s. (  p  =.14)



This study supports the use of discount usability engineering techniques and shows that they can indeed cause measurable improvements in usability. However, the results also indicate that one should be cautious in setting the goals for usability engineering work. Those usability parameters that have no goals set for improvement risk being left behind as the attention of the usability engineer is concentrated on the official goals. In this study, no negative effects in the form of actual degradation in measured usability parameters were observed but one can not always count on being so lucky.
3.2 Experiment 2: Recommendations from People without Usability Expertise
Two groups of evaluators were shown the two versions of the account statement (without being told which one was the revised version) and asked which one they would recommend management to use. All the evaluators were computer science students who had signed up for a user interface design course but who had not yet been taught anything in the course. This meant that they did not know the  usability heuristics which they might otherwise have used to evaluate the two versions.
Group A consisted of the experimenters from Experiment 1 (reported above) who had run two short experiments with each version of the account statement, while the evaluators in Group B had to make their recommendation on the basis of their own personal evaluation of the two versions. The results are reported in Table 4 and show a significant difference in the recommendations: Evaluators in Group A preferred the revised version 4 to 1 while evaluators in Group B were split equally between the two versions. This latter result is probably a reflection of the fact that the two versions are almost equally subjectively satisfying according to the measurement results reported in Table 3.

Table 4 
	Result of Experiment 2: asking two group of evaluators to recommend one of the two versions of an account statement. In Group A, each person had first run an empirical test with four subjects, whereas the evaluators in Group B had no basis for their recommendation except their own subjective evaluation.
	The difference between the two groups is statistically different at the  p  <.05 level.


 
Group A
Group B


 
N=38
N=21


Recommends original
16%
48%


Recommends revised
68%
48%


No recommendation
16%
5%



If we accept the statistical measurement results in Table 3 as defining the revised version as the ""best,"" we see that Group A was dramatically better at making the correct recommendation than Group B was. This was in spite of the fact that each of the individuals in Group A had knowledge only of the experimental results from two subjects for each of the designs (the aggregate statistics were not calculated until after the recommendations had been made, so each evaluator knew only the results from the four subjects run by that individual).
So we can conclude that running even a small, cheap empirical study can help non-human factors people significantly in their evaluation of user interfaces. If we count the evaluators who did not make a recommendation as having a 50/50 chance of picking the right interface, this experiment shows that running just two subjects for each version in a small test improved the probability for recommending the best of two versions from 50% to 76%.
4 Cost-Benefit Analysis of Heuristic Evaluation: A Case Study
A cost-benefit analysis of heuristic evaluation includes two main elements: First estimating the costs in terms of time spent performing the evaluation, and second estimating the benefits in terms of increased usability (less the development costs for the redesign). Since these estimates involve some uncertainties, they will be converted into dollar amounts by using round numbers. Any given company will of course have slightly different conversion factors, depending on its exact financial circumstances.
The following case study regards a prototype user interface for a system for internal telephone company use which will be called the Integrating System in this chapter. The Integrating System is fairly complicated and understanding its details requires extensive knowledge of telephone company concepts, procedures, and databases. Since a detailed explanation is not necessary to understand the generally applicable lessons from the study, the Integrating System will only be outlined here.
Briefly, the Integrating System provides a graphical user interface to access information from several systems running on various remote computers in a uniform manner despite the differences between the backend systems. The Integrating System can be used to resolve certain problems when data inconsistencies require manual intervention by a technician because the computer systems cannot determine which information is correct. The traditional method for resolving these problems involves having the technician compare information across several of these databases by accessing them through a number of traditional alphanumeric terminal sessions. The databases reside on different computers and have different data formats and user interface designs, so this traditional method is somewhat awkward and requires the technicians to learn a large number of inconsistent user interfaces.
Performing this task involves a large amount of highly domain-specific knowledge about the way the telephone system is constructed and the structure of the different databases. Technicians need to know where to look for what data and how the different kinds of data are related. Also, the individual data items themselves are extremely obscure for people without detailed domain knowledge.
As a result of the heuristic evaluation of this interface with 11 evaluators (described in further detail in [Nielsen 1994b]), 44 usability problems were found. Forty of these problems are denoted ""core"" usability problems and were found in the part of the interface that was subjected to intensive evaluation, whereas the remaining four problems were discovered in parts of the interface that we had not planned to study as part of the heuristic evaluation.
4.1 Time Expenditure
As usual in usability engineering, the cost estimates are the easiest to get right. Table 5 accounts for the total time spent on the heuristic evaluation project in terms of person-hours. No attempt has been made to distinguish between different categories of professional staff. Practically all the person-hours listed in Table 5 were spent by usability specialists. The only exception is a small number of hours spent by development specialists in getting the prototype ready for the evaluation and in attending the debriefing session.

Table 5 
	Estimate of the total number of person-hours spent on the heuristic evaluation study described in this article. The estimate of ""time to prepare the prototype"" does not include the time needed for the initial task analysis, user interface design, or implementation of the prototype since these activities had already been undertaken independently of the heuristic evaluation.


Assessing appropriate ways to use heuristic evaluation, 4 people @ 2 hours
8


Having outside evaluation expert learn about the domain and scenario
8


Finding and scheduling evaluators, 1.8 hours + 0.2 hours per evaluator
4


Preparing the briefing
3


Preparing scenario for the evaluators
2


Briefing, 1 system expert, 1 evaluation expert, 11 evaluators @ 1.5 hours
19.5


Preparing the prototype (software and its hardware platform) for the evaluation
5


Actual evaluation, 11 evaluators @ 1 hour
11


Observing the evaluation sessions, 2 observers @ 11 hours
22


Debriefing, 3 evaluators, 3 developers, 1 evaluation expert @ 1 hour
7


Writing list of usability problems based on notes from evaluation sessions
2


Writing problem descriptions for use in severity-rating questionnaire
6


Severity rating, 11 evaluators @ 0.5 hours
5.5


Analyzing severity ratings
2


Total
105



Note that the time given for the preparation of the scenario covers only the effort of writing up the scenario in a form that would be usable by the evaluators during the evaluation. Considerable additional effort was needed to specify the scenario in the first place, but that effort was part of the general task analysis and design activities performed before the evaluation. Scenario-based design is a well-known method for user interface design [Carroll and Rosson 1990, Clarke 1991], so one will often be able to draw upon interaction scenarios that have been developed in previous stages of the usability lifecycle. Even so, we were probably lucky that the scenario developed for the present system could be used for the evaluation with such a small amount of additional effort.
The evaluation sessions were videotaped, and approximately eight hours were spent on mundane tasks like getting videotapes, learning to operate the video equipment in the specific usability laboratory used for the evaluation sessions, setting up and closing down the video equipment on each of the two days of the study, rewinding tapes, etc. This videotaping was not part of the heuristic evaluation as such, and the tapes were not reviewed for the purpose of arriving at the list of usability problems. The observers' notes were sufficient for that purpose. The videotapes were used to some extent in this research analysis of the study where an additional eight hours were spent reviewing details of some evaluation sessions, but since this use was not part of the practical application of the heuristic evaluation method, the time spent on the videotapes has not been included in Table 5.
It follows from Table 5 that the total number of person-hours spent on the evaluation can be determined by the formula



Equation 1:
			time(  i  ) = 47.8 + 5.2  i 



where  i  is the number of evaluators. This formula is not exact for large values of  i  , since some of the effort devoted to room scheduling and to the analysis of the severity ratings is partly dependent on the number of evaluators and would change with large  i  s.
The cost estimate in (Equation 1) is probably larger than necessary for future heuristic evaluations. Major reductions in both the fixed and variable costs could be achieved by reducing the team of two observers to a single observer. This observer should be the person who is familiar with the application such that the observer can answer questions from the evaluators during the evaluation. Also, even though the observer should have a certain level of usability knowledge in order to understand the comments made by the evaluators, the observer need not be a highly skilled expert specializing in usability. A major difference between heuristic evaluation and traditional user testing is that an observer of a heuristic evaluation session is mostly freed from having to interpret user actions since the evaluators are assuming the task of explicitly identifying the usability problems. In contrast, the experimenter in a traditional user test would need a higher level of usability expertise in order to translate the subject's actions and difficulties into interface-related usability problems.
This single change would result in the following, revised formula



Equation 2:
			time(  i  ) = 37.3 + 4.2  i 



Transforming the time estimates in (Equation 1) or (Equation 2) to money estimates can be done fairly simply by multiplying the number of hours by an estimate of the loaded hourly cost of professional staff. Note that the salary and benefits costs of the professional staff are not sufficient, since additional costs are incurred in form of the computer equipment and laboratory space used for the test. To use round numbers, an estimated hourly loaded cost for professional staff of $100 translates into a total cost for the heuristic evaluation of $10,500 for the 105 hours that were actually spent.
4.2 Benefit Estimation
The only way to get an exact measure of the benefits of the heuristic evaluation would be to fully implement two versions of the user interface; one without any changes and one with the changes implied by the evaluation results. These two versions should then be used by a large number of real users to perform real tasks for sufficiently long time that the steady-state level of expert performance had been reached in both cases [Gray et al. 1992]. This process would provide exact measures for the differences in learning time and expert performance. Unfortunately, the version of the interface that was evaluated only exists in a prototype form with which one cannot do any real work, and it would be unrealistic to expect significant development resources to be invested in transforming this prototype to a final product with an identical user interface now that a large number of usability problems have been documented.
Alternatively, one could build a detailed economic work-study model of the different steps involved in the users' workday in order to assess the frequency and duration of each sub-task. One could then further use formal models of user interaction times to estimate the duration of performing each step with each of a set of alternative user interface designs [Gray et al. 1992]. Such an approach would provide fairly detailed estimates but would not necessarily be accurate because of unknown durations of the operations in the model. It would also be very time-consuming to carry out.
It is thus necessary to rely on estimates of the benefits rather than hard measurement data. To get such estimates, the 11 evaluators were asked to estimate the improvements in usability from fixing all the 44 usability problems identified by the heuristic evaluation. Usability improvements were estimated with respect to two usability parameters:

Reduction of learning time: How much less time would the users need to spend learning to use the system? Learning time considered as a usability parameter represents a one-time loss of productive time for each new user to learn the system, so any savings would be realized only once.
Speedup in expert performance: Once the users have reached a steady state of expert performance, how much faster would they be able to perform their work when using a system with all the usability problems fixed than when using a system with all the problems still in place? Expert performance considered as a usability parameter represents a continued advantage for the use of the improved interface, so any savings would be realized throughout the lifetime of the system.

Other usability parameters of interest include frequency of user errors and the users' subjective satisfaction, but these parameters were not estimated. Since several of the usability problems we found were related to error-prone circumstances, it is likely that the number of user errors would go down.
Ten of the 11 evaluators provided learning time estimates and all 11 provided expert speedup estimates. Histograms of the distribution of these estimates are shown in Figure 3. Nielsen and Phillips [1993] found that estimates of changes in user performance made by usability specialists were highly variable, as also seen in the figure here, but that mean values of at least three independent estimates were reasonably close to the values measured by controlled experiments.

Figure 3 
	Histograms showing the distribution of the evaluators' estimates of savings in learning time (top) and expert performance speedup (bottom) for an interface fixing all the usability problems found in the heuristic evaluation. One evaluator did not provide a learning time estimate.






Given that the benefit estimates are based purely on subjective judgments of experts rather than on empirical evidence, it would seem prudent to be conservative in translating the evaluators' estimates into projected monetary savings. The mean values are 0.8 days for learning time reduction and 18% for expert speedup when all evaluators are considered, and 0.5 days and 16%, respectively, when excluding the perhaps overly optimistic outliers at 2 days and 40%. In order to be conservative, we will choose 0.5 days as our learning time reduction estimate and 10% as our expert speedup estimate.
The 10% expert speedup obviously only applies to time spent using the interface. Studies of the users indicate that they will spend about 1/3 of their time doing other tasks, 1/3 of their time performing the task without operating the user interface, and 1/3 of their time actually operating the interface. The 10% expert speedup thus corresponds to 3.3% of total work time.
Translating these estimates into overall savings can be done under the following assumptions: We assume that 2,000 people will be using the system. This is somewhat conservative given that about 3,000 people currently perform this job. Having 2,000 people each save 0.5 days in learning to use the system corresponds to a total of 1,000 user-days saved as a one-time saving. Furthermore, having 2,000 users perform their work 3.3% faster after having reached expert performance corresponds to 67 user-years saved each year the system is in use. Again to be conservative, we will only consider the savings for the first year, even though computer systems of the magnitude we are talking about here are normally used for more than one year. Sixty-seven user-years correspond roughly to 13,000 user-days saved. The total number of user-days saved the first year is thus about 14,000.
To value the total savings in monetary terms, we will assume that the cost of one user-day is $100, and to be conservative, we will assume that only half of the usability problems can actually be fixed, so that only half of the potential savings are actually realized. Furthermore, we need to take into account the fact that the savings in user time are not realized until the system is introduced and thus have a smaller net present value than their absolute value. Again to use round numbers, we will discount the value of the saved learning time by 20% and the value of the expert speedup in the first year by 30%. Learning time can be discounted by a smaller percentage as this saving is realized on day one after the introduction of the system. Using these conservative assumptions, we find one-year savings of $540,000.
Of course, the savings are not realized just by wishing for half of the usability problems to be fixed, so we have to reduce the savings estimate with an estimate of the cost of the additional software engineering effort needed to redesign the interface rather than just implementing the interface from the existing prototype. Assuming that the amount of software engineering time needed for this additional work is 400 hours, and again assuming that the loaded cost of a professional is $100 per hour, we find that the savings estimate needs to be reduced by $40,000. This expense is incurred here and now and thus cannot be discounted. Our final estimate of the net present value of improving the user interface is thus $500,000.
Still being conservative, we have not taken into account the value of the saved software engineering costs from not having to modify the system after its release. Assuming that the original user interface were to be fully implemented and released, is it very likely that the users would demand substantial changes in the second release, and it is well known that making software engineering changes to a released system is much more expensive than making changes at a prototype stage of the software lifecycle.
The $500,000 benefit of improving the interface should be compared with the cost of the heuristic evaluation project, estimated at $10,500. We thus see that the benefit/cost ratio is 48. This number involves significant uncertainties, but is big enough that we do not hesitate to conclude that the heuristic evaluation paid off.
As a final comment on the cost-benefit analysis we should note that the ""benefits"" do not translate to an actual cash flow. Instead, they represent the avoidance of the penalty represented by the extra time the users would have had to spend if the prototype interface had been implemented and released without further changes. It is an interesting and an important management problem to find ways to properly represent such savings in the funding of software development.
4.3 Cost-Benefit Analysis of User Testing
After the heuristic evaluation exercise, additional user testing was performed on the same interface, running four test users. A major reason for using so many more heuristic evaluators than test users was that the users of this particular application were highly specialized technicians who were difficult to get into the lab, whereas it was reasonably easy to get a large number of usability specialists to participate in the heuristic evaluation session. Four new usability problems were found by the user testing which also confirmed 17 of the problems that had already been found by heuristic evaluation.
One can discuss whether the 23 core problems that were not observed in the user test are in fact ""problems"" given that they could not be seen to bother the real users. As argued elsewhere [Nielsen 1992b], such problems can indeed be very real, but their impact may just have too short a duration to be observable in a standard user test. Problems that have the effect of slowing users down for 0.1 second or so simply cannot be observed unless data from a very large number of users is subjected to statistical analysis, but they can be very real and costly problems nevertheless. Also, some problems may occur too infrequently to have been observed with the small number of users tested here.
The main cost of the user test activity was having two professionals spend 7 hours each on the running of the test and the briefing and debriefing of the test users. No time was needed for the traditionally time-consuming activity of defining the test tasks since the same scenario was used as that developed for the previous usability work. Additionally, half an hour was spent finding and scheduling the users for the test and two hours were spent on implementing a small training interface on which the users could learn to use a mouse and standard graphical interaction techniques like pull-down windows. These activities sum to a total of 16.5 person-hours of professional staff, or a cost of $1,650.
Furthermore, the four users and their manager spent essentially a full day on the test when their travel time is taken into account. Again assuming that the cost of one user-day is $100, and furthermore assuming that the cost of one manager-day is $200, the total cost of user involvement is $600. Adding the cost of the professionals and the users gives a total estimate of $2,250 as the cost of the user testing.
The $2,250 spent on user testing could potentially have been spent on additional heuristic evaluation efforts instead. According to Equation 1, this sum corresponds to using 4.3 additional evaluators. Nielsen and Landauer [1993] showed that the finding of usability problems by  i  evaluators can be modelled by the prediction formula



Equation 3:
			ProblemsFound(  i  ) = N(1 - (1-  l  )   i   )



For the core usability problems in the present study, the best-fit values for the parameters in this equation are N=40 and  l  =0.26. Increasing the number of heuristic evaluators,  i  , from 11 to 15.3 can thus be expected to result in the finding of about 1.1 additional usability problems. This estimate shows that the available additional resources do indeed seem to have been spent better on running a user test, finding four problems, than on potentially extending the heuristic evaluation further.
We have no systematic method to estimate the benefits of having found the four additional problems that were discovered by user testing. However, one easy way to arrive at a rough estimate is to assume that the average severity of the four new problems is the same as the average severity of the 17 problems that had already been found by heuristic evaluation. As part of the heuristic evaluation study, severity was measured on a rating scale, with each usability problem being assigned a severity score from zero to four, with higher scores denoting more serious problems. The sum of the severity scores for the original 44 usability problems was 98.41, and the sum of the severity scores for the 17 problems that were seen both in the user test and in the heuristic evaluation was 41.56. We can thus estimate the relative severity of the additional four problems as compared to the original problems as 4/17 xá41.56/98.41 = 0.099.
Knowing about the additional problems found by user testing would thus add 9.9% to the total potential for improving the interface. Furthermore, we might assume that the proportion of the new problems that can be fixed, the impact of fixing them, and the cost of fixing them are all the same as the estimates for the problems found by heuristic evaluation. Under these assumptions, the benefit of having found the additional four usability problems can be valued at $500,000 x 0.099 = $49,500.
Using these estimates, the benefit/cost ratio of adding the user test after the heuristic evaluation is 22. Of course, the benefits of user testing would have been larger if we had credited it with finding the problems that were observed during the user test but had already been found by the heuristic evaluation. We should note, though, that the cost of planning the user test would have been higher if the heuristic evaluation had not been performed and had confirmed the value of the usage scenario. Also, there is no guarantee that all the observed problems would in fact have been found if there had been no prior heuristic evaluation. Now, we knew what to look for, but we might not have noticed as many problems if the user test had been our first usability evaluation activity for this interface.
If the user test were to be credited with all 17 duplicate problems as well as the four new ones, taking the higher-than-average severity of the seventeen problems into account, the benefit of the user test would be valued at $260,500. Of course, this amount would be the benefit from the user test only if no prior heuristic evaluation had been performed. Therefore, it would seem reasonable to charge this hypothetical analysis of the user test with some of the costs that were in fact spent preparing for the heuristic evaluation. Specifically, referring to Table 5, we will add the costs of assessing the appropriate way to use the method, having the outside evaluation expert learn about the domain and scenario, preparing the scenario, and preparing the software, as well as half the time spent writing the problem descriptions (since about half as many problems were found). These activities sum to 24 hours, or an additional cost of $2,400, for a total estimated cost of running the user test without prior heuristic evaluation of $4,650. This translates into a benefit/cost ratio of 56.
To provide a fair comparison, it should be noted that the benefit/cost ratio of performing the heuristic evaluation with only four evaluators would have been 53. This number is larger than the benefit/cost ratio for the full evaluation since more previously unfound usability problems are identified by the first evaluators than by the last, as shown by (EQ 3). Furthermore, the heuristic evaluation provided severity estimates that can be used to prioritize the fixing of the usability problems in the further development process, and the availability of this data probably adds to the actual value of the method as measured by delivered usability. If the time spent on the debriefing and severity ratings is deducted from the time spent on the heuristic evaluation, the benefit/cost ratio for the full eleven evaluators becomes 59 and the ratio for four evaluators becomes 71.
Thus, within the uncertainty of these estimates, it appears that user testing and heuristic evaluation have comparable cost-benefit ratios, and that doing some of each may have additional value.
5 The Evolution of Usability Engineering in Organizations
Two of the fundamental slogans of discount usability engineering are that ""any data is data"" and ""anything is better than nothing"" when it comes to usability. Therefore, I often advocate an approach to usability that focuses on getting started to use a minimum of usability methods. Even so, there are many projects that would benefit from employing more than the minimum amount of discount usability methods. I used the term ""guerrilla HCI"" in the title of this chapter because I believe that simplified usability methods can be a way for a company to gradually build up its reliance on systematic usability methods, starting with the bare minimum and gradually progressing to a more refined lifecycle approach.
Based on observing multiple companies and projects over the years, I have arrived at the following series of steps in the increased use of usability engineering in software development.

Usability does not matter.  The main focus is to wring every last bit of performance from the iron. This is the attitude leading to the world-famous error message, ""beep.""
Usability is important, but  good interfaces can surely be designed by the regular development staff  as part of their general system design. This attitude is symbolized by the famous statement made by King Frederik VI of Denmark on February 26, 1835: ""We alone know what serves the true welfare and benefit of the State and People."" At this stage, no attempt is made at user testing or at acquiring staff with usability expertise.
The desire to have the  interface blessed by the magic wand  of a usability engineer. Developers recognize that they may not know everything about usability, so they call in a usability specialist to look over their design and comment on it. The involvement of the usability specialist is often too late to do much good in the project, and the usability specialist often has to provide advice on the interface without the benefit of access to real users.
GUI panic strikes  , causing a sudden desire to learn about user interface issues. Currently, many companies are in this stage as they are moving from character-based user interfaces to graphical user interfaces and realize the need to bring in usability specialists to advise on graphical user interfaces from the start. Some usability specialists resent this attitude and maintain that it is more important to provide an appropriate interface for the task than to blindly go with a graphical interface without prior task analysis. Even so, GUI panic is an opportunity for usability specialists to get involved in interface design at an earlier stage than the traditional last-minute blessing of a design that cannot be changed much. (Update added 1999: these days, this stage is often characterized by  Web Panic Strikes  . It's the same phenomenon and should be treated the same way.)
Discount usability engineering  sporadically  used.  Typically, some projects use a few discount usability methods (like user testing or heuristic evaluation), though the methods are often used too late in the development lifecycle to do maximum good. Projects that do use usability methods often differ from others in having managers who have experienced the benefit of usability methods on earlier projects. Thus, usability acts as a kind of virus, infecting progressively more projects as more people experience its benefits.
Discount usability engineering  systematically  used.  At some point in time, most projects involve some simple usability methods, and some projects even use usability methods in the early stages of system development. Scenarios and cheap prototyping techniques seem to be very effective weapons for guerrilla HCI in this stage.
Usability group and/or usability lab founded.  Many companies decide to expand to a deluxe usability approach after having experienced the benefits of discount usability engineering. Currently, the building of  usability laboratories [Nielsen 1994a] is quite popular as is the formation of dedicated groups of usability specialists.
Usability permeates lifecycle.  The final stage is rarely reached since even companies with usability groups and usability labs normally do not have enough usability resources to employ all the methods one could wish for at all the stages of the development lifecycle. However, there are some, often important, projects that have usability plans defined as part of their early project planning and where usability methods are used throughout the development lifecycle.

This model is fairly similar to the series of organizational acceptance stages outlined by Ehrlich and Rohn [1994] but was developed independently. Stage 1-2 in the above list correspond to Ehrlich and Rohn's skepticism stage, stage 3-4 correspond to their curiosity stage, stage 5-6 correspond to their acceptance stage, and stage 7-8 correspond to their partnership stage.
Many teachers of usability engineering have described the almost religious effect it seems to have the first time students try running a user test and see with their own eyes the difficulties perfectly normal people can have using supposedly ""easy"" software. Unfortunately, organizations are more difficult to convert, so they mostly have to be conquered from within by the use of guerrilla methods like discount usability engineering that gradually show more and more people that usability methods work and improve products. It is too optimistic to assume that one can move a development organization from stage 1 or 2 in the above model to stage 7 or 8 in a single, sweeping change. In reality, almost all usability methods are extremely cheap to use compared to the benefits they provide in form of better and easier to use products, but often we have to start with the cheapest possible methods to overcome the intimidation barrier gradually.
 
Acknowledgments
The author would like to thank Raldolph Bias, Tom Landauer, and Janice Rohn for helpful comments on an earlier version of the manuscript.
References

Apple Computer (1987). Human Interface Guidelines: The Apple Desktop Interface. Addison Wesley, Reading, MA.
Apple Computer (1992). Macintosh Human Interface Guidelines. Addison Wesley, Reading, MA.
Bellotti, V. (1988). Implications of current design practice for the use of HCI techniques. In Jones, D.M. and Winder, R. (Eds.), People and Computers IV, Cambridge University Press, Cambridge, U.K., 13-34.
Boehm, B. W. (1981). Software Engineering Economics. Prentice-Hall, Englewood Cliffs, NJ.
Card, S. K., Moran, T. P., and Newell, A. (1983). The Psychology of Human-Computer Interaction, Lawrence Erlbaum Associates, Hillsdale, NJ.
Carroll, J. M., and Rosson, M. B. (1990). Human-computer interaction scenarios as a design representation. Proc. HICSS-23: Hawaii International Conference on System Science, IEEE Computer Society Press, 555-561.
Clarke, L. (1991). The use of scenarios by user interface designers. In Diaper, D., and Hammond, N. (Eds.), People and Computers VI, Cambridge University Press, Cambridge, U.K. 103-115.
Ehrlich, K., and Rohn, J. (1994). Cost-justification of usability engineering: A vendor's perspective. In Bias, R.G., and Mayhew, D.J. (Eds.), Cost-Justifying Usability. Academic Press, Boston, MA.
Gould, J. D., and Lewis, C. H. (1985). Designing for usability: Key principles and what designers think. Communications of the ACM 28, 3 (March), 300-311.
Gray, W. D., John, B. E., and Atwood, M. E. (1992). The precis of project Grace, or, an overview of a validation of GOMS. Proc. ACM CHI'92 (Monterey, CA, 3-7 May 1992), 307-312.
Jørgensen, A.H. (1989). Using the thinking-aloud method in system development. In Salvendy, G. and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems. Elsevier Science Publishers, Amsterdam, 743-750.
Karwowski, W., Kosiba, E., Benabdallah, S., and Salvendy, G. (1989). Fuzzy data and communication in human-computer interaction: For bad or for good. In Salvendy, G. and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems. Elsevier Science Publishers, Amsterdam, 402-409.
Landauer, T. K. (1988). Research methods in human-computer interaction. In Helander, M. (Ed.), Handbook of Human-Computer Interaction. North-Holland, Amsterdam, The Netherlands. 543-568.
Mantei, M. M., and Teorey, T.J. (1988). Cost/benefit analysis for incorporating human factors in the software lifecycle, Communications of the ACM 31, 4 (April), 428-439.
Milsted, U., Varnild, A., and Jørgensen, A.H. (1989). Hvordan sikres kvaliteten af brugergr¾nsefladen i systemudviklingen (""Assuring the quality of user interfaces in system development,"" in Danish), Proceedings NordDATA'89 Joint Scandinavian Computer Conference (Copenhagen, Denmark, 19-22 June), 479-484.
Molich, R., and Nielsen, J. (1990). Improving a human-computer dialogue, Communications of the ACM 33, 3 (March), 338-348.
Monk, A., Wright, P., Haber, J., and Davenport, L. (1993). Improving Your Human-Computer Interface: A Practical Technique. Prentice Hall International, Hemel Hempstead, U.K.
Nielsen, J. (1989a). Prototyping user interfaces using an object-oriented hypertext programming system, Proc. NordDATA'89 Joint Scandinavian Computer Conference (Copenhagen, Denmark, 19-22 June), 485-490.
Nielsen, J. (1989b). Usability engineering at a discount. In Salvendy, G., and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems, Elsevier Science Publishers, Amsterdam. 394-401.
Nielsen, J. (1990a). Big paybacks from 'discount' usability engineering, IEEE Software 7, 3 (May), 107-108.
Nielsen, J. (1990b). Paper versus computer implementations as mockup scenarios for heuristic evaluation, Proc. INTERACT'90 3rd IFIP Conf. Human-Computer Interaction (Cambridge, U.K., 27-31 August), 315-320.
Nielsen, J. (1992a). The usability engineering life cycle. IEEE Computer 25, 3 (March), 12-22.
Nielsen, J. (1992b). Evaluating the thinking aloud technique for use by computer scientists. In Hartson, H. R. and Hix, D. (Eds.), Advances in Human-Computer Interaction Vol. 3, Ablex, Norwood, NJ. 75-88.
Nielsen, J. (1992c). Finding usability problems through heuristic evaluation. Proc. ACM CHI'92 (Monterey, CA, 3-7 May), 373-380.
Nielsen, J. (1993).  Usability Engineering. Academic Press, Boston, MA.
Nielsen, J. (1994a).  Usability laboratories. Behaviour & Information Technology 13, 1.
Nielsen, J. (1994b). Heuristic evaluation. In Nielsen, J., and Mack, R.L. (Eds.),  Usability Inspection Methods. John Wiley & Sons, New York, NY.
Nielsen, J., and Landauer, T. K. (1993). A mathematical model of the finding of usability problems. Proc. ACM INTERCHI'93 Conf. (Amsterdam, the Netherlands, 24-29 April), 206-213.
Nielsen, J., and Levy, J. (1994). Measuring usability - preference vs. performance.  Communications of the ACM   37  , 4 (April), 66-75.
Nielsen, J., and Molich, R. (1989). Teaching user interface design based on usability engineering, ACM SIGCHI Bulletin 21, 1 (July), 45-48
Nielsen, J., and Molich, R. (1990). Heuristic evaluation of user interfaces, Proc. ACM CHI'90 (Seattle, WA, 1-5 April), 249-256.
Nielsen, J., and Phillips, V. L. (1993). Estimating the relative usability of two interfaces: Heuristic, formal, and empirical methods compared. Proc. ACM INTERCHI'93 Conf. (Amsterdam, the Netherlands, 24-29 April), 214-221.
Nielsen, J., Frehr, I., and Nymand, H. O. (1991). The learnability of HyperCard as an object-oriented programming system, Behaviour and Information Technology 10, 2 (March-April), 111-120.
Perlman, G. (1988). Teaching user interface development to software engineers, Proc. Human Factors Society 32nd Annual Meeting, 391-394.
Perlman, G. (1990). Teaching user-interface development, IEEE Software 7, 6 (November), 85-86.
Telles, M. (1990). Updating an older interface, Proc. ACM CHI'90 (Seattle, WA, 1-5 April), 243-247.
Thovtrup, H., and Nielsen, J. (1991). Assessing the usability of a user interface standard, Proc. ACM CHI'91 (New Orleans, LA, 28 April-2 May), 335-341.
Tognazzini, B. (1990). User testing on the cheap, Apple Direct 2, 6 (March), 21-27. Reprinted as Chapter 14 in   TOG on Interface, Addison-Wesley, Reading, MA, 1992.
Voltaire, F. M. A. (1764). Dictionnaire Philosophique.
Whiteside, J., Bennett, J., and Holtzblatt, K. (1988). Usability engineering: Our experience and evolution. In Helander, M. (Ed.), Handbook of Human-Computer Interaction, North-Holland, Amsterdam, 791-817.
"
121,1993-11-01,"Introduction
It has long been recognized  [Refs. 1,2,3,4]  that user interfaces should be designed iteratively in almost all cases because it is virtually impossible to design a user interface that has no usability problems from the start. Even the best usability experts cannot design perfect user interfaces in a single attempt, so a usability engineering lifecycle should be built around the concept of iteration.  [Ref. 5] 
Iterative development of user interfaces involves steady refinement of the design based on user testing and other evaluation methods. Typically, one would complete a design and note the problems several test users have using it. These problems would then be fixed in a new iteration which should again be tested to ensure that the ""fixes"" did indeed solve the problems and to find any new usability problems introduced by the changed design. The design changes from one iteration to the next are normally local to those specific interface elements that caused user difficulties. An iterative design methodology does not involve blindly replacing interface elements with alternative new design ideas. If one has to choose between two or more interface alternatives, it is possible to perform comparative testing to measure which alternative is the most usable, but such tests are usually viewed as constituting a different methodology than iterative design as such, and they may be performed with a focus on measurement instead of the finding of usability problems. Iterative design is specifically aimed at refinement based on lessons learned from previous iterations.
The user tests presented in this article were rigorous, with a fairly large number of test subjects who were measured carefully in several different ways while performing a fixed set of tasks for each system. In many practical usability engineering situations,  [Ref. 6]  it is possible to gain sufficient insight into the usability problems in a given design iteration by running a small number of test subjects, and it may not be necessary to collect quantitative measurement data. The quantitative measures that are emphasized in this article may be useful for management purposes in larger projects but they are not the main goal of usability evaluations. Instead, the main outcome of a usability evaluation in a practical development project is a list of usability problems and suggestions for improvements in the interface.
After a general discussion of the nature of iteration and usability metrics, this article presents four examples of iterative user interface design and measurement. The first example is discussed in greater depth than the others to provide some insight into how the various usability parameters are measured and further refined.
The Benefits of Iteration
The value of iteration in a usability engineering process is illustrated by a commercial development project analyzed by Karat.  [Ref. 7]  The user interface to a computer security application was tested and improved through three versions as discussed further below. A lower bound estimate of the value of the user interface improvements comes from calculating the time saved by users because of the shorter task completion times, thus leaving out any value of the other improvements. The number of users of the security application was 22,876 and each of them could be expected to save 4.67 minutes from using version three rather than version one for their first twelve tasks (corresponding to one day's use of the system), for a total time savings of 1,781 work hours. Karat estimates that this corresponds to saved personnel costs of $41,700 which compares very favorably with the increased development costs of only $20,700 due to the iterative design process. The true savings are likely to be considerably larger since the improved interface was also faster after the first day.

Figure 1.   Interface quality as a function of the number of design iterations: Measured usability will normally go up for each additional iteration, until the design potentially reaches a point where it plateaus. 
Figure 1 shows a conceptual graph of the relation between design iterations and interface usability. Ideally, each iteration would be better than the previous version, but as shown later in this article, this is not always true in practice. Some changes in an interface may turn out not to be improvements after all. Therefore, the true usability curve for a particular product would not be as smooth as the curve in Figure 1. Even so, the general nature of iterative design can probably be expected to be somewhat like Figure 1, even though it must be admitted that we do not yet have sufficiently many documented case studies to estimate the curve precisely. The first few iterations can probably be expected to result in major gains in usability as the true ""usability catastrophes"" are found and fixed. Later iterations have progressively smaller potential for improvements as the major usability problems are eliminated, and the design may eventually become so polished that very little potential for further improvement would remain. Because of the small number of documented cases, we do not yet know where this point of diminishing returns is in terms of number of iterations. It is not even known for sure whether there is in fact an upper limit on the usability of a given interface or whether it would be possible to improve some interfaces indefinitely with continued substantial gains for each iteration.
Assuming that one stays with the same basic interface and keeps refining it, my personal intuition is that there would be a limit to the level of usability one could achieve. I do believe, however, that one can often break such limits by rethinking the interface and base it on a completely new design as more experience is gained over time. For example, the task of ""programming a computer"" has been reconceptualized several times, with languages changing from octal machine code to mnemonic assembler to higher-level programming languages. Figure 1 shows a reconceptualization after a long period of stabile usability for a system, but we do not really know what leads to the creative insights necessary for a fundamentally novel and better interface design. Interface reconceptualizations may not always be immediately followed by increased usability as indicated by Figure 1 since a fundamental redesign may introduce unexpected usability problems that would have to be ironed out by a few additional iterations.
Unfortunately, interface reconceptualizations in individual development projects are mainly known through anecdotal evidence and have not been documented or had their usability impact measured. One exception is the development of an electronic white pages system by a large telephone company. The system was intended to allow customers with home computers to search for telephone numbers where search terms could be matched either exactly or phonetically and with the possibility of expanding the search to a larger geographical area. Unfortunately, even after the iterative design had been through 14 versions, test users still indicated that they were intimidated and frustrated by the system. At this stage of the iterative design it was decided to abandon the basic interface design which had been based on a command-line interface. The interface was reconceptualized based on the insight that the command line made the structure of the system invisible to the user. The interface was redesigned around a menu of services displaying all options at all times, thus making them visible and presumably easier to understand. Indeed, user satisfaction on a 1-5 rating scale (where 5=""like very much"") increased from 3.4 for the command-line based version to 4.2 for the menu-based version, indicating a drastic improvement in usability for the reconceptualized interface.
Usability Metrics and Overall Usability
The overall quality of a system is actually a sum of many quality attributes, only one of which is usability. Additionally, a system should of course be socially acceptable and be practically feasible with respect to cost, maintainability, etc. Furthermore, the system should fulfill additional requirements such as fitting the user's job needs and allowing users to produce high-quality results since that is the reason for having the system at all. I will not consider these issues here since they are related to  utility  : whether the functionality of the system in principle can do what is needed. This article focuses on usability as the question of  how well  users can use that functionality. Note that the concept of ""utility"" does not necessarily have to be restricted to work-oriented software. Educational software has high utility if students learn from using it, and an entertainment product has high utility if it is fun to use.
In spite of simplified conceptual illustrations like Figure 1, usability is not really a one-dimensional property of a user interface.  Usability has many aspects, and is often associated with the following 5 attributes:  [Ref. 6] 

Easy to learn:  The user can quickly go from not knowing the system to getting some work done with it.
Efficient to use:  Once the user has learned the system, a high level of productivity is possible.
Easy to remember:  The infrequent user is able to return to using the system after some period of not having used it, without having to learn everything all over.
Few errors:  Users do not make many errors during the use of the system, or if they do make errors they can easily recover from them. Also, no catastrophic errors should occur.
Pleasant to use:  Users are subjectively satisfied by using the system; they like it.

Focusing in on usability as a quality goal, we thus define it in terms of these five quality attributes. For each of these quality attribute, one then needs to define a metric to numerically characterize the attribute. In the usability field, these metrics are commonly defined in terms of the performance of a randomly chosen set of representative users while using the system to do a set of representative tasks, and the metrics are thus dependent on these benchmark tasks. Finally, usability measures are empirically derived numbers resulting from the application of one or more metrics to a concrete user interface. This progression from quality goals over quality attributes and their metrics to the actual measures thus makes usability steadily more concrete and operationalized.
From a usability engineering perspective, one should not necessarily pay equal attention to all usability attributes. Some are more important than others and should be the focus of the development efforts throughout the iterative design. In some cases one might even accept lower scores on certain usability metrics as long as their values remain above some minimally acceptable value.  [Ref. 8]  Each of the case studies discussed below had their own priorities with regard to the different attributes of usability and other projects might have yet other goals. It is an important early part of the usability lifecycle to set priorities for the usability attributes and decide on which ones will deserve the main consideration in a given project.
User efficiency is a measure of how fast users can use the computer system to perform their tasks. Note that user efficiency is a different concept than traditional computer response time, even though faster response times will often allow users to complete their tasks faster. Often, user efficiency is seen as the most important attribute of usability because of its direct bottom-line impact. For example, NYNEX estimates that each one second reduction in work time per call for its toll and assistance operators will save three million U.S. dollars per year.  [Ref. 9]  Other usability attributes may be critical for certain applications such as control rooms for dangerous plants or processes where an especially low frequency of user errors is desired.
Normally, the values of the chosen usability metrics are measured empirically through user testing. There are research projects aiming at analytical methods for estimating certain usability metrics, and they have had some success at dealing with user performance for expert users  [Ref. 9]  and the transfer of user learning from one system to another. These analytical methods are substantially weaker at estimating error rates and are unable to address the subjective ""pleasant to use"" dimension of usability. In any case, practical development projects are probably best advised to stay with user testing for the time being since the analytical methods can be rather difficult to apply.  [Ref. 10]  Often, about ten test users are used for usability measurements, though tests are sometimes conducted with twenty or more users if tight confidence intervals on the measurement results are desired. It is important to pick test users who are as representative as possible of the actual people who will be using the system after its release. When repeated user tests are performed as part of iterative design, one should use different users for each test to eliminate transfer of learning from previous iterations.
It is normally quite easy to conduct user testing with novice users in order to measure learnability and initial performance, error rates, and subjective satisfaction. Usability measures for expert users are unfortunately harder to come by, as are measures of the ability of experienced users to return to the system after a period of absence. The basic problem is the need for users with actual experience in using the system. For some user interfaces, full expertise may take several years to acquire, and even for simpler interfaces, one would normally need to train users for several weeks or months before their performance plateaued out at the expert level. Such extended test periods are of course infeasible during the design of new products, so expert performance tends to be much less studied than novice performance. Two ways of obtaining expert users without having to train them are to involve expert users of any prior release of the product and to use the developers themselves as test users. Of course, one should keep in mind that a developer has a much more extensive understanding of the system than a user would have, so one should always test with some real users also.
Quantifying Relative Changes in Usability
In order to compare the different iterative design projects later in this article, a uniform way of scoring usability improvements is desirable. The approach used here is to normalize all usability measures with respect to the values measured for the initial design. Thus, the initial design is by definition considered to have a normalized usability value of 100, and the subsequent iterations are normalized by dividing their measured values by those measured for the initial design. For example, if users made four errors while completing a set task with version one and three errors with version two of an interface, the normalized usability of interface two with respect to user errors was 133% of the usability of interface one, indicating that a 33% improvement in usability had been achieved. In just the same way, the normalized usability of interface two would still be 133% if users made eight errors with interface one and six errors with interface two.
A particular problem is how to convert measures of subjective satisfaction into improvement scores, since subjective satisfaction is often measured with questionnaires and rating scales. It would not be reasonable just to take the ratio of the raw rating scale scores. Doing so would mean, say, that a system rated 2 on a 1-5 scale would be twice as good as a system rated 1 on a 1-5 scale, but also that a system rated 2 on a 1-7 scale would be twice as good as a system rated 1 on a 1-7 scale, even though it is clearly easier to achieve a rating of 2 on a 1-7 scale than on a 1-5 scale.
The underlying problem is that the rating scales are not ratio scales, so it is theoretically impossible to arrive at a perfect formula to calculate how much better one rating is relative to another. Assuming that the goal of usability engineering is purely economical, it might be possible to gather empirical data to estimate how various subjective satisfaction ratings translate to increased sales of a product or increased employee happiness (and one could then further measure the contribution to a company's profits from having happier employees). I am not aware of any such data, so I proceeded with calculating relative subjective satisfaction scores as well as possible, using a method that had proven successful in a larger study of subjective preference measures,  [Ref. 11]  even though the resulting formula is admittedly not perfect. This transformation method is discussed further in the box.
The case studies in this article all used a given set of test tasks to measure usability throughout the iterative design process. This was possible because they had their basic functionality defined before the start of the projects. In contrast, some development projects follow a more exploratory model where the tasks to be performed with a system may change during the development process as more is learned about the users and their needs. For such projects, it may be more difficult to use a measurement method like the one outlined here, but the general results with respect to the improvements from iterative design should still hold.

Transforming Subjective Ratings to a Ratio Scale
To ensure uniform treatment of all subjective rating scale data, the following three transformations were applied to the raw rating scale scores:


The various rating scales were linearly rescaled to map onto the interval from -1 to +1 with zero as the midpoint and +1 as the best rating. This transformation provided a uniform treatment of scales independent of their original scale intervals.
The  arcsine  function was applied to the rescaled scores. This transformation was intended to compensate for the fact that rating scales are terminated at each end, making it harder for an average rating to get close to one of the ends of the scales. For example, an average rating of 4.6 on a 1-5 scale might result from four users rating the system 4 and six users rating it 5.0. Now, even if all ten users like some other system better and would like to increase their rating by one point, the second system would only get an average rating of 5.0 (an increase of 0.4 instead of 1.0 as ""deserved"") because six of the users were already at the end of the scale. Because of this phenomenon, changes in ratings towards the end of a rating scale should be given extra weight, which is achieved by the way the  arcsine  function stretches the ends of the interval.
The exponential function  e   x  was applied to the stretched scores to achieve numbers for which ratios were meaningful. Without such a transformation, it would seem that, say, a score of 0.2 was twice as good as a score of 0.1, whereas a score of 0.7 would only be 17% better than a score of 0.6, even though the improvement was the same in both cases. After the exponential transformation, a score of 0.2 is 10.5% better than a score of 0.1, and a score of 0.7 is also 10.5% better than a score of 0.6.

Admittedly, the choice of functions for these transformations is somewhat arbitrary. For example, an exponential function with another basis than  e  would have achieved the same effect while resulting in different numbers. Remember, however, that the purpose of the present analysis is to compare  relative  ratings of the various interface iterations, and that the same transformations were applied to each of the original ratings. Nowhere do we use the value of a single transformed rating; we always look at the ratio between two equally transformed ratings, and all the transformations are of course monotonic. Also, an analysis of a larger body of subjective satisfaction data indicated that the use of these transformations resulted in reasonable statistical characteristics.  [Ref. 11] 
Calculating Scores for Overall Usability
It is often desirable to have a single measure of overall usability of a user interface. Typically, interface designers can choose between several alternative solutions for various design issues and would like to know how each alternative impacts the overall usability of the resulting system. As another example, a company wanting to choose a single spreadsheet as its corporate standard would need to know which of the many available spreadsheets was the most usable. In both examples, usability would then need to be traded off against other considerations such as implementation time for the design alternatives and purchase price for the spreadsheets.
Ultimately, the measure of usability should probably be monetary in order to allow comparisons with other measures such as implementation or purchase expenses. For some usability metrics, economic equivalents are fairly easy to derive, given data about the number of users, their loaded salary costs, and their average work day. Loaded salaries include not just the money paid out to employees but also any additional costs of having them employed, such as social security taxes and medical and pension benefits. For example, I recently performed a usability evaluation of a telephone company application. Improvements in the user interface based on this study were estimated to result in a reduction in training time of half a day per user and a speedup in expert user performance of 10%. Given the number of users and their loaded salaries, these numbers translate to savings of $40,000 from reduced training costs and about $500,000 from increased expert user performance in the first year alone. In this example, the overall value of the usability improvements is obviously dominated by expert user performance -- especially since the system will probably be used for more than one year.
Other usability metrics are harder to convert into economic measures. User error rates can in principle be combined with measures of the seriousness of each error and estimates of the impact on corporate profits from each error. The impact of some errors is easy to estimate: For example, an error in the use of a print command causing a file to be printed on the wrong printer would have a cost approximately corresponding to the time needed for the user to discover the error and to walk to the wrong printer to retrieve the output. An error causing a cement plant to burn down would have a cost corresponding to the value of the plant plus any business lost while it was being rebuilt. In other cases, error costs are harder to estimate. For example, an error causing a customer to be shipped a different product from that ordered would have direct costs corresponding to the administrative costs of handling the return plus the wasted freight charges. However, there could also very well be much larger indirect costs if the company got a reputation of being an unreliable supplier causing customers to prefer doing business elsewhere.
Finally, subjective satisfaction is perhaps the usability attribute that has the least direct economic impact in the case of software for in-house use, even though it may be one of the most important factors influencing individual purchases of shrinkwrap software. Of course, it is conceptually easy to consider the impact of increased user satisfaction on software sales or employee performance, but actual measurements are difficult to make and were not available in the case studies discussed below.
Due to the difficulties of using monetary measures of all usability attributes, an alternative approach has been chosen here. Overall usability is calculated in relative terms as the geometric mean (the  n  'th root of the product) of the normalized values of the relative improvements in the individual usability metrics. Doing so involves certain weaknesses: First, all the usability metrics measured for a given project are given equal weight even though some may be more important than others. Second, there is no use of cut-off values where any further improvement in a given usability metric would be of no practical importance. For example, once error rates have been reduced to, say, one error per ten thousand user transactions, it may make no practical difference to reduce them further. Even so, reducing the error rate to, say, one error per twenty thousand transactions would double the usability score for the user error metric as long as no cut-off value has been set for user errors. Using geometric means rather than arithmetic means (the sum of values divided by the number of values) to calculate the overall usability somewhat alleviates this latter problem as a geometric mean gives comparatively less weight to uncommonly large numbers. The geometric mean increases more by improving all the usability metrics a little than by improving a single metric a lot and leaving the others stagnant.
Case Studies in Iterative User Interface Design
The following sections present data from the four case studies in iterative user interface design summarized in Table 1. The first example is described in somewhat greater depth than the others in order to provide some insight into how the various usability metrics are measured and further refined into estimates of improvements in overall usability.

Table 1.   Four case studies of iterative design. 









Name of System
Interface Technology
Versions Tested
Subjects per Test
Overall Improvement


Home banking
Personal-computer graphical user interface
5
8
242%


Cash register
Specialized hardware with character-based interface
5
9
87%


Security
Mainframe character-based interface
3
9
882%


Hypertext
Workstation graphical user interface
3
10
41%



Home Banking
The home banking system was a prototype of a system to allow Danish bank customers access to their accounts and other bank services from a home computer using a modem. The interface was explicitly designed to explore the possibilities for such a system when customers owned a personal computer supporting a graphical user interface and a mouse.
The prototype system was developed on a single personal computer platform under the assumption that alternative versions for other personal computers with graphical user interfaces would use similar designs and have similar usability characteristics. This assumption was not tested, however. Prototyping aims at generating a running user interface much faster than standard programming by accepting some limitations on the generality of the code. As one such prototyping limitation, the system was implemented on a stand-alone personal computer without actual on-line access to bank computers. Instead, the user interface only provided access to a limited number of accounts and other pre-defined database information stored on the personal computer.
The relevant usability attributes for this application include user task performance, the users' subjective satisfaction, and the number of errors made by users. Task performance is less important for this application than for many others, as the users will not be paid employees. Even so, users will be running up telephone bills and taking up resources on the central computer while accessing the system, so task performance is still of interest as a usability attribute. Subjective satisfaction may be the most important usability attribute, as usage of a home banking system is completely discretionary, and since the reputation for being ""user friendly"" (or rather, ""customer friendly"") is an important competitive parameter for a bank. User errors are also important, but most important of all is probably the avoidance of usage catastrophes, defined as situations where the user does not complete a task correctly. User errors are certainly unpleasant and should be minimized, but it is considerably worse when a user makes an error without realizing it, and thus without correcting it.
Each test user was asked to perform a specified set of tasks with the system, accessing some of the dummy accounts that had been set up for the prototype. In general, it is important to pick benchmark tasks that span the expected use of the system since it would be easy for a design to score well on a single, limited task while being poor for most other tasks.
Four sets of tasks were used:

Basic tasks operating on the customer's own accounts
	
Find out the balance for all your accounts.
Transfer an amount from one of your accounts to the other.
Investigate whether a debit card transaction has been deducted from the account yet.


Money transfers to accounts owned by others
	
Order an electronic funds transfer to pay your March telephone bill.
Set up a series of electronic funds transfers to pay monthly installments for a year on a purchase of a stereo set.
Investigate whether the telephone bill transfer has taken place yet.


Foreign exchange and other rare tasks
	
Order Japanese Yen corresponding to the value of 2,000 Danish Kroner.
Order 100 Dutch Guilders.
Order an additional account statement for your savings account.


Special tasks
	
Given the electronic funds transfers you have set up, what will the balance be on August 12th? (Do not calculate this manually; find out through the system).
You returned the stereo set to the shop, so cancel the remaining installment payments.



Table 2 shows the measures of task time for each of the four task types measured in the user testing of the home banking system. The best way of calculating a score for task efficiency would be to weigh the measured times for the individual sub-tasks relative to the expected frequency with which users would want to perform those sub-tasks in field use of the system. To do this perfectly, it would be necessary to perform true, contextual field studies of how consumers actually access their bank account and other bank information. Unfortunately, it is rarely easy to use current field data to predict frequencies of use for features in a future system, because the very introduction of the system changes the way the features are used.

Table 2.   Absolute values of the constituent measurements of the ""task time"" usability metric for the home-banking system (all times in seconds).
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Basic Tasks
			(own account)
Transfers to Others' Accounts
Foreign Exchange
Special Tasks
Total Task Time


1
199
595
245
182
1,220 


2
404
442
296
339
1,480 


3
221
329
233
317
1,090 


4
206
344
225
313
1,088 


5
206
323
231
208
967 



For the home banking example, it is likely that most users would access their own account more frequently than they would perform foreign currency tasks or the special tasks. On the other hand, some small business users of this kind of system have been known to rely extensively on a foreign exchange feature, so a proper weighting scheme is by no means obvious. In this article, I will simply assume that all four tasks are equally important and should thus have the same weight. This means that task efficiency is measured by the total task time which is simply the sum of the times for the four task groups in Table 2.
Regular errors and catastrophes were measured by having an experimenter count the number of each, observed as the users were performing the specified tasks. Subjective satisfaction was measured by giving the test users a two-question questionnaire after they had performed the tasks with the system:
How did you like using the bank system? 

Very pleasant
Somewhat pleasant
Neutral
Somewhat unpleasant
Very unpleasant

If you had to perform a task that could be done with this system, would you prefer using the system or would you contact the bank in person? 

Definitely use the system
Likely use the system
Don't know
Likely contact the bank
Definitely contact the bank

The overall subjective satisfaction score was computed as the average of numeric scores for the user's replies to these two questions. The raw data for the subjective satisfaction and error metrics is listed in Table 3.

Table 3.   Absolute values of the usability parameters for the home-banking system. Subjective satisfaction was measured on a 1 to 5 scale, where 1 indicated the highest satisfaction.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Subjective Satisfaction
			(1-5 scale; 1 best)
Errors Made per User
Catastrophes per User


1
1.92
9.2
2.56


2
1.83
4.3
1.00


3
1.78
2.5
0.44


4
1.86
2.5
0.43


5
1.67
1.5
0.17



The usability test showed that the first version of the user interface had serious usability problems. Both error rates and catastrophes were particularly high. Even so, users expressed a fairly high degree of subjective satisfaction which might have been due to the pleasing appearance of graphical user interfaces compared with the types of banking interfaces they were used to. One major problem was the lack of explicit error messages (the system just beeped when the users made an error), which made it hard for the users to learn from their mistakes. Another major problem was the lack of a help system. Also, many dialog boxes did not have ""cancel"" buttons, so users were trapped once they had chosen a command. Furthermore, several menu options had names that were difficult to understand and were easy to confuse (for example the two commands ""Transfer money"" and ""Move an amount""), and there were several inconsistencies. For example, users had to type in account numbers as digits only (without any dashes), even though dashes were used to separate groups of digits in listings of account numbers shown by the computer.
(One of the anonymous referees of this article questioned whether this example was real or contrived: ""Would anyone  really  design a dialog box without a Cancel button?"" I can confirm that the example is real, and that the designer indeed had forgotten this crucial dialogue element in the first design. Not only did this supposedly trivial problem occur for real in this case study, it occurred again for another designer in another design case I am currently studying. Also, several commercial applications have been released with similar problems.)
To reduce the number of catastrophes, confirming dialog boxes were introduced in version two to allow the user to check the system's interpretation of major commands before they were executed. For example, one such dialog read,  ""Each month we will transfer 2,000.00 Kroner from your savings account to The LandLord Company Inc. The first transfer will be made April 30, 1992. OK/Cancel.""  Also, written error messages in alert boxes replaced the beep, and a special help menu was introduced.
Version one had used three main menus: Functions, Accounts, and Information. The first menu included commands for ordering account statements, for initiating electronic funds transfers, and for viewing a list of funds transfers. The second menu included commands for viewing an account statement, for transferring money between the user's own accounts, and for viewing a list of the user's accounts and their current balance. The third menu included commands for showing current special offers from the bank and for accessing foreign currency exchange rates. The test had shown that the users did not understand the conceptual structure of this menu design but instead had to look through all three menus every time they wanted to activate a command. Therefore, the menu structure was completely redesigned for version two, with only two main menus: Accounts (for all commands operating on the user's accounts) and Other (for all other commands).
The user test of version two showed that users had significantly fewer problems with finding commands in the menus and that they also made fewer errors. At the same time, the help system was somewhat confusingly structured, and users wasted a fair amount of time reading help information which they did not need to complete their current tasks. As can be seen from Table 3, version two did succeed in its major goal of reducing the very high levels of user errors and catastrophes from version one, but as shown by Table 2, this improvement was made at the cost of slower task completion for most tasks. Transfers to others' accounts were faster in version two due to the elimination of errors when users typed account numbers the way they were normally formatted.
For version three, the help system was completely restructured. Also, several of the error messages introduced in version two were rewritten to make them constructive and better inform users how they can correct their errors.
To further reduce the error rates, version four identified the user's own accounts by their account type (e.g. checking account or savings account instead of just their account number). Also, the dialog box for transferring money was expanded with an option to show the available balance on the account from which the transfer was to be made. Furthermore, a minor change was made to the confirmation dialog box for electronic funds transfers to prevent a specific error that had been observed to occur fairly frequently with users not entering the correct date for transfers that were to be scheduled for future execution. As mentioned above, the confirming dialog normally stated the date on which the transfer was to be made, but when the transfer was specified to happen immediately, the text was changed to read ""The transfer will take place today."" Using the word ""today"" rather than listing a date made the message more concrete and easy to understand and also made it different from the message used for future transfers. Note how this iterative design further modified an interface change that had been introduced in version two.
In the final version, version five, the help system was completely restructured for the second time. The help system introduced in version three was structured according to the menu commands in the system and allowed users to get help about any command by selecting it from a list. The revised help system presented a single conceptual diagram of the tasks that were supported by the system, linking them with a list of the available menu commands, and providing further levels of information through a simplistic hypertext access mechanism.
In addition to the changes outlined here, a large number of smaller changes were made to the user interface in each iteration. Almost no user interface elements survived unchanged from version one to version five.
Table 4 finally shows the normalized values for the four usability metrics as well the overall usability computed as their geometric mean. It can be seen that most usability metrics improved for each iteration but that there also were cases where an iteration scored worse than its predecessor on some metric. As mentioned, many of these lower scores led to further redesigns in later versions. Also, there were many smaller instances of iterative changes that had to be further modified due to observations made during the user testing, even if they were not large enough to cause measurable decreases in the usability metrics.

Table 4.   Normalized improvements in usability parameters for the home-banking system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Efficiency
			(inverse task time)
Subjective Satisfaction
Correct Use
			(inverse error frequency)
Catastrophe Avoidance
Overall Usability Improvement


1
0
0
0
0
0 


2
-18%
+6%
+114%
+156%
+48% 


3
+12%
+9%
+268%
+582%
+126% 


4
+12%
+4%
+513%
+595%
+155% 


5
+26%
+17%
+513%
+1,406%
+242% 



A Cash Register System
The cash register system was a point of sales application for a chain of men's clothes stores in Copenhagen. A computerized cash register with a built-in alphanumeric screen allowed sales clerks to perform tasks like selling specific pieces of merchandise with payment received as cash, debit card, credit card, check, foreign currencies converted according to the current exchange rate, and gift certificates, as well as combinations of these, and also accepting returned goods, issuing gift certificates, and discounting prices by a specified percentage.
The usability metrics of interest for this application again included time needed for the users to perform 17 typical tasks, subjective satisfaction, and errors made while completing the tasks. Furthermore, an important metric was the number of times the users requested help from a supervisor while performing the task. This last metric was especially important because the users would often be temporary sales staff taken on for sales or holiday shopping seasons where the shop would be very busy and such help requests would slow down the selling process and inconvenience shoppers. The values measured for these usability metrics for each of the five versions of the system are shown in Table 5.

Table 5.   Absolute values of usability metrics for the cash-register system. Subjective satisfaction was measured on a 1 to 5 scale, where 1 indicated the highest satisfaction.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 









Version
Time on Task
			(seconds)
Subjective Satisfaction
			(1-5 scale; 1 best)
Errors Made per User During 17 Tasks
Help Requests per User During 17 Tasks


1
2,482
2.38
0.0
2.7


2
2,121
1.58
0.1
1.1


3
2,496
1.62
0.2
1.2


4
2,123
1.45
0.2
0.9


5
2,027
1.69
0.0
0.4



Table 6 shows the normalized improvements in usability for each iteration. Since the error rate was zero for the first version,  any  occurrence of errors would in principle constitute an infinite degradation in usability. However, as mentioned above, very small error rates (such as one error per 170 tasks, which was the measured error rate for version two) are not really disastrous for an application like the one studied here, so a 0.1 increase in error rate has been represented in the table as a 10% degradation in usability. One can obviously argue whether another number would have been more reasonable, but as it turns out, the error rate for the last version was also zero, so the method chosen to normalize the error rates does not impact on the conclusion with respect to overall improvement due to the total iterative design process.

Table 6.   Normalized improvements in usability parameters for the cash-register system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Efficiency
			(inverse task time)
Subjective Satisfaction
Correct Use
			(inverse error frequency)
Help Avoidance (inverse help requests)
Overall Usability Improvement


1
0
0
0
0
0 


2
+17%
+61%
-10%
+145%
+43% 


3
-1%
+56%
-20%
+125%
+29% 


4
+17%
+77%
-20%
+200%
+49% 


5
+22%
+49%
0%
+575%
+87% 



The main changes across the iterations regarded the wording of the field labels that were changed several times to make them more understandable. Also, some field labels were made context sensitive in order to improve their fit with the user's task, and fields that were not needed for given tasks were removed from the screen when users were performing those tasks. Several shortcuts were introduced to speed up common tasks, and these shortcuts also had to be modified over subsequent iterations.
A Security Application
The security application  [Ref. 7]  was the sign-on sequence for remote access to a large data entry and inquiry mainframe application for employees at the branch offices of a major computer company. Users already used their alphanumeric terminals to access an older version of the system, and a main goal of the development of the new system was to ensure that the transition between the two systems happened without disrupting the users in the branch offices. Therefore, the most important usability attribute was the users' ability to successfully use the new security application to sign on to the remote mainframe without any errors. Other relevant attributes were the time needed for users to sign on and their subjective satisfaction. Three usability metrics were measured: User performance was measured as time needed to complete twelve sign-ons on the system, success rate was measured as proportion of users who could sign on error free after the third attempt, and the users' subjective attitudes towards the system was measured as the proportion of test users who believed that the product was good enough to deliver without any further changes. Table 7 shows the result of measuring these metrics for the three versions of the interface using test users who had experience with the old system but not with the new interface.

Table 7.   Absolute values of usability metrics for three iterations of a computer security application in a major computer company.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Time to Complete Task
			(minutes)
Success Rate
			(percent)
Subjective Satisfaction (percent)


1
5.32
20
0


2
2.23
90
60


3
0.65
100
100



The table shows substantial improvements on all three usability metrics. A further test with an expert user showed that the optimum time to complete a sign-on for the given system was 6 seconds. The test users were able to complete their twelfth sign-on attempt in 7 seconds with the third iteration of the interface, indicating that there would not be much room for further improvement without changing the fundamental nature of the system. Table 8 shows the normalized improvements in usability for the security application over its three versions.

Table 8.   Normalized improvements in usability metrics for the security application. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 









Version
Efficiency
			(inverse task time)
Success Rate
Subjective Satisfaction
Overall Usability Improvement


1
0
0
0
0 


2
+139%
+350%
+448%
+298% 


3
+718%
+400%
+2,214%
+882% 



A Hypertext System
The hypertext system  [Ref. 12]  was an interface designed to facilitate access to large amounts of text in electronic form. Typical applications include technical reference manuals, online documentation, and scientific journal articles converted into online form. The system ran on a standard workstation using multiple windows and various other graphical user interface features such as the ability for the user to click on a word in the text with the mouse to find more information about that word.
For the use of this hypertext system for technical reference texts, the most important usability attributes were the search time for users to find information about their problems as well as the search accuracy (the proportion of times they found the correct information). Both of these attributes were measures by having test users answer 32 given questions about a statistics software package on the basis of a hypertext version of its reference manual. The results of these tests for the three versions of the hypertext system are shown in Table 9.

Table 9.   Absolute values of usability metrics for the hypertext system. (A version numbering scheme starting with version 0 has been used in other papers about this system, but for internal consistency in this document, I gave the first version the number 1 in this table.).
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 







Version
Search Time
			(seconds)
Search Accuracy
			(seconds)


1
7.6
69


2
5.4
75


3
4.3
78



A hypertext system for technical reference texts will obviously have to compete with text presented in traditional printed books. The developers of the hypertext system therefore conducted a comparative test where a group of test users were asked to perform the same tasks with the printed version of the manual.  [Ref. 12]  The search time for paper was 5.9 minutes with a search accuracy of 64%, indicating that paper actually was better than the initial version of the hypertext system. This example clearly shows that iterative design should not just try to improve a system with reference to itself, but should also aim at arriving at better usability characteristics than the available competition.
Table 10 shows the normalized improvements in usability for the hypertext system. Many of the changes from version 1 to version 2 were based on encouraging users to use a search strategy that had proven effective by making it both easier to use and more explicitly available. For version 3, several additional changes were made, including an important shortcut that automatically performed a second step that users almost always did after a certain first step in the initial tests. Both improvements illustrate the important ability of iterative design to mold the interface according to user strategies that do not become apparent until after testing has begun. Users always find new and interesting ways to use new computer systems, so it is not enough to make an interface usable according to preconceived notions about how they will be used.

Table 10.   Normalized improvements in usability parameters for the hypertext system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Search Efficiency
			(inverse task time)
Search Accuracy
Overall Usability Improvement


1
0
0
0 


2
+41%
+9%
+24% 


3
+77%
+13%
+41% 



Conclusions
The median improvement in overall usability from the first to the last version for the case studies discussed here was 165%. A study of 111 pairwise comparisons of user interfaces found that the median difference between two interfaces that had been compared in the research literature was 25%.  [Ref. 11]  Given that the mean number of iterations in the designs discussed here was three, one might expect the improvement from first to last interface to be around 95% based on an average improvement of 25% for each iteration. As mentioned, the measured improvement was larger, corresponding to an average improvement of 38% from one version to the next.
(Three iterations, each improving usability by 25%, lead to an improvement from version one to version four of 95% rather than 75%, due to a compounding effect similar to that observed when calculating interest on a bank account across several years.)
There is no fundamental conflict between the estimate of 25% average usability difference between interfaces compared in the research literature and 38% difference between versions in iterative design. Picking the best of two proposed interfaces to perform the same task is actually a very primitive usability engineering method that does not allow one to combine appropriate features from each of the designs. In contrast, iterative design relies much more on the intelligence and expertise of the usability specialist throughout the design process and allows designers to combine features from previous versions based on accumulated evidence of what works under what circumstances. It is thus not surprising that iterative design might lead to larger improvements between versions than that found simply by picking the best of two average design alternatives.
Of course, the 38% improvement between versions in iterative design should only be considered a rough estimate as it was only based on four case studies. Furthermore, there is a large variability in the magnitude of usability improvement from case to case, so one should not expect to realize 38% improvements in all cases. Finally, one should not expect to be able to sustain exponential improvements in usability across iterations indefinitely. If a very large number of iterations were to be used, usability improvements would probably follow a curve somewhat like Figure 1, since it is probably not possible to achieve arbitrary improvements in usability just by iterating sufficiently many times. It is currently an open research question how much usability can be improved and how good an ""ultimate user interface"" can get, since practical development projects always stop iterating before perfection is achieved.
The median improvement from version 1 to version 2 was 45%, whereas the median improvement from version 2 to version 3 was ""only"" 34%. In general, one can probably expect the greatest improvements from the first few iterations as usability catastrophes get discovered and removed. It is recommended to continue beyond the initial iteration, however, since one sometimes introduces new usability problems in the attempt to fix the old ones. Also, user testing may turn up new interface strategies that would have to be refined through further iterations.
The projects discussed in this article included several cases where at least one usability attribute had reduced scores from one version to the next. Also, it was often necessary to redesign a given user interface element more than once before a usable version had been found. Three versions (two iterations) should probably be the minimum in an iterative design process. Of course, as in our cash register case study, the third version will sometimes test out as worse than the second version, so one cannot always follow a plan to stop after version three. The actual results of the iterative design and testing must guide the decisions on how to proceed in each individual project.
Update added 2006: 
By way of comparison, across a large number of  much newer Web design projects I studied, the average improvement in measured usability in a redesign was 135%.
See also my paper on  parallel design for a way to kickstart an iterative design project by testing multiple alternative design suggestions at the same time during the first iteration.

Acknowledgments
This paper was written while the author was a member of the Applied Research Division of Bell Communications Research (Bellcore). Data on the home banking and cash register projects analyzed in this article was collected while the author was on the faculty of the Technical University of Denmark and these systems do not represent Bellcore products. The author would like to thank his students, Jens Rasmussen and Frederik Willerup, for helping collect data on these projects. The author would like to thank Susan T. Dumais, Michael C. King, and David S. Miller as well as several anonymous  Computer  referees for helpful comments on this article.
Originally published in IEEE Computer Vol. 26, No. 11 (November 1993), pp. 32–41. 
References

Bury, K.F. The iterative development of usable computer interfaces. In Proceedings of  IFIP INTERACT'84 International Conference on Human-Computer Interaction  (London, U.K., 4-7 September 1984), 743-748.
Buxton, W., and Sniderman, R. Iteration in the design of the human-computer interface. In  Proceedings of the 13th Annual Meeting of the Human Factors Association of Canada, 1980, pp. 72-81.
Gould, J.D., and Lewis, C.H. Designing for usability: Key principles and what designers think.  Communications of the ACM   28, 3 (March 1985), 300-311.
Tesler, L. Enlisting user help in software design.  ACM SIGCHI Bulletin   14, 3 (January 1983), 5-9.
Nielsen, J. The usability engineering life cycle.  IEEE Computer   25, 3 (March 1992), 12-22.
Nielsen, J.   Usability Engineering. Academic Press, San Diego, CA, 1993.
Karat, C.-M. Cost-benefit analysis of iterative usability testing. In  Proceedings of IFIP INTERACT'90 Third International Conference on Human-Computer Interaction  (Cambridge, U.K., 27-31 August 1990), pp. 351-356.
Good, M., Spine, T.M., Whiteside, J., and George, P. User-derived impact analysis as a tool for usability engineering. In  Proceedings of ACM CHI'86 Conference on Human Factors in Computing Systems  (Boston, MA, 13-17 April 1986), pp. 241-246.
Gray, W.D., John, B.E., Stuart, R., Lawrence, D., and Atwood, M.E. GOMS meets the phone company: Analytic modeling applied to real-world problems. In  Proceedings of IFIP INTERACT'90 Third International Conference on Human-Computer Interaction  (Cambridge, U.K., 27-31 August 1990), pp. 29-34.
Carroll, J.M., and Campbell, R.L. Softening up hard science: Reply to Newell and Card.  Human-Computer Interaction   2, 3 (1986), 227-249.
Nielsen, J., and Levy, J. Measuring usability - preference vs. performance.  Communications of the ACM   37, 4 (April 1994), 66-75.
Egan, D.E., Remde, J.R., Gomez, L.M., Landauer, T.K., Eberhardt, J., and Lochbaum, C.C. Formative design-evaluation of SuperBook.  ACM Transactions on Information Systems   7, 1 (January 1989), 30-57.
"
122,1993-08-01,"Redmond, WA, 21-23 July 1993
The second annual UPA conference sponsored by the  Usability Professionals Association was hosted by Microsoft on their well-groomed corporate campus outside of Seattle. Having the conference at an actual member company site continued the tradition started by the first conference which was hosted by WordPerfect in Utah. I highly appreciated the opportunity to see the environment in which so much software is produced and where the everyday activities of the usability professionals take place.
The UPA conference had grown to 374 participants this year from about 140 last year, so if the growth rate keeps up even one or two more years, it will soon become impossible for the conference to be hosted at a single company. Anyway, those of us who have had the experience these last two years have benefited tremendously from the hospitality of the host organizations. In addition to taking in the general ambiance and the ubiquitous fridges stocked with free soft drinks, the conference benefited from a  usability lab tour. In fact, only a few rooms were included on the tour since  Microsoft has an extensive facility spread over two buildings.
Compared with the WordPerfect facility we toured last year, the individual rooms in  Microsoft's usability lab  were smaller but there were more of them. In general, the impression was that of a facility set up for large-scale testing of many products with an attempt to minimize overhead. For example, the control room could be run by a single person who combined the roles of video tape producer, experimenter, and event logger. This was possible because the video taping mostly relied on fixed camera positions that could be determined in advance of the test instead of being changed during the user's work with the system. Event logging was done by a home-made software tool that allowed the observer to select encodings of the user's activities from dynamically expanding hierarchical menus. These codes were synchronized with the video record, allowing the retrieval of video clips of specific events even though the video was currently being kept on analog video tapes instead of being directly integrated with the test data in a multimedia database. It is typical of the state of the art in the CAUSE field (CAUSE = computer-aided usability engineering) that this logging tool was home-made for this specific laboratory rather than being a generally available system.
The conference was very rich in terms of topics and insights, but there were two issues that struck me as being prevalent in several of the presentations: the need for improved communication between usability evaluation and system design, and the advantages of faster and cheaper usability testing as well as other ""discount usability engineering"" methods aimed at getting usability results fed back to the design cycle in a matter of days rather than weeks or months.
For many years, usability professionals have been complaining that developers and system designers don't pay sufficient attention to usability. Jared Spool from User Interface Engineering (a consulting company) addressed this issue in the opening plenary, saying that developers were not inherently evil, so if they did not understand us, it was probably our fault for not communicating clearly enough. In other words, he suggested that we might be wise to take some of our own medicine and consider the developers as the ""users"" of usability engineering results, remembering our standard premise that if the users don't understand something is not because they are stupid but because the system is wrong.
The main approach to improving communication with developers seem to be to involve them directly in usability engineering activities. For example, Michael Kahn and colleagues from Hewlett-Packard had designed a new  usability inspection method that was mostly based on involving developers as the interface evaluators. One major reason for the success of usability inspections at HP was the ability to piggyback onto the existing methods for code inspection and other forms of inspection that were already widely used and respected in the company. One more form of inspection was seen as reasonable and as a comprehensible activity which developers and managers were motivated to undertake. For this very reason, HP used the term ""usability inspection"" combined with standard terminology from code inspection in order to allow participants to feel on familiar ground. Within the usability engineering field, however, the term ""usability inspection"" is used as a generic term to encompass methods like heuristic evaluation, cognitive walkthroughs, standards inspection, etc., so I prefer using the term ""formal usability inspections"" for the HP method to highlight its differences from many other methods.
Formal usability inspections rely heavily on the use of a separate moderator to manage the inspection meetings and to plan them in advance. Basically, the inspectors are given instructions prepared by the moderator and the ""owner"" of the interface with profiles of the main user categories and descriptions of some typical task scenarios. The inspectors then work through the scenarios on their own, noting usability problems at each step. They do so by applying a combination of several other methods, including a set of heuristics like those used in heuristic evaluation combined with a user goal-action-feedback model that can be viewed as a simplified version of cognitive walkthroughs. Finally, the inspectors meet with the moderator and combine their lists of usability problems. This last stage is a major educational experience for many of the developers who participate as inspectors, since they are exposed to a large number of usability problems which they had not found themselves. Thus, in addition to achieving an enhanced list of usability problems, the coordination meeting serves to emphasize the phenomenon that different people find different usability problems and that your own design intuitions are insufficient to cover the ground. A typical formal inspection of a mid-sized project takes about 142 staff-hours, which is somewhat more than the time needed for a heuristic evaluation which may only take on the order of 40-80 hours due to its less formal nature. Even so, the more formal approach was appreciated by the engineers.
As another example of involving developers to increase communication, Mary Beth Butler (now Rettger) from Lotus (now MathWorks) reported on a team method for categorizing usability problems. During user testing, she had some members of the development team sit in the background and note the usability problems they observed on index cards. Each developer would observe a few tests, giving as many developers as possible the chance to participate. The day after the tests, the usability staff and the participating developers would meet for a 90 minute debriefing session where all the index cards with usability problems were pasted onto the walls of a meeting room. Everybody would then move cards around until reasonable categories of usability problems emerged, after which the problem categories would be rated for severity. After the meeting, the usability specialist would write up a report with the categorized usability problems and any other data from the user test for distribution the following day.
By using this method, Lotus had succeeded in cutting the turnaround time for usability reports from about two weeks to two days, with some information getting to the development team the very day after the user test. In general, a major trend at the conference was the need to speed up usability work in order to impact design while it is happening. Kay Chalupnik from IDS Financial Services (an American Express company) mentioned that they needed instant evaluation turnaround to enable them to fax home the same night what was learned on a field study.
Since 1988, I myself have advocated  cutting user testing to about 3-5 subjects per test  (and then test more iterations). Even though one company said that they needed six subjects, the general feeling at the conference was that studies with larger numbers of test users were becoming rare. This approach was confirmed by Claus Neugebauer from SAP in Germany where they also used 3-5 users, since additional users only added about 15% to what they learned with three. Neugebauer also discussed several ways of  reducing the turnaround time  from the beginning of a usability study to the delivery of the final results. Most of the cuts stemmed from the notion of  constancy in usability testing  since they had many products that were similar in many ways. By always using the same questionnaires and other test instruments as well as holding the task outlines (if not the detailed tasks) constant, the preparation time for a study had been reduced from two weeks to two days, and data analysis had been cut from five to three days due to the ability to reuse templates in statistics and business graphics programs.
A further reason to speed up the usability process is to  increase the amount of actual usability data that is gathered. I have become concerned that usability specialists spend much more time writing reports, going to meetings, and such than they spend on actually creating new usability information. We might use the term ""exposure hours"" to denote the time spent with the users in user testing (but not setting up the test), inspecting the interface in heuristic evaluation (but not polishing the report), observing real users working in field studies (but not traveling to the customer site or talking with the users' managers), etc. From personal experience and from talking with people at the conference, it may well turn out that most usability specialists get  exposure hours that amount to no more than 10% of their working day. Better communication mechanisms and more efficient usability methods are needed to increase this percentage. Unfortunately, one problem with smaller studies and faster data reporting that was mentioned in questions is the difficulty of communicating usability results to other projects and people working on later releases several months or years later. A possible answer to these concerns might be an integrated CAUSE system for computer-aided usability engineering to handle design rationale hypertexts linking to usability test data that was collected automatically (or with minimal overhead) as well as other low-overhead reports that could make sense if they were given more context. Unfortunately, almost no research is underway on such environments.
The conference was also filled with practical advice and variations on common usability engineering issues. For example, Monty Hammontree from Sun presented a way to involve widely scattered users like system administrators in user testing without having to travel to their site. Instead of a traditional usability laboratory, Hammontree used a ""distributed usability lab"" running over a computer network, where an exact copy of the user's screen was slaved to the experimenter's screen and the user's comments were relayed by computerized audioconferencing. Nadine Nayak from HP mentioned that she had used a similar method to test international usability with users in Germany without leaving the U.S. (though she used a speakerphone rather than computerized voice transmission for the user's comments).
As a matter of fact, the difficulty of recruiting representative users for usability testing is so prevalent that a session was dedicated to a presentation by Liz Cowan from User Interface Engineering entitled ""stalking the wily test subject."" Out of the approximately 170 people in the room for that session, only 6 had a specialized person helping them with the job of  finding and recruiting test subjects  (I am happy to report that I was one of the 6, having a very competent person help me with that job). Again from the perspective of speeding up turnaround time for user testing, it really helps when one can just say ""I need 5 subjects of this-and-this kind"" instead of having to spend time coming up with a way of finding them. Microsoft had a database of 3,000 local users with various levels of experience in their products who were willing to come in for user testing, and normally each user was used a maximum of three times to avoid getting ""professional test users."" Cowan mentioned that the most powerful motivator for recruiting subjects often was the promise of exposure to cool technology in their field rather than cash payment though one should normally give the users something (either money or a T-shirt) to show good faith. Usually, Cowan needed slightly more than a week to find a set of inexperienced users for a test and as much as three weeks when specific kinds of experience was needed (e.g., experience running a special kind of printing press for color jobs). Even though managers were often inappropriate as test subjects for software intended for their staff, Cowan found that it was often a shortcut to convince a manager that the test was valuable since that could make it easy to get several subjects from that manager's staff. (See also later results from  two hundred companies' experience recruiting subjects for user tests)
In general, the conference was highly oriented towards practitioners with only a few researchers in evidence (though maybe they were just hiding since it was mentioned at the opening plenary that people without Ph.D.'s were often better at usability engineering). An interesting phenomenon was the presence of participants from at least seven major popular  computer magazines  (MacUser, MacWorld, PC/Computing, PC Magazine U.K., PC Week, PC World, Windows Magazine), many of which had multiple participants at the conference. Several of these magazines have recently started including usability test results or other usability engineering data in their review articles, and the rest are presumably thinking about doing so. PC/Computing has even instituted a ""PC/Computing Usability Seal of Approval"" awarded to products passing a specified level of usability on several quality criteria. One session was dedicated to discussing this trend based on a talk on PC World's experience entitled ""usability makes the difference to 4 million buyers""-referring to the number of readers who make their software purchasing decisions based on what they read in the reviews. Magazines conduct extensive surveys of their readers (who after all are their users), and PC World had found that readers indicated that usability was as important a review parameter as the more traditional issues of speed and features. Initial usability reviews were fairly informal (e.g., have a few users send a fax with fax software to get some usability anecdotes for a review), but recent tests have been more elaborate in employing usability labs (including borrowing an American Airlines simulator for a test of the usability of notebook computers on board airplanes). A major concern in reporting summative evaluations was the difficulty in reporting statistics like confidence intervals which are incomprehensible for the average reader. The solution so far has been to convey much of the same information in natural language to express the s trength of the editors' belief in their conclusions. In the two weeks following this session, I have already seen the word ""usability"" used in major headlines on the covers of two leading PC magazines.
In a session on consulting jobs, John Claudy from American Institutes for Research mentioned that they were often asked to perform summative evaluations for advertising purposes when a vendor wanted to claim that a certain user interface was X% more usable than the competition and wanted a credible source to cite. Initially, they had refused such contracts, but now they did the work under the condition that they could approve all advertising using their name. Of course, a company that loses a comparative usability test will just not advertise the result. A person from the audience commented that some companies do comparative benchmarking for internal use to see whether certain features or interface elements in competing products are any good, but again such results are of course not published, depriving the field of much valuable data. In this session, a distinction was made between consulting and contracting, where contracting involves doing regular work on a project (with the client's main benefit being the added flexibility of using outside staff instead of permanent staff) and consulting involves a more advisory role. Even though nobody wanted to reveal their actual  pricing  schedules, typical charges mentioned for usability contracting were $75 per hour and a $25,000 fixed price for usability testing with 20 subjects, whereas consultants are known to charge anywhere between $170 and $500 per hour. Of course, as mentioned above, most user tests involve significantly fewer than 20 subjects, and the cost of a ""discount user test"" with 3-5 users would be much lower than $25,000.
A fun part of the conference was a design competition organized by Jared Spool and Deborah Mrazek from Hewlett-Packard. Unfortunately, I was too busy with other obligations at the conference to actively participate in the competition, but about 70 people formed teams to design a customer-operated ordering kiosk for a hypothetical taco chain. After several elimination rounds, the three winning designs competed in an open session, where each of them was subjected to live user testing by subjects who were timed as they entered a rather large and complex order. The winner was the design that allowed the user to complete the order in the shortest amount of time. Of course, since the winning times were determined by a single test user for each design, there is no guarantee that the winning design would in fact be the fastest over a range of users, but then this competition was more in the spirit of a fun conference event than a commercial comparative evaluation. The design and testing were done with a  low-fidelity prototyping approach, using colored notepads for the buttons and signs on the kiosk, and using humans to simulate the computational processes. For example, one design used highlighting on the display to indicate the ""current item"" in the order (that could be modified by, e.g., requesting diet or light versions of the item), and this highlighting was simulated by having one of the designers hold a yellow transparency cutout over the line being ""highlighted."" Not only was the design competition entertaining for the audience and a motivating team-building exercise for the participants, it also taught many people the power of low-fidelity prototyping techniques compared with the elaborate computerized tools used by most usability groups.
In general, UPA'93 was a very enjoyable conference with a wealth of valuable information. Due to the increased number of participants and activity in the field, this year's conference instituted dual tracks making it impossible to take in the entire conference. Last year's conference was single-track and felt more like a large specialized workshop than a conference which I personally enjoyed more. On the other hand, it is hard to turn people away when they want to come and the field is in an explosive expansive phase.
All conference sessions were videotaped by Microsoft's extremely professional A/V crew, resulting in a set of 18 VHS video tapes that is available for $255.00 from American Production Services, 2247 15th Ave. W, Seattle, WA 98119, tel. (206) 282-1776, fax (206) 282-3535. Actually, the videos are cheaper than the conference registration fee, but then they do not include the great coffee and extensive networking opportunities offered to those physically present at the conference.
The list of videos (and thus the list of conference sessions) is as follows:

Volume 1, Opening Panel: An Exploration of the Hot Issues of Usability Don Ballman (Mead Data Central), Mary Dieli (Microsoft), Jakob Nielsen (Bellcore), Janice Rohn (SunSoft), Jared Spool (User Interface Engineering)
Volume 2, Practical Tips for Enhancing the Power of UI Groups in Software Development Organizations Bob Vallone (GO Corporation)
Volume 3, Discount Usability Engineering Six Years Later Jakob Nielsen (Bellcore)
Volume 4, Stalking the Wily Test Subject (how to get the right subjects) Liz Cowan (User Interface Engineering)
Volume 5, Tailoring Data Collection Methodologies to Meet the Research and Business Objectives of Usability Practitioners Bradford Hesse (American Institute for Research)
Volume 6, Usability Makes the Difference to Four Million Buyers (usability in the trade press) Dean Andrews, Thomas Grubb, and Greg Smith (PC World Magazine)
Volume 7, Using Low Fidelity Prototyping as a Usability Tool Jared Spool (User Interface Engineering)
Volume 8, Engineering Usability into the Lab Design Process Denise C.R. Benel (National Information Technology Center), Richard Horst (Man-Made Systems Corp.), Russell Benel (The Mitre Corp.)
Volume 9, Working with Many Masters (being an interface consultant) John Claudy (American Institute for Research), Larry Marine (IQ Cognitive Engineering), Joan Lasselle (Ramsey Inc.), Judith Ramey (University of Washington)
Volume 10, Structuring Usability within Organizations Sue Braun (Northwestern Mutual Life), Janice Rohn (SunSoft Inc.)
Volume 11, Usability Lab Tools Kent Sullivan (Microsoft), Nigel Bevan (National Physical Laboratory), Mary Beth Butler (Lotus), Monty Hammontree (SunSoft)
Volume 12, Methods for Investigating Usability During Early Product Design Judith Ramey (University of Washington), Stephanie Rosenbaum (Ted-Ed)
Volume 13, What's Your Problem (how to define what aspects of usability to look for) Amy Kanerva, Jill Rojek (Microsoft)
Volume 14, Usability Inspections at Hewlett-Packard Michael Kahn, Rose Marchetti, and Amanda Prail (Hewlett-Packard)
Volume 15, The Designer's View Mary Beth Butler (Lotus), Ken Dye (Microsoft), Mark Gowans (WordPerfect), Andrew Kwatinetz (Microsoft), Alice Mead (Lotus), Marshall McClintock (Microsoft), Jack Young (WordPerfect)
Volume 16, Why Test Documentation? JoAnn Hackos (Com Tech Services)
Volume 17, Usability Testing: It Seems to be a HIT (speeding up usability) Claus Neugebauer, Nicola Spielmann (Ergonomics Group SAP AG)
Volume 18, Closing Panel: Usability Horoscope of the 90s Kay Chalupnik (IDS American Express), John Claudy (American Institute for Research), Julie Humburg (Intuit Inc.), Jakob Nielsen (Bellcore), Irene Wong (Apple), Jack Young (WordPerfect)

"
123,1994-01-01,"One of the oldest jokes in computer science goes as follows:
Q:   How many programmers does it take to change a light bulb? 
A:   None; it is a hardware problem! 
When asking how many usability specialists it takes to change a light bulb, the answer might well be four: Two to conduct a field study and task analysis to determine whether people really need light, one to observe the user who actually screws in the light bulb, and one to control the video camera filming the event. It is certainly true that one should study user needs before implementing supposed solutions to those problems. Even so, the perception that anybody touching usability will come down with a bad case of budget overruns is keeping many software projects from achieving the level of usability their users deserve.
1 The Intimidation Barrier
It is well known that people rarely use the recommended usability engineering methods [Nielsen 1993; Whiteside et al. 1988] on software development projects in real life. This includes even such basic usability engineering techniques as early focus on the user, empirical measurement, and iterative design which are used by very few companies. Gould and Lewis [1985] found that only 16% of developers mentioned all three principles when asked what one should do when developing and evaluating a new computer system for end users. Twenty-six percent of developers did not mention a single of these extremely basic principles. A more recent study found that only 21% of Danish software developers knew about the thinking aloud method and that only 6% actually used it [Milsted et al. 1989]. More advanced usability methods were not used at all.
One important reason usability engineering is not used in practice is the cost of using the techniques. Or rather, the reason is the perceived cost of using these techniques, as this chapter will show that many usability techniques can be used quite cheaply. It should be no surprise, however, that practitioners view usability methods as expensive considering, for example, that a paper in the widely read and very respected journal Communications of the ACM estimated that the ""costs required to add human factors elements to the development of software"" was $128,330 [Mantei and Teorey 1988]. This sum is several times the total budget for usability in most smaller companies, and one interface evangelist has actually found it necessary to warn such small companies against believing the CACM estimate [Tognazzini 1990]. Otherwise, the result could easily be that a project manager would discard any attempt at usability engineering in the belief that the project's budget could not bear the cost. Table 1 shows the result of adjusting a usability budget according to the discount usability engineering method discussed below. The numbers in Table 1 are for a medium scale software project (about 32,000 lines of code). For small projects, even cheaper methods can be used, while really large projects might consider additional funds to usability and the full-blown traditional methodology, though even large projects can benefit considerably from using discount usability engineering.

Table 1 
	Cost savings in a medium scale software project by using the discount usability engineering method instead of the more thorough usability methods sometimes recommended.


Original usability cost estimate by [Mantei and Teorey 1988]
$128,330


Scenario developed as paper mockup instead of on videotape
- $2,160


Prototyping done with free hypertext package
- $16,000


All user testing done with 3 subjects instead of 5
- $11,520


Thinking aloud studies analyzed by taking notes instead of by video taping
- $5,520


Special video laboratory not needed
- $17,600


Only 2 focus groups instead of 3 for market research
- $2,000


Only 1 focus group instead of 3 for accept analysis
- $4,000


Questionnaires only used in feedback phase, not after prototype testing
- $7,200


Usability expert brought in for heuristic evaluation
+ $3,000


Cost for ""discount usability engineering"" project
$65,330



British studies [Bellotti 1988] indicate that many developers don't use usability engineering because HCI (human-computer interaction) methods are seen as too time consuming and expensive and because the techniques are often intimidating in their complexity. The ""discount usability engineering"" approach is intended to address these two issues. Further reasons given by Bellotti were that there were sometimes no perceived need for HCI and a lack of awareness about appropriate techniques. These two other problems must be addressed by education [Perlman 1988, 1990; Nielsen and Molich 1989] and propaganda [Nielsen 1990a], but even for that purpose, simpler usability methods should help. Also, time itself is on the side of increasing the perceived need for HCI since the software market seems to be shifting away from the ""features war"" of earlier years [Telles 1990]. Now, most software products have more features than users will ever need or learn, and Telles [1990] states that the ""interface has become an important element in garnering good reviews"" of software in the trade press.
As an example of ""intimidating complexity,"" consider the paper by Karwowski et al. [1989] on extending the GOMS model [Card et al. 1983] with fuzzy logic. Note that I am not complaining that doing so is bad research. On the contrary, I find it very exciting to develop methods to extend models like GOMS to deal better with real-world circumstances like uncertainty and user errors. Unfortunately, the fuzzy logic GOMS and similar work can easily lead to intimidation when software people without in-depth knowledge of the HCI field read the papers. These readers may well believe that such methods represent ""the way"" to do usability engineering even though usability specialists would know that the research represents exploratory probes to extend the field and should only serve as, say, the fifth or so method one would use on a project. There are many simpler methods one should use first [Nielsen 1992a, 1993].
I certainly can be guilty of intimidating behavior too. For example, together with Marco Bergman, I recently completed a research project on iterative design where we employed a total of 99 subjects to test various versions of a user interface at a total estimated cost of $62,786. People reading papers reporting on this and similar studies might be excused if they think that iterative design and user testing are expensive and overly elaborate procedures. In fact, of course, it is possible to use considerably fewer subjects and get by with much cheaper methods, and we took care to say so explicitly in our paper. A basic problem is that with a few exceptions, published descriptions of usability work normally describe cases where considerable extra efforts were expended on deriving publication-quality results, even though most development needs can be met in much simpler ways.
As one example, consider the issue of statistical significance. I recently had a meeting to discuss usability engineering with the head of computer science for one of the world's most famous laboratories, and when discussing the needed number of subjects for various tests, he immediately referred to the need for test results to be statistically significant to be worth collecting. Certainly, for much research, you need to have a high degree of confidence that your claimed findings are not just due to chance. For the development of usable interfaces, however, one can often be satisfied by less rigorous tests.
Statistical significance is basically an indication of the probability that one is not making the wrong conclusion (e.g., a claim that a certain result is significant at the p<.05 level indicates that there is a 5% probability that it is false). Consider the problem of choosing between two alternative interface designs [Landauer 1988]. If no information is available, you might as well choose by tossing a coin, and you will have a 50% probability of choosing the best interface. If a small amount of user testing has been done, you may find that interface A is better than interface B at the 20% level of significance. Even though 20% is considered ""not significant,"" your tests have actually improved your chance of choosing the best interface from 50/50 to 4-to-1, meaning that you would be foolish not to take the data into account when choosing. Furthermore, even though there remains a 20% probability that interface A is not better than interface B, it is very unlikely that it would be much worse than interface B. Most of the 20% accounts for cases where the two interfaces are equal or where B is slightly better than A, meaning that it would almost never be a really bad decision to choose interface A. In other words, even tests that are not statistically significant are well worth doing since they will improve the quality of decisions substantially.
2 The Discount Usability Engineering Approach
Usability specialists will often propose using the best possible methodology. Indeed, this is what they have been trained to do in most universities. Unfortunately, it seems that ""le mieux est l'ennemi du bien"" (the best is the enemy of the good) [Voltaire 1764] to the extent that insisting on using only the best methods may result in having no methods used at all. Therefore, I will focus on achieving ""the good"" with respect to having some usability engineering work performed, even though the methods needed to achieve this result are definitely not ""the best"" method and will not give perfect results.
It will be easy for the knowledgable reader to put down the methods proposed here with various well-known counter-examples showing important usability aspects that will be missed under certain circumstances. Some of these counter-examples are no doubt true and I do agree that better results can be achieved by applying more careful methodologies. But remember that such more careful methods are also more expensive -- often in terms of money, and always in terms of required expertise (leading to the intimidation factor discussed above). Therefore, the simpler methods stand a much better chance of actually being used in practical design situations and they should therefore be viewed as a way of serving the user community.
The ""discount usability engineering"" [Nielsen 1989b, 1990a, 1993] method is based on the use of the following three techniques:

Scenarios
Simplified thinking aloud
Heuristic evaluation

Additionally, the basic principle of early focus on users should of course be followed. It can be achieved in various ways, including simple visits to customer locations.
2.1 Scenarios
Scenarios are a special kind of prototyping as shown in Figure 1. The entire idea behind prototyping is to cut down on the complexity of implementation by eliminating parts of the full system. Horizontal prototypes reduce the level of functionality and result in a user interface surface layer, while vertical prototypes reduce the number of features and implement the full functionality of those chosen (i.e. we get a part of the system to play with).

Figure 1 
	The concept of a  scenario  compared to vertical and horizontal prototypes as ways to make rapid prototyping simpler.






Scenarios take prototyping to the extreme by reducing both the level of functionality and the number of features. By reducing the part of interface being considered to the minimum, a scenario can be very cheap to design and implement, but it is only able to simulate the user interface as long as a test user follows a previously planned path.
Since the scenario is small, we can afford to change it frequently, and if we use cheap, small thinking aloud studies, we can also afford to test each of the versions. Therefore scenarios are a way of getting quick and frequent feedback from users.
Scenarios can be implemented as  paper mock-ups [Nielsen 1990b] or in simple prototyping environments [Nielsen 1989a] that may be easier to learn than more advanced programming environments [Nielsen et al. 1991]. This is an additional savings compared to more complex prototypes requiring the use of advanced software tools.
2.2 Simplified Thinking Aloud
Traditionally, thinking aloud studies are conducted with psychologists or user interface experts as experimenters who videotape the subjects and perform detailed protocol analysis. This kind of method certainly may seem intimidating for ordinary developers. However, it is possible to run user tests without sophisticated labs, simply by bringing in some real users, giving them some typical test tasks, and asking them to think out loud while they perform the tasks. Those developers who have used the thinking aloud method are happy about it [Jørgensen 1989, Monk et al. 1993], and my studies [Nielsen 1992b] show that computer scientists are indeed able to apply the thinking aloud method effectively to evaluate user interfaces with a minimum of training, and that even fairly methodologically primitive experiments will succeed in finding many usability problems.
I have long claimed that one learns the most from the first few test users, based on several case studies. In earlier papers, I have usually recommended using between three and five test users per test as a way of simplifying user testing while gaining almost the same benefits as one would get from more elaborate tests with large numbers of subjects. Recently, Tom Landauer and I developed a mathematical model of the number of usability problems [Nielsen and Landauer 1993], and when plugging in typical budget figures from different kinds of user testing, we derived curves like the ones shown in Figure 2 for the ratio between the benefits of user testing and the cost of the test for medium-sized development projects. The curves basically show that the benefits from user testing are much larger than the costs, no matter how many subjects are used. The maximum benefit-cost ratio is achieved when using between three and five subjects, confirming my earlier experience.

Figure 2 
	Cost-benefit trade-off curve for a ""typical"" project, varying the number of test users, using the model and average parameters described by Nielsen and Landauer [1993]. The curve shows the ratio of benefits to costs, that is, how many times the benefits are larger than the costs. For example, a benefit-to-cost ratio of 50 might correspond to costs of $10,000 and benefits of $500,000.






Besides reducing the number of subjects, another major difference between simplified and traditional thinking aloud is that data analysis can be done on the basis of the notes taken by the experimenter instead of by videotapes. Recording, watching, and analyzing the videotapes is expensive and takes a lot of time which is better spent on running more subjects and on testing more iterations of redesigned user interfaces. Video taping should only be done in those cases (such as research studies) where absolute certainty is needed. In discount usability engineering we don't aim at perfection anyway, we just want to find most of the usability problems, and a survey of 11 software engineers [Perlman 1988] found that they rated simple tests of prototypes as almost twice as useful as video protocols.
2.3 Heuristic Evaluation
Current user interface standards and collections of usability guidelines typically have on the order of one thousand rules to follow and are therefore seen as intimidating by developers. For the discount method I advocate cutting the complexity by two orders of magnitudes and instead rely on a small set of heuristics such as the  ten basic usability principles (listed on a separate page).
These principles can be presented in a single lecture and can be used to explain a very large proportion of the problems one observes in user interface designs. Unfortunately it does require some experience with the principles to apply them sufficiently thoroughly [Nielsen 1992c], so it might be necessary to spend some money on getting outside usability consultants to help with a heuristic evaluation. On the other hand, even non-experts can find many usability problems by heuristic evaluation and many of the remaining problems would be revealed by the simplified thinking aloud test. It can also be recommended to let several different people  perform a heuristic evaluation as different people locate different usability problems [Nielsen and Molich 1990]. This is another reason why even discount usability engineers might consider setting aside a part of their budget for outside usability consultants.
3 Validating Discount Usability Engineering
In one case, I used the discount usability engineering method to redesign a set of account statements [Nielsen 1989b]. I tested eight different versions (the original design plus seven redesigns) before I was satisfied. Even so, the entire project required only about 90 hours, including designing seven versions of twelve different kinds of statements (not all the forms were changed in each iteration, however) and testing them in simplified thinking aloud experiments. Most versions were tested with just a single user. To validate the redesign, a further experiment was done using traditional statistical measurement methods. It should be stressed that this validation was a research exercise and not part of the discount usability engineering method itself: The usability engineering work ended with the development of the improved account statements, but as a check of the usability engineering methods used, it was decided to conduct a usability measurement of one of the new designs compared with the original design.
3.1 Experiment 1: Double Blind Test Taking Usability Measurements
The validation was done using a double blind test: 38 experimenters each ran four subjects (for a total of 152 subjects) in a between-subjects design. Neither the experimenters nor the subjects knew which was the original account statement and which was the new. The results which are reported in Table 3 show clear and highly statistically significant improvements in measurement values for the new statement with respect to the understandability of the information in the statement as measured by the average number of correct answers to four questions concerning the contents of the statement. The value had indeed been the usability parameter which had been monitored as a goal during the iterative design. Two other usability parameters which had not been considered goals in the iterative design process (efficiency of use and subjective satisfaction) were also measured in the final test, and the two versions of the statement got practically identical scores on those.

Table 3 
	Result of Experiment 1: a double blind test (N=152) comparing the original and the revised version of a bank account statement. The values measured are: How many of the subjects could correctly answer each of four questions about the contents of the statement (and the combined average for those four questions), the average time needed by subjects to review the statement and answer the questions, and the subjects' average subjective rating (scale: 1 [bad] to 5 [good]).
	The rightmost column indicates whether the difference between the two account statements is statistically significant according to a  t  -test.


 
Original design
Revised design
Significance of Difference


""Size of deposit""
79%
95%
p  <.01


""Commission""
34%
53%
p  <.05


""Interest rates""
20%
58%
p  <.01


""Credit limit""
93%
99%
p  <.05


Average correct
56%
76%
p  <.01


Task time (sec.)
315
303
n.s. (  p  =.58)


Subjective satisfaction [1-5 scale]
2.8
3.0
n.s. (  p  =.14)



This study supports the use of discount usability engineering techniques and shows that they can indeed cause measurable improvements in usability. However, the results also indicate that one should be cautious in setting the goals for usability engineering work. Those usability parameters that have no goals set for improvement risk being left behind as the attention of the usability engineer is concentrated on the official goals. In this study, no negative effects in the form of actual degradation in measured usability parameters were observed but one can not always count on being so lucky.
3.2 Experiment 2: Recommendations from People without Usability Expertise
Two groups of evaluators were shown the two versions of the account statement (without being told which one was the revised version) and asked which one they would recommend management to use. All the evaluators were computer science students who had signed up for a user interface design course but who had not yet been taught anything in the course. This meant that they did not know the  usability heuristics which they might otherwise have used to evaluate the two versions.
Group A consisted of the experimenters from Experiment 1 (reported above) who had run two short experiments with each version of the account statement, while the evaluators in Group B had to make their recommendation on the basis of their own personal evaluation of the two versions. The results are reported in Table 4 and show a significant difference in the recommendations: Evaluators in Group A preferred the revised version 4 to 1 while evaluators in Group B were split equally between the two versions. This latter result is probably a reflection of the fact that the two versions are almost equally subjectively satisfying according to the measurement results reported in Table 3.

Table 4 
	Result of Experiment 2: asking two group of evaluators to recommend one of the two versions of an account statement. In Group A, each person had first run an empirical test with four subjects, whereas the evaluators in Group B had no basis for their recommendation except their own subjective evaluation.
	The difference between the two groups is statistically different at the  p  <.05 level.


 
Group A
Group B


 
N=38
N=21


Recommends original
16%
48%


Recommends revised
68%
48%


No recommendation
16%
5%



If we accept the statistical measurement results in Table 3 as defining the revised version as the ""best,"" we see that Group A was dramatically better at making the correct recommendation than Group B was. This was in spite of the fact that each of the individuals in Group A had knowledge only of the experimental results from two subjects for each of the designs (the aggregate statistics were not calculated until after the recommendations had been made, so each evaluator knew only the results from the four subjects run by that individual).
So we can conclude that running even a small, cheap empirical study can help non-human factors people significantly in their evaluation of user interfaces. If we count the evaluators who did not make a recommendation as having a 50/50 chance of picking the right interface, this experiment shows that running just two subjects for each version in a small test improved the probability for recommending the best of two versions from 50% to 76%.
4 Cost-Benefit Analysis of Heuristic Evaluation: A Case Study
A cost-benefit analysis of heuristic evaluation includes two main elements: First estimating the costs in terms of time spent performing the evaluation, and second estimating the benefits in terms of increased usability (less the development costs for the redesign). Since these estimates involve some uncertainties, they will be converted into dollar amounts by using round numbers. Any given company will of course have slightly different conversion factors, depending on its exact financial circumstances.
The following case study regards a prototype user interface for a system for internal telephone company use which will be called the Integrating System in this chapter. The Integrating System is fairly complicated and understanding its details requires extensive knowledge of telephone company concepts, procedures, and databases. Since a detailed explanation is not necessary to understand the generally applicable lessons from the study, the Integrating System will only be outlined here.
Briefly, the Integrating System provides a graphical user interface to access information from several systems running on various remote computers in a uniform manner despite the differences between the backend systems. The Integrating System can be used to resolve certain problems when data inconsistencies require manual intervention by a technician because the computer systems cannot determine which information is correct. The traditional method for resolving these problems involves having the technician compare information across several of these databases by accessing them through a number of traditional alphanumeric terminal sessions. The databases reside on different computers and have different data formats and user interface designs, so this traditional method is somewhat awkward and requires the technicians to learn a large number of inconsistent user interfaces.
Performing this task involves a large amount of highly domain-specific knowledge about the way the telephone system is constructed and the structure of the different databases. Technicians need to know where to look for what data and how the different kinds of data are related. Also, the individual data items themselves are extremely obscure for people without detailed domain knowledge.
As a result of the heuristic evaluation of this interface with 11 evaluators (described in further detail in [Nielsen 1994b]), 44 usability problems were found. Forty of these problems are denoted ""core"" usability problems and were found in the part of the interface that was subjected to intensive evaluation, whereas the remaining four problems were discovered in parts of the interface that we had not planned to study as part of the heuristic evaluation.
4.1 Time Expenditure
As usual in usability engineering, the cost estimates are the easiest to get right. Table 5 accounts for the total time spent on the heuristic evaluation project in terms of person-hours. No attempt has been made to distinguish between different categories of professional staff. Practically all the person-hours listed in Table 5 were spent by usability specialists. The only exception is a small number of hours spent by development specialists in getting the prototype ready for the evaluation and in attending the debriefing session.

Table 5 
	Estimate of the total number of person-hours spent on the heuristic evaluation study described in this article. The estimate of ""time to prepare the prototype"" does not include the time needed for the initial task analysis, user interface design, or implementation of the prototype since these activities had already been undertaken independently of the heuristic evaluation.


Assessing appropriate ways to use heuristic evaluation, 4 people @ 2 hours
8


Having outside evaluation expert learn about the domain and scenario
8


Finding and scheduling evaluators, 1.8 hours + 0.2 hours per evaluator
4


Preparing the briefing
3


Preparing scenario for the evaluators
2


Briefing, 1 system expert, 1 evaluation expert, 11 evaluators @ 1.5 hours
19.5


Preparing the prototype (software and its hardware platform) for the evaluation
5


Actual evaluation, 11 evaluators @ 1 hour
11


Observing the evaluation sessions, 2 observers @ 11 hours
22


Debriefing, 3 evaluators, 3 developers, 1 evaluation expert @ 1 hour
7


Writing list of usability problems based on notes from evaluation sessions
2


Writing problem descriptions for use in severity-rating questionnaire
6


Severity rating, 11 evaluators @ 0.5 hours
5.5


Analyzing severity ratings
2


Total
105



Note that the time given for the preparation of the scenario covers only the effort of writing up the scenario in a form that would be usable by the evaluators during the evaluation. Considerable additional effort was needed to specify the scenario in the first place, but that effort was part of the general task analysis and design activities performed before the evaluation. Scenario-based design is a well-known method for user interface design [Carroll and Rosson 1990, Clarke 1991], so one will often be able to draw upon interaction scenarios that have been developed in previous stages of the usability lifecycle. Even so, we were probably lucky that the scenario developed for the present system could be used for the evaluation with such a small amount of additional effort.
The evaluation sessions were videotaped, and approximately eight hours were spent on mundane tasks like getting videotapes, learning to operate the video equipment in the specific usability laboratory used for the evaluation sessions, setting up and closing down the video equipment on each of the two days of the study, rewinding tapes, etc. This videotaping was not part of the heuristic evaluation as such, and the tapes were not reviewed for the purpose of arriving at the list of usability problems. The observers' notes were sufficient for that purpose. The videotapes were used to some extent in this research analysis of the study where an additional eight hours were spent reviewing details of some evaluation sessions, but since this use was not part of the practical application of the heuristic evaluation method, the time spent on the videotapes has not been included in Table 5.
It follows from Table 5 that the total number of person-hours spent on the evaluation can be determined by the formula



Equation 1:
			time(  i  ) = 47.8 + 5.2  i 



where  i  is the number of evaluators. This formula is not exact for large values of  i  , since some of the effort devoted to room scheduling and to the analysis of the severity ratings is partly dependent on the number of evaluators and would change with large  i  s.
The cost estimate in (Equation 1) is probably larger than necessary for future heuristic evaluations. Major reductions in both the fixed and variable costs could be achieved by reducing the team of two observers to a single observer. This observer should be the person who is familiar with the application such that the observer can answer questions from the evaluators during the evaluation. Also, even though the observer should have a certain level of usability knowledge in order to understand the comments made by the evaluators, the observer need not be a highly skilled expert specializing in usability. A major difference between heuristic evaluation and traditional user testing is that an observer of a heuristic evaluation session is mostly freed from having to interpret user actions since the evaluators are assuming the task of explicitly identifying the usability problems. In contrast, the experimenter in a traditional user test would need a higher level of usability expertise in order to translate the subject's actions and difficulties into interface-related usability problems.
This single change would result in the following, revised formula



Equation 2:
			time(  i  ) = 37.3 + 4.2  i 



Transforming the time estimates in (Equation 1) or (Equation 2) to money estimates can be done fairly simply by multiplying the number of hours by an estimate of the loaded hourly cost of professional staff. Note that the salary and benefits costs of the professional staff are not sufficient, since additional costs are incurred in form of the computer equipment and laboratory space used for the test. To use round numbers, an estimated hourly loaded cost for professional staff of $100 translates into a total cost for the heuristic evaluation of $10,500 for the 105 hours that were actually spent.
4.2 Benefit Estimation
The only way to get an exact measure of the benefits of the heuristic evaluation would be to fully implement two versions of the user interface; one without any changes and one with the changes implied by the evaluation results. These two versions should then be used by a large number of real users to perform real tasks for sufficiently long time that the steady-state level of expert performance had been reached in both cases [Gray et al. 1992]. This process would provide exact measures for the differences in learning time and expert performance. Unfortunately, the version of the interface that was evaluated only exists in a prototype form with which one cannot do any real work, and it would be unrealistic to expect significant development resources to be invested in transforming this prototype to a final product with an identical user interface now that a large number of usability problems have been documented.
Alternatively, one could build a detailed economic work-study model of the different steps involved in the users' workday in order to assess the frequency and duration of each sub-task. One could then further use formal models of user interaction times to estimate the duration of performing each step with each of a set of alternative user interface designs [Gray et al. 1992]. Such an approach would provide fairly detailed estimates but would not necessarily be accurate because of unknown durations of the operations in the model. It would also be very time-consuming to carry out.
It is thus necessary to rely on estimates of the benefits rather than hard measurement data. To get such estimates, the 11 evaluators were asked to estimate the improvements in usability from fixing all the 44 usability problems identified by the heuristic evaluation. Usability improvements were estimated with respect to two usability parameters:

Reduction of learning time: How much less time would the users need to spend learning to use the system? Learning time considered as a usability parameter represents a one-time loss of productive time for each new user to learn the system, so any savings would be realized only once.
Speedup in expert performance: Once the users have reached a steady state of expert performance, how much faster would they be able to perform their work when using a system with all the usability problems fixed than when using a system with all the problems still in place? Expert performance considered as a usability parameter represents a continued advantage for the use of the improved interface, so any savings would be realized throughout the lifetime of the system.

Other usability parameters of interest include frequency of user errors and the users' subjective satisfaction, but these parameters were not estimated. Since several of the usability problems we found were related to error-prone circumstances, it is likely that the number of user errors would go down.
Ten of the 11 evaluators provided learning time estimates and all 11 provided expert speedup estimates. Histograms of the distribution of these estimates are shown in Figure 3. Nielsen and Phillips [1993] found that estimates of changes in user performance made by usability specialists were highly variable, as also seen in the figure here, but that mean values of at least three independent estimates were reasonably close to the values measured by controlled experiments.

Figure 3 
	Histograms showing the distribution of the evaluators' estimates of savings in learning time (top) and expert performance speedup (bottom) for an interface fixing all the usability problems found in the heuristic evaluation. One evaluator did not provide a learning time estimate.






Given that the benefit estimates are based purely on subjective judgments of experts rather than on empirical evidence, it would seem prudent to be conservative in translating the evaluators' estimates into projected monetary savings. The mean values are 0.8 days for learning time reduction and 18% for expert speedup when all evaluators are considered, and 0.5 days and 16%, respectively, when excluding the perhaps overly optimistic outliers at 2 days and 40%. In order to be conservative, we will choose 0.5 days as our learning time reduction estimate and 10% as our expert speedup estimate.
The 10% expert speedup obviously only applies to time spent using the interface. Studies of the users indicate that they will spend about 1/3 of their time doing other tasks, 1/3 of their time performing the task without operating the user interface, and 1/3 of their time actually operating the interface. The 10% expert speedup thus corresponds to 3.3% of total work time.
Translating these estimates into overall savings can be done under the following assumptions: We assume that 2,000 people will be using the system. This is somewhat conservative given that about 3,000 people currently perform this job. Having 2,000 people each save 0.5 days in learning to use the system corresponds to a total of 1,000 user-days saved as a one-time saving. Furthermore, having 2,000 users perform their work 3.3% faster after having reached expert performance corresponds to 67 user-years saved each year the system is in use. Again to be conservative, we will only consider the savings for the first year, even though computer systems of the magnitude we are talking about here are normally used for more than one year. Sixty-seven user-years correspond roughly to 13,000 user-days saved. The total number of user-days saved the first year is thus about 14,000.
To value the total savings in monetary terms, we will assume that the cost of one user-day is $100, and to be conservative, we will assume that only half of the usability problems can actually be fixed, so that only half of the potential savings are actually realized. Furthermore, we need to take into account the fact that the savings in user time are not realized until the system is introduced and thus have a smaller net present value than their absolute value. Again to use round numbers, we will discount the value of the saved learning time by 20% and the value of the expert speedup in the first year by 30%. Learning time can be discounted by a smaller percentage as this saving is realized on day one after the introduction of the system. Using these conservative assumptions, we find one-year savings of $540,000.
Of course, the savings are not realized just by wishing for half of the usability problems to be fixed, so we have to reduce the savings estimate with an estimate of the cost of the additional software engineering effort needed to redesign the interface rather than just implementing the interface from the existing prototype. Assuming that the amount of software engineering time needed for this additional work is 400 hours, and again assuming that the loaded cost of a professional is $100 per hour, we find that the savings estimate needs to be reduced by $40,000. This expense is incurred here and now and thus cannot be discounted. Our final estimate of the net present value of improving the user interface is thus $500,000.
Still being conservative, we have not taken into account the value of the saved software engineering costs from not having to modify the system after its release. Assuming that the original user interface were to be fully implemented and released, is it very likely that the users would demand substantial changes in the second release, and it is well known that making software engineering changes to a released system is much more expensive than making changes at a prototype stage of the software lifecycle.
The $500,000 benefit of improving the interface should be compared with the cost of the heuristic evaluation project, estimated at $10,500. We thus see that the benefit/cost ratio is 48. This number involves significant uncertainties, but is big enough that we do not hesitate to conclude that the heuristic evaluation paid off.
As a final comment on the cost-benefit analysis we should note that the ""benefits"" do not translate to an actual cash flow. Instead, they represent the avoidance of the penalty represented by the extra time the users would have had to spend if the prototype interface had been implemented and released without further changes. It is an interesting and an important management problem to find ways to properly represent such savings in the funding of software development.
4.3 Cost-Benefit Analysis of User Testing
After the heuristic evaluation exercise, additional user testing was performed on the same interface, running four test users. A major reason for using so many more heuristic evaluators than test users was that the users of this particular application were highly specialized technicians who were difficult to get into the lab, whereas it was reasonably easy to get a large number of usability specialists to participate in the heuristic evaluation session. Four new usability problems were found by the user testing which also confirmed 17 of the problems that had already been found by heuristic evaluation.
One can discuss whether the 23 core problems that were not observed in the user test are in fact ""problems"" given that they could not be seen to bother the real users. As argued elsewhere [Nielsen 1992b], such problems can indeed be very real, but their impact may just have too short a duration to be observable in a standard user test. Problems that have the effect of slowing users down for 0.1 second or so simply cannot be observed unless data from a very large number of users is subjected to statistical analysis, but they can be very real and costly problems nevertheless. Also, some problems may occur too infrequently to have been observed with the small number of users tested here.
The main cost of the user test activity was having two professionals spend 7 hours each on the running of the test and the briefing and debriefing of the test users. No time was needed for the traditionally time-consuming activity of defining the test tasks since the same scenario was used as that developed for the previous usability work. Additionally, half an hour was spent finding and scheduling the users for the test and two hours were spent on implementing a small training interface on which the users could learn to use a mouse and standard graphical interaction techniques like pull-down windows. These activities sum to a total of 16.5 person-hours of professional staff, or a cost of $1,650.
Furthermore, the four users and their manager spent essentially a full day on the test when their travel time is taken into account. Again assuming that the cost of one user-day is $100, and furthermore assuming that the cost of one manager-day is $200, the total cost of user involvement is $600. Adding the cost of the professionals and the users gives a total estimate of $2,250 as the cost of the user testing.
The $2,250 spent on user testing could potentially have been spent on additional heuristic evaluation efforts instead. According to Equation 1, this sum corresponds to using 4.3 additional evaluators. Nielsen and Landauer [1993] showed that the finding of usability problems by  i  evaluators can be modelled by the prediction formula



Equation 3:
			ProblemsFound(  i  ) = N(1 - (1-  l  )   i   )



For the core usability problems in the present study, the best-fit values for the parameters in this equation are N=40 and  l  =0.26. Increasing the number of heuristic evaluators,  i  , from 11 to 15.3 can thus be expected to result in the finding of about 1.1 additional usability problems. This estimate shows that the available additional resources do indeed seem to have been spent better on running a user test, finding four problems, than on potentially extending the heuristic evaluation further.
We have no systematic method to estimate the benefits of having found the four additional problems that were discovered by user testing. However, one easy way to arrive at a rough estimate is to assume that the average severity of the four new problems is the same as the average severity of the 17 problems that had already been found by heuristic evaluation. As part of the heuristic evaluation study, severity was measured on a rating scale, with each usability problem being assigned a severity score from zero to four, with higher scores denoting more serious problems. The sum of the severity scores for the original 44 usability problems was 98.41, and the sum of the severity scores for the 17 problems that were seen both in the user test and in the heuristic evaluation was 41.56. We can thus estimate the relative severity of the additional four problems as compared to the original problems as 4/17 xá41.56/98.41 = 0.099.
Knowing about the additional problems found by user testing would thus add 9.9% to the total potential for improving the interface. Furthermore, we might assume that the proportion of the new problems that can be fixed, the impact of fixing them, and the cost of fixing them are all the same as the estimates for the problems found by heuristic evaluation. Under these assumptions, the benefit of having found the additional four usability problems can be valued at $500,000 x 0.099 = $49,500.
Using these estimates, the benefit/cost ratio of adding the user test after the heuristic evaluation is 22. Of course, the benefits of user testing would have been larger if we had credited it with finding the problems that were observed during the user test but had already been found by the heuristic evaluation. We should note, though, that the cost of planning the user test would have been higher if the heuristic evaluation had not been performed and had confirmed the value of the usage scenario. Also, there is no guarantee that all the observed problems would in fact have been found if there had been no prior heuristic evaluation. Now, we knew what to look for, but we might not have noticed as many problems if the user test had been our first usability evaluation activity for this interface.
If the user test were to be credited with all 17 duplicate problems as well as the four new ones, taking the higher-than-average severity of the seventeen problems into account, the benefit of the user test would be valued at $260,500. Of course, this amount would be the benefit from the user test only if no prior heuristic evaluation had been performed. Therefore, it would seem reasonable to charge this hypothetical analysis of the user test with some of the costs that were in fact spent preparing for the heuristic evaluation. Specifically, referring to Table 5, we will add the costs of assessing the appropriate way to use the method, having the outside evaluation expert learn about the domain and scenario, preparing the scenario, and preparing the software, as well as half the time spent writing the problem descriptions (since about half as many problems were found). These activities sum to 24 hours, or an additional cost of $2,400, for a total estimated cost of running the user test without prior heuristic evaluation of $4,650. This translates into a benefit/cost ratio of 56.
To provide a fair comparison, it should be noted that the benefit/cost ratio of performing the heuristic evaluation with only four evaluators would have been 53. This number is larger than the benefit/cost ratio for the full evaluation since more previously unfound usability problems are identified by the first evaluators than by the last, as shown by (EQ 3). Furthermore, the heuristic evaluation provided severity estimates that can be used to prioritize the fixing of the usability problems in the further development process, and the availability of this data probably adds to the actual value of the method as measured by delivered usability. If the time spent on the debriefing and severity ratings is deducted from the time spent on the heuristic evaluation, the benefit/cost ratio for the full eleven evaluators becomes 59 and the ratio for four evaluators becomes 71.
Thus, within the uncertainty of these estimates, it appears that user testing and heuristic evaluation have comparable cost-benefit ratios, and that doing some of each may have additional value.
5 The Evolution of Usability Engineering in Organizations
Two of the fundamental slogans of discount usability engineering are that ""any data is data"" and ""anything is better than nothing"" when it comes to usability. Therefore, I often advocate an approach to usability that focuses on getting started to use a minimum of usability methods. Even so, there are many projects that would benefit from employing more than the minimum amount of discount usability methods. I used the term ""guerrilla HCI"" in the title of this chapter because I believe that simplified usability methods can be a way for a company to gradually build up its reliance on systematic usability methods, starting with the bare minimum and gradually progressing to a more refined lifecycle approach.
Based on observing multiple companies and projects over the years, I have arrived at the following series of steps in the increased use of usability engineering in software development.

Usability does not matter.  The main focus is to wring every last bit of performance from the iron. This is the attitude leading to the world-famous error message, ""beep.""
Usability is important, but  good interfaces can surely be designed by the regular development staff  as part of their general system design. This attitude is symbolized by the famous statement made by King Frederik VI of Denmark on February 26, 1835: ""We alone know what serves the true welfare and benefit of the State and People."" At this stage, no attempt is made at user testing or at acquiring staff with usability expertise.
The desire to have the  interface blessed by the magic wand  of a usability engineer. Developers recognize that they may not know everything about usability, so they call in a usability specialist to look over their design and comment on it. The involvement of the usability specialist is often too late to do much good in the project, and the usability specialist often has to provide advice on the interface without the benefit of access to real users.
GUI panic strikes  , causing a sudden desire to learn about user interface issues. Currently, many companies are in this stage as they are moving from character-based user interfaces to graphical user interfaces and realize the need to bring in usability specialists to advise on graphical user interfaces from the start. Some usability specialists resent this attitude and maintain that it is more important to provide an appropriate interface for the task than to blindly go with a graphical interface without prior task analysis. Even so, GUI panic is an opportunity for usability specialists to get involved in interface design at an earlier stage than the traditional last-minute blessing of a design that cannot be changed much. (Update added 1999: these days, this stage is often characterized by  Web Panic Strikes  . It's the same phenomenon and should be treated the same way.)
Discount usability engineering  sporadically  used.  Typically, some projects use a few discount usability methods (like user testing or heuristic evaluation), though the methods are often used too late in the development lifecycle to do maximum good. Projects that do use usability methods often differ from others in having managers who have experienced the benefit of usability methods on earlier projects. Thus, usability acts as a kind of virus, infecting progressively more projects as more people experience its benefits.
Discount usability engineering  systematically  used.  At some point in time, most projects involve some simple usability methods, and some projects even use usability methods in the early stages of system development. Scenarios and cheap prototyping techniques seem to be very effective weapons for guerrilla HCI in this stage.
Usability group and/or usability lab founded.  Many companies decide to expand to a deluxe usability approach after having experienced the benefits of discount usability engineering. Currently, the building of  usability laboratories [Nielsen 1994a] is quite popular as is the formation of dedicated groups of usability specialists.
Usability permeates lifecycle.  The final stage is rarely reached since even companies with usability groups and usability labs normally do not have enough usability resources to employ all the methods one could wish for at all the stages of the development lifecycle. However, there are some, often important, projects that have usability plans defined as part of their early project planning and where usability methods are used throughout the development lifecycle.

This model is fairly similar to the series of organizational acceptance stages outlined by Ehrlich and Rohn [1994] but was developed independently. Stage 1-2 in the above list correspond to Ehrlich and Rohn's skepticism stage, stage 3-4 correspond to their curiosity stage, stage 5-6 correspond to their acceptance stage, and stage 7-8 correspond to their partnership stage.
Many teachers of usability engineering have described the almost religious effect it seems to have the first time students try running a user test and see with their own eyes the difficulties perfectly normal people can have using supposedly ""easy"" software. Unfortunately, organizations are more difficult to convert, so they mostly have to be conquered from within by the use of guerrilla methods like discount usability engineering that gradually show more and more people that usability methods work and improve products. It is too optimistic to assume that one can move a development organization from stage 1 or 2 in the above model to stage 7 or 8 in a single, sweeping change. In reality, almost all usability methods are extremely cheap to use compared to the benefits they provide in form of better and easier to use products, but often we have to start with the cheapest possible methods to overcome the intimidation barrier gradually.
 
Acknowledgments
The author would like to thank Raldolph Bias, Tom Landauer, and Janice Rohn for helpful comments on an earlier version of the manuscript.
References

Apple Computer (1987). Human Interface Guidelines: The Apple Desktop Interface. Addison Wesley, Reading, MA.
Apple Computer (1992). Macintosh Human Interface Guidelines. Addison Wesley, Reading, MA.
Bellotti, V. (1988). Implications of current design practice for the use of HCI techniques. In Jones, D.M. and Winder, R. (Eds.), People and Computers IV, Cambridge University Press, Cambridge, U.K., 13-34.
Boehm, B. W. (1981). Software Engineering Economics. Prentice-Hall, Englewood Cliffs, NJ.
Card, S. K., Moran, T. P., and Newell, A. (1983). The Psychology of Human-Computer Interaction, Lawrence Erlbaum Associates, Hillsdale, NJ.
Carroll, J. M., and Rosson, M. B. (1990). Human-computer interaction scenarios as a design representation. Proc. HICSS-23: Hawaii International Conference on System Science, IEEE Computer Society Press, 555-561.
Clarke, L. (1991). The use of scenarios by user interface designers. In Diaper, D., and Hammond, N. (Eds.), People and Computers VI, Cambridge University Press, Cambridge, U.K. 103-115.
Ehrlich, K., and Rohn, J. (1994). Cost-justification of usability engineering: A vendor's perspective. In Bias, R.G., and Mayhew, D.J. (Eds.), Cost-Justifying Usability. Academic Press, Boston, MA.
Gould, J. D., and Lewis, C. H. (1985). Designing for usability: Key principles and what designers think. Communications of the ACM 28, 3 (March), 300-311.
Gray, W. D., John, B. E., and Atwood, M. E. (1992). The precis of project Grace, or, an overview of a validation of GOMS. Proc. ACM CHI'92 (Monterey, CA, 3-7 May 1992), 307-312.
Jørgensen, A.H. (1989). Using the thinking-aloud method in system development. In Salvendy, G. and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems. Elsevier Science Publishers, Amsterdam, 743-750.
Karwowski, W., Kosiba, E., Benabdallah, S., and Salvendy, G. (1989). Fuzzy data and communication in human-computer interaction: For bad or for good. In Salvendy, G. and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems. Elsevier Science Publishers, Amsterdam, 402-409.
Landauer, T. K. (1988). Research methods in human-computer interaction. In Helander, M. (Ed.), Handbook of Human-Computer Interaction. North-Holland, Amsterdam, The Netherlands. 543-568.
Mantei, M. M., and Teorey, T.J. (1988). Cost/benefit analysis for incorporating human factors in the software lifecycle, Communications of the ACM 31, 4 (April), 428-439.
Milsted, U., Varnild, A., and Jørgensen, A.H. (1989). Hvordan sikres kvaliteten af brugergr¾nsefladen i systemudviklingen (""Assuring the quality of user interfaces in system development,"" in Danish), Proceedings NordDATA'89 Joint Scandinavian Computer Conference (Copenhagen, Denmark, 19-22 June), 479-484.
Molich, R., and Nielsen, J. (1990). Improving a human-computer dialogue, Communications of the ACM 33, 3 (March), 338-348.
Monk, A., Wright, P., Haber, J., and Davenport, L. (1993). Improving Your Human-Computer Interface: A Practical Technique. Prentice Hall International, Hemel Hempstead, U.K.
Nielsen, J. (1989a). Prototyping user interfaces using an object-oriented hypertext programming system, Proc. NordDATA'89 Joint Scandinavian Computer Conference (Copenhagen, Denmark, 19-22 June), 485-490.
Nielsen, J. (1989b). Usability engineering at a discount. In Salvendy, G., and Smith, M.J. (Eds.), Designing and Using Human-Computer Interfaces and Knowledge Based Systems, Elsevier Science Publishers, Amsterdam. 394-401.
Nielsen, J. (1990a). Big paybacks from 'discount' usability engineering, IEEE Software 7, 3 (May), 107-108.
Nielsen, J. (1990b). Paper versus computer implementations as mockup scenarios for heuristic evaluation, Proc. INTERACT'90 3rd IFIP Conf. Human-Computer Interaction (Cambridge, U.K., 27-31 August), 315-320.
Nielsen, J. (1992a). The usability engineering life cycle. IEEE Computer 25, 3 (March), 12-22.
Nielsen, J. (1992b). Evaluating the thinking aloud technique for use by computer scientists. In Hartson, H. R. and Hix, D. (Eds.), Advances in Human-Computer Interaction Vol. 3, Ablex, Norwood, NJ. 75-88.
Nielsen, J. (1992c). Finding usability problems through heuristic evaluation. Proc. ACM CHI'92 (Monterey, CA, 3-7 May), 373-380.
Nielsen, J. (1993).  Usability Engineering. Academic Press, Boston, MA.
Nielsen, J. (1994a).  Usability laboratories. Behaviour & Information Technology 13, 1.
Nielsen, J. (1994b). Heuristic evaluation. In Nielsen, J., and Mack, R.L. (Eds.),  Usability Inspection Methods. John Wiley & Sons, New York, NY.
Nielsen, J., and Landauer, T. K. (1993). A mathematical model of the finding of usability problems. Proc. ACM INTERCHI'93 Conf. (Amsterdam, the Netherlands, 24-29 April), 206-213.
Nielsen, J., and Levy, J. (1994). Measuring usability - preference vs. performance.  Communications of the ACM   37  , 4 (April), 66-75.
Nielsen, J., and Molich, R. (1989). Teaching user interface design based on usability engineering, ACM SIGCHI Bulletin 21, 1 (July), 45-48
Nielsen, J., and Molich, R. (1990). Heuristic evaluation of user interfaces, Proc. ACM CHI'90 (Seattle, WA, 1-5 April), 249-256.
Nielsen, J., and Phillips, V. L. (1993). Estimating the relative usability of two interfaces: Heuristic, formal, and empirical methods compared. Proc. ACM INTERCHI'93 Conf. (Amsterdam, the Netherlands, 24-29 April), 214-221.
Nielsen, J., Frehr, I., and Nymand, H. O. (1991). The learnability of HyperCard as an object-oriented programming system, Behaviour and Information Technology 10, 2 (March-April), 111-120.
Perlman, G. (1988). Teaching user interface development to software engineers, Proc. Human Factors Society 32nd Annual Meeting, 391-394.
Perlman, G. (1990). Teaching user-interface development, IEEE Software 7, 6 (November), 85-86.
Telles, M. (1990). Updating an older interface, Proc. ACM CHI'90 (Seattle, WA, 1-5 April), 243-247.
Thovtrup, H., and Nielsen, J. (1991). Assessing the usability of a user interface standard, Proc. ACM CHI'91 (New Orleans, LA, 28 April-2 May), 335-341.
Tognazzini, B. (1990). User testing on the cheap, Apple Direct 2, 6 (March), 21-27. Reprinted as Chapter 14 in   TOG on Interface, Addison-Wesley, Reading, MA, 1992.
Voltaire, F. M. A. (1764). Dictionnaire Philosophique.
Whiteside, J., Bennett, J., and Holtzblatt, K. (1988). Usability engineering: Our experience and evolution. In Helander, M. (Ed.), Handbook of Human-Computer Interaction, North-Holland, Amsterdam, 791-817.
"
124,1993-11-01,"Introduction
It has long been recognized  [Refs. 1,2,3,4]  that user interfaces should be designed iteratively in almost all cases because it is virtually impossible to design a user interface that has no usability problems from the start. Even the best usability experts cannot design perfect user interfaces in a single attempt, so a usability engineering lifecycle should be built around the concept of iteration.  [Ref. 5] 
Iterative development of user interfaces involves steady refinement of the design based on user testing and other evaluation methods. Typically, one would complete a design and note the problems several test users have using it. These problems would then be fixed in a new iteration which should again be tested to ensure that the ""fixes"" did indeed solve the problems and to find any new usability problems introduced by the changed design. The design changes from one iteration to the next are normally local to those specific interface elements that caused user difficulties. An iterative design methodology does not involve blindly replacing interface elements with alternative new design ideas. If one has to choose between two or more interface alternatives, it is possible to perform comparative testing to measure which alternative is the most usable, but such tests are usually viewed as constituting a different methodology than iterative design as such, and they may be performed with a focus on measurement instead of the finding of usability problems. Iterative design is specifically aimed at refinement based on lessons learned from previous iterations.
The user tests presented in this article were rigorous, with a fairly large number of test subjects who were measured carefully in several different ways while performing a fixed set of tasks for each system. In many practical usability engineering situations,  [Ref. 6]  it is possible to gain sufficient insight into the usability problems in a given design iteration by running a small number of test subjects, and it may not be necessary to collect quantitative measurement data. The quantitative measures that are emphasized in this article may be useful for management purposes in larger projects but they are not the main goal of usability evaluations. Instead, the main outcome of a usability evaluation in a practical development project is a list of usability problems and suggestions for improvements in the interface.
After a general discussion of the nature of iteration and usability metrics, this article presents four examples of iterative user interface design and measurement. The first example is discussed in greater depth than the others to provide some insight into how the various usability parameters are measured and further refined.
The Benefits of Iteration
The value of iteration in a usability engineering process is illustrated by a commercial development project analyzed by Karat.  [Ref. 7]  The user interface to a computer security application was tested and improved through three versions as discussed further below. A lower bound estimate of the value of the user interface improvements comes from calculating the time saved by users because of the shorter task completion times, thus leaving out any value of the other improvements. The number of users of the security application was 22,876 and each of them could be expected to save 4.67 minutes from using version three rather than version one for their first twelve tasks (corresponding to one day's use of the system), for a total time savings of 1,781 work hours. Karat estimates that this corresponds to saved personnel costs of $41,700 which compares very favorably with the increased development costs of only $20,700 due to the iterative design process. The true savings are likely to be considerably larger since the improved interface was also faster after the first day.

Figure 1.   Interface quality as a function of the number of design iterations: Measured usability will normally go up for each additional iteration, until the design potentially reaches a point where it plateaus. 
Figure 1 shows a conceptual graph of the relation between design iterations and interface usability. Ideally, each iteration would be better than the previous version, but as shown later in this article, this is not always true in practice. Some changes in an interface may turn out not to be improvements after all. Therefore, the true usability curve for a particular product would not be as smooth as the curve in Figure 1. Even so, the general nature of iterative design can probably be expected to be somewhat like Figure 1, even though it must be admitted that we do not yet have sufficiently many documented case studies to estimate the curve precisely. The first few iterations can probably be expected to result in major gains in usability as the true ""usability catastrophes"" are found and fixed. Later iterations have progressively smaller potential for improvements as the major usability problems are eliminated, and the design may eventually become so polished that very little potential for further improvement would remain. Because of the small number of documented cases, we do not yet know where this point of diminishing returns is in terms of number of iterations. It is not even known for sure whether there is in fact an upper limit on the usability of a given interface or whether it would be possible to improve some interfaces indefinitely with continued substantial gains for each iteration.
Assuming that one stays with the same basic interface and keeps refining it, my personal intuition is that there would be a limit to the level of usability one could achieve. I do believe, however, that one can often break such limits by rethinking the interface and base it on a completely new design as more experience is gained over time. For example, the task of ""programming a computer"" has been reconceptualized several times, with languages changing from octal machine code to mnemonic assembler to higher-level programming languages. Figure 1 shows a reconceptualization after a long period of stabile usability for a system, but we do not really know what leads to the creative insights necessary for a fundamentally novel and better interface design. Interface reconceptualizations may not always be immediately followed by increased usability as indicated by Figure 1 since a fundamental redesign may introduce unexpected usability problems that would have to be ironed out by a few additional iterations.
Unfortunately, interface reconceptualizations in individual development projects are mainly known through anecdotal evidence and have not been documented or had their usability impact measured. One exception is the development of an electronic white pages system by a large telephone company. The system was intended to allow customers with home computers to search for telephone numbers where search terms could be matched either exactly or phonetically and with the possibility of expanding the search to a larger geographical area. Unfortunately, even after the iterative design had been through 14 versions, test users still indicated that they were intimidated and frustrated by the system. At this stage of the iterative design it was decided to abandon the basic interface design which had been based on a command-line interface. The interface was reconceptualized based on the insight that the command line made the structure of the system invisible to the user. The interface was redesigned around a menu of services displaying all options at all times, thus making them visible and presumably easier to understand. Indeed, user satisfaction on a 1-5 rating scale (where 5=""like very much"") increased from 3.4 for the command-line based version to 4.2 for the menu-based version, indicating a drastic improvement in usability for the reconceptualized interface.
Usability Metrics and Overall Usability
The overall quality of a system is actually a sum of many quality attributes, only one of which is usability. Additionally, a system should of course be socially acceptable and be practically feasible with respect to cost, maintainability, etc. Furthermore, the system should fulfill additional requirements such as fitting the user's job needs and allowing users to produce high-quality results since that is the reason for having the system at all. I will not consider these issues here since they are related to  utility  : whether the functionality of the system in principle can do what is needed. This article focuses on usability as the question of  how well  users can use that functionality. Note that the concept of ""utility"" does not necessarily have to be restricted to work-oriented software. Educational software has high utility if students learn from using it, and an entertainment product has high utility if it is fun to use.
In spite of simplified conceptual illustrations like Figure 1, usability is not really a one-dimensional property of a user interface.  Usability has many aspects, and is often associated with the following 5 attributes:  [Ref. 6] 

Easy to learn:  The user can quickly go from not knowing the system to getting some work done with it.
Efficient to use:  Once the user has learned the system, a high level of productivity is possible.
Easy to remember:  The infrequent user is able to return to using the system after some period of not having used it, without having to learn everything all over.
Few errors:  Users do not make many errors during the use of the system, or if they do make errors they can easily recover from them. Also, no catastrophic errors should occur.
Pleasant to use:  Users are subjectively satisfied by using the system; they like it.

Focusing in on usability as a quality goal, we thus define it in terms of these five quality attributes. For each of these quality attribute, one then needs to define a metric to numerically characterize the attribute. In the usability field, these metrics are commonly defined in terms of the performance of a randomly chosen set of representative users while using the system to do a set of representative tasks, and the metrics are thus dependent on these benchmark tasks. Finally, usability measures are empirically derived numbers resulting from the application of one or more metrics to a concrete user interface. This progression from quality goals over quality attributes and their metrics to the actual measures thus makes usability steadily more concrete and operationalized.
From a usability engineering perspective, one should not necessarily pay equal attention to all usability attributes. Some are more important than others and should be the focus of the development efforts throughout the iterative design. In some cases one might even accept lower scores on certain usability metrics as long as their values remain above some minimally acceptable value.  [Ref. 8]  Each of the case studies discussed below had their own priorities with regard to the different attributes of usability and other projects might have yet other goals. It is an important early part of the usability lifecycle to set priorities for the usability attributes and decide on which ones will deserve the main consideration in a given project.
User efficiency is a measure of how fast users can use the computer system to perform their tasks. Note that user efficiency is a different concept than traditional computer response time, even though faster response times will often allow users to complete their tasks faster. Often, user efficiency is seen as the most important attribute of usability because of its direct bottom-line impact. For example, NYNEX estimates that each one second reduction in work time per call for its toll and assistance operators will save three million U.S. dollars per year.  [Ref. 9]  Other usability attributes may be critical for certain applications such as control rooms for dangerous plants or processes where an especially low frequency of user errors is desired.
Normally, the values of the chosen usability metrics are measured empirically through user testing. There are research projects aiming at analytical methods for estimating certain usability metrics, and they have had some success at dealing with user performance for expert users  [Ref. 9]  and the transfer of user learning from one system to another. These analytical methods are substantially weaker at estimating error rates and are unable to address the subjective ""pleasant to use"" dimension of usability. In any case, practical development projects are probably best advised to stay with user testing for the time being since the analytical methods can be rather difficult to apply.  [Ref. 10]  Often, about ten test users are used for usability measurements, though tests are sometimes conducted with twenty or more users if tight confidence intervals on the measurement results are desired. It is important to pick test users who are as representative as possible of the actual people who will be using the system after its release. When repeated user tests are performed as part of iterative design, one should use different users for each test to eliminate transfer of learning from previous iterations.
It is normally quite easy to conduct user testing with novice users in order to measure learnability and initial performance, error rates, and subjective satisfaction. Usability measures for expert users are unfortunately harder to come by, as are measures of the ability of experienced users to return to the system after a period of absence. The basic problem is the need for users with actual experience in using the system. For some user interfaces, full expertise may take several years to acquire, and even for simpler interfaces, one would normally need to train users for several weeks or months before their performance plateaued out at the expert level. Such extended test periods are of course infeasible during the design of new products, so expert performance tends to be much less studied than novice performance. Two ways of obtaining expert users without having to train them are to involve expert users of any prior release of the product and to use the developers themselves as test users. Of course, one should keep in mind that a developer has a much more extensive understanding of the system than a user would have, so one should always test with some real users also.
Quantifying Relative Changes in Usability
In order to compare the different iterative design projects later in this article, a uniform way of scoring usability improvements is desirable. The approach used here is to normalize all usability measures with respect to the values measured for the initial design. Thus, the initial design is by definition considered to have a normalized usability value of 100, and the subsequent iterations are normalized by dividing their measured values by those measured for the initial design. For example, if users made four errors while completing a set task with version one and three errors with version two of an interface, the normalized usability of interface two with respect to user errors was 133% of the usability of interface one, indicating that a 33% improvement in usability had been achieved. In just the same way, the normalized usability of interface two would still be 133% if users made eight errors with interface one and six errors with interface two.
A particular problem is how to convert measures of subjective satisfaction into improvement scores, since subjective satisfaction is often measured with questionnaires and rating scales. It would not be reasonable just to take the ratio of the raw rating scale scores. Doing so would mean, say, that a system rated 2 on a 1-5 scale would be twice as good as a system rated 1 on a 1-5 scale, but also that a system rated 2 on a 1-7 scale would be twice as good as a system rated 1 on a 1-7 scale, even though it is clearly easier to achieve a rating of 2 on a 1-7 scale than on a 1-5 scale.
The underlying problem is that the rating scales are not ratio scales, so it is theoretically impossible to arrive at a perfect formula to calculate how much better one rating is relative to another. Assuming that the goal of usability engineering is purely economical, it might be possible to gather empirical data to estimate how various subjective satisfaction ratings translate to increased sales of a product or increased employee happiness (and one could then further measure the contribution to a company's profits from having happier employees). I am not aware of any such data, so I proceeded with calculating relative subjective satisfaction scores as well as possible, using a method that had proven successful in a larger study of subjective preference measures,  [Ref. 11]  even though the resulting formula is admittedly not perfect. This transformation method is discussed further in the box.
The case studies in this article all used a given set of test tasks to measure usability throughout the iterative design process. This was possible because they had their basic functionality defined before the start of the projects. In contrast, some development projects follow a more exploratory model where the tasks to be performed with a system may change during the development process as more is learned about the users and their needs. For such projects, it may be more difficult to use a measurement method like the one outlined here, but the general results with respect to the improvements from iterative design should still hold.

Transforming Subjective Ratings to a Ratio Scale
To ensure uniform treatment of all subjective rating scale data, the following three transformations were applied to the raw rating scale scores:


The various rating scales were linearly rescaled to map onto the interval from -1 to +1 with zero as the midpoint and +1 as the best rating. This transformation provided a uniform treatment of scales independent of their original scale intervals.
The  arcsine  function was applied to the rescaled scores. This transformation was intended to compensate for the fact that rating scales are terminated at each end, making it harder for an average rating to get close to one of the ends of the scales. For example, an average rating of 4.6 on a 1-5 scale might result from four users rating the system 4 and six users rating it 5.0. Now, even if all ten users like some other system better and would like to increase their rating by one point, the second system would only get an average rating of 5.0 (an increase of 0.4 instead of 1.0 as ""deserved"") because six of the users were already at the end of the scale. Because of this phenomenon, changes in ratings towards the end of a rating scale should be given extra weight, which is achieved by the way the  arcsine  function stretches the ends of the interval.
The exponential function  e   x  was applied to the stretched scores to achieve numbers for which ratios were meaningful. Without such a transformation, it would seem that, say, a score of 0.2 was twice as good as a score of 0.1, whereas a score of 0.7 would only be 17% better than a score of 0.6, even though the improvement was the same in both cases. After the exponential transformation, a score of 0.2 is 10.5% better than a score of 0.1, and a score of 0.7 is also 10.5% better than a score of 0.6.

Admittedly, the choice of functions for these transformations is somewhat arbitrary. For example, an exponential function with another basis than  e  would have achieved the same effect while resulting in different numbers. Remember, however, that the purpose of the present analysis is to compare  relative  ratings of the various interface iterations, and that the same transformations were applied to each of the original ratings. Nowhere do we use the value of a single transformed rating; we always look at the ratio between two equally transformed ratings, and all the transformations are of course monotonic. Also, an analysis of a larger body of subjective satisfaction data indicated that the use of these transformations resulted in reasonable statistical characteristics.  [Ref. 11] 
Calculating Scores for Overall Usability
It is often desirable to have a single measure of overall usability of a user interface. Typically, interface designers can choose between several alternative solutions for various design issues and would like to know how each alternative impacts the overall usability of the resulting system. As another example, a company wanting to choose a single spreadsheet as its corporate standard would need to know which of the many available spreadsheets was the most usable. In both examples, usability would then need to be traded off against other considerations such as implementation time for the design alternatives and purchase price for the spreadsheets.
Ultimately, the measure of usability should probably be monetary in order to allow comparisons with other measures such as implementation or purchase expenses. For some usability metrics, economic equivalents are fairly easy to derive, given data about the number of users, their loaded salary costs, and their average work day. Loaded salaries include not just the money paid out to employees but also any additional costs of having them employed, such as social security taxes and medical and pension benefits. For example, I recently performed a usability evaluation of a telephone company application. Improvements in the user interface based on this study were estimated to result in a reduction in training time of half a day per user and a speedup in expert user performance of 10%. Given the number of users and their loaded salaries, these numbers translate to savings of $40,000 from reduced training costs and about $500,000 from increased expert user performance in the first year alone. In this example, the overall value of the usability improvements is obviously dominated by expert user performance -- especially since the system will probably be used for more than one year.
Other usability metrics are harder to convert into economic measures. User error rates can in principle be combined with measures of the seriousness of each error and estimates of the impact on corporate profits from each error. The impact of some errors is easy to estimate: For example, an error in the use of a print command causing a file to be printed on the wrong printer would have a cost approximately corresponding to the time needed for the user to discover the error and to walk to the wrong printer to retrieve the output. An error causing a cement plant to burn down would have a cost corresponding to the value of the plant plus any business lost while it was being rebuilt. In other cases, error costs are harder to estimate. For example, an error causing a customer to be shipped a different product from that ordered would have direct costs corresponding to the administrative costs of handling the return plus the wasted freight charges. However, there could also very well be much larger indirect costs if the company got a reputation of being an unreliable supplier causing customers to prefer doing business elsewhere.
Finally, subjective satisfaction is perhaps the usability attribute that has the least direct economic impact in the case of software for in-house use, even though it may be one of the most important factors influencing individual purchases of shrinkwrap software. Of course, it is conceptually easy to consider the impact of increased user satisfaction on software sales or employee performance, but actual measurements are difficult to make and were not available in the case studies discussed below.
Due to the difficulties of using monetary measures of all usability attributes, an alternative approach has been chosen here. Overall usability is calculated in relative terms as the geometric mean (the  n  'th root of the product) of the normalized values of the relative improvements in the individual usability metrics. Doing so involves certain weaknesses: First, all the usability metrics measured for a given project are given equal weight even though some may be more important than others. Second, there is no use of cut-off values where any further improvement in a given usability metric would be of no practical importance. For example, once error rates have been reduced to, say, one error per ten thousand user transactions, it may make no practical difference to reduce them further. Even so, reducing the error rate to, say, one error per twenty thousand transactions would double the usability score for the user error metric as long as no cut-off value has been set for user errors. Using geometric means rather than arithmetic means (the sum of values divided by the number of values) to calculate the overall usability somewhat alleviates this latter problem as a geometric mean gives comparatively less weight to uncommonly large numbers. The geometric mean increases more by improving all the usability metrics a little than by improving a single metric a lot and leaving the others stagnant.
Case Studies in Iterative User Interface Design
The following sections present data from the four case studies in iterative user interface design summarized in Table 1. The first example is described in somewhat greater depth than the others in order to provide some insight into how the various usability metrics are measured and further refined into estimates of improvements in overall usability.

Table 1.   Four case studies of iterative design. 









Name of System
Interface Technology
Versions Tested
Subjects per Test
Overall Improvement


Home banking
Personal-computer graphical user interface
5
8
242%


Cash register
Specialized hardware with character-based interface
5
9
87%


Security
Mainframe character-based interface
3
9
882%


Hypertext
Workstation graphical user interface
3
10
41%



Home Banking
The home banking system was a prototype of a system to allow Danish bank customers access to their accounts and other bank services from a home computer using a modem. The interface was explicitly designed to explore the possibilities for such a system when customers owned a personal computer supporting a graphical user interface and a mouse.
The prototype system was developed on a single personal computer platform under the assumption that alternative versions for other personal computers with graphical user interfaces would use similar designs and have similar usability characteristics. This assumption was not tested, however. Prototyping aims at generating a running user interface much faster than standard programming by accepting some limitations on the generality of the code. As one such prototyping limitation, the system was implemented on a stand-alone personal computer without actual on-line access to bank computers. Instead, the user interface only provided access to a limited number of accounts and other pre-defined database information stored on the personal computer.
The relevant usability attributes for this application include user task performance, the users' subjective satisfaction, and the number of errors made by users. Task performance is less important for this application than for many others, as the users will not be paid employees. Even so, users will be running up telephone bills and taking up resources on the central computer while accessing the system, so task performance is still of interest as a usability attribute. Subjective satisfaction may be the most important usability attribute, as usage of a home banking system is completely discretionary, and since the reputation for being ""user friendly"" (or rather, ""customer friendly"") is an important competitive parameter for a bank. User errors are also important, but most important of all is probably the avoidance of usage catastrophes, defined as situations where the user does not complete a task correctly. User errors are certainly unpleasant and should be minimized, but it is considerably worse when a user makes an error without realizing it, and thus without correcting it.
Each test user was asked to perform a specified set of tasks with the system, accessing some of the dummy accounts that had been set up for the prototype. In general, it is important to pick benchmark tasks that span the expected use of the system since it would be easy for a design to score well on a single, limited task while being poor for most other tasks.
Four sets of tasks were used:

Basic tasks operating on the customer's own accounts
	
Find out the balance for all your accounts.
Transfer an amount from one of your accounts to the other.
Investigate whether a debit card transaction has been deducted from the account yet.


Money transfers to accounts owned by others
	
Order an electronic funds transfer to pay your March telephone bill.
Set up a series of electronic funds transfers to pay monthly installments for a year on a purchase of a stereo set.
Investigate whether the telephone bill transfer has taken place yet.


Foreign exchange and other rare tasks
	
Order Japanese Yen corresponding to the value of 2,000 Danish Kroner.
Order 100 Dutch Guilders.
Order an additional account statement for your savings account.


Special tasks
	
Given the electronic funds transfers you have set up, what will the balance be on August 12th? (Do not calculate this manually; find out through the system).
You returned the stereo set to the shop, so cancel the remaining installment payments.



Table 2 shows the measures of task time for each of the four task types measured in the user testing of the home banking system. The best way of calculating a score for task efficiency would be to weigh the measured times for the individual sub-tasks relative to the expected frequency with which users would want to perform those sub-tasks in field use of the system. To do this perfectly, it would be necessary to perform true, contextual field studies of how consumers actually access their bank account and other bank information. Unfortunately, it is rarely easy to use current field data to predict frequencies of use for features in a future system, because the very introduction of the system changes the way the features are used.

Table 2.   Absolute values of the constituent measurements of the ""task time"" usability metric for the home-banking system (all times in seconds).
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Basic Tasks
			(own account)
Transfers to Others' Accounts
Foreign Exchange
Special Tasks
Total Task Time


1
199
595
245
182
1,220 


2
404
442
296
339
1,480 


3
221
329
233
317
1,090 


4
206
344
225
313
1,088 


5
206
323
231
208
967 



For the home banking example, it is likely that most users would access their own account more frequently than they would perform foreign currency tasks or the special tasks. On the other hand, some small business users of this kind of system have been known to rely extensively on a foreign exchange feature, so a proper weighting scheme is by no means obvious. In this article, I will simply assume that all four tasks are equally important and should thus have the same weight. This means that task efficiency is measured by the total task time which is simply the sum of the times for the four task groups in Table 2.
Regular errors and catastrophes were measured by having an experimenter count the number of each, observed as the users were performing the specified tasks. Subjective satisfaction was measured by giving the test users a two-question questionnaire after they had performed the tasks with the system:
How did you like using the bank system? 

Very pleasant
Somewhat pleasant
Neutral
Somewhat unpleasant
Very unpleasant

If you had to perform a task that could be done with this system, would you prefer using the system or would you contact the bank in person? 

Definitely use the system
Likely use the system
Don't know
Likely contact the bank
Definitely contact the bank

The overall subjective satisfaction score was computed as the average of numeric scores for the user's replies to these two questions. The raw data for the subjective satisfaction and error metrics is listed in Table 3.

Table 3.   Absolute values of the usability parameters for the home-banking system. Subjective satisfaction was measured on a 1 to 5 scale, where 1 indicated the highest satisfaction.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Subjective Satisfaction
			(1-5 scale; 1 best)
Errors Made per User
Catastrophes per User


1
1.92
9.2
2.56


2
1.83
4.3
1.00


3
1.78
2.5
0.44


4
1.86
2.5
0.43


5
1.67
1.5
0.17



The usability test showed that the first version of the user interface had serious usability problems. Both error rates and catastrophes were particularly high. Even so, users expressed a fairly high degree of subjective satisfaction which might have been due to the pleasing appearance of graphical user interfaces compared with the types of banking interfaces they were used to. One major problem was the lack of explicit error messages (the system just beeped when the users made an error), which made it hard for the users to learn from their mistakes. Another major problem was the lack of a help system. Also, many dialog boxes did not have ""cancel"" buttons, so users were trapped once they had chosen a command. Furthermore, several menu options had names that were difficult to understand and were easy to confuse (for example the two commands ""Transfer money"" and ""Move an amount""), and there were several inconsistencies. For example, users had to type in account numbers as digits only (without any dashes), even though dashes were used to separate groups of digits in listings of account numbers shown by the computer.
(One of the anonymous referees of this article questioned whether this example was real or contrived: ""Would anyone  really  design a dialog box without a Cancel button?"" I can confirm that the example is real, and that the designer indeed had forgotten this crucial dialogue element in the first design. Not only did this supposedly trivial problem occur for real in this case study, it occurred again for another designer in another design case I am currently studying. Also, several commercial applications have been released with similar problems.)
To reduce the number of catastrophes, confirming dialog boxes were introduced in version two to allow the user to check the system's interpretation of major commands before they were executed. For example, one such dialog read,  ""Each month we will transfer 2,000.00 Kroner from your savings account to The LandLord Company Inc. The first transfer will be made April 30, 1992. OK/Cancel.""  Also, written error messages in alert boxes replaced the beep, and a special help menu was introduced.
Version one had used three main menus: Functions, Accounts, and Information. The first menu included commands for ordering account statements, for initiating electronic funds transfers, and for viewing a list of funds transfers. The second menu included commands for viewing an account statement, for transferring money between the user's own accounts, and for viewing a list of the user's accounts and their current balance. The third menu included commands for showing current special offers from the bank and for accessing foreign currency exchange rates. The test had shown that the users did not understand the conceptual structure of this menu design but instead had to look through all three menus every time they wanted to activate a command. Therefore, the menu structure was completely redesigned for version two, with only two main menus: Accounts (for all commands operating on the user's accounts) and Other (for all other commands).
The user test of version two showed that users had significantly fewer problems with finding commands in the menus and that they also made fewer errors. At the same time, the help system was somewhat confusingly structured, and users wasted a fair amount of time reading help information which they did not need to complete their current tasks. As can be seen from Table 3, version two did succeed in its major goal of reducing the very high levels of user errors and catastrophes from version one, but as shown by Table 2, this improvement was made at the cost of slower task completion for most tasks. Transfers to others' accounts were faster in version two due to the elimination of errors when users typed account numbers the way they were normally formatted.
For version three, the help system was completely restructured. Also, several of the error messages introduced in version two were rewritten to make them constructive and better inform users how they can correct their errors.
To further reduce the error rates, version four identified the user's own accounts by their account type (e.g. checking account or savings account instead of just their account number). Also, the dialog box for transferring money was expanded with an option to show the available balance on the account from which the transfer was to be made. Furthermore, a minor change was made to the confirmation dialog box for electronic funds transfers to prevent a specific error that had been observed to occur fairly frequently with users not entering the correct date for transfers that were to be scheduled for future execution. As mentioned above, the confirming dialog normally stated the date on which the transfer was to be made, but when the transfer was specified to happen immediately, the text was changed to read ""The transfer will take place today."" Using the word ""today"" rather than listing a date made the message more concrete and easy to understand and also made it different from the message used for future transfers. Note how this iterative design further modified an interface change that had been introduced in version two.
In the final version, version five, the help system was completely restructured for the second time. The help system introduced in version three was structured according to the menu commands in the system and allowed users to get help about any command by selecting it from a list. The revised help system presented a single conceptual diagram of the tasks that were supported by the system, linking them with a list of the available menu commands, and providing further levels of information through a simplistic hypertext access mechanism.
In addition to the changes outlined here, a large number of smaller changes were made to the user interface in each iteration. Almost no user interface elements survived unchanged from version one to version five.
Table 4 finally shows the normalized values for the four usability metrics as well the overall usability computed as their geometric mean. It can be seen that most usability metrics improved for each iteration but that there also were cases where an iteration scored worse than its predecessor on some metric. As mentioned, many of these lower scores led to further redesigns in later versions. Also, there were many smaller instances of iterative changes that had to be further modified due to observations made during the user testing, even if they were not large enough to cause measurable decreases in the usability metrics.

Table 4.   Normalized improvements in usability parameters for the home-banking system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Efficiency
			(inverse task time)
Subjective Satisfaction
Correct Use
			(inverse error frequency)
Catastrophe Avoidance
Overall Usability Improvement


1
0
0
0
0
0 


2
-18%
+6%
+114%
+156%
+48% 


3
+12%
+9%
+268%
+582%
+126% 


4
+12%
+4%
+513%
+595%
+155% 


5
+26%
+17%
+513%
+1,406%
+242% 



A Cash Register System
The cash register system was a point of sales application for a chain of men's clothes stores in Copenhagen. A computerized cash register with a built-in alphanumeric screen allowed sales clerks to perform tasks like selling specific pieces of merchandise with payment received as cash, debit card, credit card, check, foreign currencies converted according to the current exchange rate, and gift certificates, as well as combinations of these, and also accepting returned goods, issuing gift certificates, and discounting prices by a specified percentage.
The usability metrics of interest for this application again included time needed for the users to perform 17 typical tasks, subjective satisfaction, and errors made while completing the tasks. Furthermore, an important metric was the number of times the users requested help from a supervisor while performing the task. This last metric was especially important because the users would often be temporary sales staff taken on for sales or holiday shopping seasons where the shop would be very busy and such help requests would slow down the selling process and inconvenience shoppers. The values measured for these usability metrics for each of the five versions of the system are shown in Table 5.

Table 5.   Absolute values of usability metrics for the cash-register system. Subjective satisfaction was measured on a 1 to 5 scale, where 1 indicated the highest satisfaction.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 









Version
Time on Task
			(seconds)
Subjective Satisfaction
			(1-5 scale; 1 best)
Errors Made per User During 17 Tasks
Help Requests per User During 17 Tasks


1
2,482
2.38
0.0
2.7


2
2,121
1.58
0.1
1.1


3
2,496
1.62
0.2
1.2


4
2,123
1.45
0.2
0.9


5
2,027
1.69
0.0
0.4



Table 6 shows the normalized improvements in usability for each iteration. Since the error rate was zero for the first version,  any  occurrence of errors would in principle constitute an infinite degradation in usability. However, as mentioned above, very small error rates (such as one error per 170 tasks, which was the measured error rate for version two) are not really disastrous for an application like the one studied here, so a 0.1 increase in error rate has been represented in the table as a 10% degradation in usability. One can obviously argue whether another number would have been more reasonable, but as it turns out, the error rate for the last version was also zero, so the method chosen to normalize the error rates does not impact on the conclusion with respect to overall improvement due to the total iterative design process.

Table 6.   Normalized improvements in usability parameters for the cash-register system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 










Version
Efficiency
			(inverse task time)
Subjective Satisfaction
Correct Use
			(inverse error frequency)
Help Avoidance (inverse help requests)
Overall Usability Improvement


1
0
0
0
0
0 


2
+17%
+61%
-10%
+145%
+43% 


3
-1%
+56%
-20%
+125%
+29% 


4
+17%
+77%
-20%
+200%
+49% 


5
+22%
+49%
0%
+575%
+87% 



The main changes across the iterations regarded the wording of the field labels that were changed several times to make them more understandable. Also, some field labels were made context sensitive in order to improve their fit with the user's task, and fields that were not needed for given tasks were removed from the screen when users were performing those tasks. Several shortcuts were introduced to speed up common tasks, and these shortcuts also had to be modified over subsequent iterations.
A Security Application
The security application  [Ref. 7]  was the sign-on sequence for remote access to a large data entry and inquiry mainframe application for employees at the branch offices of a major computer company. Users already used their alphanumeric terminals to access an older version of the system, and a main goal of the development of the new system was to ensure that the transition between the two systems happened without disrupting the users in the branch offices. Therefore, the most important usability attribute was the users' ability to successfully use the new security application to sign on to the remote mainframe without any errors. Other relevant attributes were the time needed for users to sign on and their subjective satisfaction. Three usability metrics were measured: User performance was measured as time needed to complete twelve sign-ons on the system, success rate was measured as proportion of users who could sign on error free after the third attempt, and the users' subjective attitudes towards the system was measured as the proportion of test users who believed that the product was good enough to deliver without any further changes. Table 7 shows the result of measuring these metrics for the three versions of the interface using test users who had experience with the old system but not with the new interface.

Table 7.   Absolute values of usability metrics for three iterations of a computer security application in a major computer company.
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Time to Complete Task
			(minutes)
Success Rate
			(percent)
Subjective Satisfaction (percent)


1
5.32
20
0


2
2.23
90
60


3
0.65
100
100



The table shows substantial improvements on all three usability metrics. A further test with an expert user showed that the optimum time to complete a sign-on for the given system was 6 seconds. The test users were able to complete their twelfth sign-on attempt in 7 seconds with the third iteration of the interface, indicating that there would not be much room for further improvement without changing the fundamental nature of the system. Table 8 shows the normalized improvements in usability for the security application over its three versions.

Table 8.   Normalized improvements in usability metrics for the security application. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 









Version
Efficiency
			(inverse task time)
Success Rate
Subjective Satisfaction
Overall Usability Improvement


1
0
0
0
0 


2
+139%
+350%
+448%
+298% 


3
+718%
+400%
+2,214%
+882% 



A Hypertext System
The hypertext system  [Ref. 12]  was an interface designed to facilitate access to large amounts of text in electronic form. Typical applications include technical reference manuals, online documentation, and scientific journal articles converted into online form. The system ran on a standard workstation using multiple windows and various other graphical user interface features such as the ability for the user to click on a word in the text with the mouse to find more information about that word.
For the use of this hypertext system for technical reference texts, the most important usability attributes were the search time for users to find information about their problems as well as the search accuracy (the proportion of times they found the correct information). Both of these attributes were measures by having test users answer 32 given questions about a statistics software package on the basis of a hypertext version of its reference manual. The results of these tests for the three versions of the hypertext system are shown in Table 9.

Table 9.   Absolute values of usability metrics for the hypertext system. (A version numbering scheme starting with version 0 has been used in other papers about this system, but for internal consistency in this document, I gave the first version the number 1 in this table.).
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 







Version
Search Time
			(seconds)
Search Accuracy
			(seconds)


1
7.6
69


2
5.4
75


3
4.3
78



A hypertext system for technical reference texts will obviously have to compete with text presented in traditional printed books. The developers of the hypertext system therefore conducted a comparative test where a group of test users were asked to perform the same tasks with the printed version of the manual.  [Ref. 12]  The search time for paper was 5.9 minutes with a search accuracy of 64%, indicating that paper actually was better than the initial version of the hypertext system. This example clearly shows that iterative design should not just try to improve a system with reference to itself, but should also aim at arriving at better usability characteristics than the available competition.
Table 10 shows the normalized improvements in usability for the hypertext system. Many of the changes from version 1 to version 2 were based on encouraging users to use a search strategy that had proven effective by making it both easier to use and more explicitly available. For version 3, several additional changes were made, including an important shortcut that automatically performed a second step that users almost always did after a certain first step in the initial tests. Both improvements illustrate the important ability of iterative design to mold the interface according to user strategies that do not become apparent until after testing has begun. Users always find new and interesting ways to use new computer systems, so it is not enough to make an interface usable according to preconceived notions about how they will be used.

Table 10.   Normalized improvements in usability parameters for the hypertext system. (Version 1 is the baseline and has zero improvement by definition. All numbers except version numbers are percentages, relative to the baseline metrics for version 1.)
	Color-coding indicates whether a redesign led to an improvement or a degradation in a usability metric:  green cells show when a design was better than the previous iteration, and  rose cells show designs that are worse than the previous iteration. 








Version
Search Efficiency
			(inverse task time)
Search Accuracy
Overall Usability Improvement


1
0
0
0 


2
+41%
+9%
+24% 


3
+77%
+13%
+41% 



Conclusions
The median improvement in overall usability from the first to the last version for the case studies discussed here was 165%. A study of 111 pairwise comparisons of user interfaces found that the median difference between two interfaces that had been compared in the research literature was 25%.  [Ref. 11]  Given that the mean number of iterations in the designs discussed here was three, one might expect the improvement from first to last interface to be around 95% based on an average improvement of 25% for each iteration. As mentioned, the measured improvement was larger, corresponding to an average improvement of 38% from one version to the next.
(Three iterations, each improving usability by 25%, lead to an improvement from version one to version four of 95% rather than 75%, due to a compounding effect similar to that observed when calculating interest on a bank account across several years.)
There is no fundamental conflict between the estimate of 25% average usability difference between interfaces compared in the research literature and 38% difference between versions in iterative design. Picking the best of two proposed interfaces to perform the same task is actually a very primitive usability engineering method that does not allow one to combine appropriate features from each of the designs. In contrast, iterative design relies much more on the intelligence and expertise of the usability specialist throughout the design process and allows designers to combine features from previous versions based on accumulated evidence of what works under what circumstances. It is thus not surprising that iterative design might lead to larger improvements between versions than that found simply by picking the best of two average design alternatives.
Of course, the 38% improvement between versions in iterative design should only be considered a rough estimate as it was only based on four case studies. Furthermore, there is a large variability in the magnitude of usability improvement from case to case, so one should not expect to realize 38% improvements in all cases. Finally, one should not expect to be able to sustain exponential improvements in usability across iterations indefinitely. If a very large number of iterations were to be used, usability improvements would probably follow a curve somewhat like Figure 1, since it is probably not possible to achieve arbitrary improvements in usability just by iterating sufficiently many times. It is currently an open research question how much usability can be improved and how good an ""ultimate user interface"" can get, since practical development projects always stop iterating before perfection is achieved.
The median improvement from version 1 to version 2 was 45%, whereas the median improvement from version 2 to version 3 was ""only"" 34%. In general, one can probably expect the greatest improvements from the first few iterations as usability catastrophes get discovered and removed. It is recommended to continue beyond the initial iteration, however, since one sometimes introduces new usability problems in the attempt to fix the old ones. Also, user testing may turn up new interface strategies that would have to be refined through further iterations.
The projects discussed in this article included several cases where at least one usability attribute had reduced scores from one version to the next. Also, it was often necessary to redesign a given user interface element more than once before a usable version had been found. Three versions (two iterations) should probably be the minimum in an iterative design process. Of course, as in our cash register case study, the third version will sometimes test out as worse than the second version, so one cannot always follow a plan to stop after version three. The actual results of the iterative design and testing must guide the decisions on how to proceed in each individual project.
Update added 2006: 
By way of comparison, across a large number of  much newer Web design projects I studied, the average improvement in measured usability in a redesign was 135%.
See also my paper on  parallel design for a way to kickstart an iterative design project by testing multiple alternative design suggestions at the same time during the first iteration.

Acknowledgments
This paper was written while the author was a member of the Applied Research Division of Bell Communications Research (Bellcore). Data on the home banking and cash register projects analyzed in this article was collected while the author was on the faculty of the Technical University of Denmark and these systems do not represent Bellcore products. The author would like to thank his students, Jens Rasmussen and Frederik Willerup, for helping collect data on these projects. The author would like to thank Susan T. Dumais, Michael C. King, and David S. Miller as well as several anonymous  Computer  referees for helpful comments on this article.
Originally published in IEEE Computer Vol. 26, No. 11 (November 1993), pp. 32–41. 
References

Bury, K.F. The iterative development of usable computer interfaces. In Proceedings of  IFIP INTERACT'84 International Conference on Human-Computer Interaction  (London, U.K., 4-7 September 1984), 743-748.
Buxton, W., and Sniderman, R. Iteration in the design of the human-computer interface. In  Proceedings of the 13th Annual Meeting of the Human Factors Association of Canada, 1980, pp. 72-81.
Gould, J.D., and Lewis, C.H. Designing for usability: Key principles and what designers think.  Communications of the ACM   28, 3 (March 1985), 300-311.
Tesler, L. Enlisting user help in software design.  ACM SIGCHI Bulletin   14, 3 (January 1983), 5-9.
Nielsen, J. The usability engineering life cycle.  IEEE Computer   25, 3 (March 1992), 12-22.
Nielsen, J.   Usability Engineering. Academic Press, San Diego, CA, 1993.
Karat, C.-M. Cost-benefit analysis of iterative usability testing. In  Proceedings of IFIP INTERACT'90 Third International Conference on Human-Computer Interaction  (Cambridge, U.K., 27-31 August 1990), pp. 351-356.
Good, M., Spine, T.M., Whiteside, J., and George, P. User-derived impact analysis as a tool for usability engineering. In  Proceedings of ACM CHI'86 Conference on Human Factors in Computing Systems  (Boston, MA, 13-17 April 1986), pp. 241-246.
Gray, W.D., John, B.E., Stuart, R., Lawrence, D., and Atwood, M.E. GOMS meets the phone company: Analytic modeling applied to real-world problems. In  Proceedings of IFIP INTERACT'90 Third International Conference on Human-Computer Interaction  (Cambridge, U.K., 27-31 August 1990), pp. 29-34.
Carroll, J.M., and Campbell, R.L. Softening up hard science: Reply to Newell and Card.  Human-Computer Interaction   2, 3 (1986), 227-249.
Nielsen, J., and Levy, J. Measuring usability - preference vs. performance.  Communications of the ACM   37, 4 (April 1994), 66-75.
Egan, D.E., Remde, J.R., Gomez, L.M., Landauer, T.K., Eberhardt, J., and Lochbaum, C.C. Formative design-evaluation of SuperBook.  ACM Transactions on Information Systems   7, 1 (January 1989), 30-57.
"
125,1993-08-01,"Redmond, WA, 21-23 July 1993
The second annual UPA conference sponsored by the  Usability Professionals Association was hosted by Microsoft on their well-groomed corporate campus outside of Seattle. Having the conference at an actual member company site continued the tradition started by the first conference which was hosted by WordPerfect in Utah. I highly appreciated the opportunity to see the environment in which so much software is produced and where the everyday activities of the usability professionals take place.
The UPA conference had grown to 374 participants this year from about 140 last year, so if the growth rate keeps up even one or two more years, it will soon become impossible for the conference to be hosted at a single company. Anyway, those of us who have had the experience these last two years have benefited tremendously from the hospitality of the host organizations. In addition to taking in the general ambiance and the ubiquitous fridges stocked with free soft drinks, the conference benefited from a  usability lab tour. In fact, only a few rooms were included on the tour since  Microsoft has an extensive facility spread over two buildings.
Compared with the WordPerfect facility we toured last year, the individual rooms in  Microsoft's usability lab  were smaller but there were more of them. In general, the impression was that of a facility set up for large-scale testing of many products with an attempt to minimize overhead. For example, the control room could be run by a single person who combined the roles of video tape producer, experimenter, and event logger. This was possible because the video taping mostly relied on fixed camera positions that could be determined in advance of the test instead of being changed during the user's work with the system. Event logging was done by a home-made software tool that allowed the observer to select encodings of the user's activities from dynamically expanding hierarchical menus. These codes were synchronized with the video record, allowing the retrieval of video clips of specific events even though the video was currently being kept on analog video tapes instead of being directly integrated with the test data in a multimedia database. It is typical of the state of the art in the CAUSE field (CAUSE = computer-aided usability engineering) that this logging tool was home-made for this specific laboratory rather than being a generally available system.
The conference was very rich in terms of topics and insights, but there were two issues that struck me as being prevalent in several of the presentations: the need for improved communication between usability evaluation and system design, and the advantages of faster and cheaper usability testing as well as other ""discount usability engineering"" methods aimed at getting usability results fed back to the design cycle in a matter of days rather than weeks or months.
For many years, usability professionals have been complaining that developers and system designers don't pay sufficient attention to usability. Jared Spool from User Interface Engineering (a consulting company) addressed this issue in the opening plenary, saying that developers were not inherently evil, so if they did not understand us, it was probably our fault for not communicating clearly enough. In other words, he suggested that we might be wise to take some of our own medicine and consider the developers as the ""users"" of usability engineering results, remembering our standard premise that if the users don't understand something is not because they are stupid but because the system is wrong.
The main approach to improving communication with developers seem to be to involve them directly in usability engineering activities. For example, Michael Kahn and colleagues from Hewlett-Packard had designed a new  usability inspection method that was mostly based on involving developers as the interface evaluators. One major reason for the success of usability inspections at HP was the ability to piggyback onto the existing methods for code inspection and other forms of inspection that were already widely used and respected in the company. One more form of inspection was seen as reasonable and as a comprehensible activity which developers and managers were motivated to undertake. For this very reason, HP used the term ""usability inspection"" combined with standard terminology from code inspection in order to allow participants to feel on familiar ground. Within the usability engineering field, however, the term ""usability inspection"" is used as a generic term to encompass methods like heuristic evaluation, cognitive walkthroughs, standards inspection, etc., so I prefer using the term ""formal usability inspections"" for the HP method to highlight its differences from many other methods.
Formal usability inspections rely heavily on the use of a separate moderator to manage the inspection meetings and to plan them in advance. Basically, the inspectors are given instructions prepared by the moderator and the ""owner"" of the interface with profiles of the main user categories and descriptions of some typical task scenarios. The inspectors then work through the scenarios on their own, noting usability problems at each step. They do so by applying a combination of several other methods, including a set of heuristics like those used in heuristic evaluation combined with a user goal-action-feedback model that can be viewed as a simplified version of cognitive walkthroughs. Finally, the inspectors meet with the moderator and combine their lists of usability problems. This last stage is a major educational experience for many of the developers who participate as inspectors, since they are exposed to a large number of usability problems which they had not found themselves. Thus, in addition to achieving an enhanced list of usability problems, the coordination meeting serves to emphasize the phenomenon that different people find different usability problems and that your own design intuitions are insufficient to cover the ground. A typical formal inspection of a mid-sized project takes about 142 staff-hours, which is somewhat more than the time needed for a heuristic evaluation which may only take on the order of 40-80 hours due to its less formal nature. Even so, the more formal approach was appreciated by the engineers.
As another example of involving developers to increase communication, Mary Beth Butler (now Rettger) from Lotus (now MathWorks) reported on a team method for categorizing usability problems. During user testing, she had some members of the development team sit in the background and note the usability problems they observed on index cards. Each developer would observe a few tests, giving as many developers as possible the chance to participate. The day after the tests, the usability staff and the participating developers would meet for a 90 minute debriefing session where all the index cards with usability problems were pasted onto the walls of a meeting room. Everybody would then move cards around until reasonable categories of usability problems emerged, after which the problem categories would be rated for severity. After the meeting, the usability specialist would write up a report with the categorized usability problems and any other data from the user test for distribution the following day.
By using this method, Lotus had succeeded in cutting the turnaround time for usability reports from about two weeks to two days, with some information getting to the development team the very day after the user test. In general, a major trend at the conference was the need to speed up usability work in order to impact design while it is happening. Kay Chalupnik from IDS Financial Services (an American Express company) mentioned that they needed instant evaluation turnaround to enable them to fax home the same night what was learned on a field study.
Since 1988, I myself have advocated  cutting user testing to about 3-5 subjects per test  (and then test more iterations). Even though one company said that they needed six subjects, the general feeling at the conference was that studies with larger numbers of test users were becoming rare. This approach was confirmed by Claus Neugebauer from SAP in Germany where they also used 3-5 users, since additional users only added about 15% to what they learned with three. Neugebauer also discussed several ways of  reducing the turnaround time  from the beginning of a usability study to the delivery of the final results. Most of the cuts stemmed from the notion of  constancy in usability testing  since they had many products that were similar in many ways. By always using the same questionnaires and other test instruments as well as holding the task outlines (if not the detailed tasks) constant, the preparation time for a study had been reduced from two weeks to two days, and data analysis had been cut from five to three days due to the ability to reuse templates in statistics and business graphics programs.
A further reason to speed up the usability process is to  increase the amount of actual usability data that is gathered. I have become concerned that usability specialists spend much more time writing reports, going to meetings, and such than they spend on actually creating new usability information. We might use the term ""exposure hours"" to denote the time spent with the users in user testing (but not setting up the test), inspecting the interface in heuristic evaluation (but not polishing the report), observing real users working in field studies (but not traveling to the customer site or talking with the users' managers), etc. From personal experience and from talking with people at the conference, it may well turn out that most usability specialists get  exposure hours that amount to no more than 10% of their working day. Better communication mechanisms and more efficient usability methods are needed to increase this percentage. Unfortunately, one problem with smaller studies and faster data reporting that was mentioned in questions is the difficulty of communicating usability results to other projects and people working on later releases several months or years later. A possible answer to these concerns might be an integrated CAUSE system for computer-aided usability engineering to handle design rationale hypertexts linking to usability test data that was collected automatically (or with minimal overhead) as well as other low-overhead reports that could make sense if they were given more context. Unfortunately, almost no research is underway on such environments.
The conference was also filled with practical advice and variations on common usability engineering issues. For example, Monty Hammontree from Sun presented a way to involve widely scattered users like system administrators in user testing without having to travel to their site. Instead of a traditional usability laboratory, Hammontree used a ""distributed usability lab"" running over a computer network, where an exact copy of the user's screen was slaved to the experimenter's screen and the user's comments were relayed by computerized audioconferencing. Nadine Nayak from HP mentioned that she had used a similar method to test international usability with users in Germany without leaving the U.S. (though she used a speakerphone rather than computerized voice transmission for the user's comments).
As a matter of fact, the difficulty of recruiting representative users for usability testing is so prevalent that a session was dedicated to a presentation by Liz Cowan from User Interface Engineering entitled ""stalking the wily test subject."" Out of the approximately 170 people in the room for that session, only 6 had a specialized person helping them with the job of  finding and recruiting test subjects  (I am happy to report that I was one of the 6, having a very competent person help me with that job). Again from the perspective of speeding up turnaround time for user testing, it really helps when one can just say ""I need 5 subjects of this-and-this kind"" instead of having to spend time coming up with a way of finding them. Microsoft had a database of 3,000 local users with various levels of experience in their products who were willing to come in for user testing, and normally each user was used a maximum of three times to avoid getting ""professional test users."" Cowan mentioned that the most powerful motivator for recruiting subjects often was the promise of exposure to cool technology in their field rather than cash payment though one should normally give the users something (either money or a T-shirt) to show good faith. Usually, Cowan needed slightly more than a week to find a set of inexperienced users for a test and as much as three weeks when specific kinds of experience was needed (e.g., experience running a special kind of printing press for color jobs). Even though managers were often inappropriate as test subjects for software intended for their staff, Cowan found that it was often a shortcut to convince a manager that the test was valuable since that could make it easy to get several subjects from that manager's staff. (See also later results from  two hundred companies' experience recruiting subjects for user tests)
In general, the conference was highly oriented towards practitioners with only a few researchers in evidence (though maybe they were just hiding since it was mentioned at the opening plenary that people without Ph.D.'s were often better at usability engineering). An interesting phenomenon was the presence of participants from at least seven major popular  computer magazines  (MacUser, MacWorld, PC/Computing, PC Magazine U.K., PC Week, PC World, Windows Magazine), many of which had multiple participants at the conference. Several of these magazines have recently started including usability test results or other usability engineering data in their review articles, and the rest are presumably thinking about doing so. PC/Computing has even instituted a ""PC/Computing Usability Seal of Approval"" awarded to products passing a specified level of usability on several quality criteria. One session was dedicated to discussing this trend based on a talk on PC World's experience entitled ""usability makes the difference to 4 million buyers""-referring to the number of readers who make their software purchasing decisions based on what they read in the reviews. Magazines conduct extensive surveys of their readers (who after all are their users), and PC World had found that readers indicated that usability was as important a review parameter as the more traditional issues of speed and features. Initial usability reviews were fairly informal (e.g., have a few users send a fax with fax software to get some usability anecdotes for a review), but recent tests have been more elaborate in employing usability labs (including borrowing an American Airlines simulator for a test of the usability of notebook computers on board airplanes). A major concern in reporting summative evaluations was the difficulty in reporting statistics like confidence intervals which are incomprehensible for the average reader. The solution so far has been to convey much of the same information in natural language to express the s trength of the editors' belief in their conclusions. In the two weeks following this session, I have already seen the word ""usability"" used in major headlines on the covers of two leading PC magazines.
In a session on consulting jobs, John Claudy from American Institutes for Research mentioned that they were often asked to perform summative evaluations for advertising purposes when a vendor wanted to claim that a certain user interface was X% more usable than the competition and wanted a credible source to cite. Initially, they had refused such contracts, but now they did the work under the condition that they could approve all advertising using their name. Of course, a company that loses a comparative usability test will just not advertise the result. A person from the audience commented that some companies do comparative benchmarking for internal use to see whether certain features or interface elements in competing products are any good, but again such results are of course not published, depriving the field of much valuable data. In this session, a distinction was made between consulting and contracting, where contracting involves doing regular work on a project (with the client's main benefit being the added flexibility of using outside staff instead of permanent staff) and consulting involves a more advisory role. Even though nobody wanted to reveal their actual  pricing  schedules, typical charges mentioned for usability contracting were $75 per hour and a $25,000 fixed price for usability testing with 20 subjects, whereas consultants are known to charge anywhere between $170 and $500 per hour. Of course, as mentioned above, most user tests involve significantly fewer than 20 subjects, and the cost of a ""discount user test"" with 3-5 users would be much lower than $25,000.
A fun part of the conference was a design competition organized by Jared Spool and Deborah Mrazek from Hewlett-Packard. Unfortunately, I was too busy with other obligations at the conference to actively participate in the competition, but about 70 people formed teams to design a customer-operated ordering kiosk for a hypothetical taco chain. After several elimination rounds, the three winning designs competed in an open session, where each of them was subjected to live user testing by subjects who were timed as they entered a rather large and complex order. The winner was the design that allowed the user to complete the order in the shortest amount of time. Of course, since the winning times were determined by a single test user for each design, there is no guarantee that the winning design would in fact be the fastest over a range of users, but then this competition was more in the spirit of a fun conference event than a commercial comparative evaluation. The design and testing were done with a  low-fidelity prototyping approach, using colored notepads for the buttons and signs on the kiosk, and using humans to simulate the computational processes. For example, one design used highlighting on the display to indicate the ""current item"" in the order (that could be modified by, e.g., requesting diet or light versions of the item), and this highlighting was simulated by having one of the designers hold a yellow transparency cutout over the line being ""highlighted."" Not only was the design competition entertaining for the audience and a motivating team-building exercise for the participants, it also taught many people the power of low-fidelity prototyping techniques compared with the elaborate computerized tools used by most usability groups.
In general, UPA'93 was a very enjoyable conference with a wealth of valuable information. Due to the increased number of participants and activity in the field, this year's conference instituted dual tracks making it impossible to take in the entire conference. Last year's conference was single-track and felt more like a large specialized workshop than a conference which I personally enjoyed more. On the other hand, it is hard to turn people away when they want to come and the field is in an explosive expansive phase.
All conference sessions were videotaped by Microsoft's extremely professional A/V crew, resulting in a set of 18 VHS video tapes that is available for $255.00 from American Production Services, 2247 15th Ave. W, Seattle, WA 98119, tel. (206) 282-1776, fax (206) 282-3535. Actually, the videos are cheaper than the conference registration fee, but then they do not include the great coffee and extensive networking opportunities offered to those physically present at the conference.
The list of videos (and thus the list of conference sessions) is as follows:

Volume 1, Opening Panel: An Exploration of the Hot Issues of Usability Don Ballman (Mead Data Central), Mary Dieli (Microsoft), Jakob Nielsen (Bellcore), Janice Rohn (SunSoft), Jared Spool (User Interface Engineering)
Volume 2, Practical Tips for Enhancing the Power of UI Groups in Software Development Organizations Bob Vallone (GO Corporation)
Volume 3, Discount Usability Engineering Six Years Later Jakob Nielsen (Bellcore)
Volume 4, Stalking the Wily Test Subject (how to get the right subjects) Liz Cowan (User Interface Engineering)
Volume 5, Tailoring Data Collection Methodologies to Meet the Research and Business Objectives of Usability Practitioners Bradford Hesse (American Institute for Research)
Volume 6, Usability Makes the Difference to Four Million Buyers (usability in the trade press) Dean Andrews, Thomas Grubb, and Greg Smith (PC World Magazine)
Volume 7, Using Low Fidelity Prototyping as a Usability Tool Jared Spool (User Interface Engineering)
Volume 8, Engineering Usability into the Lab Design Process Denise C.R. Benel (National Information Technology Center), Richard Horst (Man-Made Systems Corp.), Russell Benel (The Mitre Corp.)
Volume 9, Working with Many Masters (being an interface consultant) John Claudy (American Institute for Research), Larry Marine (IQ Cognitive Engineering), Joan Lasselle (Ramsey Inc.), Judith Ramey (University of Washington)
Volume 10, Structuring Usability within Organizations Sue Braun (Northwestern Mutual Life), Janice Rohn (SunSoft Inc.)
Volume 11, Usability Lab Tools Kent Sullivan (Microsoft), Nigel Bevan (National Physical Laboratory), Mary Beth Butler (Lotus), Monty Hammontree (SunSoft)
Volume 12, Methods for Investigating Usability During Early Product Design Judith Ramey (University of Washington), Stephanie Rosenbaum (Ted-Ed)
Volume 13, What's Your Problem (how to define what aspects of usability to look for) Amy Kanerva, Jill Rojek (Microsoft)
Volume 14, Usability Inspections at Hewlett-Packard Michael Kahn, Rose Marchetti, and Amanda Prail (Hewlett-Packard)
Volume 15, The Designer's View Mary Beth Butler (Lotus), Ken Dye (Microsoft), Mark Gowans (WordPerfect), Andrew Kwatinetz (Microsoft), Alice Mead (Lotus), Marshall McClintock (Microsoft), Jack Young (WordPerfect)
Volume 16, Why Test Documentation? JoAnn Hackos (Com Tech Services)
Volume 17, Usability Testing: It Seems to be a HIT (speeding up usability) Claus Neugebauer, Nicola Spielmann (Ergonomics Group SAP AG)
Volume 18, Closing Panel: Usability Horoscope of the 90s Kay Chalupnik (IDS American Express), John Claudy (American Institute for Research), Julie Humburg (Intuit Inc.), Jakob Nielsen (Bellcore), Irene Wong (Apple), Jack Young (WordPerfect)

"
