,timestamp,content
0,2022-10-23,"Interviewing stakeholders provides helpful information about their context, allows us to identify business goals they are concerned with, and increases their support.
A stakeholder is anyone who has interest in your project or with whom you need to work with in some way to complete the project. Understanding your stakeholders and their perspectives is key to your project’s success and is commonly done through stakeholder interviews.
Definition: A stakeholder interview is a conversation with a person who has a vested interested in a project with a goal of gathering insights to drive the project’s success.
In a user interview, a researcher asks a user questions about a topic of interest (e.g., use of a system, behaviors and habits) with the goal of learning about that topic. Similarly, in a stakeholder interview, the UX team member asks an internal or external stakeholders questions meant to shape the design process, define success metrics, and ultimately meet their expectations.
Why Do a Stakeholder Interview?
Stakeholder interviews drive successful stakeholder engagement and give us an understanding of our project’s landscape. This understanding can help us navigate obstacles before they arise and keep everyone engaged and supportive.
Stakeholder interviews help us:

Gather context and history. Every project comes with history: the origin of the project, known constraints, solutions tried before. If you work in a large organization, there is almost always politics at play. Stakeholder interviews can help us uncover and organize contextual variables and historical baggage that accompanies a project, prior to kickstarting it. Having this knowledge upfront will often lead to a more robust design process, save time, and avoid friction.
Identify business goals. As UX practitioners, we focus on our users and their needs. In contrast, our stakeholders focus on the business and its needs. Stakeholder interviews can provide us with data and insights about business problems and can pinpoint business objectives and key results. This knowledge ensures that you track the right metrics to prove project success, both from the business’s and users’ point of view.
Align on a shared vision. Understanding your stakeholders’ vision, beyond success metrics, is another benefit of stakeholder interviews. You can assess where it differs from your own vision, then align on a shared, collaborative vision for your work and the initiative at large.
Increase buy-in and communication. Ultimately, stakeholder interviews help stakeholders feel heard. They make them more likely to buy into the project and its output. They also help establish an open communication channel throughout the design process.  

When Should You Do Them?
It’s never too late for a stakeholder interview; however, the earlier in the process you can speak to stakeholders, the better. Ideally, stakeholder interviews kick off a project or initiative, welcome a new stakeholder, or offer a reset when something goes awry. The benefit of doing them early in the engagement process is two-fold: the stakeholder feels heard and you gather insight when it’s most helpful.
How to Conduct a Stakeholder Interview
Set a Goal
Similar to a user interview, you’ll want to set a goal for your stakeholder interview. What are you hoping to learn from the stakeholder? What are they uniquely positioned to help with? A concrete goal for the interview will provide structure to your interview.  
Your goal can range from to obtaining knowledge to securing their support or identifying their communication preferences. Some examples of goals include:

Identifying potential concerns or obstacles
Assessing technological limitations and previous explorations
Understanding competitors and potential market challenges
Outlining communication and involvement preferences
Establishing buy-in and support
Signing them on as a project champion
Creating shared success metrics

Prepare Questions
There are three types of interviews: unstructured, semistructured, and structured. Stakeholder interviews are most often semistructured:  instead of a script (which is used in structured interviews), you will use a discussion guide that provides direction but doesn’t need to be followed precisely.
Preparing a guide for your stakeholder interview ensures that your questions and topics are relevant to your interview goal. A good interview guide paves the way for a deep, free-flowing conversation. Use your interview goal to outline high-level topics to cover. If you’re not sure where to start, we recommend questions that cover these four high-level topics:

Success metrics: what success looks like, tangibly, in their eyes
Priorities: what they know or hear from users or customers and want to address
History and expertise: questions that target their unique perspective or role
Process and workflow: how they want to be kept in the loop

From here, using your high-level topics as a guide, create a rough list of questions to ask your stakeholder. Below is an example we often start with, then adjust depending on the stakeholder and their role or expertise.

[Introduction] I don’t believe we’ve had the opportunity to work together before. Can you tell me a little bit about your role?
[Introduction] What are you currently working on here at <organization name>?
[Success metrics] What are you aiming to achieve with that work? Why is that important to the organization?
[Success metrics] Do you have specific goals you’re trying to achieve or metrics you’re tracking?
[Success metrics] What does success look like for you and your team? (You can timebox this for the year, quarter, half year, etc.)
[Priorities] What challenges or business issues are currently top priorities for you? Your team?
[Priorities] I’m currently <designing/researching/conducting discovery> on <insert problem, project, product, area of the business>.  Do you have any insight you’d like to share related to those efforts?
[History and expertise] Can you tell me about any constraints related to the work I’m doing that you think would be important for me to know about?
[History and expertise] Can you tell me about any solutions to this problem or similar projects you’ve tried before? Could be technical, process-related, people-related, and so on? How did it turn out? Why do you think it succeeded or missed the mark?
[Process and workflow] What is your ideal level of engagement in the project?
[Process and workflow] What is your preferred communication method?
[Ending] Is there anything else that would be helpful for me to know?

Introduce Yourself and Create Rapport
You’ll notice above that we included a few questions as a part of an introduction. These questions are important for creating rapport and aid in building a long-term successful relationship with the stakeholder. We like to include a few sentences about ourselves, the purpose of the interview, and a request for transparency and honesty.
Hello. Thank you for agreeing to speak with me today. My name is <your name>, and I work with <team or manager> and we’re working on a <project/initiative> to learn more about <problem, project, feature, etc.>
I’m speaking with <other leaders, team members, stakeholders, people> in the organization this week to get your perspectives on the current experiences. We’re excited to get your perspective.
I’ll ask you a few questions today to get your feedback and input. There are no right or wrong answers; feel free to be open and honest; anything you share today will not be distributed or associated back to you. You also won’t hurt my feelings with any negative feedback; we need it to learn. We will take about <XX> minutes today, is that acceptable with your schedule?
Conduct the Interview
When conducting the interview, aim to speak to stakeholders one on one. Reach out to them  to schedule time via email. Include the purpose of the meeting, what to expect, and how much time you’re looking for. It is helpful to also include a sentence or two about the value of speaking to them.
Hi <stakeholder name>,
[Use if you don’t know the stakeholder] I’m <your name>, a <role> on <new project or initiative>. It’s nice to meet you and I’m looking forward to working with you.
As we kickoff <project name>, I’m working to gather important perspectives and existing knowledge about <users, previous solutions, etc>. We are hoping to leverage your expertise, establish success metrics, and gather any other insight that you think may be helpful before we begin.
Do you have <x time> available in the coming week or two to speak with me? I’ll have a handful of topics and questions prepared. We appreciate your time—it will help us be as effective as possible moving forward, as well as ensure a successful outcome.
Looking forward to speaking with you,
<Your name>
Consider framing the interview as a virtual (or in-person) “coffee.” The informal atmosphere helps relax both parties and, thus, makes the stakeholder more likely to open up.
As you conduct the interview, remain as neutral as possible and focus on active listening. Stakeholders will respond differently to interviews. Some may be quick to open and share insights, while others will need coaching. Ask open-ended probing questions to dig deeper on insights. For example, to gather more information, use prompts like:

Tell me more about that…?
Can you expand on that…?
Can you give me an example…?
Can you tell me about the last time that you did…?
How do you feel about that…?

To explore an underlying idea mentioned, use prompts like:

Tell me why you felt that way.
Tell me why you did that.
Why is that important to you?
Why does that stand out in your memory?

Stakeholders will all respond differently to interviews. Don’t forget your basic interview etiquette:

Help the stakeholder feel heard by taking notes, making eye contact, and offering words of acknowledgment like I see, and That’s helpful.
Even if the stakeholder is long-winded, don’t interrupt them. Try not to rush them and ask questions in a calm, relaxed cadence. This attitude helps communicate that you have time to listen.

Ending and Followup
As a final wrapup question, we like to ask Who else should I speak to? The answer to that question gives us insight into who they think may be valuable to the project (and potentially an introduction to another key stakeholder). Once finished with your questions, thank the stakeholder for their time and for sharing their expertise.
We also like to send a followup thank-you email. This message confirms that their time was productive and establishes an open communication channel, should any future questions arise.
Alternative Approach: Email Questions
If your stakeholder is unable to give you synchronous meeting time, try emailing your questions. Rather than sending them the whole interview guide, we suggest narrowing it to the top 4-5 questions that would be most useful for you.  
If you are interviewing a large group of stakeholders, you can also create a stakeholder survey. A survey will make data collection and analysis more streamlined. However, it is less likely your stakeholders will be as responsive as if you sent them an individual email.
Using the Insights
Analysis of insights from stakeholder interviews can be lightweight and informal or robust and systematic. We see teams take an informal approach when 5 or fewer stakeholders are interviewed. With this small number, it is easy to quickly outline major themes and concerns across the group and share them with the immediate team. These high-level insights are usually captured in a straightforward document.
In contrast, when 5 or more stakeholders are interviewed, teams may need to  conduct more rigorous data analysis, using thematic analysis. Thematic analysis is a systematic method of breaking down and organizing data from qualitative research by tagging individual observations and quotations with appropriate codes, to facilitate the discovery of significant themes. Working with the data from their stakeholder interviews, immediate team members will identify concerns, success metrics, and ideas mentioned by multiple stakeholders.
The takeaways that emerge from this analysis should help inform all aspect of your project: UX research, ideation, priorities, timeline, resources, and stakeholder-engagement plans.
Conclusion
UX-stakeholder interviews help us gather any information that may help shape the design process, define success metrics, and ultimately meet stakeholder expectations. They save us time and resources by minimizing redundant work and lay the foundation for successful relationships with stakeholders."
1,2022-10-23,"The practice of shadowing is widespread in the corporate world. Interns, apprentices, or new hires will observe colleagues to understand how they do their job. A short period of shadowing an experienced staff member is often used to help people learn how to execute a particular task.
Field studies are a UX research method that is rooted in the practice of shadowing. Field studies involve observing and interviewing participants in their environment while they engage in their natural behavior and complete their daily routines. A field study will often target a specific activity and audience. For example, you might want to see how people make purchase decisions, use devices, or interact with others at work. The crucial aspect is for you to go wherever your users are — in an office, a vehicle, a store, on the street, or on a production line.
When shadowing a user in a field study, you follow her around, take notes of what’s happening, and choose whether to interfere and ask questions on the spot or wait until the end of the activity (or the session) and ask questions afterward. Two types of field studies are contextual inquiry and direct observation. In contextual inquiries, researchers can intervene by clarifying questions during the activity; in direct observation, the interaction between the researcher and the participant is kept at a minimum during the activity of interest, and questions are usually reserved for the end of the activity or the session. 
In theory, it seems simple to watch activities, ask questions, and discuss what happened. However, in practice, one might face multiple obstacles — from interpersonal conflicts and local organizational politics to issues with budget and recruitment. Below are five common hurdles, alongside recommendations on how to avoid them.
#1: One Length Does Not Fit All
How long should a field visit be? The answer, of course, depends on the activities and behaviors you are interested in. Estimate the length of the session by approximating the duration of each of the tasks of interest and then adding some extra time for discussion.
That being said, depending on the type of field study, long sessions may not be feasible. For example, if you’re running an in-person field study, you may be able to manage longer sessions, especially if they involve following the user from one meeting to another. But if you’re running a remote field study, it may be difficult to sustain attention and engage in observation sessions longer than 60 minutes due to screen fatigue. If you can interrupt, ask clarifying questions, and interact with the user, longer remote sessions are feasible. 
As a rule of thumb, avoid scheduling more than 2 hours with a user. It is better to schedule additional sessions rather than to rush and try to fit multiple activities into one. Remember: less is more; the quality of observation and analysis is more important than the sheer number of tasks observed. 
Therefore, in the initial interview before the field visit, discuss the participant’s schedule and routines so you don’t join them on a lunch break and miss their work time when they use the software you are interested in observing.
#2: Field Visits Become Complaint Sessions
There is a chance that your sessions with a participant might become too focused on “wish lists” if they believe your role is to document their complaints — finally, there is someone ready to hear about all their pains with a system or process! To mitigate this danger, you need to clarify that the purpose of a session is not to document all the problems with an application or service but to understand the participant’s tasks and daily practices. Some participants will come to the observation session with a list of problems to go through with you. Thank the participant for their thoughtfulness, take that list of complaints, assure them that these will be passed over to the team, and return to observing the habitual actions and processes that you’re interested in. Pass along the list of complaints and follow up with the participant with the latest updates. Remember that while users make feature requests, they might not think of the product as a whole, so instead of asking them what they want, focus on understanding their behaviors and activities and the contexts in which these unfold.
#3: Individual Sessions Transform into Focus Groups
In a field-study session, we do not want to observe more than one person at a time because we need to immerse ourselves in that person’s activities and context. Avoid group sessions and resist the temptation to save time by observing several people at once. 
There is a time and place for focus groups; however, focus groups do not provide enough context for what people do in their daily life. This does not mean that a field study should ignore interactions that the participant naturally has with other people; rather, we do not want other people to be invited to jointly participate in the session with the purpose of explaining the system to the researcher. On the rare occasion when two people always work together as a pair (e.g., in pair programming), you can include them both in the field-study session. 
#4: The Relationship Model Is Skewed
During a field study, we want to create a partnership with the participant so they guide us through and feel comfortable sharing their experiences. However, this relationship might slide into behavioral patterns that bias our sessions. For example, if questions start taking most of the time and the session feels like an interview, refocus on the activity and pause the discussion. 
Another potentially biasing situation tends to happen in private spaces such as one’s home. Your study participant might naturally want to be a considerate and welcoming host and, as a result, might focus too much on the observer’s needs by offering food and drinks or engaging them in conversation. (The extent to which they do so may be very much culture-specific.) These patterns alter normal user behavior. Consider accepting a glass of water to make the participants feel that they fulfilled their role as hosts and allow them to focus on the task at hand. 
#5: Abstract Descriptions Hide Concrete Behaviors
When people tell you what they usually do instead of doing it or when they share what they might do in the future, there is a danger of becoming too abstract and l glossing over details of specific events. One way to notice abstraction is to look for phrases such as “in general…,” “in our company we…,” and “we usually…”. Researchers should try to steer the participant away from abstraction and toward specific instances by asking the participant to walk them through their activities and using probing questions. For instance, if you ask somebody how they spend their mornings, you will get a very different level of detail than if you asked them how they spent this morning. 
Conclusion
Field studies help teams stay grounded in the real-life experiences of their customers. This research method is quite flexible and inherently more intimate as it involves spending time with our users in their natural environment. However, some stakeholders are skeptical about ethnography-inspired methods because of the observer effect –- that is, because participants may alter their behavior if they know that they are being watched. To account for the observer effect, traditional ethnographers spend a long period of time together with their study participants to let them get accustomed to the researchers’ presence. 
Even though UX researchers don’t have the luxury of observing participants for a long time, they can still get valuable insights from field studies since, with the proper setup (i.e., study duration, setting expectations, and creating a partnership model), participants quickly get into their workflow and forget that you are there. During moments of concentration and deep work, people eventually return to their default behaviors and reactions, allowing researchers to derive valuable insights that can ultimately drive UX improvements.
Reference
Hugh Beyer. and Karen Holtzblatt., 2017. Contextual Design: Design for Life Ed. 2, Morgan Kaufmann Publishers."
2,2022-10-02,"Here’s a list of NN/g’s most useful articles and videos about working with UX stakeholders. Within each section, the resources are in recommended reading order.
For hands-on training, check out our full day course on building successful stakeholder relationships.
UX Stakeholders: Overview
A stakeholder is anyone who has interest in your project or with whom you need to work with in some way to complete the project.
Your CEO, the marketing director, the account manager, or even your manager could all be stakeholders. Stakeholders can be internal to the organization or external to it. If you’re not sure who your stakeholders are, start by asking yourself who is interested in your project and who has power, influence, or control over it.
If you’re new to considering how to include stakeholders in your UX work, start here:




Number


Link


Format


Description




1


Stakeholder Analysis for UX Projects


Article


How to identify your key stakeholders and how they impact your work  




Involving Stakeholders in UX Research




Number


Link


Format


Description




1


Involving Stakeholders in User Testing


Article


How to get all design-team members to observe usability testing to increase buy-in and empathy




2


Collaborating with Stakeholders


Video


An 8-step process for facilitating collaboration between UX professionals and their stakeholders




3


How to Collaborate with Stakeholders in UX Research


Article


How to convince stakeholders to participate in design and research projects




4


Tips for Motivating Stakeholders to Participate in User Research


Video




Workshops Involving Stakeholders




Number


Link


Format


Description




1


How to Get Stakeholders to Sketch


Article


How to make group sketching more productive and effective with stakeholders




2


How to Get Stakeholders to Sketch


Video




3


Facilitating UX Workshops with Stakeholders in the Room


Video


How to maximize the positive contribution of a variety of stakeholders in workshops  while minimizing any disruptive or negative impact




Communicating and Presenting to Stakeholders




Number


Link


Format


Description




1


How to Sell UX: Translating UX to Business Value


Video


How to translate UX work into a business impact that stakeholders will understand




2


Presenting Low-Fidelity Prototypes to Stakeholders


Video


Sharing low fidelity user-interface prototypes with stakeholders is a great way to transfer knowledge and get buy-in early




3


UX Stories Communicate Designs


Article


Using well-crafted UX stories to help internal audiences empathize with users and buy into your design goals




4


Presenting to Stakeholders


1-hr Talk


How to build empathy and communicate value to stakeholders




5


How to Handle Bad UX Requests


Video


How to address unreasonable requests by focusing on outcome goals and  return on investment of proper UX effort




6


Making a Case for UX in 3 Steps


Video


How to effectively make a case for UX by Understanding your audience, speaking its language, and calculating potential savings wherever possible"
3,2022-09-11,"Distributed Cognition and User Experience 
In the 1998 article called “The Extended Mind,” philosophers Andy Clark and David Chalmers wrote about the active role of the environment in cognitive processes. The idea that “cognitive processes ain’t (all) in the head!” has become foundational for the distributed cognition theory (DCog). The critical point of that theory is that thinking is not confined to the limits of one person and their brain but occurs when people interact with each other and with material artifacts. 
DCog has been successfully used to explain how complex systems of people and technology work together. For example, the distributed-cognition approach was applied to examine the design of ship-navigation systems, air-traffic–control rooms, and shared-database systems. One of the most well-known examples is Ed Hutchins’s work on commercial-airplane cockpits. He showed that to understand what happens in a cockpit, one needs to look at both the rules and instructions, the layout of the space, and the communication among people in the air and on the ground. For instance, Hutchins observed how pilots relied on external scaffolding: distinct interface design with buttons of different shapes and sizes, pieces of paper, and “speed bugs” — physical markers on speed indicators that correspond to specific speed targets for takeoff, landing, and maneuvers. 

Commercial-aircraft cockpit 

Aerosavvy.com: Image of the plane's speed indicator with small markers called speed “bugs” (red and white in the image) that pilots set before takeoff and landing to remind themselves of the maneuvers they are supposed to do as the plane reaches different speeds
Context CUEs in UX: A Framework for Observations and Analysis
Field studies aim to study how a group or individual functions in their natural environment while doing an activity of interest (e.g., interacting with a product).
When conducting field studies, UX professionals face an avalanche of data that must be observed, recorded, and analyzed. DCog can help decide what needs to be captured so data can be easily clustered later during the analysis. 
One way to apply DCog is through the context CUEs framework, which helps to guide the collection of the complex qualitative data by parsing it into three categories: the culture, the unspoken (e.g., practices and assumptions), and the environment (CUE). Each of those categories serves as a prompt to observe users in the field and take holistic notes about the context of use. Below we explain what each letter stands for:
C — Culture: These are observations that reveal a shared implicit and explicit set of behaviors, beliefs, concepts, rules, and values of a particular social, ethnic, or age group. Questions to ask oneself when probing for culture: 

What and how do users do <X>?
When do users reach out to others? 
Why do they do <X>? 
How do they explain <X>?  

Where X is the phenomenon you are interested to understand.
U — The Unspoken: These are observations of practices and assumptions rooted in tacit knowledge about how the group or product functions. Questions to ask oneself when probing for the unspoken: 

When was the last conflict in the group, and how was it mitigated? 
What language one can or cannot use?
What are the questions new group members or new users ask and do not have a resource to go to?
What happens when a group member leaves? 
Are there any issues involving access to necessary information (e.g., files, daily tasks)?

E — Environment: These are observations related to the material, social, and online surroundings of the user or of the group of interest. Questions to ask oneself when recording the environment: 

Where are we? 
How does it look, smell, and feel?
Is this the only space where <X> happens?
 How accessible is this place?

Context CUEs are all around us in our daily lives. For instance, the many variations of restaurant etiquette around the globe make for a great example of how culture, unspoken rules, and the environment come together. Thankfully, in restaurants, written signs, waiters’ instructions, or the behaviors of other customers often help us navigate the space.  






How people behave in a restaurant is the product of previous experiences, written and unwritten rules, explicit instructions, other customers’ behaviors, and the affordances of the environment.
To summarize, the context CUEs framework serves as a reminder to look for cues in the field. We want to ensure that we stay attuned to how people reduce their cognitive load by interacting with others, using external memory, or creating scaffolding in their physical environment. There is no preset mandatory list of all aspects of culture, unspoken habits, and environment that one needs to use all the time. The variation comes from the specific research question of your study. 




Research Question


Contextual cues




How do people who work together memorize, share, and recall information?


Sticky notes, memos, shared notes




How do people navigate space when they travel?


Maps, landmarks, signs, look at or follow others




How do people know how to behave in a new setting, for example, at a new restaurant?


Language, signs, explicit instructions, observing others (mirroring)
 




Example: UX Professionals’ Daily Lives
Suppose we want to conduct a contextual inquiry about the daily lives of UX practitioners to understand the challenges they face, as well as the similarities and differences in their work practices across multiple disciplines, industries, and products. In that case, we can plan to divide our observations around the following aspects of distributed cognition:

Culture:
	
Demographic of the group, languages, and education levels
Communication channels used by the group (Slack, email, texting) 
Team structure and hierarchy
Holidays and celebrations


The unspoken:
	
Tacit knowledge of tools used by the group (e.g., Miro, Figma, roadmaps, wireframes, laptop)
Skills and know-hows (e.g., presenting to stakeholders, notetaking)
Jargon and abbreviations (e.g., Agile, UX, KPI, IA)


Environment:
	
Space where daily activities happen (e.g., laptop, remote meeting, whiteboard, office, home)



You might notice that something like a laptop could be both the tool and the environment, especially if we look at remote workers. It is fine to have overlapping categories.
Contextual Cues Lead to New Opportunities 
Good designs minimize cognitive load to maximize usability. As UX professionals, we want to create products that allow users to offload some of the task demands on external processes and artifacts — that is, products that facilitate and scaffold external thinking. 
Context-based observations are fodder for product improvements and opportunities. If you notice that people always refer to a physical address book to enter information in a form, you can provide them with an online directory so they can directly access that information. 
Or, if you notice that people go through the trouble of scanning a physical business card and then adding it to their list of contacts on their phone, you can, as LinkedIn did, simply provide them with the opportunity to exchange contact information via Bluetooth or by scanning a QR code.

A LinkedIn user can scan someone else’s LinkedIn QR code to connect with that person without relying on memory, business cards, or address books.
Conclusion
Field studies provide rich data that might seem overwhelming and hard to capture consistently. One way to facilitate observations is to use the context CUEs framework, which is rooted in the DCog approach. It provides three broad categories of context cues (culture, the unspoken, and the environment) that nudge practitioners to account for the complex interactions and interdependencies among people, space, and artifacts. By taking note of the dimensions of distributed cognition during a field study, we expose information flows, walkarounds, limitations, and user pain points. These details about how users behave in their natural environment can uncover opportunities for improving the overall user experience of our products. 
References
John Martin Carroll. 2003. HCI models, theories, and Frameworks: Toward a multidisciplinary science, Amsterdam: Morgan Kaufmann Publishers.
Andy Clark, David Chalmers. ""The extended mind."" Analysis 58.1 (1998): 7-19.
James Hollan, Edwin Hutchins, and David Kirsh. 2000. Distributed cognition. ACM Transactions on Computer-Human Interaction 7, 2 (2000), 174–196. DOI:http://dx.doi.org/10.1145/353485.353487 
Edwin Hutchins. ""How a cockpit remembers its speeds."" Cognitive science 19.3 (1995): 265-288."
4,2022-09-04,"With the rise of global action around data privacy and protection laws, researchers need to think about how participant privacy is maintained before, during, and after a research study. While the data collected is valuable to researchers, it is even more important to the participants in a research study. It is our responsibility to make sure we do not violate research- ethics principles and we respect participant involvement every step of the way.
Why Protecting Participant Data Is Important
As data collection and dependence on the Internet have risen, so have data breaches and cyber threats. Research participants are more vulnerable than ever as researchers conduct remote usability tests, use third party applications, or store and share data online. In 2021, according to Politico, nearly 50 million people in the U.S. faced a health-data breach. Data breaches pose huge privacy and security concerns for consumers and cost the health-care industry billions of dollars. To avoid such data losses and privacy infringements, regardless of industry, we need to develop privacy and security practices that seamlessly fit into the user-research process.
Key Terms
Data-privacy terminology isn’t always the easiest to understand. Before discussing best practices for data privacy and protection, we need to define a few terms.

Encryption is the process of scrambling data or converting it into code. Encrypted formats make the data unreadable, which is important when transferring and sharing data online. Encrypted data comes with a key provided by the sender. This often takes the form of a password or passphrase.
Confidentiality vs. anonymity: These terms are often used interchangeably but understanding the difference between them is critical for protecting research participants. Confidentiality is the state of keeping information private. When data is confidential, it means that it is known and associated with a particular participant, but that information is not shared with others. In contrast, the information is anonymous if it cannot be linked back to the identity of the participant who was the source of that information.
Informed consent is the process through which research participants (1) are informed of what data will be collected and how it will be used and (2) agree to these terms.

6 Best Practices for Protecting Participant Data in User Research
While data-protection efforts should be considered carefully for each study, below you will find a list of foundational best practices to follow before, during, and after a study.
Before Data Collection Begins
1. Establish a data-management process.
When it comes to data, you want to be proactive.  Create guidelines about how data should be collected, stored, protected, and shared with others. Then make sure you communicate these to all the members of a team. A designated editor can update the guidelines as laws or company policies change. Guidelines should include information about:

Consent forms and what they should include
Storing and sharing participant data
Deleting data when a study is complete
A plan in case there is a data breach

2. Develop a data-collection plan for preserving participants’ confidentiality.
Before collecting your data, understand the laws and regulations that require data to be confidential. Laws are dependent on where you live, but a good place to start is European Union’s General Data Protection Regulation (GDPR), which is considered the strictest privacy and security law in the world. While laws and regulations are complex, they exist to minimize the risk of data breaches and cyber threats. Researchers need to follow the law and use it as a guiding framework when developing a data-collection plan. Your data-collection plan should include what data will be collected, how it will be used, and who it will potentially be shared with. Developing a data-collection plan around confidentiality requires researchers to ask themselves the following questions:

Do I need to collect this identifiable data? Will this data affect my results?  As a researcher, you need to be able to justify the identifiable data you are collecting. A good way to do this is to make a list of identifiers you are collecting and describe why you need each and how it will be used. This information will be helpful for creating participant-consent forms, as well.
What tools should I use to collect data safely and securely? When multiple people are conducting research, you often end up with multiple tools and, as a result, multiple ways in which data is stored. Teams need to decide which tools to use before data collection, to avoid having data stored in multiple places and, thus, manage the risk associated with a data breach.

3. Informed Consent
Informed consent creates a two-way street between the researcher and the participant before the start of data collection. Researchers inform participants about what their involvement in the study entails, what data will be collected, and how it will be used. Then, participants are given the opportunity to make an informed decision about their involvement based on the information that is provided. This communication is typically presented as a consent form, which should include the following:

Information about the study and the activities involved
What data will be collected
How the data collected will be used
Steps researchers will take to secure their data

During and After Data Collection
4. Maintain participant anonymity. 
Anonymity should be preserved while taking notes, while cleaning data and preparing it for storage, or while disseminating results. Qualitative researchers need to pay close attention to how they present participants’ personal details. While they may not be using names and other key identifiers, personal information can still be deduced based on individual or group traits represented in the data. With that in mind, research teams should follow these best practices:

Be intentional about the data they collect and ask only for information they really need from participants.
Do not use participant names or any other key identifies (e.g., Social Security number, date of birth) in notes and file names.
In usability testing, pause the recording while participants type in usernames, passwords, addresses, or other identifying information. You can also consider using fake credentials if appropriate for the study.
If, in spite of precautions, a video or audio recording does include identifying information, make sure to delete that part of the recording or blur it as soon as possible.

5. Share files in a secure way with only those people who need them.
In general, researchers should control who has access to data and when they have access to it. Access should only be given to people who actually need it. Researchers should not collect and share data on cloud-storage services like Google Drive, Dropbox, and One Drive. The challenging part about cloud storage is that you don’t have complete control over data. For example, if cloud services are down or hacked, you don’t have the control to fix the issue. While using cloud services makes it easy to share information, researchers do not have complete control over data stored in the cloud and further expose research participants to data breaches and cyber threats. Instead, researchers should consider using an external drive to storing encrypted data and find more-secure ways to share data (e.g., using secure file-transfer services like Hightail).  
6. Delete data that is no longer needed.
It can be tempting to hold on to data but one of the best ways to protect data is to delete it once you are done with it. This practice reduces data-breach risks. There will also be instances where your client or participant wants data permanently removed. To make this as easy as possible, researchers should have:

A clear way to identify and retrieve a participant’s individual data. For example, researchers could follow a standard format for how they name data files and folders. This practice makes it easier to navigate to the appropriate folder with all the data from that participant. If you include participant data in a presentation or other deliverables, you could use tags in data files to indicate where that data point was used externally.
No duplications of data in multiple places
A consistent way of storing data so it can be found easily in the case it needs to be deleted immediately. Researchers shouldn’t be searching for places where the data files could be. They should already know where it is. Consistency helps researchers keep track of the data effectively and efficiently.

Applying Best Practices
Applying privacy and security best practices shouldn’t involve extra work. Rather, it should fit seamlessly into the 5 steps of the user-research process:

Develop a research plan
Recruit participants
Conduct research
Synthesize Data
Share Data



How privacy and security best practices can be integrated into the 5-step user-research process




How privacy and security best practices can be integrated into the 5-step user-research process


To heighten the value and impact of privacy and security of research participants at scale, these best practices should be implemented into existing Research Operations (ResearchOps). ResearchOps streamlines dedicated roles and efforts toward managing operational aspects associated with privacy and security. Building in these practices and finding ways to operationalize them allows researchers to spend more time conducting studies and uncovering insights at scale, in a safe and secure way.
Conclusion
Maintaining participant data privacy and security should be a priority for all researchers. As researchers continue to develop new technologies and work with more research participants to do so, following these best practices is an important place to start.
Reference
Kaiser, K., 2009. Protecting Respondent Confidentiality in Qualitative Research. [ebook] Chicago: Qualitative Health Research. (Nov. 2009), DOI: https://doi.org/10.1177/1049732309350879"
5,2022-08-28,"The number of mobile augmented-reality (AR) applications and their popularity are rapidly increasing. It is only a matter of time until companies in various domains, including health, ecommerce, education, and gaming, incorporate AR features into their mobile apps.
As with any digital product, AR apps benefit from user testing before and after launch.  While conducting an AR usability study has many similarities with other mobile studies, certain factors can impact the quality of the study and of the findings. For example, some AR apps involve (sometimes intense) physical movement, while others require the participant to share a camera capture of their surroundings with the facilitator.

Plaicise –- AR Fitness Game (left) and Home Court- Basketball Training (right) are two AR apps that involved physical exercise. For both these apps, users had to stand far away from their device.


During the test of the Augmented Berlin, an AR educational storytelling app about the Berlin Wall, the participants could see the story elements projected in their room. That required not only ample space around the participant, but also that the remote-testing users be willing to show their surroundings on camera and share them with the facilitator.

We recently ran a usability-testing study of mobile AR applications. The study involved 7 in-person sessions and 4 remote moderated sessions. In this article we share the lessons we learned about setting up such a study. In particular, we focus on the following main components:

Tasks
Study setting
Session duration
Recording equipment
Participants’ profile(s) and recruitment
Informed consent
Running the study

Tasks: Wording and Order  
Since users are often new to AR technology, tasks should be clear and straightforward to reduce potential confusion or misunderstanding.
For example, a task in our study required users to ""download the application ARLOOPA and take a photo of a few animals in the room.” This task was intended to get the participant to use the AR feature to add a few virtual animals to the user’s environment. However, the participant misunderstood the task and to the facilitator’s surprise, tried using the app to take some pictures of his pets (a cute dog and fish tank). The facilitator had to patiently redirect the participant and ask him to add some virtual animals to the room. Eventually, the user added an AR dinosaur to the scene.

ARLOOPA app: A study participant misunderstood the task of “taking a few pictures of the animals in the room” and, at first, took a picture of his dog (left). Once the facilitator asked for virtual animals, he was able to include an AR dinosaur (right).

You should be ready to provide more details than in a regular user test and may even have to guide participants through the initial steps of the task — for example, you may need to help them identify the AR feature within the application.
Alternate tasks that require physical activity with tasks that do not. Even participants who are physically fit may get tired after doing several physically demanding tasks in a row. Alternatively, if you are testing only physical tasks, consider offering one or more breaks to users during the session. Make sure you have water on hand, as well.
Consider categorizing the tasks based on priority for the study, need for physical activity, and space requirements, to make it easy to select the appropriate tasks for each participant.
Study Setting
Some AR apps require ample or open space to function. If you are running an in-person study, make sure that there is enough space in the testing room for the tasks to be accomplished without participants bumping into objects. If your study is remote, depending on the types of apps you’re testing, you might need to let participants know in advance that they will need to have plenty of space around them.
While working with an AR app, participants typically focus on the task and the AR objects and may disregard their surroundings. Thus, prevent hazardous situations by thinking ahead about potential risk factors, such as sharp objects, moving furniture such as chairs, or slippery surfaces. Also, keep in mind that some activities are designed for outdoor setting and might not work properly in a closed room.
If you’re testing applications where the device needs to be far away from the participants, you may also need to have a device stand to enable participants to still see their device’s screen while interacting with the AR app.
Piloting the test beforehand, while recommended for any usability study, is crucial for AR studies because it can help you determine not only whether your tasks are clear, but also whether your room is safe and, also, how long the study session should be (see below).
Session Duration
You might need to allocate a longer time to an AR-testing session compared to a regular mobile-testing session, as the participants are likely to be less familiar with the AR technology and it might take them longer to complete the tasks.
Moreover, some AR apps can take a long time to download, and that can also increase session time if participants will need to install apps on their devices during the session.
You might consider sending study participants a list of apps to download beforehand. However, the danger of this approach is that, in a desire to “do well” during the test, participants will play with the apps before the session and, thus, contaminate your findings. Therefore, if you do decide on sending apps to users in advance, make sure you do it only shortly before the session (15-20 minutes before).
Alternatively, you might ask the participant to start the download at the very beginning of the session, after you introduced yourself, and then, as the apps load, you can talk about the study goals and explain the protocol to the participant.
To save time, you can also ask participants to sign consent forms ahead of time. Make sure, however, that participants do understand that they will be recorded and how their data will be used.
Recording Equipment
Often AR activities require participants to move around.  These interactions with the physical environment can uncover important limitations or opportunities in the AR interface.
If your application involves such interactions with the physical environment, it’s a good idea to run the study in person and record both the participant’s phone screen and their movement in the testing room.  In mobile-usability studies, we often use document cameras to record participant’s fingers as they are interacting with their phones. However, depending on the study’s goal, that may not be enough for your AR study.

The setup of our in-person AR usability-test session: The participant uses his personal phone, which is running screen-sharing software and is connected to the facilitator’s computer for screen recording.


The setup of our in-person AR usability-test session: A second camera is recording the participant’s activities in the room.

While the phone screen can be recorded using Zoom or another similar software, we recommend that you use one or two tripod-mounted cameras to capture participants’ interactions with the physical environment. Have backup batteries and memory cards for these cameras. Also, doublecheck the recording limitations of the camera. Often DSLR cameras have a 29–59-minute recording-duration limit, which can interrupt the process of usability testing. If this is the case, the facilitator will have to move to the camera and restart the recording, which can be distracting for participants and impact their natural workflow.
Participant movement in the room can also impact the quality of your audio recording.  Make sure that the participant's voice can be heard and recorded when the camera or the phone are far from the participant — for example by having the participant use a wearable microphone.
Note that some AR experiences use storytelling and narration throughout the experience. Thus, these narrations might also be recorded in parallel to the participant's comments, which will make hearing the participant's voice difficult. Either adjust the audio configuration of your recording software so it prioritizes human voices over the device’s output or rely on the external camera for reviewing the participant's comments.
Choosing the Right Participants
AR apps can involve a lot of movement; some are even intended as fitness games. Based on the goal of the study and the type of tasks, your screening criteria should include factors such as participants’ age, health conditions, or physical ability. You do not want to jeopardize participants’ health, especially since some participants may feel pressured to do tasks that they are not totally comfortable with during a user test.

Plaicise – AR Fitness Game required extensive physical activity and movement, such as jumping, push-ups, and side-to-side movement (bottom right and left). Even though the participant was a healthy 32-year-old, he was exhausted by the end of the task. This type of task can have potential health risks for older adults or people that are not physically ready.

During the test, pay close attention to participants’ reactions and look for any signs of discomfort or other unforeseen dangers. Should such signs occur, stop the activities immediately.
Even minor details such as wearing glasses can impact the performance of the participant throughout an AR study. For instance, if the participant is playing a physical-exercise game and the device needs to be placed farther away, they may not be able to read instructions because they may not have their glasses on (or may not want to wear them during the physical activity).

Home Court- Basketball Training app: The participant didn’t have her glasses and was not able to read the instructions for how to use the app because the phone (placed on the floor, next to a chair) was too far away. She said, “It wasn’t clear, from the first screen how to do this. […]’’And then, like at this point, I usually wear glasses for distance.”

Informed Consent
It’s always a good idea to let participants know what to expect during the study, especially if doing so does not risk compromising the validity of your research findings.
Therefore, before the study, inform participants of whether:

They will need to do any physical activities.
They will need to wear contacts instead of glasses or have glasses with them and be okay with wearing them during physical activities.
Their surroundings will be recorded (for a remote study — since some people may be uncomfortable with sharing their personal spaces on camera).
They will need to have ample space around them (for a remote study).

Allow users to withdraw from the study — before the study or during the study. And, if your tasks involve physical movement, remind them to charge their phones before the session — a plugged-in phone can be a tripping hazard.
Helpers
If possible, have an observer or another person who could help the facilitator during the session. While the facilitator is focused on moderating the session, the helper can take notes, monitor the cameras, change any memory cards, or restart the recording as needed.
During the Session
Creating a customized checklist of the steps to take during the session can help you keep track of the many moving parts of an AR study.
The rule of thumb is to be calm and patient with the participants — even more so during an AR study than in a regular user test. Participants are often new to the world of AR and might not be familiar with AR patterns. This unfamiliarity will make some of the tasks confusing to them. Resist the urge to rush people through the activities and give them the chance to explore the app as they would do in their natural setting.
Remember that some people may not even know what AR is. You might occasionally need to guide them to move the test along. Make a note of all the situations where participants needed help, as those are improvement opportunities.
Conclusion
Mobile AR technology has a lot of potential in various domains and proper usability testing can ensure the success of the product and the return on investment. While conducting an AR-usability study can seem a complex process at first glance, with proper planning and preparation you can achieve your research goals. A significant part of this process is understanding your participants and putting yourself in their shoes, as many will not be familiar with AR technologies or may not be physically able to perform certain activities Explain your tasks clearly, using words that they can understand. Be aware of the setting and of potential safety hazards. Use an external camera to record the participant’s movements. Pilot the test to make sure that the study room or setup and the study duration are reasonable and that the tasks are all easy to comprehend. And last but not least, be patient with your participants, explain what’s going to be required of them during the session, and resist the urge to rush through the tasks."
6,2022-08-21,"

















For further detail, see When to Use Which User-Experience Research Methods."
7,2022-08-17,"Uncovering themes in qualitative data can be daunting and difficult. Summarizing a quantitative study is relatively clear: you scored 25% better than the competition, let’s say. But how do you summarize a collection of qualitative observations?
In the discovery phase, exploratory research is often carried out. This research often produces a lot of qualitative data, which can include:
Qualitative attitudinal data, such as people’s thoughts, beliefs and self-reported needs obtained from user interviews, focus groups and even diary studies
Qualitative behavioral data, such as observations about people’s behavior collected through contextual inquiry and other ethnographic approaches
Thematic analysis, which anyone can do, renders important aspects of qualitative data visible and makes uncovering themes easier.
What Is a Thematic Analysis?
Definition: Thematic analysis is a systematic method of breaking down and organizing rich data from qualitative research by tagging individual observations and quotations with appropriate codes, to facilitate the discovery of significant themes.
As the name implies, a thematic analysis involves finding themes.
Definition: A theme:

is a description of a belief, practice, need, or another phenomenon that is discovered from the data
emerges when related findings appear multiple times across participants or data sources

Challenges with Analyzing Qualitative Data
Many researchers feel overwhelmed by qualitative data from exploratory research conducted in the early stages of a project. The table below highlights some common challenges and resulting issues.





CHALLENGES
RESULTING ISSUES



Large quantity of data: Qualitative research results in long transcripts and extensive field notes that can be time-consuming to read; you may have a hard time seeing patterns and remembering what’s important.


Superficial analysis: Analysis is often done very superficially, just skimming topics, focusing on only memorable events and quotes, and missing large sections of notes.




Rich data: There are lots of detail within every sentence or paragraph. It can be hard to see which details are useful and which are superfluous.


Analysis becomes a description of many details: The analysis simply becomes a regurgitation of what participants’ may have said or done, without any analytical thinking applied to it.




Contradicting data: Sometimes the data from different participants or even from the same participant contains contradictions that researchers have to make sense of.


Findings are not definitive: Analysis is not definitive because participant feedback is conflicting, or, worse, viewpoints that don't fit with the researcher's belief are ignored.



No goals set for the analysis: The aims of the initial data collection are lost because researchers can easily become too absorbed in the detail.
Wasted time and misdirected analysis: The analysis lacks focus and the research reports on the wrong thing.



Without some form of systematic process, the problems outlined easily arise when analyzing qualitative data. Thematic analysis keeps researchers organized and focused and gives them a general process to follow when analyzing qualitative data.
Tools and Methods for Conducting Thematic Analysis
A thematic analysis can be done in many different ways. The best tool or method for this process is determined based on the:

data
context and constraints of the data-analysis phase
the researcher’s personal style of work

3 common methods include:

Using software
Journaling
Using affinity diagramming techniques

Using Software
Researchers often use data-analysis software for analyzing large amounts of qualitative data. Researchers upload their raw data (such as transcripts or field notes) into the software and then use the software’s features to code the data. Some tools even support transcription of the video or audio recordings. Examples of data-analysis software include:

Dovetail
EnjoyHQ
Delve
Aurelius
Nvivo
Dedoose
MAXQDA

Benefits

The analysis is very thorough.
The analysis can be done collaboratively.
The raw data and the results of the analysis are always accessible in the software and can be revisited when needed.

Drawbacks

The analysis can be time-consuming, as it results in many codes which need to be condensed into a small, manageable list.
Subscriptions or licenses can be expensive
Some learning of the software is required.

Journaling
Writing thought processes and ideas you have about a text is common among researchers practicing grounded-theory methodology. Journaling as a form of thematic analysis is based on this methodology and involves manual annotation and highlighting of the data, followed by writing down the researchers’ ideas and thought processes. The notes are known as memos (not to be confused with the office memo delivering news to employees).
Benefits

The process encourages reflection through the writing of detailed notes.
Researchers have a record of how they arrived at their themes.
The analysis is cheap and flexible.

Drawbacks

Hard to do collaboratively

Affinity-Diagramming Techniques
The data is highlighted, cut out physically or digitally, and reassembled into meaningful groups until themes emerge on a physical or digital board. (See a video demonstrating affinity-diagramming.)
Benefits

Can be done collaboratively
Quick arriving at themes
Cheap and flexible
Visual, and supports an iterative-analysis process

Drawbacks

Not as thorough as other methods as often segments of text aren’t coded multiple times
Hard to do when data is very varied, or there is a lot of data

Codes and Coding
All methods of thematic analysis assume some amount of coding (not to be confused with writing a program in a programming language).
Definition: A code is a word or phrase that acts as a label for a segment of text.
A code describes what the text is about and is a shorthand for more complicated information. (A good analogy is that a code describes data like a keyword describes an article or like a hashtag describes a tweet.) Often, qualitative researchers will not only have a name for each code but will also have a description of what the code means and examples of text that fit or don’t fit the code. These descriptions and examples are especially useful if more than one person is responsible for coding the data or if coding is done over a longer period of time.
Definition: Coding refers to the process of labeling segments of text with the appropriate codes.
Once codes are assigned, it’s easy to identify and compare segments of text that are about the same thing. The codes allow us to sort information easily and to analyze data to uncover similarities, differences, and relationships among segments. We can then arrive at an understanding of the essential themes.

A thematic analysis starts with coding qualitative data. Through a systematic process of comparing segments of text within and between codes, the researcher arrives at themes.

Code Types: Descriptive and Interpretive
Codes can be:

Descriptive: They describe what the data is about
Interpretive: They are an analytical reading of the data, adding the researcher’s interpretive lens to it.

To see examples of descriptive and interpretive codes, let’s look at a quote from an interview I performed with a UX practitioner earlier this year (as part of our UX Careers research, to be published in our UX Careers report).

“I was petrified about facilitating a meeting and my company offered a day-and-a-half– long course. So, I went in there and the instructor did something that I felt was horrible at the time, but I've since really come to appreciate it. The first thing that we did was we filled out a sheet of paper with our name and wrote down our worst fear of moderating or facilitating and we turned it in and then he said, okay, tomorrow you're going to act out this situation (…) the next day we came back and I would leave the room while the rest of the team read, they read my worst fear, figured out how they'd act it out, and then I'd walk in and facilitate for 10 minutes with that. And that really helped me realize that there isn't anything to be afraid of, that our fears are really in our head most of the time and facing that made me realize I can handle these situations.”

Here are possible descriptive and interpretive codes for the text above:
Descriptive code: how skills are acquired
Rationale behind the code label: Participants were asked to describe how they came to possess certain skills.
Interpretive code: self-reflection
Rationale behind the code label: The participant describes how this experience changed her beliefs about facilitation and how she reflected on her fear.
Steps to Conduct a Thematic Analysis
Regardless of which tool you use (software, journaling, or affinity diagraming), the act of conducting a thematic analysis can be broken down into 6 steps.


A thematic analysis involves 6 different phases: gathering the data, reading all the data from beginning to end, coding the text based on what it’s about, creating new codes that encapsulate candidate themes, taking a break and coming back to the analysis later, and evaluating your themes for good fit.




A thematic analysis involves 6 different phases: gathering the data, reading all the data from beginning to end, coding the text based on what it’s about, creating new codes that encapsulate candidate themes, taking a break and coming back to the analysis later, and evaluating your themes for good fit.


Step 1: Gather All Your Data
Start with the raw data, such as interview or focus-group transcripts, field notes, or diary study entries. I recommended transcribing audio recordings from interviews and using the transcriptions for analysis instead of relying on patchy memory or notes.
Step 2: Read All Your Data from Beginning to End
Familiarize yourself with the data before you begin the analysis, even if you were the one to perform the research. Read all your transcripts, field notes, and other data sources before analyzing them. At this step, you can involve your team in the project. Involving your team instills knowledge of users and empathy for them and their needs.
Run a workshop (or a series of workshops if your team is very large or you have a lot of data). Follow these steps:

Before your team members engage with the data, write your research questions on a whiteboard or piece of flipchart paper to make the questions easy to refer to while working.
Give each member a transcript or one field- or diary-study entry. Tell people to highlight anything they think is important.
Once team members have completed reading their entries, they can pass their transcript or entry to someone else and receive a new one from another team member. This step is repeated until all team members have engaged with all the data.
Discuss as a group what you noticed or found surprising.


A workshop where each team member reads each diary- or field-study entry and highlights important bits is a good way of getting team members to actively engage with the text, as opposed to just reading it and letting it wash over them.

While it’s best if your team observes all your research sessions, that may not be possible if you have a lot of sessions or a big team. When individual team members observe only a handful of sessions, they sometimes walk away with an incomplete understanding of the findings. The workshop can solve that problem, since everyone will read all the session transcripts.
Step 3: Code the Text Based on What It’s About
In the coding step, highlighted sections need to be categorized so that the highlighted sections can be easily compared.
At this stage, remind yourself of your research objectives. Print your research questions out. Stick them up on a wall or on a whiteboard in the room where you’re conducting the analysis.
If you have adequate time, you can involve your team in this initial coding step. If time is limited and there is a lot of data to work through, then do this step by yourself and invite your team later to review your codes and help flesh out the themes.
As you are coding, review each segment of text and ask yourself “What is this about?” Give the fragment a name that describes the data (a descriptive code). You can also add interpretive codes to the text at this stage. However, these will typically become easier to assign later.
The code can be created before or after you have grouped the data. The next two sections of this step describe how and when you may add the codes.
Traditional Method: Create Codes Before Grouping
In the traditional approach, as you highlight segments of the data, like sentences, paragraphs, phrases, you code them. It’s helpful to keep a record of all the codes used and outline what they are, so you can refer to this list when coding further sections of the text (especially if multiple people are coding the text). This approach avoids creating multiple codes (that will later need to be consolidated) for the same type of issue.
Once all the text has been coded, you can group all the data that has the same code.
If you’re using software for this process, then it will automatically log the codes you assign while coding, so you can use them again. It will then provide a way for you to view all text coded with the same code.

An example from Dovetail is shown above. The highlighted sections show which segments of the transcript have been coded. The codes appear on the right. Double-clicking on a code will display all segments that have been coded with the same code.

Quick Method: Group Segments of Text, Then Assign a Code
Rather than coming up with a code when you highlight text, you cut up (physically or digitally) and cluster all the similar highlighted segments (similarly to how different stickies may be grouped in an affinity map). The groupings are then given a code. If you’re doing the clustering digitally, you might pull coded sections into a new document or a visual collaboration platform.
In the pictures below, the grouping was done manually. Transcripts were cut up, fixed to stickies, and moved around the board until they fell into natural topic groups. The researcher then assigned a pink sticky with a descriptive code to the grouping.

The highlighted sections were physically cut up with scissors and taped to stickies.


The participant number or the data type (i.e., interview vs. field study) was written on the sticky (but could also be communicated through the color of the sticky). This practice facilitates an easy return to the full data, as well as comparisons across participants and data sources. Stickies allow the segments of text to be easily moved around a board or wall.


The highlighted segments were clustered by the text topic and given a descriptive code.

At the end of this step, you should have data grouped by topics and codes for each topic.
Let’s look at an example. I interviewed 3 people about their experience of cooking at home. In these interviews, participants talked about how they chose to cook certain things and not others. They talked about specific challenges they faced while cooking (e.g., dietary requirements, tight budgets, lack of time and physical space) and about solutions for some of these challenges.
After grouping the highlighted clippings from my interviews by topic, I ended up with 3 broad descriptive codes and corresponding groupings:

Cooking experiences: memorable positive and negative experiences related to cooking
Pain points: anything that stops someone from cooking or makes cooking difficult (including navigating dietary restrictions, limited budgets, etc.)
Things that help: what helps (or is believed to possibly help) someone overcome specific challenges or pain points

Step 4: Create New Codes that Encapsulate Potential Themes
Look across all the codes and explore any causal relationships, similarities, differences, or contradictions to see if you can uncover underlying themes. While doing so, some of the codes will be set aside (either archived or deleted) and new interpretive codes will be created. If you’re using a physical-mapping approach like that discussed in step 3, then some of these initial groupings may collapse or expand as you look for themes.
Ask yourself the following questions:

What’s going on in each group?
How are these codes related?
How do these relate to my research questions?

Returning to our cooking topic, when analyzing the text within each grouping and looking for relationships between the data, I noticed that two participants said that they liked ingredients that can be prepared in different ways and go well with other different ingredients. A third participant talked about wishing she could have a set of ingredients that can be used for many different meals throughout the week, rather than having to buy separate ingredients for each meal plan. Thus, a new theme about the flexibility of ingredients emerged. For this theme, I came up with the code one ingredient fits all, for which I then wrote a detailed description.

In this research example, a new grouping was formed; the grouping included quotes mentioning a need for ingredients that can be flexibly used — either because they can be prepared in several ways or because they can be used in several different meals throughout a week. The grouping was labeled with the interpretive code one ingredient fits all. The researcher then fleshed out the description of this code.

Step 5: Take a Break for a Day, then Return to the Data
It almost always is a good idea to take a break and come back and look at the data with a fresh pair of eyes. Doing so sometimes helps you to see significant patterns in the data clearly and derive breakthrough insights.
Step 6: Evaluate Your Themes for Good Fit
In this step, it can be useful to have others involved to help you review your codes and emerging themes. Not only are new insights drawn out, but your conclusions can be challenged and critiqued by fresh eyes and brains. This practice reduces the potential for your interpretation to be colored by personal biases.
Put your themes under scrutiny. Ask yourself these questions:

Is the theme well supported by the data? Or could you find data that don’t support your theme?
Is the theme saturated with lots of instances?
Do others agree with the themes you have found in the data after analyzing the data separately?

If the answer to these questions is no, it might mean that you need to return to the analysis board. Assuming you collected sound data, there is almost always something to be learned, so spending more time with your team repeating steps 4–6 will be worthwhile.
Conclusion
A thematic analysis can uncover the major themes from your research. There’s no one way to do a thematic analysis. Choose a method of analysis that suits the kind and volume of data you’ve collected. When possible, invite others into the analysis process to both increases the accuracy of the analysis and your team’s knowledge of your users’ behaviors, motivations, and needs. Analysis can be a lengthy process, so a good rule of thumb is to budget at least as much time as you had for the data collection to complete the analysis.
Learn more: User Interviews, Advanced techniques to uncover values, motivations, and desires, a full-day course at the UX Conference or the week-long series on Qualitative Research."
8,2022-07-31,"It is no secret that the field of user experience often favors objective, observational research methods over subjective, attitudinal methods. After all, when something is observed, with proof that it has actually happened, it can be hard to argue against it. However, it takes more than observational research to truly empathize and understand the full complexity of a person’s experience, which includes emotional experiences, mindsets, values, and belief systems. Since there is no other way to gather this data (at the writing of this article, mind reading with neural implants is not possible) researchers must use attitudinal methods to solicit the thoughts and opinions of target customers. A focus group is one of these methods.
What Is a Focus Group?
Definition: A focus group is a qualitative, attitudinal research method in which a facilitator conducts a meeting or workshop (typically about 1–2 hours long) with a group of 6–9 people to discuss issues and concerns about their experiences with a product or service. The term “focus” relates to the role of the facilitator, who maintains the group’s focus on certain topics during discussions.
Traditionally, focus groups have been a market-research method, used to get a sense of some aspect of a product, service, or concept. In these settings, the focus would typically be on certain words, graphics, videos, or other noninteractive media. All participants are presented with the media as a group and then prompted to provide their thoughts to the facilitator and the rest of the group.
Generally speaking, focus groups can provide useful information about your customers’ overall opinions and their impressions of a product or service.
Limitations and Risks of Focus Groups
Focus groups are notoriously problematic and often improperly used. Here are some of their limitations:

They do not provide detailed insights on usability. People will comment on what is shown or remembered and offer opinions, so, by their nature, focus groups cannot provide any objective information on behavior when using a product or service. Thus, they cannot provide detailed usability insights, which would be best found with a usability test or field study. Even if there are some usability insights uncovered when presenting a design, products are almost never used by a whole committee; they’re used individually.
People don’t always know what they will do or what will MOST benefit them in the future. In many focus groups, participants are asked whether they would use a particular product. But users do not always do what they say they will do. So, while it’s helpful to listen to customers’ concerns, preferences, or requests for features or product offerings — especially to uncover unmet user needs — the requests themselves are not always going to be the best solutions to address customers’ needs in a systematic and prioritized way.
As is the case with any self-reported data, human memory is fallible. Certain biases will limit the focus of participants’ accounts and make these an unreliable source of information about sentiment or satisfaction.
	
Negativity bias often results in people more readily recalling what was bad about an experience (particularly if it was not a great one), which can skew the discussion negatively for everyone else.
The peak-end rule can cause people to overly focus on the most memorable and most recent moments, at the expense of other possibly more-meaningful ones.
Priming can cause participants to overemphasize an aspect of their experience, because it so happened that someone else mentioned it and made them remember it.


Group dynamics may impact how much (or how little) people share. Strong personalities in the group may affect what and how much is shared. Depending on the focus group’s format, it may disproportionately represent the opinions of those who are more talkative or quick to answer. Groupthink is also more likely to occur in these settings if only verbal contributions are given attention. To paraphrase my colleague Sarah Gibbons: a poorly run focus group can be a great way to pay 9 people for the opinions of three.

Given these limitations, focus groups should NOT be utilized in the following contexts:

Evaluating a design’s usability 
Evaluating workflows
Creating a list of design requirements
Determining a UI’s impact on emotions
Quantifying satisfaction or other sentiments 

Benefits of Focus Groups
Despite these shortcomings, there are some good reasons to consider a focus group:

Participants with similar goals or perspectives can build on each other’s responses or recall experiences in greater detail. Sometimes during interviews, a participant might have trouble recalling all the details of an experience. However, hearing another participant mention something related may trigger the recollection of an important detail, which would otherwise get skimmed over in an interview.
They can help teams clarify users’ mental models and language (vocabulary) around the problem space during discovery phases, before conducting further research. While you should generally run a pilot study for most research studies anyway, a focus group can help researchers rework a research plan or facilitator guide with language that could be more user-centered.
They are a time-efficient method for the researcher. Rather than dedicate 9–12 hours interviewing 6–9 individual participants, a researcher can dedicate 1–2 hours to gather the perspectives of 6–9 people at the same time. It can be a quick way to learn from many people and perspectives (and certainly a 100% improvement to conducting no research at all). These can be especially time-efficient if the researcher is facilitating the focus group online rather than in person.
When run properly, they can yield rich qualitative insights due to a format similar to semistructured interviews. Unlike questionnaires — which can sometimes limit the level of detail covered — focus groups give facilitators the flexibility to explore topics in which the participants are interested. This format is especially useful if the team is still in early stages of product development and trying to discover new information about the problem space.

Given these benefits, focus groups are BEST utilized in the following contexts:

Early discovery research to gauge customer familiarity or interest in a concept and initial impressions
Understanding users’ mental models and expectations
Cocreation workshops with sponsored customers

You Can Run an Effective Focus Group
It’s fair to say that focus groups are often unfairly maligned, considering the many benefits they can yield with relatively less time commitment compared to other methods. The key to reaping these benefits and mitigating limitations is to use a combination of other research methods (like other behavioral or attitudinal methods), and having a strong research plan.
Here are the key things to consider when planning your focus group:
1. Recruit participants that are representative of your target audience(s).
Who do you want to learn about? What specific segment of users are you interested in? Even if your user is “everybody,” use personas, archetypes, or jobs-to-be-done to identify key recruiting criteria. Recruiting is a tricky balance of finding similar user motivations and goals (not demographics) while inviting a mix of backgrounds to reduce bias from other sources — such as having an overly westernized sample when studying a global offering.
2. Note potential sources of bias from the focus group’s structure.
Note who is not included, and why, for consideration during analysis and when strategizing future research. Is it a different segment that’s intentionally excluded? Lack of response? Lack of interest/trust? Bias is difficult to totally eliminate, but awareness of sources of bias can help during analysis and might inform future research. For example:

With online focus groups, there may be potential participants who are excluded from participating (be it due to a poor internet connection, lack of a desktop device, or low literacy in certain digital tools). Thus, they may not be able join a video chat or, if they do join, they may be less likely to participate when using an unfamiliar online-meeting tool or whiteboard platform for the first time.
With in-person focus groups, it’s fair to assume that the study will only involve participants from the immediate commutable vicinity (i.e., within the city or state), especially if travel is not funded by the study or if insufficient notice is provided for those commuting from further distances.
Is your focus group accessible? This is relevant for both in-person and online focus groups. Can disabled participants get into the facility and participate readily? Can nonverbal participants contribute?

3. Treat your focus group like a workshop. Make participants comfortable with participating, verbally or nonverbally.
As you plan your agenda for the focus group, remember that most of your participants likely do not know each other and will be asked to speak honestly, potentially revealing information that may make them feel vulnerable or unlike others. Some people may do it, others may not. Consider having the following in your focus group:

Easy warmup activity (e.g. participants could write their first names on a folded sheet of paper) 
	Start your session with a low-stake prompt or icebreaker question that does not have “correct” answers. This serves two purposes:
	
It gives a structured way for participants to build rapport with the facilitator and with each other.
It builds participants’ confidence, in themselves and in the format (particularly if you’re using online-meeting tools or digital whiteboards).
Note: Do not “break the ice” with highly personal or sensitive topics, which will likely cause participants to withdraw instead. (In fact, those types of answers are probably not going to come easily in a focus-group format, even with the most “warmed up” group. These types of questions are better suited to 1:1 interviews).


Both written and verbal participation opportunities
	As with any other UX workshop, offer multiple methods of engagement (verbal and nonverbal) to encourage maximum participation and contribution. This ensures that less vocal or nonnative speakers feel comfortable contributing. You can also use the diverge–converge technique to maximize participation while decreasing bias potential.
Note: If covering sensitive topics, offer an anonymous way to contribute (or, again, consider a different, more-private method altogether).

4. Have a (written) plan and guide.
Construct your prompts in advance to avoid leading or biasing participants.  As with semistructured interviews, focus-group questions should use the funnel technique: be open-ended and broad at the beginning and progressively build detail and specificity with concepts as the conversation progresses. On a related note: remember to frame followup questions both positively and negatively to avoid leading participants, particularly when the conversation may naturally skew in one of these directions.
Conclusion
Focus groups don’t accurately predict future behavior. However, they can help gauge attitudes and guide future exploration, thus avoiding wasted research time. Still, they should be considered a starting point to further research, rather than a validation step. The good news? If your focus-group participants are willing, not only will you have their input to guide your further research, you may also have a group of customers willing and able to test what you create to further guide your design."
9,2022-07-24,"The funnel technique has been around since qualitative interviews emerged as a research method. This technique involves asking broad open-ended questions before gradually introducing more narrowly-scoped open-ended questions, as well as closed questions. 
This idea of starting broad before getting more specific is valuable in other types of studies besides user interviews. This technique can help you organize:

Interview questions
Followup questions
Usability tasks
Research in a multimethod study

This article discusses how the funnel technique can be used in user interviews and in moderated usability tests. 
Why Is It Called the Funnel Technique?
A funnel is broad at the top and narrow at the bottom, which makes it easy to pour a substance (for example, oil or rice) into a container with a narrow opening (for example, a bottle or jar). 
Similarly, the funnel technique in user research involves moving from broad to narrow — in other words, from general to specific. The funnel is a fitting metaphor because a user interview or usability-test session should start with broad, exploratory questions or tasks before introducing specific, narrowly scoped questions or tasks. 
The purpose of the funnel technique is to avoid influencing user behavior or perceptions as much as possible. When we ask specific questions or give specific tasks too early in a research session, we risk introducing bias and missing important data.


A funnel starts broad and gradually gets narrower. In qualitative interviews and usability tests, starting with broad, exploratory questions or tasks can generate lots of unbiased information about users’ natural thoughts and behaviors. As we introduce specific questions or tasks, we gather less information but more specific details.


The Funnel Technique in User Interviews
An interview guide for a user interview will usually consist of 5–8 open-ended questions that get participants to share relevant stories or experiences. These are followed by followup questions, which can be open-ended or closed.
An interview should start with broad, open-ended questions like Tell me about the last time you ordered movie tickets.
Starting with broad open-ended questions:

Gets the participant comfortable with talking
Allows the participant to begin sharing stories
Generates lots of new, unanticipated information
Avoids the researcher priming the participant

After the participant responds, the interviewer should ask open-ended followup questions (known as probing questions) to dig into areas of the participant’s response that the researcher wants to learn more about. These include questions like:

Can you expand on that?
What do you mean by that?
How do you feel about that?
Why do you think that?

The interviewer may also want to gather additional information and details that the participant omitted, by introducing closed questions, such as: 

When did you see this movie?
Did you go alone?
How long was the movie?

For each main question prepared in the interview guide, the interviewer is moving from broad open-ended questions to closed questions. With each new question in the guide, the funneling process repeats.


Each main interview question is usually broad and open-ended; it is followed by open-ended followup questions and gradually more closed questions. With each new interview question, the funnel process repeats.


When we make use of the funnel technique, we allow space for us to learn new things before gathering detail. This approach to interviewing is the best way to build rapport and learn what users really think without priming them.
The Funnel Technique in Qualitative Usability Tests
In qualitative usability testing, funneling can be used in: 

Creating and ordering tasks
Devising the sequence of followup questions after a participant completes a task

Funneling Tasks
The same process of moving from broad to specific can apply to constructing and ordering tasks for a qualitative usability test. Broad exploratory tasks — which allow us to learn how people do things on their own — should be given before specific, directed tasks that ask users to find or do something that they might not otherwise do.
Another metaphor for the funnel technique is a mouse in a maze. Imagine a researcher in a lab wants to see if a mouse can find a piece of cheese in a large maze. They might start off by giving the mouse the entire maze and seeing whether it finds the cheese. After some time, if the mouse hasn’t arrived at the cheese, they could slowly start closing off incorrect pathways, increasing the chances that the mouse will find the cheese.



Applying the funnel technique can work a bit like guiding a mouse in a maze — we're first seeing if the mouse can find its way to the cheese on its own, but then closing off some pathways as needed.






Applying the funnel technique can work a bit like guiding a mouse in a maze — we're first seeing if the mouse can find its way to the cheese on its own, but then closing off some pathways as needed.



In this metaphor, the mouse is the participant, and the cheese is the aspect of the design that we’re interested in studying. It’s better to first see if our participants get there on their own, but we might have to nudge them a bit.




Task to give the participant


Rationale




Task 1: Using the site, see if there’s a recipe you might like to cook.


Broad exploratory task: We’re allowing the user to define the end point of the task and complete it in their own way.




Task 2: Use the site to find an easy recipe you might like to cook for dinner.


Directed task: This task has a defined end point and success criterion. We’re asking participants to find something that they might not otherwise look for.




Task 3: Using the search, find a recipe for chicken noodle soup.


Directed task: This task asks users to use a specific feature to do a task. We’re asking users to find something they might not otherwise look for and use a feature they might not use otherwise.




If we started with task 3, and moved in the reverse order, we would prime participants on how to behave in task 1 and task 2. It’s likely that we would miss out on learning how they would do things on their own without prompting. Funneling tasks, by starting with broad exploratory tasks before introducing directed tasks, ensures you get valid data about behavior.


Starting with broad exploratory tasks helps us learn about natural behavior, while directed tasks give us less information about how users would do something but more about a specific UI element.


The funnel technique is the idea behind stepped tasks. Stepped tasks are multistep tasks that start broad and then provide more specific instructions as necessary. 
For example, let’s imagine that we’re interested in studying the comparison feature on an ecommerce site. 




Task to give the participant


Rationale




Task 4: Choose a pair of headphones you might like to buy and add them to your cart.


Broad task: We’re giving participants a very natural task and hoping that they happen to use the feature we’re interested in along the way.




Task 4.1:  Your friend is considering buying the Apple Airpods or Apple Airpods Pro but isn’t sure which one to get. Use the site to decide which one you’d recommend to your friend and why.


If the participant doesn’t naturally discover and use the comparison feature on their own, we might give them a followup instruction like this. The goal is to put the participant in a more specific situation that might be less natural but will encourage them to use the feature.




Task 4.2: Can you find a way to compare those two headphones without having to switch back and forth between pages?


If the participant still doesn’t engage with the feature of interest, we might get even more direct. In this example, we’re strongly hinting that the comparison feature exists and asking the participant to use it. 




If we get to Task 4.2 in the stepped task example above, we might conclude that there are some discoverability or desirability issues with the feature. However, providing this level of specificity would allow us to make those conclusions while also getting a chance to see if there are any interaction-design problems with the feature itself.
Funneling in Followup Questions
In qualitative usability tests, the facilitator will make use of the funnel technique when asking followup questions. For example, we might ask the following questions after the participant has finished a task:




Question to give the participant


Rationale




Question 1: Do you have any thoughts about doing this activity on the website?


This broad, open-ended question allows participants to share anything related to the task they have completed.




Question 2: Was there anything easy or difficult?


This more specific open-ended question is focused on the ease of use. Here we’re prompting users to recall anything they might have liked using or had difficulty doing.




Question 3: What did you think about the filters you were using?


This open-ended question is even more specific. Here, we’re drawing the user’s attention to a specific UI element that they used in the task.




In the above example, we are starting with broad, open-ended questions before narrowing the scope to specific, open-ended questions that relate to a UI element that we want more feedback on. 
We shouldn’t start with Question 3 and move in the reverse direction, because it’s always better if participants volunteer information unprompted. We’re hoping the participant will organically provide feedback without our specific direction, because unsolicited feedback is more likely to be the participant’s true opinion. Asking participants for feedback on specific elements can be risky — there’s always a chance that the participant will simply make up an opinion for the researcher. For example, if the participant said the filters were really helpful in response to question 1, we can be more confident that they really think so than if they gave us a similar response to question 3.
Summary
The funnel technique can be used when administering questions or tasks in user interviews or usability tests. Start with broad open-ended questions or tasks before introducing more specific questions or tasks. This approach helps to ensure that you don’t miss out on important information and you avoid prompting or priming participants too early.
 
To learn more about interviews and moderating usability tests, explore our 5-day course on Qualitative Research or our 1-day courses on User Interviews and Usability Testing."
10,2022-07-17,"The field of user experience has a wide range of research methods available, ranging from tried-and-true methods such as lab-based usability testing to those that have been more recently developed, such as unmoderated UX assessments.
While it's not realistic to use the full set of methods on a given project, nearly all projects would benefit from multiple research methods and from combining insights. Unfortunately, many design teams only use one or two methods that they are most familiar with. The key question is what to use when. To better understand when to use which method, it is helpful to view them along a 3-dimensional framework with the following axes:

Attitudinal vs. Behavioral
Qualitative vs. Quantitative
Context of Use

The following chart illustrates where 20 popular methods appear along these dimensions:

Each dimension provides a way to distinguish among studies in terms of the questions they answer and the purposes they are most suited for. The methods placed in the middle of the quantitative–qualitative axis can be used to gather both qualitative and quantitative data.

The Attitudinal vs. Behavioral Dimension
This distinction can be summed up by contrasting ""what people say"" versus ""what people do"" (very often the two are quite different). The purpose of attitudinal research is usually to understand or measure people's stated beliefs, but it is limited by what people are aware of and willing to report.
While most usability studies should rely on behavior, methods that use self-reported information can still be quite useful to designers. For example, card sorting provides insights about users' mental model of an information space and can help determine the best information architecture for your product, application, or website. Surveys measure and categorize attitudes or collect self-reported data that can help track or discover important issues to address. Focus groups tend to be less useful for usability purposes, for a variety of reasons, but can provide a top-of-mind view of what people think about a brand or product concept in a group setting.
On the other end of this dimension, methods that focus mostly on behavior seek to understand ""what people do"" with the product or service in question. For example A/B testing presents changes to a site's design to random samples of site visitors but attempts to hold all else constant, in order to see the effect of different site-design choices on behavior, while eyetracking seeks to understand how users visually interact with a design or visual stimulus.
Between these two extremes lie the two most popular methods we use: usability studies and field studies. They utilize a mixture of self-reported and behavioral data and can move toward either end of this dimension, though leaning toward the behavioral side is generally recommended.
The Qualitative vs. Quantitative Dimension
The distinction here is an important one and goes well beyond the narrow view of qualitative as in an open-ended survey question. Rather, studies that are qualitative in nature generate data about behaviors or attitudes based on observing or hearing them directly, whereas in quantitative studies, the data about the behavior or attitudes in question are gathered indirectly, through a measurement or an instrument such as a survey or an analytics tool. In field studies and usability testing, for example, researchers directly observe how people use (or do not use) technology to meet their needs or to complete tasks. These observations give them the ability to ask questions, probe on behavior, or possibly even adjust the study protocol to better meet study objectives. Analysis of the data is usually not mathematical.
In contrast, the kind of data collected in quantitative methods is predetermined — it could include task time, success, whether the user has clicked on a given UI element or whether they selected a certain answer to a multiple-choice question. The insights in quantitative methods are typically derived from mathematical analysis, since the instrument of data collection (e.g., survey tool or analytics tool) captures such large amounts of data that are automatically coded numerically.
Due to the nature of their differences, qualitative methods are much better suited for answering questions about why or how to fix a problem, whereas quantitative methods do a much better job answering how many and how much types of questions. Having such numbers helps prioritize resources, for example to focus on issues with the biggest impact. The following chart illustrates how the first two dimensions affect the types of questions that can be asked:

The Context of Product Use
The third distinction has to do with how and whether participants in the study are using the product or service in question. This can be described as:

Natural or near-natural use of the product
Scripted use of the product
Limited in which a limited form of the product is used to study a specific aspect of the user experience
Not using the product during the study (decontextualized)

When studying natural use of the product, the goal is to minimize interference from the study in order to understand behavior or attitudes as close to reality as possible. This provides greater external validity but less control over what topics you learn about. Many ethnographic field studies attempt to do this, though there are always some observation biases. Intercept surveys and data mining or other analytic techniques are quantitative examples of this.
A scripted study of product usage is done in order to focus the insights on specific product areas, such as a newly redesigned flow. The degree of scripting can vary quite a bit, depending on the study goals. For example, a benchmarking study is usually very tightly scripted, so that it can produce reliable usability metrics by ensuring consistency across participants.
Limited methods use a limited form of a product to study a specific or abstracted aspect of the experience. For example, participatory-design methods allow users to interact with and rearrange design elements that could be part of a product experience, in order discuss how their proposed solutions would meet their needs and why they made certain choices. Concept-testing methods employ an expression of the idea of a product or service that gets at the heart of what it would provide (and not at the details of the experience) in order to understand if users would want or need such a product or service.  Card sorting and tree testing focus on how the information architecture is or could be arranged to best make sense to participants and make navigation easier.
Studies where the product is not used are conducted to examine issues that are broader than usage and usability, such as a study of the brand or discovering the aesthetic attributes that participants associate with a specific design style.
Many of the methods in the chart can move along one or more dimensions, and some do so even in the same study, usually to satisfy multiple goals. For example, field studies can focus a little more on what people say (ethnographic interviews) or emphasize studying what they do (extended observations); concept testing, desirability studies, and card sorting have both qualitative and quantitative versions; and eyetracking can be natural or scripted.
Phases of Product Development (the Time Dimension)
Another important distinction to consider when making a choice among research methodologies is the phase of product development and its associated objectives.  For example, in the beginning of the product-development process, you are typically more interested in the strategic question of what direction to take the product, so methods at this stage are often generative in nature, because they help generate ideas and answers about which way to go.  Once a direction is selected, the design phase begins, so methods in this stage are well-described as formative, because they inform how you can improve the design.  After a product has been developed enough to measure it, it can be assessed against earlier versions of itself or competitors, and methods that do this are called summative. This following table describes where many methods map to these stages in time:




Product-Development Stage




Strategize


Design


Launch & Assess




Research goal: 
			Find new directions and opportunities


Research goal:
			Improve usability of design


Research goal:
			Measure product performance against itself or its competition




Generative research methods


Formative research methods


Summative research methods




Example methods




Field studies, diary studies, interviews, surveys, participatory design, concept testing


Card sorting, tree testing, usability testing, remote testing (moderated and unmoderated)


Usability benchmarking, unmoderated UX testing, A/B testing, clickstream / analytics, surveys




Art or Science?
While many user-experience research methods have their roots in scientific practice, their aims are not purely scientific and still need to be adjusted to meet stakeholder needs. This is why the characterizations of the methods here are meant as general guidelines, rather than rigid classifications.
In the end, the success of your work will be determined by how much of an impact it has on improving the user experience of the website or product in question. These classifications are meant to help you make the best choice at the right time.
20 UX Methods in Brief
Here’s a short description of the user research methods shown in the above chart:
Usability testing (aka usability-lab studies): Participants are brought into a lab, one-on-one with a researcher, and given a set of scenarios that lead to tasks and usage of specific interest within a product or service.
Field studies: Researchers study participants in their own environment (work or home), where they would most likely encounter the product or service being used in the most realistic or natural environment.
Contextual inquiry: Researchers and participants collaborate together in the participants own environment to inquire about and observe the nature of the tasks and work at hand. This method is very similar to a field study and was developed to study complex systems and in-depth processes.
Participatory design: Participants are given design elements or creative materials in order to construct their ideal experience in a concrete way that expresses what matters to them most and why.
Focus groups: Groups of 3–12 participants are led through a discussion about a set of topics, giving verbal and written feedback through discussion and exercises.
Interviews: a researcher meets with participants one-on-one to discuss in depth what the participant thinks about the topic in question.
Eyetracking: an eyetracking device is configured to precisely measure where participants look as they perform tasks or interact naturally with websites, applications, physical products, or environments.
Usability benchmarking: tightly scripted usability studies are performed with larger numbers of participants, using precise and predetermined measures of performance, usually with the goal of tracking usability improvements of a product over time or comparing with competitors.
Remote moderated testing: Usability studies are conducted remotely, with the use of tools such as video conferencing, screen-sharing software, and remote-control capabilities.
Unmoderated testing: An automated method that can be used in both quantitative and qualitative studies and that uses a specialized research tool to capture participant behaviors and attitudes, usually by giving participants goals or scenarios to accomplish with a site, app, or prototype. The tool can  record a video stream of each user session, and can gather usability metrics such as success rate, task time, and perceived ease of use.
Concept testing: A researcher shares an approximation of a product or service that captures the key essence (the value proposition) of a new concept or product in order to determine if it meets the needs of the target audience. It can be done one-on-one or with larger numbers of participants, and either in person or online.
Diary studies: Participants are using a mechanism (e.g., paper or digital diary, camera, smartphone app) to record and describe aspects of their lives that are relevant to a product or service or simply core to the target audience. Diary studies are typically longitudinal and can be done only for data that is easily recorded by participants.
Customer feedback: Open-ended and/or close-ended information is provided by a self-selected sample of users, often through a feedback link, button, form, or email.
Desirability studies: Participants are offered different visual-design alternatives and are expected to associate each alternative with a set of attributes selected from a closed list. These studies can be both qualitative and quantitative.
Card sorting: A quantitative or qualitative method that asks users to organize items into groups and assign categories to each group. This method helps create or refine the information architecture of a site by exposing users’ mental models.
Tree testing: A quantitative method of testing an information architecture to determine how easy it is to find items in the hierarchy. This method can be conducted on an existing information architecture to benchmark it and then again, after the information architecture is improved with card sorting, to demonstrate improvement.
Analytics: Analyzing data collected from user behavior like clicks, form filling, and other recorded interactions. It requires the site or application to be instrumented properly in advance.
Clickstream analytics:  A particular type of analytics that involves analyzing the sequence of pages that users visit as they use a site or software application.
A/B testing (aka multivariate testing, live testing, or bucket testing): A method of scientifically testing different designs on a site by randomly assigning groups of users to interact with each of the different designs and measuring the effect of these assignments on user behavior.
Surveys: A quantitative measure of attitudes through a series of questions, typically more closed-ended than open-ended.  A survey that is triggered during the use of a site or application is an intercept survey, often triggered by user behavior. More typically, participants are recruited from an email message or reached through some other channel such as social media.
In-Depth Course
More details about the methods and the dimensions of use in the full-day training course User Research Methods: From Strategy to Requirements to Design and the article A Guide to Using User-Experience Research Methods."
11,2022-07-03,"To ensure that user research provides meaningful data, UX teams must get feedback from representative customers. You can certainly reach out to a customer for a quick phone call about their experiences without calling it a research session or needing a consent form. However, as soon as you transcribe quotes from that call or share that person’s information with other people on your team, you are technically (mis)using that person’s data in a manner to which they did not agree.
In other words, by not informing your participants about how you will use their data and by misleading them into believing that they were having a private conversation, you may be violating research ethics principles and taking advantage of that person’s kindness and willingness to help.
Informed consent is an exchange of information in which both of the following must occur:

The researcher informs the participant about what involvement in the study entails and the potential consequences of participating.
The participant fully understands the terms of the study and can make an informed decision about whether they would like to voluntarily participate.

A research consent form (also known as an informed consent form) serves as written documentation of this exchange.
(A downloadable example of a research consent form can be found at the end of this article.)
Why Is Informed Consent Important?
While researchers may be well-intended in their hopes of learning about user behaviors, the intent does not matter nearly as much as the execution of the study and its impact. There are many documented cases of unethical research and its dangerous outcomes. One famous example of research abuse is the Nuremberg doctors’ trial. The doctors were placed on trial for war crimes, due to harmful and lethal experiments on inmates at Nazi concentration camps. This trial led to the creation of the Nuremberg Code, which is a set of ethics principles for human experimentation.
The Tuskegee syphilis study is also notorious for its lethal impact on more than 100 of the 400 African American participants, all of whom were not fully informed on the nature of the experiment, which studied the effects of untreated syphilis. This trial led to creation of the National Research Act and the Belmont Report, which outlines additional “ethical principles and guidelines for the protection of human subjects” for both biomedical and behavioral research.
To avoid further harm, most research institutions (like universities) have established institutional review boards (IRBs) that provide ethical oversight and approval authority on any planned research. Whether your organization has an IRB or not, informed consent is a fundamental part of an ethical research program that respects participants and does not take advantage of (or deceive) them.
Informed Consent Means Respecting and Protecting Participants
While most user research with digital products will typically be nonclinical in nature (and, thus, relatively low-risk), there is still risk to the participants' wellbeing, especially if the tasks they conduct during the study cause temporary hardship, distress, discomfort, or pain. This is particularly true for vulnerable populations who are already experiencing these things due to injury, abuse, trauma, or general misfortune. To be clear, we can still do research on sensitive topics or with vulnerable or disadvantaged populations, but we must take extra precautions and communicate the study’s potential impact in advance to protect participants’ wellbeing.
It’s worth noting that there are populations who are vulnerable or incapable of granting informed consent, due to the criteria of the second bullet point: the participant’s understanding and voluntary participation. For example, children/minors, adults with cognitive impairments, low or no literacy, or mentally incapacitated individuals (e.g., unconscious or under the influence of alcohol or drugs) may not be capable of fully understanding the terms of the study or lack the full agency needed to make voluntary decisions (at times relying upon guardians to make these decisions). Thus, these parties cannot offer informed consent on their own, and researchers need the consent of their guardian before conducting any research, no matter how low-risk.
Similarly, inmates and prisoners are already performing an involuntary task, so the second criteria, voluntary participation, is not always possible. The decisions with these populations are rarely fully voluntary, and voluntary withdrawal may not be seen as a valid option due to fear of authorities or retribution. Furthermore, there may be additional legal requirements like consent of a prisoner representative and approval from an IRB before any research is conducted.
Are Consent Forms Really Necessary?
Writing out consent forms may seem like unnecessary work when you could just tell the participant what the study will be like. However, written consent forms are critical for a number of reasons:

They consistently inform every participant with the same level of detail, whether they are the first or the 1,000th participant in the study and regardless of which researcher works with them. The written documentation serves as a paper trail to ensure that the researcher satisfactorily informed all participants and that all the key study details were covered every single time.
They hold researchers accountable to protect the welfare and wellbeing of study participants and their data. A participant may forget what they initially consented to, but a signed form empowers the participant to be in control of their participation and protect their own welfare, wellbeing, and data, should the researcher deviate from the original plan.
They prevent the research team (to some extent) from running a poor study. Granted, there can still be poor studies with consent forms, but consent-form writing helps researchers think through all the nuances of their study, including the activities the participant will do, the data that is truly needed, and how that data will be protected during and after the session.

That said, there are exceptions where a consent form is not necessary. A/B testing, for example, can be characterized as routine use of a product, and therefore, does not require additional involvement on the part of the participant. Plus, data from any single user is used only as part of a bigger statistic.
You may also not need a full-length consent form for an intercept study, which is the equivalent of interrupting someone while they are in the middle of a task to get their thoughts on that particular task. That said, you should still begin the intercept study with an agreement page containing a brief summary of how the information they provide will be used, stored, shared, and deleted. Also, if you are physically intercepting them in a public location, you should still get written permission from any physical establishment in which you are recruiting, to avoid getting in trouble for soliciting on a private property — which is illegal in many countries.
What Belongs in a Consent Form?
At the end of this article, you’ll find a consent-form template that you can repurpose for future research, but remember that your consent forms should be carefully considered and crafted based on the unique needs and format of each study.
 In general, a consent form should have the following information:

High-level purpose of the study
	Provide just enough detail to inform the participant about the nature of the study, without giving away any hypotheses or details that may skew behavior during the study. That said, this statement shouldn’t be as vague as “to learn about our customers.” Consider mentioning the objective of the research — for instance, to improve an existing product or strategize for future service development.
Format of the study (activities, tasks, and duration)
	Describe the types of activities and tasks the participant may complete. This is where you can say how the data will be collected and to what extent the participant can expect to interact with the researcher(s). With that, the duration of the study should be clearly stated. “Umbrella” consent forms in which the user consents to an indefinite timeframe of research for current and future studies can be very problematic in their lack of specificity and susceptibility to abuse (and, generally, participants probably will not want to participate in something that has no defined end point).
Voluntary-participation clause
	While research participants usually understand that they have volunteered to participate in the study, it may not always be clear that withdrawal is voluntary, or that it’s permissible for them to change their minds. Signing paperwork often can feel very “final,” so this clause helps to reassure that the participant is always in control of their participation.
Participant-data handling
	Explain how personal data, findings, and recordings (if applicable) will be handled by answering key questions like:
	
Will data be anonymized? If so, how much of it will be anonymized?
How will it be used, accessed, and stored? Who has access to this information?
Will it ever be published anywhere?
When will the raw, recorded data be deleted (if ever)?
Can the participant request that information is deleted?


Consent statement
	While many consent forms will simply have a signature block for participants to indicate consent or a single checkbox labeled, I consent to the terms of this study, this section should be carefully formatted and worded to instill confidence and give the participant agency and choices over their involvement.
Multiple checkboxes 
	Instead of requiring the participant to agree to a blanket statement, consider a modular consent structure, allowing participants to consent to certain parts or aspects of the study but not others. For example, participants may consent to participating in the study but may opt out of recordings or data publication. So, instead of I agree and consent to the terms of this study, understanding that our session may be recorded, your consent form may include a series of checkboxes:
	
I agree to the terms of this study and consent to participate.
I consent to audio being recorded during the session for analysis purposes.
I consent to video being recorded during the session for analysis purposes.



This modular consent form increases the likelihood that the participant will participate and contribute data to the study, even if they do not consent to recordings.
Other optional information to include, if applicable:

Compensation 
	If you will be providing compensation for the participants’ time, be it monetary or as free products or services, clearly state it in the consent form. Regardless which method you choose, make it clear that compensation is not based on the participants’ abilities or on whether they provide positive feedback. Especially during any form of “testing,” participants can sometimes feel like they are being tested for their abilities and performance, rather than the design.
	Tip: For in-person testing, it helps to reinforce this idea by paying participants at the beginning of the session, before tasks begin, right after they sign the consent form.
Parent or guardian involvement and approval
	If your participant is a minor or an adult with a guardian, a parent or guardian will likely accompany them into the session. Include a designated Parent or guardian signature block, to make it clear the signature belongs to a parent or guardian and that the participant was not coerced into signing. Any special considerations regarding studies with minors or other adults with guardians should be clearly communicated in the consent form.
Information (or debrief) sheet
	Some study formats may warrant an additional information sheet (separate from the signed consent form) which contains a more detailed description of the study’s purpose. Such an information sheet is usually given to the participant at the end of the study, to avoid priming them to behave differently during the study.

Lastly, regardless if you use a separate information sheet or not, you should always give the participant a copy of the signed consent form, so they can refer to it after the study. This ensures the participant knows what they signed and who to contact for further questions about the research.
Conclusion
Human-centered design begins with human-centered research, and, as such, improvements to a design should never come at the cost of another person’s wellbeing. Getting informed consent from participants is critical for researchers to carry out fair, transparent, and accurate research while doing their part to minimize harm. By writing a thorough consent form, researchers can think through important details of their study and build trust and confidence with participants.
Resources
Shuster, E. Fifty Years Later: The Significance of the Nuremberg Code. New England Journal of Medicine 337, 20 (1997), 1436-1440.
The Belmont Report. Office for Human Research Protections, 1979.
https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html."
12,2022-04-03,"There is a misconception in the UX world that research data speaks for itself and shouldn’t need the addition of a narrative or extra polish to convince others. We often think that by laying out all the facts in front of our teams and stakeholders, they will come to the same conclusions that we did. Unfortunately, that doesn’t always happen.
Telling an interesting story and sharing findings in an engaging way doesn’t have to be the exclusive advantage of live presentations. Whether we share insights from research or sell others on a recommendation, a compelling story can arise interest and get buy-in. When our team members and stakeholders care about users, it’s less of an uphill battle to sell them on our ideas and recommendations.
But how do we communicate a compelling story and keep our audience engaged with a report? Whether the report is a lengthy Word document or a presentation deck, it’s meant to be consumed asynchronously and the author doesn’t have the opportunity to insert a meaningful anecdote to illuminate some of the data like they would in a conversation. This article discusses ideas for how to make your asynchronous research deliverables engaging for your audience.
Using Storytelling in Asynchronous Mediums
If you consistently share research findings in the form of research reports, use the following storytelling techniques to make them engaging to your audience.
Structure Your Report for 3 Audiences
When you share reports with others at your organization, you want to structure them in a way that will cover three different types of reading:

Just the headlines: The headlines should be aimed towards someone who is likely not involved in the day-to-day inner working of the project, but needs to be kept in the loop to make high-level decisions. If they were to read only the headlines, they should still be able to make sense of the research.
The big themes: These include the top takeaways, such as strengths and opportunities for improvement, but not the nitty-gritty details of how you got there. Product managers and day-to-day stakeholders may skim the report for these big ideas rather than read all the details.
In-depth details: Data-driven team members — whether they’re fellow researchers, designers, data scientists, subject-matter experts, or developers — appreciate the details on process and methodology and are more likely to explore the depths of the report.

Use Anecdotes to Persuade
People can depersonalize large numbers and forget that such numbers stand for real people. However, if they only have one person to focus on, they’re more likely to empathize and make decisions with that person in mind. When you want to sell others on your ideas, anecdotes can sometimes persuade more than numbers.
Think about how your data could affect just one person. Anecdotes trigger an emotional reaction and will spark empathy from your audience. This isn’t to say that you should rely only on anecdotes though. Anecdotes can lead to narrative bias, so it’s best to use them to illustrate or support your data.

Couple your numerical data (top) with an anecdote or two (bottom) to remind your audience that there are real people behind the numbers. Further illustrate the number or showcase interesting outliers, like in the example above.

Find Analogies to Which Your Audience Can Relate
If you don’t have a data-based story about a specific number, you can still help people visualize that number by finding a real-life analogy that people could relate to. 
For example, let’s say that your research showed that 70% of customers abandon purchases because of a bad user experience.
That’s a pretty high number, right? Some stakeholders may not perceive how dramatic that number is and it could be worthwhile to make it more tangible and less abstract by providing some context.
Follow these steps to add context to the number:

Start by introducing a relatable scenario: Imagine a typical 737 airplane with first class, premium economy, and economy seats. You could add relevant numbers (how many seats are in each class, for example), but make sure they don’t overcomplicate the ultimate point you’re trying to make.  
Describe the comparison, analogy, or metaphor: Picture the entire economy class walking off the plane.
Tie it to your research: That large proportion of people in the economy class is the same as the 70% of customers that are abandoning purchases on our website because of a bad user experience.

Whenever possible, use compelling visuals to quickly convey your scenario instead of relying on lengthy word descriptions.

Using a recognizable visual to help illustrate the number can effectively demonstrate the impact of that number, like with this seat-selection graphic.

By making your audience visualize the number with a relatable experience or analogy, you’ll make a bigger impact when you present your metrics.
Break Up Heavy Text-Based Content
Sometimes the easiest way to increase engagement with reports is to go beyond the typical bullet points and text-heavy paragraphs and find some nonstandard way to display your content.
Think about the content you’re presenting and ask yourself the following questions:

Could this content be a checklist or a bulleted list?
Are there pros and cons of this recommendation that could be compared side-by-side?
Is this data better read as a chart or graph?
Is there a quote or video clip of a user making this point that I could use?


These examples of report pages show the difference between blocks of text (top) and varied text (bottom). The varied text makes it easy to scan for main points and identify participant quotes.


These slide examples show the difference between blocks of text (top) and varied content (bottom). The information is comparable, but the bottom example sums up the main ideas and calls attention to the exact part of the page being tested.

By varying the presentation of your content, you’ll create visual interest in the report and allow readers to take more information in. Varied content is also easily scannable and helps those readers who are prone to skimming.
Bonus: Present Material in a Nonlinear Format
Have you ever read a choose-your-own-adventure book? In this type of book, readers get to a point in the story where they must make a decision. That decision will take them in one of two directions in the story. Then readers will have to make more decisions until they get to the conclusion (the butler did it, or maybe it was the old Count).
Creating reports and presentation decks that allow for choose-your-own-adventure stories is a challenge, but can pay off if you have broad research to share with several different teams or areas of the business. This type of deliverable allows you to create one artifact that will serve several audiences.

Shopify UX Blog: Shopify created a research deliverable that delivered findings in a choose-your-own-adventure format. The front-end experience (top) required readers to make a decision, which led them on one of several different paths with those research insights that were most interesting to their team. The back-end mapping (bottom) highlighted the many different research insights that could be discovered.

The key to a good choose-your-own-adventure deliverable is testing the different paths so each delivers value. Walk through the deck on your own and make sure that everything links correctly and maybe even pilot-test your deliverable by sitting with a few different team members to see what they get out of the experience.

Keynote: When creating a choose-your-own-adventure presentation deck, add in links to the corresponding research findings. In this example, if a product manager on the training team was reading through this report, they would be most interested in the ‘Sign up for training’ findings, which they could easily get to using this link.

Out-of-the-Box Inspiration
Teams should regularly share research insights. For example, a research team may share findings to the larger team from recent research every week or sprint in the form of a UX progress report. This deliverable can be presented live in a meeting or sent out to the team to consume asynchronously.
The downside of having a predictable cadence and template for sharing research is that, after a while, these kinds of documents become stale and less engaging.  
Don’t be afraid to change up your presentation methods into something out of the ordinary, such as:

TikTok style videos: Challenge yourself to present findings in a short-form video. This will require you to sum up your research insights into an elevator pitch. What’s one actionable thing that you want your audience to take on?
Mini museums: Set up your artifacts and findings in person or in a Miro board and have your audience walk through it at their own pace like they would walk through an exhibit in a museum. Have them post their reactions on sticky notes and then debrief later as a group.
Video screenings: Put positive and negative video clips from usability testing together (making sure you have proper consent from participants, of course) into a single movie. Encourage your team and stakeholders to hold their own movie screening so they can hear what users have to say firsthand. This can either be done synchronously (with popcorn) or sent out to the team so they can watch on their own.

It's important to keep in mind the tool that you’re using when sharing research findings. The downside of sharing findings asynchronously is that you won’t be there to see how people react and to help if things go wrong. Avoid asking people to learn new tools to consume your deliverable. If you absolutely must use an unfamiliar tool, make it easy for people to figure it out, either by relying only on basic functionality or by providing readers with help and instructions.
If your team members and stakeholders can’t quickly figure out how to use the tool, they won’t, which means your deliverable ultimately won’t be useful.
Conclusion
Sharing research in an engaging way doesn’t always have to be synchronous. Use these guidelines and ideas to help spice up your asynchronous deliverables and increase engagement with your teams and stakeholders.
Learn more about tactics and templates for sharing research in our full-day course, Storytelling to Present UX Work.
Reference
Basi, Mandeep (2018). Choose your own adventure: sharing research with a broad audience. Shopify UX. https://ux.shopify.com/choose-your-own-adventure-sharing-research-with-a-broad-audience-24ff754b4d3a"
13,2022-03-06,"User interviews are a fantastic method to uncover information about your users’ experiences, backgrounds, needs, and desires. That being said, writing interview questions requires some thought and attention. Questions that might be fit for a questionnaire are not always appropriate for a user interview. And poorly constructed questions can confuse participants or lead to inaccurate reporting of thoughts, feelings, needs, and desires and, hence, result in invalid insights. This article highlights 6 common mistakes in drafting interview questions and suggests how to improve them.
#1 Starting with Questions that Should Be Asked in a Screener Survey
Some new interviewers want to know many facts about their participants and end up with an interview guide full of closed questions. For example, imagine that we’re conducting research on people’s experiences cooking at home. It can be tempting to ask lots of questions like these at the beginning of your interview:

How often do you cook?
Do you have any dietary restrictions?
Have you tried a meal-kit service before?
How often do you shop for ingredients?

In the context of a user interview, the questions above would be considered closed questions, in that there are only a few possible responses to each of these questions. (Of course, some participants might interpret closed questions as open-ended questions and offer further explanation). The problem with having many closed questions like these at the beginning of the interview is that they don’t allow participants to share stories about their experiences and they hamper your ability to build rapport with your participant. The other issue with asking closed questions instead of open-ended ones is that you don’t learn things that you didn’t think to ask!
If it’s important to ask these closed questions to direct the focus of the interview, then incorporate them into your screener questionnaire. The responses will help you tailor your interview guide or understand where emphasis should be placed in the interview. Remember that for any user research study, you have a strict budget for time spent with your participants. (And for unmoderated methods like surveys, users will spend very little time on your study.) Therefore, you should allocate this limited time to the most important research questions and to collecting data that couldn’t be gathered more easily with simpler methods.
There’s nothing wrong with asking some closed questions in your interviews — in fact, they’re needed to get detail and provide clarification into things participants are sharing with you. But it's much better to start by asking open-ended questions that allow participants to share some of their experiences. Such questions also set the stage for a less structured communication style and prime participants to later share details. Example questions include:

Tell me about a time when you cooked a meal for yourself.
Tell me about the last time you cooked something.
Tell me about a time when you cooked a new recipe.

#2 Asking Only Questions About Typical Behaviors
Another mistake some new interviewers make is to ask only about typical behavior. For example, consider the following questions:

How do you normally decide what to eat?
Which utensils do you typically use?
What’s your typical meal-preparation process?

Asking only about typical behavior prevents you from gaining in-depth, reliable information — what people typically do (if there is such a thing) and what they think they typically do may be different things! Moreover, responses to such questions will not capture participants’ behaviors that are very much dependent on contextual factors. It’s much better to ask about specific examples than to ask them to describe what they think is typical.
That being said, it’s often customary for the interviewer to introduce a grand-tour question at the beginning of the interview. Note that this question does ask about typical behavior. Some examples of a grand-tour question are:

Walk me through what a typical day in your home looks like.
Walk me through what a typical meal looks like in your home.
Tell me about a typical day in your office.

A grand-tour question at the beginning of an interview is like setting the scene for a story: we have a preview of the landscape that we can use to build upon throughout the interview. Once we’ve asked the grand-tour question, we move on and ask about specific examples, like those we’ve covered already in #1.
#3 Asking Hypothetical Questions
Sometimes interviewers introduce questions that ask the participant to imagine a future experience, choice, or situation, and ask how the participant might respond. These are hypothetical questions. Consider the questions below:

If you chose to use a meal-kit service, why might that be?
If there was a product that could help you make new meals from scratch, would you use it?

The problem with this kind of question is that people are bad at predicting their future behavior or choices — but they’ll likely have a good answer for you! If we want to understand people’s real choices, desires, and needs, we need to ask about real experiences and choices — not imagined future ones. This might mean recruiting the right people: people who have had the experiences you are looking to learn about.
#4 Using Clarifying Questions that Introduce an Interpretation
Often, when we hear users describe their past actions, thoughts, or feelings, it’s tempting to start hypothesizing out loud why they said or did something, like in the following examples:

Did you choose that meal because it was easy to prepare? 
Was it to save time that you ordered the meal-kit service?
Was that because you liked the website that you chose that recipe?

While these questions seem fairly innocent, especially when the hypotheses all seem reasonable, they are, in fact, leading the participants towards a certain response. For this reason, avoid asking questions that use the word ‘because.’ When people are presented with leading questions, they’re more likely to agree with the question or succumb to some kind of priming effect. Instead, it’s much better to ask:

Tell me why you chose that meal.
What made you decide to order the meal kit service?
What made you choose that recipe?

#5 Asking Compound Questions
Compound questions (or double-barreled questions) contain more than one question at the same time. For example:

Tell me what you decided to cook, and why.
Tell me about your journey into cooking and your experience at culinary school.
What things made you interested in cooking and good at it?

While these questions are common in surveys, they’re not a good idea to ask in interviews. This is because participants have to store the question in their working memory while they answer part of the question, which is hard to do. Participants may incorrectly remember the question, or only remember part of it. Your participant may also feel silly asking you what the original question was. Instead, keep questions short and concise, and don’t ask people to tell you what they did and why; that’s what followup and probing questions are for!
#6 Asking Ambiguous Questions
Sometimes, interviewers ask questions that are so broad that they become ambiguous, causing participants to interpret them incorrectly or ask for clarification. For example:

Can you share with me the environment that you cook in?
Tell me about your cooking habits.
Tell me about your cooking experience.

These questions can result in participants responding with questions like “What do you mean by the environment?”, “Do you mean how frequently I cook, or how I cook?”, “Do you mean how much experience I have, or how much I like cooking?”.
When devising questions, it’s important to think about how they could be interpreted. Taking time to pilot your interview guide with a participant can also help you understand if the questions you’re asking are interpreted incorrectly or need clarification.
Summary
Devising strong interview questions makes a lot of difference in the resulting data we collect. When in doubt, pilot your interviews to make sure you’re gathering useful data and your questions are understood correctly.
To learn more about mastering interviews, take our full-day course, User Interviews, or our 5-day course, Qualitative Research Series."
14,2022-02-20,"You don’t have to do all the user-research work yourself. If somebody else already ran a study (and published it), grab it!
Have you ever completed a project only to find out that something very similar has already been done in your organization a couple of years ago? That situation is common, especially with rising employee-churn rates, and fueled the popularity of research repositories (e.g., Microsoft Human Insights System) and the growth of the research-operations community. It should also inspire practitioners to do more secondary research.
Secondary research, also known as desk research or, in academic contexts, literature review, refers to the act of gathering prior research findings and other relevant information related to a new project. It is a foundational part of any emerging research project and provides the project with background and context. Secondary research allows us to stand on the shoulders of giants and not to reinvent the wheel every time we initiate a new program or plan a study.
This article provides a step-by-step guide on how to conduct secondary research in UX. The key takeaway is that this type of research is not solely an intellectual exercise, but a way to minimize research costs, win internal stakeholders and get scaffolding for your own projects.
Academic publications include a literature review at the beginning to showcase context or known gaps and to justify the motivation for the research questions. However, the task of incorporating previous results is becoming more and more challenging with a growing number of publications in all fields. Therefore, practitioners across disciplines (for instance in eHealth, business, education, and technology) develop method guidelines for secondary research.  
When to Conduct Secondary Research?
Secondary research should be a standard first step in any rigorous research practice, but it’s also often cost-effective in more casual settings. Whether you are just starting a new project, joining an existing one, or planning a primary research effort for your team, it is always good to start with a broad overview of the field and existent resources. That would allow you to synthesize findings and uncover areas where more research is needed. 
Secondary research shows which topics are particularly popular or important for your organization and what problems other researchers are trying to solve. This research method is widely discussed in library and information sciences but is often neglected in UX. Nonetheless, secondary research can be useful to uncover industry trends and to inspire further studies. For example, Jessica Pater and her colleagues looked at the foundational question of participant compensation in user studies. They could have opted for user interviews or a costly large-scale survey, yet through secondary research, they were able to review 2250 unique user studies across 1662 manuscripts published in 2018-2019. They found inconsistencies in participant compensation and suggested changes to the current practices and further research opportunities.
Types of Secondary Research
Secondary research can be divided into two main types: internal and external research.
Internal secondary research involves gathering all relevant research findings already available in your organization. These might include artifacts from the past primary research projects, maps (e.g., customer-journey map, service blueprint), deliverables from external consultants, or results from different kinds of workshops (e.g., discovery, design thinking, etc.). Hopefully, these will be available in a research repository. 
External secondary research is focused on sources outside of your organization, such as academic journals, public libraries, open data repositories, internet searches, and white papers published by reputable organizations. For example, external resources for the field of human-computer interaction (HCI) can be found at the Association for Computing Machinery (ACM) digital library, Journal of Usability Studies (JUS), or research websites like ours. University libraries and labs like UCSD Geisel Library, Carnegie Mellon University Libraries, MIT D-Lab, Stanford d.school, and specialized portals like Google Scholar offer another avenue for directed search. 
How to Conduct Secondary Research?
Our goal is to have the necessary depth, rigor, and usefulness for practitioners. Here are the 4 steps for conducting secondary research:

Choose the topic of research & write a problem statement. 

Write a concise description of the problem to be solved. For example, if you are doing a website redesign, you might want to both learn the current standards and look at all the previous design iterations to avoid issues that your team already identified.

Identify external and internal resources.

Peer-reviewed publications (such as those published in academic journals and conferences) are a fairly reliable source. They always include a section describing methods, data-collection techniques, and study limitations. If a study you plan to use does not include such information, that might be a red flag and a reason to further scrutinize that source. Public datasets also often present some challenges because of errors and inclusion criteria, especially if they were collected for another purpose. 
One should be cautious of the seemingly reputable “research” findings published across different websites in a form of blog posts, which could be opinion pieces, not backed up by primary research. If you encounter such a piece, ask yourself — is the conclusion of the writeup based on a real study? If the study was quantitative, was it properly analyzed (e.g., at the very least, are confidence intervals reported, and was statistical significance evaluated?). For all studies, was the method sound and nonbiased (e.g., did the study have internal and external validity)?
A more nuanced challenge involves evaluating findings based on a different audience, which might not be always generalizable to your situation, but may form hypotheses worthy of investigating. For example, if a design pattern is found okay to use by young adults, you may still want to know if this finding will also be valid for older generations.

Collect and analyze data from external and internal resources.

Remember that secondary research involves both the existing data and existing research. Both of those categories become helpful resources when they are critically evaluated for any inherent biases, omissions, and limitations. If you already have some secondary data in your organization, such as customer service logs or search logs, you should include them in secondary research alongside any existent analysis of such logs and previous reports. It is helpful to revisit previous findings, compare how they have or have not been implemented to refresh institutional memory and support future research initiatives.

Refine your problem statement and determine what still needs to be investigated.

Once you collected the relevant information, write a summary of findings, and discuss them with your team. You might need to refine your problem statement to determine what information you still need to answer your research questions. Next time your team is planning to adopt a trendy new design pattern, it may be a good idea to go back and search the web or an academic database for any evaluations of that pattern.
It is important to note that secondary research is not a substitute for primary research. It is always better to do both. Although secondary research is often cost-effective and quick, its quality depends to a large extent on the quality of your sources. Therefore, before using any secondary sources, you need to identify their validity and limitations. 
Summary
Secondary (or desk) research involves gathering existing data from inside and outside of your organization. A literature review should be done more frequently in UX because it is a viable option even for researchers with limited time and budget. The most challenging part is to persuade yourself and your team that the existing data is worth being summarized, compared, and collated to increase the overall effectiveness of your primary research. 
References
Jessica Pater, Amanda Coupe, Rachel Pfafman, Chanda Phelan, Tammy Toscos, and Maia Jacobs. 2021. Standardizing Reporting of Participant Compensation in HCI: A Systematic Literature Review and Recommendations for the Field. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, Article 141, 1–16. https://doi.org/10.1145/3411764.3445734
Hannah Snyder. 2019. Literature review as a research methodology: An overview and guidelines. Journal of business research 104, 333-339. DOI: https://doi.org/10.1016/j.jbusres.2019.07.039."
15,2021-11-07,"Recruiting participants for research studies is a difficult task: you have to attract interested participants, schedule times to meet for the study, remind them to come to the study, and then hope that they do, in fact, remember to come to their scheduled session. To make matters worse, sometimes participants who do show up are not good candidates for the study because they simply do not have relevant life experiences to contribute meaningful feedback or insights, even with their best efforts. Suboptimal study participants negatively affect the quality of your research and of your design decisions.
Screening surveys (also known as “screeners”) are questionnaires that gather information about candidate participants’ experiences to:

quickly identify and prioritize optimal candidates that are representative for your target audience
exclude any candidates who may not be a “good fit” for your research study

In this article we discuss the importance of screening in the user-research recruitment process and how to incorporate it into your recruitment strategy.
Biased Sampling
First, for any user study, you want to make sure that you recruit people who are representative for your audience. In order to do so, you need to be aware of the fact that the recruiting process may be biased towards certain types of participants.
For example, whenever you are using a remote-testing platform which includes its own participant panel,  you may run into “professional testers” — people who make a significant portion of their income by participating in different kinds of user research. While these participants are not inherently “bad” for a study, their motivations may lead to behavior that skews research results. For example, some may answer questions or perform tasks very quickly (or not at all) during unmoderated tests, rather than making an honest effort to do them in a realistic way. Others may know what researchers are looking for in user tests and deliberately exacerbate feedback to respond to the perceived study needs.
While there is no foolproof method to evade professional testers, you can alleviate the problem by further vetting screening-survey answers to determine whether they are reasonable and honest  (e.g.,  participants did not type “abc” instead of a meaningful sentence). To be clear, the intent behind screening surveys is to alleviate some of the manual work of vetting candidates, but some effort is still required (whether by the researcher or a professional recruiter) to ensure that quality candidates get selected.
Similarly, if you rely upon your personal network to recruit participants (a “convenience” sample), these people will already have somewhat of a relationship with you and may feel reluctant to give honest negative feedback. Testing with coworkers can also bias results because they may be familiar with the project, the organization, and even with different types of user research and their goals.
In general, you should also screen out UX professionals (or those who are “UX-adjacent” with an interest in interface design) since they will be too sensitive to UX issues and more likely to offer an expert review than realistic user feedback.
Even if your customers are “everybody” (that is, they draw from the general audience and cover a mix of genders and ages), you need to make sure that your study has external validity — namely, that your participants match the goals and interests of your audience. For example, someone who is not at all into sports or hiking may not be too motivated to shop on an outdoor-equipment website.
How to Screen Participants
1.  Define eligibility criteria.
First, you and your team should identify participant criteria for your study. Think about both the demographics of your target audience and its goals as they use your products. (For a thorough breakdown of this process, check out our free report How to Recruit Participants for Usability Studies.) These criteria will determine your recruitment strategy and your screener.
If you use automated-recruiting platforms, be careful about overly restricting your survey by having extraneous elimination criteria.  For example, say you were concerned about professional testers and you included an exclusionary question like “When did you last participate in a research study?” You may exclude more participants than necessary, like second-time participants who just happened to participate in a completely different type of study that same month.
In a similar vein, in an attempt to recruit marketing professionals, you might choose to accept only individuals who select Advertising and Marketing as their industry. However, marketing professionals are not exclusive to advertising and marketing agencies and, more often than not, they do marketing functions within nonmarketing organizations (like grocery or apparel stores). Thus, it would be more productive to accept people from multiple industries who have the term “marketing” within job titles or descriptions.
2.  Construct your screener. 
When writing your questions, consider using open-ended or multiple-choice questions to avoid giving away the study’s intent.
For example, a yes/no question like Do you play video games? hints that the study might be related to video games and that the desirable answer is yes. However, if you phrased the question as Which of these activities have you done in the last 4 weeks? with a list of options like hiking, reading, shopping, and video games, then the intent of the study is less obvious. Having distractor answers keeps respondents honest and prevents them from gaming the questionnaire.
Place your most important exclusion criteria near the beginning of the survey so that you can quickly eliminate obvious misfits without wasting their time.
Survey logic can expedite the screening process, so consider picking a survey tool with strong branching capabilities.
3.  Pick your strategy for recruiting participants.
You could find participants for your study in a number of different ways. Each method has its strengths and weaknesses, and, depending on your study’s research questions, you may want to recruit from multiple sources.

Professional recruiting agency
	There are many professional recruiters whose sole purpose is to help UX or market researchers find qualified research candidates; most will even take over some of the work of interacting with prospective and final participants (e.g., scheduling, communication, payment). These agencies have a relatively wide reach in their ability to recruit “general audience” participants (e.g., a mix of genders, backgrounds, financial status, and ages).

	They are especially useful for addressing highly specialized recruiting needs or find specialized user groups (i.e., finding participants with a certain disability, or someone with a very specific type of context or background). They often vet participants in advance, a practice that results in a low no-show rate. Consequently, they are often more costly than automated tools or leveraging existing pools of users.
	 
Automated recruiting platform
	There are many online platforms for user research on the market, and some of them offer automated screening capabilities or recruiting services. If you target a “general audience,” then this method could be beneficial, due to its relatively low cost, ability to outsource some recruiting work, and its relatively quick turnaround (due to automation).

	However, with an automated recruiting platform, there is a greater risk of recruiting “professional testers” and sometimes screening-configuration options (like survey branching and logic) are limited within the tool.
	 
Internal panel of existing users
	Many companies will opt to recruit from their existing userbase, and establish a pool of willing volunteers who are willing to either try out new features (e.g., A/B split testing or beta testing) or participate in research opportunities in general. This approach can be great for recruiting experienced users of a specific product or for getting insights about employee-facing products.

	This method has lower immediate recruiting costs (both monetary and time-related): candidates are already semiqualified and there are no external recruiting fees. You would, however, most often need provide participants with some compensation — monetary or of some other type such as products — for their time. There is also the cost of creating a user panel and maintaining it, as well as doing the work of scheduling and coordinating participants. Some larger companies have a dedicated in-house research recruiter — staff hired to create and maintain the user panel and respond to various internal research needs as needed.

	The reach of an internal panel is limited to participants who already are familiar with your brand and offerings. Most likely, with such a recruiting pool you will not capture new-user perspectives. There might also be some sampling and confirmation bias due to participants’ existing brand loyalty — since they already have a relationship with the brand, they may already like it and provide positive feedback
	 
Online forums and groups (e.g., discussion boards, professional networking groups, or other social media groups)
	When the existing userbase and automated panels fail to provide the degree of specialization required, turning to online groups and forums can identify participants who have certain experiences, interests, or backgrounds. These groups can allow you to address specialized recruiting needs at a lower cost than with recruiting agencies or platforms.

	While these can help provide a group of highly motivated participants, the reach will also be narrow and limited to the members of those groups, which are not always representative of all individuals in the targeted audience. Particularly when the groups themselves have certain predominant viewpoints or vocal community members, there is a higher likelihood of sampling bias or groupthink. There is also going to be more time and effort required to communicate with participants and establish eligibility. If you decide to contact members of an internet group or social-media platform, make sure to request permission in advance from the group moderator.
	 
Intercept studies (or ""hallway recruiting"")
	If you ever heard or read the phrase “Would you be willing to take 5 minutes participate in a quick survey?” you know what an intercept study is. These types of studies can be done virtually (via a popup or modal dialog), on the phone (via interactive voice response (IVR) systems), or in person (in a shopping mall, office hallway, or coffee shop). They are ideal for recruiting visitors or existing customers and for finding participants with a specific goal or task in mind (like using a specific feature). Recruiting can thus be automated for unmoderated studies or surveys.

	Unfortunately, these studies can also result in some wasted researcher time (for moderated or in-person sessions, the researcher has to be available and ready to go whether a participant is found or not). Depending on the specificity of your needs, they might have greater turnaround time — for example, it may be hard to recruit 100 participants for a quantitative study of people who subscribe to a newsletter. Like internal panels, such studies (especially if online) can be subject to sampling and confirmation bias because they involve customers who have already decided to interact with the brand.






Recruiting Method


Reach


Cost


Effort


Time


Bias Risk




Professional recruiter


Wide
			General users
			Specialized users


High


Low


Med


Low




Automated recruiting platforms


Wide
			General users


Med


Low-Med


Low


Low




Internal user panels


Narrow
			Existing users
			Power users
			Employees


Low


Med


Low


Med-High




Online forums and groups


Narrow
			Specialized users


Low


High


Med


Med-High




Intercept studies


Narrow
			Visitors (new/existing)
			Task-oriented users


Low


High


Med


Med










Recruiting Method


Cost


Effort


Time


Bias
			Risk




Professional recruiter


High


Low


Med


Low




Automated platforms


Med

Low-Med

Low


Low




Internal user panels


Low


Med


Low


Med-High




Online forums and groups


Low


High


Med


Med-High




Intercept studies


Low


High


Med


Med





 
4.  Adapt your recruitment strategy and screening survey to attract the right participant.
Time-poor or high-earning professionals (like night-shift or swing-shift workers, executives, doctors, or lawyers) may need high incentives to justify time spent away from work or their personal lives. They also might be less likely to spend a lot of time filling out a lengthy screener survey.
 
5.  If you are recruiting two or more types of user segments for the study...
You can attempt to use one screening survey for all these sets of criteria, but there are tradeoffs:

On the plus side, participants don’t have to fill out the survey more than once, which increases the likelihood you’ll get a sufficient sample size.
You may also end up with a pretty complicated, branching screener or may need to manually filter candidates even after they filled out the screener

For example, let’s say you wanted to recruit two user types: power users who do not work in the technology sector and novice users who work in tech. You could have survey logic that routes tech professionals through the novice qualification criteria and routes nontech professionals through the power-user qualification criteria. Or, you could ask everybody whether they work in tech and take them through both the power-user and novice questions; then look at the answers and manually decide whether they fit your combination of requirements.
6.  Review the survey responses of both qualified and disqualified candidates before finalizing your approved list of participants. 
(If you work with a recruitment agency, ask them to provide all the filled screeners for you or at least those of the near matches.)
By reviewing the survey responses submitted, you can identify any close-fit study applicants that you may want to consider as a backup candidate if they have most, but not all the criteria for your target audience.
For example, if your target audience was parents with multiple children, a primary–caregiver aunt or uncle may have been disqualified, yet acceptable for the study. Alternatively, some participants may have been screened out because they picked one of several possible answers (e.g., they may have answered Android to the question Which type of phone do you own? even though they own both an Android and an iPhone).
7.  If you’re unsure whether eligible participants are truly a good fit for the study (because, for instance, your recruit is highly specific)...
It might be worth breaking the study into two parts: a 15-minute screening interview and a 30- or 60-minute research session. This screening interview can serve two purposes:

evaluating candidates to clarify their screening-survey responses and validate whether they are a good fit for the interview
preparing the selected participants for the main study session by reviewing key logistic details (e.g., consent forms, communication methods, expectations for participating, device requirements, applications that need to be downloaded, and other setup work like account creation).

8.  Finally, notify your fully qualified and approved participants, and begin scheduling sessions for your study.
Avoid scheduling anything with semiqualified candidates unless you know for sure you will invite them to the study. Otherwise, once you promise a spot, you are technically obliged to provide compensation if you cancel the appointment.
Conclusion
No matter who your users are, you must screen research participants to ensure you are using your research time and budget wisely. After all, your design decisions will only as good as your data is. By recruiting representative study participants, your team can reduce bias and build experiences tailored to your specific users’ needs."
16,2021-10-31,"One common question I get when teaching User Interviews, a full-day course at our UX Conference, is how many people do I need to interview? Unfortunately, there isn’t a golden number. In this article, I’ll highlight some factors that will help you decide.
Common Misconceptions
Some UX professionals assume that the recommendation to test with 5 users applies to interview-based studies as well. In fact, for many exploratory-research studies, 5 participants are too few. Others have been taught to recruit 5 people per persona, a rule of thumb to ensure that the sample is representative and large enough. However, this rule can result in many more interviews than necessary — especially when you have 5 or more personas. An experienced interviewer can extract in-depth insights from a much smaller number of interview sessions.
For quantitative research studies, it’s possible to perform sample-size calculations to tell us how many participants to recruit in order to confidently make generalizations to the wider population. But interviews are a qualitative research method. Qualitative research aims to understand the human experience in detail, not to determine how many people have had a given experience or express a particular need. (That’s what a follow-up quantitative study can tell us.) It’s not possible to know exactly how many participants is enough to gain this in-depth understanding.
Saturation
In qualitative research, the sample size is often determined by the point at which saturation is reached.
Definition: Saturation in a qualitative study is a point where themes emerging from the research are fleshed out enough such that conducting more interviews won’t provide new insights that would alter those themes.
(There are slightly different interpretations of saturation depending on the school of thought, but that’s beyond the scope of this article.)
After a certain number of interviews, there are diminishing returns — little new information is learned about the topic of study. This is the point where saturation is reached.

As more participants are added to the study, fewer new insights are gained about the topic of study.

The same phenomenon of saturation occurs when recruiting users for a usability test.  When modeling the usability issues uncovered from 11 studies, Jakob Nielsen and Tom Landauer found that testing with 5 people uncovered 85% of the usability issues in the interface being tested, and with each additional new participant added to the sample, fewer and fewer new usability issues were noted.
However, in an interview-based study, we’re looking to understand people’s experiences and needs rather than discover issues in an interface. Thus, because there’s more variability in the kinds of information we’re aiming to collect, the point of saturation is often higher than for user tests. Thus, 5 interviews are often not enough.
Unfortunately, there isn’t consensus amongst academic researchers on how many interview participants are needed to reach saturation. For example, Mark Mason analyzed abstracts of over 2,000 PhD theses that utilized qualitative interviews and found that the sample size ranged from as little as 1 to as many as 95. Although the median sample was 31, the standard deviation was large: 18.7. Over the years, researchers have offered different sample-size recommendations based on their own experience. These recommendations range from as little as 5 to 50. Marketing researchers Abbie Griffin and John Hauser analyzed and modeled the number of customer needs uncovered from their interviews on the topic of portable food-carrying devices and estimated that 20–30 interviews achieve saturation by uncovering 90–95% of all customer needs. Researchers Greg Guest, Arwen Bunce, and Laura Johnson conducted 60 interviews with women in Africa on the topic of sexual health and performed a thematic analysis after sets of 6 interviews. They then analyzed the number of new codes and code alterations after each round of analysis. The researchers observed that, of the 36 high-frequency codes (that led to the development of themes) that were added, 34 were added after 6 transcripts and 35 were added after 12 transcripts. Additionally, after analyzing 12 transcripts, the codes created remained complete and stable, despite subsequent interviews. These findings led them to conclude that saturation had occurred by the time they had conducted and analyzed 12 interviews, but they acknowledged that if they were interested in high-level overarching themes, a sample of 6 would have been sufficient for their project. (If this was a UX project, we would stop after 6 interviews for practical reasons. Although we may miss one theme and some further detail, the time taken to conduct and code 6 additional interviews could be better spent on researching other pressing problems.)
Two Large Factors Affecting Saturation
How many participants you need to interview to achieve saturation depends on:

The breadth and scope of your research goals
The diversity of the study population

A research project that is very exploratory and targets a diverse study population will require more interviews to reach saturation than a study with a small-scope research goal and a homogeneous study population. For example, a study looking at the experience of the general population accessing healthcare might require a sample of 20–30 (or more) people to see saturation. This is because the population is very varied (from young to old and healthy to sick) and the scope of the research (the experience of accessing healthcare) is quite broad. On the other hand, a study looking at the experience of patients with Type II Diabetes undergoing treatment could require as few as 5 interviews since it has a smaller scope (receiving treatment for a specific disease) and a more-homogeneous study population (each participant suffers from the same disease and will receive the same or very similar treatment).
Of course, there are some additional factors that can affect saturation. These include:

How experienced the interviewer is: Experienced interviewers can extract more insights by asking well-designed probing and followup questions and can uncover more themes when analyzing the data.
How much expertise recruited participants have: People with a lot of experience in a particular domain will be able to share more information about it.
How structured the interviews are: If interviews are unstructured, then the same topics or questions may not always be covered in each interview, making saturation harder to achieve with smaller samples. If the interviews are semistructured and the same few open-ended questions are asked of each interview participant, it’s more likely that overlaps will occur and saturation will be reached early on.

Considering these factors can give you an idea of your sample size.
Start Small and Analyze as You Go
Since it’s hard to know in advance when saturation will be reached and since discoveries are often too short, it’s best to start with a small representative sample (say 5-6), and analyze your interviews as you go. If you’re still learning new things and generating new codes, you can recruit a few more participants until you reach the point where your themes are complete and little new insight is being discovered with each new interview.
Make sure you recruit people who are your target audience and put an emphasis on relevant characteristics that could affect your research questions. For example, if you’re researching the end-to-end experience of booking vacations online, relevant characteristics could include income, frequency of travel, and age. These characteristics could affect people’s preferences, attitudes, and behaviors.
Of course, it’s good to get a mix of genders, ages, and ethnicities if you can, but since interview-based studies have small samples, it’s unlikely that your sample will match the general population proportionately on all demographic factors – and that’s okay! Some characteristics will be overrepresented in the sample.  For example, if you care about inclusive design, you should have people with low digital skills and accessibility needs “overrepresented” in your sample.
Lastly, if you need to set a sample size before starting your interviews (for budgeting or because a stakeholder wants to know), give a range. In this way, you can set expectations that, if you reach saturation early, you can stop.
Summary
How many interviews are enough depends on when you reach saturation, which, in turn, depends on your research goals and the people you’re studying. To avoid doing more interviews than you need, start small and analyze as you go, so you can stop once you’re no longer learning anything new.
References
Abbie Griffin and John R. Hauser. 1993. The Voice of the Customer. Marketing Science, Vol. 12. No. 1.                        
Greg Guest, Arwen Bunce & Laura Johnson. 2006. How many interviews are enough? An experiment with data saturation and variability. Field Methods, Vol. 18, No. 1, 59-82.
Mark Mason. 2010. Sample Size and Saturation in PhD Studies Using Qualitative Interviews. Forum Qualitative Sozialforschung / Forum: Qualitative Social Research, Vol. 11, No. 3. https://doi.org/10.17169/fqs-11.3.1428
Nielsen, Jakob, and Landauer, Thomas K.: ""A mathematical model of the finding of usability problems,"" Proceedings of ACM INTERCHI'93 Conference (Amsterdam, The Netherlands, 24-29 April 1993), pp. 206-213."
17,2021-10-24,"Context methods (such as field and diary studies) provide insights about a users’ real-life environment and behaviors and shed light on how products are used in a natural context. 
Here’s a list of NN/g’s most useful introductory articles and videos about context methods (field studies and diary studies), as well as some related topics. Within each section, the resources are in recommended reading order.
Context Methods: An Overview
Many UX-research methods involve asking users to pretend they’re in a realistic but hypothetical situation. For example, in a usability test, participants may be given the task to buy a new car. While we hope that users will behave as if they really were making this purchase, there might some important contextual details we’d miss out on with this method. 
Field and diary studies use a very different approach. They involve observing users’ behaviors in their real-life context. Participants are not asked to do anything special, except perhaps answer a few questions. 

In a diary study, participants document their experiences (thoughts, feelings, and behaviors) over a set period of time (a few days, a few weeks, or longer).
A field study is conducted in the user’s environment (e.g., home or office). Researchers follow each participant around and observe the participant’s normal daily behaviors and activities.

Returning to the car-buying example, a field study may involve observing participants in their homes while they research models of cars and dealerships. A diary study may have each participant log car-shopping activities such as visiting a dealership or discussing options with a partner.  
Field and diary studies are particularly useful during the discovery phase of a design project, when we’re trying to build up our understanding of our users and opportunities to improve their experiences. They’re also commonly used to help develop customer-journey maps.
For more information on both methods, consider our week-long series on Qualitative Research.
Diary Studies




Number


Link


Format


Description




1


Diary Studies: Understanding Long-Term User Behavior and Experiences


Article


When and how to conduct a diary study, plus tips for keeping respondents engaged and motivated 




2


Diary Studies


Video


What diary studies are and how they work




3


5 Steps for Effective Diary Studies in Customer Journey Research


Video


How to conduct diary studies to learn about the user journeys related to your product or service




4


What Could an Intelligent Assistant Do for You? A Diary Study of User Needs


Article


An example diary study focused on how people want to use intelligent assistants like Siri



5
How to Analyze Qualitative Data from UX Research: Thematic Analysis
Article
How to identify themes in unstructured data



Field Studies & Contextual Inquiry
The terms “field study” and “contextual inquiry” are often used interchangeably. Typically, a contextual inquiry involves more interviewing and conversation with the participant.




Number


Link


Format


Description




1


Field Studies


Article


What a field study is, when to run one, and how to plan it




2


Context Adds Value to UX Artifacts


Video


Why context matters in UX




3


Field Studies vs. Ethnographic Studies vs. Contextual Inquiry


Video


Differences between these three terms, which are often used interchangeably 




4


Ethnography in UX


Video


5 steps to rapid corporate ethnography in UX




5


What Are Contextual Inquiries?


Video


A method to define requirements, improve processes, learn what is important to users, and spark ideas for future projects




6


Contextual Inquiry: Leave Your Office to Find Design Ideas


Video


A particular type of field study that is more interview-heavy and its benefits




7


Contextual Inquiry: Inspire Design by Observing and Interviewing Users in Their Context


Article


When and how to conduct contextual inquiry




8


Contextual Inquiry Pitfalls


Video


How to overcome the main challenges with contextual inquiries




9


Field Studies Done Right: Fast and Observational


Article


Tips for running field studies without influencing user behavior




10


Doing Field Studies Remotely


Video


How to run field studies remotely, if needed




11


Field Studies Should Inform Intranet Redesign


Article


Examples of how field studies have helped designers improve intranet systems"
18,2021-08-29,"The following tables contain links to some of our articles and videos related to quantitative user research. Within each section, the resources are in recommended reading order. 
Quantitative vs. Qualitative UX Research
In UX, we often use qualitative research to gather insights or observations about users. This type of research is useful for discovering problems and determining design solutions. (We also have a study guide for qualitative usability testing.) 
With quantitative research, our focus is different. We collect UX metrics — numerical representations of different aspects of the experience. Quantitative research is great for determining the scale or priority of design problems, benchmarking the experience, or comparing different design alternatives in an experimental way.
4-minute video: Quantitative vs. Qualitative UX Research
Topics and Methods Covered in This Article

UX Benchmarking and Return on Investment
Quantitative Usability Testing
Analytics and A/B Testing
Surveys
Card Sorting and Tree Testing
Analyzing Quantitative Data
Visualizing and Presenting Quantitative Data

UX Benchmarking and Return on Investment (ROI)
UX benchmarking refers to evaluating a product or service’s user experience by using metrics to gauge its relative performance against a meaningful standard. Teams use benchmarking to track improvements to the user experience over time or to compare  against competitors. 
Benchmarking metrics are often also used to the calculate return on investment (ROI) of UX work; this type of calculation helps UX professionals prove their value and argue for more resources.




Number


Link


Format


Description




1


The Benefits of Benchmarking Your Product’s UX


Video


Track how well your design performs over time




2


Benchmarking UX: Tracking Metrics


Article


How benchmarking works at a high level




3


7 Steps to Benchmark Your Product’s UX


Article


Specific steps to follow to get started with benchmarking




4


Calculating ROI for Design Projects


Video


Using metrics to estimate the value of a design change




5


Calculating ROI for Design Projects in 4 Steps


Article




6


Three Myths About Calculating the ROI of UX


Article


Common mistakes people make when they get started with ROI calculations




7


Average UX Improvements Are Shrinking Over Time


Article


An analysis of benchmarking trends since 2006, meant to set expectations for how much your metrics might change over time




For more in-depth help, check out our report and full-day course. (Unlike the articles and videos in this study guide, these resources are not free.)
Report: UX Metrics and ROI
Full-day course: Measuring UX and ROI
Quantitative Usability Testing
In quantitative usability testing, researchers collect metrics (like time on task, success rates, and satisfaction scores) while participants perform tasks. This version of usability testing requires more participants and a more rigorous study structure than qualitative usability testing.




Number


Link


Format


Description




1


Quantitative vs. Qualitative UX Research


Video


How to determine when you need a quantitative study




2


Quantitative vs. Qualitative Usability Testing


Article


Differences between quantitative user testing and (the more-common) qualitative usability testing 




3


How Many Participants for Quantitative Usability Studies: A Summary of Sample-Size Recommendations
 


Article


The reasoning between the 40-participant guideline for quant user testing and why you may see other recommendations




4


Why You Cannot Trust Numbers from Qualitative Usability Studies
 


Article


Why it’s a mistake to  think you can collect quant metrics during qual studies 




5


Why 5 Participants Are Okay in a Qualitative Study, but Not in a Quantitative One
 


Article


Why sample sizes differ in quantitative vs. qualitative user testing




6


Writing Tasks for Quantitative and Qualitative Usability Studies
 


Article


The differences between tasks for quant vs. qual user testing and why good quant tasks are specific and concrete




7


Success Rate: The Simplest Usability Metric


Article


How to analyze task completion when you have multiple levels of success 




8


Risks of Quantitative Studies
 


Article


The reason why quantitative usability studies can’t replace qualitative studies, and how qual studies can complement the findings from quant studies




9


Between-Subjects vs. Within-Subjects Study Design


Article


How to choose between two alternative study setups in quant usability testing that compare two different designs 




10


How to Measure Learnability of a User Interface


Article


Quantifying the learnability of complex products that take a while for new users to learn by looking at how much time it takes people to learn the interface




Analytics and A/B Testing
Analytics data describe what people do with your live product — where they go, what they click on, what features they use, where they come from, and on which pages they decide to leave the site or app. This information can support a wide variety of UX activities —  it can help you monitor the performance of various content, UIs, or features in your product and identify what doesn’t work.




Number


Link


Format


Description




1


Analytics vs. Quantitative Usability Testing


Video


Comparing the information obtained from these two sources of quantitative metrics for UX




2


Three Uses for Analytics in User-Experience Practice


Article


How to avoid feeling lost in your analytics data and make it meaningful




3


Macro & Microconversions as Metrics in Analytics


Video


How to use both high-value user actions (macroconversions) and smaller-value, frequent user actions (microconversions) as analytics metrics to track the performance of your site and identify issues




4


Translating UX Goals into Analytics Measurement Plans


Article


Advice for choosing the right analytics metrics for your specific UX goals




5


Turning Analytics Findings into Usability Studies


Video


Pairing analytics with qualitative research to learn the “why” behind those problems identified through analytics




6


In Analytics, What do the Numbers Really Mean?


Video


How to understand analytics metrics that require interpretation




7


How to Interpret User Time Spent and Page Views


Video


When and how to use two key analytics metrics (time spent and page views) to evaluate whether your users are efficient or engaged




8


Vanity Metrics: Add Context to Add Meaning


Article


Why metrics that only go up (like total visitors) aren’t very useful and how to avoid these feel-good vanity metrics




9


5 Information Architecture Warning Signs in Your Analytics Reports


Article


How to use analytics to  discover potential problems in your product’s information architecture




10


Bounces vs. Exits in Web Analytics


Video


The difference between  two metrics that  people often confuse




While you can use analytics metrics to monitor your product’s, you can also create experiments that detect how different UI designs affect those metrics — either through A/B testing or multivariate testing.




Number


Link


Format


Description




1


A/B Testing 101


Video


How A/B testing works




2


Define Stronger A/B Test Variations Through UX Research


Article


How to ground your A/B testing experiments in research to develop well informed design variations




3


Don’t A/B Test Yourself Off a Cliff


Video


Why relying on A/B testing alone is likely to result in  design mistakes. 




4


Putting A/B Testing in Its Place


Article




5


A/B Testing vs. Multivariate Testing for Design Optimization


Video


When you need multivariate testing vs. A/B testing and why multivariate testing requires more traffic 




6


Multivariate vs. A/B Testing: Incremental vs. Radical Changes


Article




Full-day course: Analytics and User Experience
Surveys
Quantitative surveys involve asking a large number of users to answer a standardized set of questions. These surveys often involve selecting a response on a rating scales and are used to quantify users’ perceptions. 




Number


Link


Format


Description




1


User Satisfaction vs. Performance Metrics


Article


Why user satisfaction and performance metrics (like time on task) often correlate, but don’t always 




2


Survey Response Biases in User Research


Article


Biases which might cause problems in your survey data




3


Keep Online Surveys Short


Article


Why online surveys must be short to collect many high-quality responses




4


Iterative Design of a Survey Question: A Case Study


Article


An example of how to design and refine your own survey




5


Rating Scales in UX Research: Likert or Semantic Differential?


Article


When to use each of the two most popular types of rating scales




6


How Useful is the System Usability Scale (SUS) in UX Projects?


Video


Jakob Nielsen’s thoughts on one of the most popular and longest-standing UX questionnaires




7


Net Promoter Score: What a Customer-Relations Metric Can Tell You About Your User Experience


Article


The Net Promoter Score (NPS) is a popular marketing metric with limited relevance for UX




8


Beyond the NPS: Measuring Perceived Usability with the SUS, NASA-TLX, and the Single Ease Question After Tasks and Usability Tests


Article


A set of questionnaires to consider as alternatives to the NPS




Card Sorting and Tree Testing
Card sorting and tree testing are both useful methods for assessing and improving your product’s information architecture.
In a card-sorting study, participants are given content items (sometimes written on index cards) and asked to group and label those items in a way that makes sense to them. This test can either be conducted in person, using physical cards, or remotely using a card-sorting platform. Card sorting can have qualitative and quantitative components.
In a tree test, participants complete tasks using only the category structure of your site. It’s essentially a way to evaluate your information architecture by isolating it away from all other aspects of your UI.
 




Number


Link


Format


Description




1


The Difference Between Information Architecture (IA) and Navigation


Article


What information architecture is and how it relates to site navigation




2


Card Sorting: Uncover Users' Mental Models for Better Information Architecture


Article


An introduction to card sorting




3


Card Sorting: How to Best Organize Product Offerings


Video




4


Card Sorting: How Many Users to Test


Article


How many participants to include in your card-sorting study




5


 Open vs. Closed Card Sorting
 


Video


How to choose between these two variations of card sorting




6


Tree Testing: Fast, Iterative Evaluation of Menu Labels and Categories


Article


An introduction to tree testing




7


Tree Testing to Evaluate Information Architecture Categories


Video




8


Tree Testing Part 2: Interpreting the Results


Article


How to make decisions based on your tree testing data




9


Quantifying UX Improvements: A Case Study


Article


An example of how one team used tree testing when redesigning a B2B site’s information architecture




Full-day course: Information Architecture
Analyzing Quantitative Data
To draw conclusion and interpret quantitative data, you’ll need to understand some statistics and study-design concepts. The following resources will introduce you to those concepts.
These resources won’t give you step-by-step instructions for calculating things like confidence intervals or statistical significance — these are too complex to be covered in a short article. If you want to learn those analysis procedures, please see our full-day course below.




Number


Link


Format


Description




1


Internal vs. External Validity of UX Studies


Article


Why validity matters in UX studies




2


Why Confidence Intervals Matter for UX


Video


Why you should calculate confidence intervals for your quantitative metrics




3


Confidence Intervals, Margins of Error, and Confidence Levels in UX


Article


Detailed explanations of these three important analysis concepts




4


Statistical Significance in UX


Video


What statistical significance means, and why you should calculate statistical significance when comparing two designs quantitatively




5


Understanding Statistical Significance


Article




6


Handling Insignificance in UX Data


Video


What to do when your findings are not statistically significant




Full day course: How to Interpret UX Numbers
Visualizing and Presenting Quantitative Data




Number


Link


Format


Description




1


Better Charts for Analytics & Quantitative UX Data


Video

An overview of why you should customize your data charts to enhance Context, Clutter, and Contrast



2


Choosing Chart Types: Consider Context


Article

Clearly visualize your UX data by providing context and contrast, while avoiding clutter.


3
Clutter-Free: One of the 3 Cs for Better Charts
Article
Cut out the chartjunk; eliminate elements that distract from your data visualization.


4
Contrast: One of the 3Cs for Better Charts
Article
Direct your viewer’s attention to your point. Use color, callouts, and titles to communicate your key takeaway.


5
Dashboards: Making Charts and Graphs Easier to Understand
Article
Visualizations should leverage human cognition to communicate quantitative information quickly."
19,2021-08-01,"“Not everything that can be counted counts, and not everything that counts can be counted.”
– (Attributed to) Albert Einstein

A fairly common objection to qualitative UX research (especially from statistically literate audiences) is that small sample sizes result in anecdotal evidence or a few people’s subjective assessments, rather than data proper. Many UXers that work in domains such as healthcare, natural science, or even just “data-driven” organizations may find that it is difficult to build buy-in to conduct small-n research in the first place; even if they are able to do the testing, it’s often hard to build credibility about the recommendations that result from the findings.
Common objections include:

Comparisons between design options in studies with 5 or 10 users aren’t statistically significant (which is true).
Small sample sizes mean that we cannot confidently generalize things like time on task or success rates from a small study (also true).
Since we aren’t measuring things, our interpretations are therefore inherently subjective (indeed a potential hazard, but one that proper methods and good researchers account for).

While some of these objections are true (and are why we don’t recommend reporting numbers from qualitative studies), it’s a big jump to assert that qualitative research is anecdotal or lacks rigor. It is simply the case that qualitative research is a rather different mode of investigation.
If you are a UXer that is facing this sort of pushback, consider making the following points to your coworkers.
Qualitative Methods Are a Necessary Complement to Quantitative Measurement
People with a science education are usually familiar with experiments that use carefully controlled quantitative measurement as a way of evaluating hypotheses; this is typically known as probative research. For people with this background, the idea of talking to a small number of people (and perhaps even changing the study procedure slightly each time!) as a means of drawing conclusions may seem inherently unscientific, prone to bias, and unlikely to generalize to the general population.
But the goal of qualitative research is different: we’re not trying to disprove a hypothesis, but we’re looking to understand the nature of a problem in detail. Qualitative research doesn’t try to make quantitative claims that generalize to the whole target audience. Just because 6 people in a 10-person study are able to, for example, easily use a feature in an app does not mean that we can say that 60% of the overall population will have a similar experience. But, in that study, we can identify the issues that the 4 other people encountered (and also those that the 6 others struggled with, yet overcome) and understand the reasons behind them, with the goal of fixing those problems. (These issues are something that we can only speculate about if we look solely at a quantitative study or analytics data...)
Different goals require different methods of investigation: knowing how many people are having a problem requires a large sample size to be confident that the number we measure isn’t distorted by random chance, but knowing that a problem may occur and why requires that we observe behaviors and elicit users’ thoughts. Most important, how to redesign the UI to fix the problem requires these qualitative insights.
The goal of qualitative research is to gather insights that drive decision making, especially when measurement is unfeasible or impossible. While, of course, we can come up with some type of survey instrument for measuring satisfaction, emotional state, and other internal phenomena, those tools don’t tell us why the user feels that way at the moment or how we can better support their needs.
Qualitative Research Is Rigorous and Systematic
Still, an important question is: how can we know if qualitative research is rigorous and dependable to give us true insights about our users?
Rigor in quantitative research is seen as being comprised of a few major attributes:

Validity — is the thing we’re measuring a good representation of the thing we care about? Can our conclusions generalize beyond this experiment?
Reliability — if we repeat the research, will we get similar results?
Objectivity — do we have a way of ensuring that our observations aren’t clouded by our biases?

These characteristics are relatively straightforward for quantitative research, but are not easy to establish for most studies with small sample- sizes.
Social scientists Yvonna Lincoln and Egon Guba created a parallel set of characteristics for qualitative research that have become a standard way of assessing rigor:

Credibility: Did we accurately describe what we observed?
Transferability: Are our conclusions applicable in other contexts?
Dependability: Are our findings consistent and repeatable?
Confirmability: Did we avoid bias in our analysis?

We can satisfy those criteria by being systematic. That is the factor that makes the data we collect data, not anecdotes that happen by chance. If the CEO hears from a friend that the company’s app looks outdated, that’s an anecdote — there wasn’t a systematic process to gather that observation, it happened by chance and is only one person’s subjective opinion. If a UX researcher systematically recruits 5 participants and several of them struggle to understand the branded terms in the navigation, that is data.
Good qualitative researchers take many steps to ensure their work is systematic:

They relate their work to an evidence-based theoretical framework about how UIs should be designed and to a cognitive-psychology and human-computer–interaction body of knowledge about how users sense the world and mentally process it, behave, and engage in specific interactions with various forms of technology.
They formulate specific research questions before choosing the appropriate method.
They carefully sample by recruiting participants that represent a variety of perspectives, so they can learn about unknown unknowns.
They facilitate sessions using open-ended prompts to elicit participants’ thoughts and reactions with minimal bias and following up on intriguing incomplete statements from participants without unconsciously indicating to them how to respond.
They don’t just take users’ opinions at face value — they build up an understanding of why a user may request a feature or why something might not look appealing, for example.
They analyze our data by systematically coding the insights (again, drawing on the overarching theoretical framework of heuristics, known best practices, and so on). Coding is often done using inductive reasoning techniques borrowed from grounded-theory methodology — themes emerge from the data in a bottom-up analysis, rather than by starting with a list of codes that is then force-fit to the data. They then try to establish conceptual connections between the coded findings in the data and look for patterns.
When they encounter something unusual or extraordinary, they use triangulation to ensure that our conclusions are supported (i.e., they investigate the same thing via a different method or they have other trained researchers analyze the same data independently). Extraordinary claims require extraordinary evidence, after all.

Small Sample Sizes Are Fine, Depending on What You’re Looking At
But, you may be saying, What about those small sample sizes? Don’t they have an inherent sensitivity to outliers? Maybe a problem you observe is real, but rare, and you might overstate its importance due to a small sample.
These are all real concerns. So, how do qualitative researchers protect against that sort of overrepresentation of rare events in our conclusions?
Once again, we can point back to a robust theoretical framework we have in UX full of evidence-based principles about how users sense, think, behave, and interact with technology. If we observe even one person having a problem that is an exemplar of a known principle, we are able to be reasonably confident that it is a real problem. Of course, we still won’t be able to say precisely how many people will encounter that problem.
If the number of people affected by a problem is a real factor that we need to consider (e.g., if the problem will be expensive to fix and will take a lot of resources), then yes, we may need to do some form of quantitative experiment to figure that out. On the other hand, it is often cheaper (and more sensible) to simply fix the design problem without quantifying just how bad it is, if we’ve identified it early in the design process.
For example, if I design a deep fryer meant for consumer use, I will (hopefully) do some safety testing before selling it. If the first tester accidentally burns themselves on the fry basket because the handle is right over a heating element, I will probably not keep testing it with a large sample to figure out exactly what proportion of users will also burn themselves and sue me. In this case, I’ve found a major problem with a sample size of 1. Now, this example is obviously very simplified, and a small sample size will not be appropriate for every research question, but this approach will be often the best use of resources, especially when we’re looking for major blockers.
That is one of the main reasons we have consistently recommended small sample size studies, done early (and repeated with several iterations of a design): they are a relatively inexpensive way to find and address major usability issues that we would otherwise learn about from angry customers if we shipped the product without testing. It would be a waste of time and resources to confirm a major flaw in the design with many participants, especially if we’re working on a fast-moving Agile team.
A reasonable question might be asked: Why not use simply larger sample sizes for qualitative studies, in order to be more confident that, for example, the expectations and needs expressed by our study participants are commonplace, not unusual outliers? Fundamentally, this comes down to cost: to recruit more participants and also to moderate the study sessions. Eliciting users’ inner thoughts often requires a skilled facilitator who may need to do some improvisation during each session in order to adjust to the specifics of each study participant. Moreover, because the session protocol will be a little different every time, we would not be able to soundly compare and aggregate the data from all the sessions, since each “trial” would be different.In practice, qualitative researchers often land on a specific sample size based on how many participants it takes to reach a saturation point in the findings (i.e., they continue the study in small batches until they’re unlikely to learn enough new core insights to be worth the added delay to the project). Especially for interviews, field studies, and other forms of discovery-oriented research, this is the goal — rather than trying to determine how common the core findings are.
Empathy and Humanity Aren’t Easily Counted, but They Count
Last, but definitely not least, qualitative research allows us to build a real, empathetic understanding of users as human beings. When we view human interactions with technology primarily through the lens of metrics such as engagement, bounce rate, or time on task, we aren’t very concerned with the users’ well-being. (It might be in the back of our minds, but certainly not a primary consideration.) The tech industry is just beginning to reckon with the ethics of what we do and to realize that how we design our products has a real impact on the life of many, many human beings.
Moderated qualitative research requires that we engage with other humans (and even unmoderated studies still involve observing people). We typically need to build some form of rapport to get participants comfortable with expressing their inner thought process. We often discover that they experience the world differently than we do — in ways both small and subtle, and huge and overt. These studies provide the opportunity to empathize with them.
I don’t want to overstate the power of qualitative research here. It will not automatically generate empathy for users — I’ve certainly witnessed teams laughing while watching users struggle. Doing qualitative research will not fix ethical problems baked into a business model. Qualitative research certainly will not replace the critical need for just and inclusive hiring practices for your team, to ensure that decisions are made by people with a variety of backgrounds and lived experiences.
On the other hand, I also don’t want to undersell the value of the empathy built through this sort of research — for example, simply through noticing how frustrated one user gets and hearing them casually questioning if they are stupid because they couldn’t figure out a confusing design. That (unfortunately commonplace) reaction tells me that the problem is real and fixing it needs to be a priority, even if I don’t have a huge sample size.
Summary
Qualitative research is rigorous and systematic, but it has different goals than quantitative measurement. It illuminates a problem space with data about human experience — expectations, mental models, pain points, confusions, needs, goals, and preferences. Sample sizes are typically smaller than for quantitative experiments, because the goal isn’t to suggest that our sample participants will represent the whole populations proportionally; instead, we’re looking to find problems, identify needs, and improve designs. UX research is a mixed-methods discipline because these two approaches are complementary: measuring how much and understanding why can both help us build better products, which is the main goal of any UX research.
References
Juliet Corbin and Anselm Strauss. 1990. Grounded Theory Research: Procedures, Canons, and Evaluative Criteria. Qualitative Sociology, Vol. 13, No. 1, 1990
Yvonna Lincoln and Egon Guba. 1985. Naturalistic inquiry. Sage, Newbury Park, CA.
Saunders, B., Sim, J., Kingstone, T. et al. 2018. Saturation in qualitative research: exploring its conceptualization and operationalization. Qual Quant 52, 1893–1907. https://doi.org/10.1007/s11135-017-0574-8
Mike Hughes. 2011. Reliability and Dependability in Usability Testing, Retrieved from: https://www.uxmatters.com/mt/archives/2011/06/reliability-and-dependability-in-usability-testing.php"
20,2021-06-06,"User interviews are a research tool that can provide insights into users’ needs and beliefs, while also building empathy.  
Not all interviews qualify as research interviews. While journalistic interviews may have some similarities with user interviews, research interviews focus on nonjudgmentally and objectively gathering information about user needs, expectations, and interactions with a product or class of products. So, not every conversation with a user can be considered a research interview.
Unfortunately, even with the best intentions, it can be easy to bias or influence your participants’ responses. User interviews require a lot of attention to detail, and can fail for a number of different reasons, but if you focus on avoiding these common mistakes, you can ensure that your gathered data is objective, unbiased, and methodologically sound.
Common Interview-Facilitation Mistakes
1. Insufficient Rapport-Building
To some people, small talk might feel like an unnecessary chore or a waste of valuable interview time. However, rushing in and diving straight into a user interview without spending enough time building rapport will limit the quality (and quantity) of the data you’ll get from that interview.
During in-person interviews, one good way to build rapport without taking too much time is to personally pick up the participant from the waiting area (rather than delegating that task to a different person). Even if you don’t have a “waiting area,” or are doing interviews remotely, make sure to spend some time setting the tone and making your participant feel comfortable.
That said, it is also easy to take rapport building too far. Some interviewers — in their effort to build maximum rapport and seem as likable as possible — might start to share their own experiences to build solidarity and empathy with the participant. However, this approach can skew participants’ responses by shedding light on what the interviewer is interested in and may also cause participants to withdraw if there is an experience that they do not share with the interviewer.
To avoid this mistake: 

Introduce yourself and ask participants about their day, beyond the scope of the interview. This type of interaction can provide valuable additional context about your customers and the lives they live outside of using our products and services.
Speak calmly and slowly and try not to talk more than the participant. Participants will often ""mirror"" the researcher, so speaking slowly will help them remain calm as well. It also will give participants time to think about their responses. Aim to have the participant speak for roughly 80% of the interview time.
Avoid the word “interview” in your interactions with the participant; refer to the interview as a “chat” instead, to make it seem less judgmental. The word “interview” is often associated with job interviews and can increase anxiety (hindering rapport as a result).

2. Not Enough ""Probing"" Questions
Probing, or the act of asking followup questions to gain specific, in-depth information, is effective for uncovering the motivations and rationale behind certain behaviors, attitudes, and perspectives. Probing questions like “How does that make you feel?”, “Why do you think that is?”, or “Can you tell me more about that?” can get participants to share more information or to clarify what they meant in a prior statement.
These types of questions can feel a bit awkward or intrusive if you are not accustomed to asking them often; however, not asking probing questions can limit the depth and specificity of your participants’ responses and can lead to misunderstandings and ambiguity in your research data. The researchers may find themselves trying to extrapolate meaning from user quotes (rather than relying on a direct explanation from the participant).
To avoid this mistake:

Plan optional followup questions for each interview question as you’re writing your interview guide.  Even if you don’t end up asking them, it’s good to have some prepared followup questions to fall back on. These should be specific to the issue you are investigating (within this scope) and range in levels of specificity from broader (good at not assuming an answer) to very precise (something you really want to know, but don’t want to bring up yourself unless absolutely needed).
Keep a generic probes bank at the top (or bottom) of your interview guide so that you can fall back on them when needed, rather than repetitively asking “Why?” Generic questions are good at not leading the user, but you do need that bank of them, because it quickly gets annoying for a respondent to repeatedly be asked the same generic question (e.g., “How does that make you feel?”)

3. Multitasking and Note Taking During the Interview
When you are the sole researcher on your team, it can be an especially difficult to  commit your full attention to what the user is saying. Still, this is important, not only because it allows you to best interpret your participant’s words and gestures, but also because it builds the right kind of relationship with the participant. Preparation is key: the more you already know what to do next, the less you have to think about what comes next and can “live in the moment” and pay full attention to the participant.
Diverting your eyes from the participant to check your phone, answer a text message, or checking your watch can signal disinterest; so, turn off unnecessary applications and silence your devices. That said, the most common culprit of disrupted interviews is taking notes while facilitating the interview. This practice is problematic not only because it makes it hard to keep up with what your participant is saying and the participant has to wait for you, but also because it takes your attention and eye contact away from the participant and erodes the rapport you’ve been struggling to build.
To make matters worse, taking notes only when participants talk about something related to your research question can signal to them that some information is more desirable or interesting than other. Participants will often try to “please” the interviewer or be as helpful as possible, so they might alter their behavior to provide what seems to be most interesting or desirable to the researcher (rather than what is most representative of their experience).
To avoid this mistake:

When possible, record the interview and get it transcribed afterward. A recording helps avoid misquotes and gives other researchers on your team access to the same “raw” data for later analysis. Be mindful that your data-management practices respect your participants’ privacy, so anonymize the recording as much as possible, and remind participants that what is shared is confidential. (Even if this information is stated in the emailed instructions, these disclaimers can sometimes seem like insincere “boilerplate” text, so be sure to also say it out loud).
If you cannot record, assign a designated notetaker. If, for any reason, it is not possible to record the interview, have an assigned notetaker, whose primary role is to take notes on the entire interview (not just areas of interest).

4. Allowing Observers to Influence the Interview
While interest in user research is good, an observer in the interview room can hinder the ability to build rapport, which, again, can limit how willing your participant will be to share intimate details about their experiences.
An interview is meant to be a friendly one-on-one conversation; as you add observers who can insert their own questions,  it can feel a lot more like an interrogation. While it is not necessarily a deal breaker to have an observer or two present during the interview, the less noticeable they are to your participant, the less awkward your interview will be.
To avoid this mistake:

Record the interviews (as stated above) and offer recordings to any interested stakeholders or team members afterward. This tactic keeps observers completely out of the room and ensures best conditions for building rapport. That said, it is also not ideal for helping stakeholders or clients build empathy and common ground, since they may watch only fragments of the interview (or not watch it at all).
Set expectations and ground rules for observers. The interview facilitator should be the only person asking questions of the participant. Since observers are not actively building rapport with participants (and sometimes, may not be trained facilitators), the questions they may ask might not be received positively or might yield only terse responses. Set expectations and ground rules in advance (perhaps in a separate email or calendar invitation), and hold a short debrief for observers after the session (or series of sessions) so that any additional research questions can be factored into future interview iterations. (Note: If you're running the interviews via video chat, it may help to choose a meeting tool which hides the list of attendees from participants.)
Limit the number of interview staff in the room to 3 people and keep them out of sight. — either through a two-way mirror or by placing them off to the side, out of the participant’s range of vision. If running interviews on video chat and you're unable to hide attendee lists, consider having observers remove their profile picture, or rename themselves to something less recognizable, like a random string of letters/numbers. Still, participants may be affected by the knowledge that they are being watched.

As a more general point, most stakeholders are only willing to devote limited time to observing user research. Even actual members of UX team may not have time to observe all research, especially if they are not researchers themselves. In both cases, it’s better to allocate most of these people’s limited time to observing behavioral sessions (such as usability testing) where they can watch users actually use the product.
5. Leading the Participant
The most compromising of all facilitator mistakes is inadvertently skewing participant behavior or priming. Some researchers may accidentally “show their cards” and reveal the intent of the research study too early, thus biasing the participant’s contributions.  
For example, if we told the participant, “We’re studying how much word-of-mouth recommendations impact real-estate purchases,” the participant may focus their responses on recent word-of-mouth recommendations. On the other hand, if we said, “We are studying how people go about their home-buying process,” the participant might reveal that they don’t use word-of-mouth recommendations at all, but spend a lot of time independently browsing online real-estate listings.
As seen earlier, body language can also prime participants. A slow head nod changing into a vigorous one might hint to a participant that their response suddenly got interesting. Similarly, how questions are phrased can change participants’ responses. For example, the question “How important is price when looking at lawn-maintenance equipment?” might yield a predictable response of “Yes, price is very important,” because it implies that price should be considered and most people would like to respond in a way that is socially desirable and acceptable. It also reminds people of price, even though some may not necessarily think of that immediately. A better question would be “What factors do you use when selecting lawn-maintenance equipment?”
To avoid this mistake:

When introducing yourself and the study, keep the study’s purpose relatively vague, to avoid priming users.
Keep questions as open-ended as possible and start topics broadly before narrowing down with followup questions. 
Mind your body language and try to stay consistent throughout the entire interview. You don’t need to be stern or robotic, but ensure you stay relatively neutral, yet friendly and interested throughout.

Conclusion
Interviews are a method to learn about how your users view the world. To maximize the quality of your research,  spend the time to build rapport and focus on being an open-minded listener."
21,2021-04-18,"Designing for global audiences with different cultural backgrounds can be challenging.
To thrive in an international market, sites must go beyond translation and localization and gain first-hand data on how your users in your markets interact with your products. Even though the main usability guidelines stay the same across countries, testing with international audiences can reveal usability problems specific to those cultures. There are two main reasons to test with international users:

International users may interact with your products differently or rely on specific features more heavily than local audiences.
Mental models and how people interact with technology or organizations can vary from country to country.

Usability Problems Associated with Features More Likely to Be Used by International Users
International usability testing can uncover issues with important site functionalities rarely used by your major group.
For instance, domestic users may rarely if ever switch the site language. But for international users, the findability of this feature is crucial. During our user study in China, a female participant tried to shop at the mobile version of the South Korean clothes retailer Maybe-Baby. Soon, she realized the page was written in English and Korean, none of which she could read. Then she wondered if there was a Chinese version, tapping the navigation menu with no luck. She took a long time to discover the language switcher, which was in the page footer. She kept scrolling on the homepage, and after scrolling for about 13 screenfuls, she finally saw a row of national flags at the bottom of the page. Clicking China’s flag took her to the Chinese version of the site. She said, “I think it’s a good design, showing national flags for different languages, because maybe I don’t know how to say ‘Languages’ in either English or Korean. But why do they put it at the end of such a long page? I may have missed it.”

The mobile version of maybe-baby.co.kr (left) was 13-screenful long, with the language switcher in the footer (zoomed-in in the right part of the screenshot).

Clothing size is another feature that can vary from country to country, as are measurement units of choice. People shopping on a foreign site may feel they need to understand sizing and use tools such as size guides more than domestic users. A study participant tried to shop for a pair of jeans at Zara.cn, but she wasn’t able to properly access it. When she clicked the Size Guide button, an overlay with a big picture of a female wearing a pair of jeans showed up. The user was supposed to choose a size in order to have the dimensions shown on top of the image. But, unfortunately, she did not understand that: the Choose a Size label and the tiny arrow associated with it were very close to the bottom of the page. She was confused, “Where is the guide? It’s just a picture.”

Zara.cn: When a participant clicked Size Guide), she saw an image of the jeans on a model (left). She was supposed to use the Choose a Size below that image to select a size and have the corresponding dimensions overlapped on the image (right). However, she never discovered that functionality because the dropdown was too close to the bottom of her screen and the associated arrow too tiny. She thought there was something wrong with the site.

Uncovering these basic usability issues not only helps with international audiences but also benefits potential domestic users that are not typically recruited in usability tests, like minorities speaking different languages. It actually improves the accessibility of your design.
Culturally Specific Issues
In addition to general usability glitches, usability testing can also identify culturally specific problems during international usability testing related to:

Context of use
Perception of the product
Consumption of content
Interaction with your organization

Context of Use
Where do people typically use your products and who is with them? What types of device limitations do they have? You can learn about these questions better when you conduct remote international usability testing where study participants perform tasks at their preferred locations (like their homes or offices) instead of traditional in-lab usability testing.
Contextual factors refer to all artifacts (physical and digital) and people that may influence how users interact with your products or services.
While it’s neither possible nor necessary to figure out how all contextual factors impact the user experience, you may be able to identify issues such as broadband limitations.
For instance, when we conducted remote usability testing with Indian participants recently, we observed that several participants had to wait for a long time to load websites. Fancy websites full of animations, pictures, and videos only made it worse.
When a participant visited the Decathlon site on his mobile phone, he waited 20 seconds for the navigation to load (and it didn’t). The navigation was very complex, with lots of pictures and even an embedded search feature. He finally turned to the site’s search bar instead of waiting for the navigation to load fully. “The website is pretty heavy to load,” he commented.
In contrast, when he tried to shop at Paytmmall.com (an Indian ecommerce platform), the navigation’s minimal design loaded immediately, and he was happy to use it to move around the site.

Decathlon.in used a complex navigation (left). For each main module, there were several interactive submodules with a search feature. The navigation took so long to load that a participant gave up on using it. The lightweight and clear navigation of paytmmall.com (right) loaded much faster and the same participant quickly began shopping.

Your product’s compatibility with users’ preferred devices or software, like operating systems or browsers, is also vital. Since people are so used to these essential tools and have no problems browsing domestic websites, if your product doesn’t work, your users will believe it’s your problem, but not theirs.
We observed several such cases when we conducted ecommerce research study with Chinese participants. For example, a female participant tried to pick a lipstick for her friend’s birthday. She had no issues choosing a lipstick when visiting the mobile version of the Chinese MAC Cosmetics, but couldn’t check out for some reason. The cart button just wouldn’t work. She was disappointed about the site and believed something was wrong with it.
It turned out that she set the Baidu browser as her default browser, and there were some compatibility issues between that browser and the MAC site. According to Baidu Statistics, 9.5% of Chinese mobile traffic goes through the Baidu browser. MAC Cosmetics China may lose this segment of customers without even being aware of it. The researcher prompted her to try with her phone’s built-in browser instead, and the MAC site worked.
(However, you shouldn’t rely only on usability testing to find out compatibility issues; for small user segments, it can be hard to identify them during testing. Teams working on international products should consider these compatibility problems early in the design and development stages by checking platform-usage data.)
Sometimes, usability testing can also reveal how users’ interpersonal interactions and communication when using your products influence their experience.
For instance, during our testing, an Indian senior participant often asked her young daughter for help when she couldn’t find something on websites, since she wasn’t good at English and wasn’t very tech-savvy.
(Typically, we don’t allow other people to interfere with participants when they perform the tasks. However, we make exceptions in cases like this, because it’s a reflection of how the user actually uses digital technology in real life. Another example would be when parents help out young kids with tech issues.)
Perception of the Product
How do your international users perceive the visual design of your products? How do they feel about your products or services before and after performing some tasks? How much do they trust your brand?
People from different cultural backgrounds have different visual-design preferences. Researchers Katharina Reinecke and Krzysztof Gajos collected 2.4 million subjective ratings on website aesthetics from more than 39,000 participants all over the world. They found out that Russian participants significantly preferred websites with lower visual complexity, while people from Mexico and Chile gave higher scores to sites with high visual complexity. The same study also found that monochrome designs are preferred by Finns, Russians, French, and Germans, while Malaysians and Chileans tend to like colorful visuals. Similarly, our previous study in China also found that Chinese participants had a higher tolerance for high visual-complexity websites than foreign users.
Testing your visual designs with your target audiences is a good way to gauge their interests.
Beyond immediate aesthetic appraisal, usability testing can also uncover how users, especially newcomers, perceive the credibility and trustworthiness of your products or services. For example, different target audiences may perceive different design components as trustworthy.
When we tested the Chinese version of the Australian pharmacy website Pharmacy Online with Chinese participants, several people praised its credibility. They appreciated that the site had a navigation UI similar to that of Chinese ecommerce websites and also localized components such as a WeChat official account. These elements made the website look familiar to them.

The Chinese version of pharmacyonline.com.au (top) compared with Taobao’s homepage (bottom): Some participants perceived this localized design as highly credible because it used components familiar to them, such as a WeChat QR-code and Taobao-like information architecture.

Consumption of Content
Can international users easily understand the information presented on your site? Do they like its tone of voice? Do they find the content helpful and how do they make use of it?
Testing your content with your global audiences is especially important if you have multiple, culturally diverse audiences using the same language version of your site (e.g., USA, Australia, UK use the same English version).
For instance, when we tested the Chinese version of the Converse website, a young male participant complained about the labels. Instead of traditional ecommerce labels like Men, Women, and Accessories, Converse China used Dude 男的, Dudette 女的. (It is hard to find English equivalents to these two labels. In Chinese, these words often have a negative connotation, instead of the casual tone indicated by the English words Dude and Dudette.)
The participant, who shopped for fashion shoes a lot and belonged to the target audience of Converse, stated, “It says Dude and Dudette instead of Men and Women. It makes me feel uncomfortable. It’s a little bit casual, and I feel they don’t respect people.”
After translating your content for people speaking different languages, be sure to review its tone of voice as it can influence your branding. Don’t listen only to marketing experts; let several users perform tasks with your design and see how they feel about your tone.
Beyond the content itself and its tone of voice, people from different cultural backgrounds have varied preferences on how the content is organized. For instance, people in high-context cultures like China are used with websites with high information density and may prefer them to those with relatively low information density.
You can also find out whether the content you offer is helpful or whether you need to provide specific, additional content that these target audiences may be used to on similar sites.
For example, an Indian participant wasn’t satisfied with the dimension information (in centimeters) provided for photo frames on Myntra. “This is funny because when we measure the wall décor, we use inches instead of centimeters. So, the size information they provided was not useful.” (Some cultures in India prefer inches, others centimeters.)

The model size provided by Myntra (an Indian ecommerce platform) was in centimeters instead of inches, which wasn’t useful to a participant at all.

During our ecommerce study in China, a participant shopping for Nike shoes wanted a popularity-filter feature, which was available on almost all domestic retailer sites. He complained, “There are no filters on popularity! I want to know which ones sell best, so that I’m wearing the most popular style.”
Interaction with Your Organization
How do your international users typically interact with your organization or with other similar organizations? Are there any channels that tend to be used more frequently than others?
For instance, Indian participants prefer one-time passwords, but Chinese participants login via third-party services like WeChat and AliPay on mobile or by scanning a scan QR code on desktop.
When we ran an in-person university website study in China, a participant tried to figure out York University’s transfer student requirements. She wasn't happy with the website's vague information and was upset to learn that she needed to communicate with the institution through emails: “I don’t think it’s a proper way to communicate with Chinese students. I’d prefer some instant communication methods like an online Open Day through Skype.”
People from different cultures can also prefer different platforms to keep updated with their favorite brands. Open-ended and exploratory usability-testing tasks such as Check out whether any of your favorite brands launched something new can reveal their preferences and browsing patterns.
For instance, during our social media usability testing in the US, we combined such questions with a cognitive-mapping activity. We found that participants believed that organizations were more active on Twitter than on Facebook. Several used Instagram to follow different fashion-related businesses. They didn’t link use Snapchat to engage with businesses at all. In China, WeChat and Weibo (the equivalent of Twitter) are the most used social-media channels for engaging with businesses.
When to Run International Usability Testing
You should conduct usability testing with global audiences early in the product-development stage, when planning your strategy and product launch in a new market. Even paper prototyping can give you rich insights on both culturally specific and general usability issues.
You don’t have to identify all issues in one round of tests. Sometimes, running several rounds of small tests with a different focus is cheaper and easier. For instance, you can test basic usability issues, content, and tone of voice for the first round, while also observing context of use. A second round can focus on testing and prioritizing localized features and components.
After launching the product, you can do testing to research the usability of global components, used for all markets, and further investigate what other features you should localize to keep competitive in the new market.
Once your product has been known to your new market for a while, running quantitative usability testing, often used for benchmarking, can also benefit you. Keep track of your chosen success metrics and monitor them for major changes to evaluate your design work and compare your performance with industry standards.
You can (and should) combine usability testing of your products with other methods such as competitive usability testing, interviews, field studies, diary studies, and other qualitative research methods, to fully understand user needs, mental models, and expectations.
Conclusion
Conducting research with your global audiences can help you uncover issues you can’t find just by testing with domestic users. Perform these studies early and often to ensure success in your international markets.
Reference
Reinecke, K., & Gajos, K. Z. (2014, April). Quantifying visual preferences around the world. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 11-20)."
22,2021-02-28,"In the discovery phase of product development, user interviews are often used to capture important information about users: their backgrounds, beliefs, motivations, desires, or needs. Typically, the interviews carried out at this stage are semistructured (referred to as “depth interviews” by market researchers) — they generally have a predefined structure, but also allow the interviewer the flexibility to follow up on significant statements made by participants.
In a semistructured interview, the interviewer uses an interview guide (also referred to as a discussion guide). Unlike an interview script — which is used in structured interviews — an interview guide can be used flexibly: interviewers can ask questions in any order they see fit, omit questions, or ask questions that are not in the guide.
A good interview guide paves the way for a deep, free-flowing conversation with participants. (Obviously, the way you conduct the interview will also be important, but that’s another article!) Interview guides should include a few concise, open-ended questions to encourage participants to tell their stories. These will be followed by carefully crafted, probing questions to uncover motivations or beliefs behind certain reported behaviors or utterances.
Constructing a good interview guide can be tricky and time-consuming. It’s not uncommon to spend a full day crafting one. However, it’s important to have one to ensure you get the most out of your interviews. Without an interview guide you run the risk of:

Asking leading questions as you try to think of questions on the spot
Not covering topics relevant to your research questions in each interview

Ultimately, without an interview guide, you are in danger of compromising the validity of your data. Here are some steps meant to help you create an interview guide if you’re new to this practice.
Step 1: Write Your Research Questions
Sometimes the research questions are clear and obvious. However, sometimes they’re not. Maybe you’ve realized you need to speak to users because you don’t know anything about them. Good! But what do you want to find out from them? These are your research questions. Write these out first before working on your interview guide, as they will shape your interview questions. Below are some examples of research questions:

What are users’ expectations in this situation?
How do users make a decision in this situation?
How have users managed to solve this problem in the past?
What aspects of this product do users care most about, and why?

Step 2: Brainstorm Interview Questions
Next, note down all interview questions that come to mind. It doesn’t matter whether they are good or poor — you’ll deal with that later. You can use mind maps, digital whiteboards, or a simple list — whatever works for you! Sometimes, further research questions pop up here. That’s fine; add them to your list of research questions.
Step 3: Broaden Your Questions
It’s typical after step 2 to have a long list of mostly closed questions. Those kinds of questions wouldn’t make for a good interview because they won’t allow for unanticipated stories and statements to emerge and can limit your ability to build rapport with the participant. (Rapport is important if you’re looking to gather accurate, in-depth information from your participants.)
Review your list of questions and ask yourself, for each interview question, is there a broader, more open-ended version of that question that you can ask instead? 
For example, consider the following closed questions that could be asked in an interview with an employee.

Do you work in an office?
Is the work mostly desk-based or paper-based?
Do you have to attend meetings during the workday?
Do you work in a team?

The above questions could be answered by asking the participant to describe a typical day at work. It’s likely that in doing so, the participant may cover all or many of the above. If the participant has not covered everything, then some of these can be asked as follow-up questions.
Example questions, that prompt the user to recall a certain event and are similar to those used in the critical-incident method, are excellent for gathering stories and unanticipated statements. For instance, imagine you’re conducting an interview to learn about people’s experiences cooking at home. The following example questions provide the opportunity for participants to tell many different stories and give you a glimpse into their lives.

Tell me about the last time you cooked at home.
Tell me about a time where you cooked something new.
Tell me about a time when you cooked something that turned out well.
Tell me about a time when you cooked something that didn’t turn out as you hoped.
Tell me about a time when you were thinking about cooking something but decided to get takeout instead?

Step 4: Fill In for Unaccounted Research Questions
Align each interview question to your research questions. If you have research questions that are not addressed by any of your interview questions, fill in the gap by crafting some more interview questions. Repeat step 3 if needed.
The interview guide can include your research questions. Some researchers like to remind themselves of the aims of the research by displaying these either at the top of the guide or alongside the interview questions.
Step 5: Arrange Your Questions
To make the conversation flow in a logical order and seem natural think about the best order for your questions. For example, if you’re talking about an experience people have had, it makes sense to move in a chronological order. If the experience has set phases (such as discover, choose, purchase, use, review) that you might have documented in a user-journey map, service blueprint, or experience map then you may want to align your questions to these phases. That’s not to say you can’t depart from this order in the interview if you need to!
You should also think about preparing some warmup questions that are open-ended and easy to answer to build rapport at the beginning of your interview. For example, “Tell me a little about yourself” is a typical opening question which gets the participant talking. Any questions that require reflection should be featured later in your guide; introducing them too early could be overwhelming and you might get stereotypical responses, as participants haven’t had a chance to recall events, feelings, and form judgments.
Step 6: Prepare Additional Probing and Followup Questions
Once you’ve ordered your questions, go through each one and prepare followup questions meant to provide you with additional detail and clarification, such as “Where were you when this happened?”, “When did that happen?”, “Tell me why you did that?”, and so on.
You can include probing questions, too, to help you to remember to ask them — for example, “Tell me more about that”, “Tell me why you felt that way”, “Why is that important to you?”
Step 7: Pilot Your Guide
Piloting your guide will give you an idea of:

Questions you should ask but aren’t yet included in your guide
Questions that need rewording
Whether the question order works
Whether you will have time for all your questions

Recruit a pilot participant and give yourself enough time to make some changes. It’s okay to make updates to your guide throughout your interviews, but the point of piloting your guide is to fix any glaring issues before commencing research.
Summary
A guide will provide focus to your interviews and ensure that they are successful. Your interview guide should consist of broad, open-ended questions that allow participants to tell you about their experience in detail. These questions will be accompanied by many probing and followup questions, used to capture further details and gain clarification. You can download an example of an interview guide to refer to as you create your own interview guides.
Learn more: User Interviews, Advanced techniques to uncover values, motivations, and desires, a full-day course at the UX Conference.
Download
Example Interview Guide (PDF)"
23,2021-02-21,"A big hurdle to doing user research is convincing stakeholders that it’s necessary. Aside from concerns that it will cost too much (it doesn’t have to!) or take too long (it can be quick!), people who haven’t experienced the benefits of doing research often raise concerns about how much it will help and whether the results can be trusted.
This last concern is especially common with small studies, where people rightfully point out that the conclusions can’t be statistically ‘proven.’ This argument can be hard  to overcome because the truth is: they’re right. A small sample size is a limitation of many qualitative usability studies. Conversely, a lack of context and meaning is a big limitation of quantitative methods like analyzing analytics data.
All research methods are limited in some way. But the solution to overcoming these limitations is not to throw up our hands and quit doing research. Instead, the best approach is to use multiple research methods, so the limitations of one method are mitigated by data from another source. This approach of applying multiple research techniques is called triangulation. According to the Encyclopedia of Research Design:

Definition: Triangulation is the practice of using multiple sources of data or multiple approaches to analyzing data, to enhance the credibility of a research study.

The term triangulation is borrowed from geometry, where knowing the precise location of 2 points allows you to determine the distance to another object. In social-science research, triangulation does not necessarily mean you need exactly 2 different methods — it just means seeking out alternative viewpoints or verification of results.
Essentially, triangulation means looking at a question from a different point of view, which lets you see part of the answer that wasn’t previously apparent. You’ve probably done this many times in your everyday life — when you asked someone else’s opinion about a situation, hoping that a different point of view will reveal something that wasn’t obvious to you.
Examples of Triangulation in UX
Triangulation can take many forms. It can be as simple as going to check your existing body of analytics or customer-satisfaction surveys.


Triangulation can take many forms; two examples of how you could triangulate UX research are shown here. Left: Triangulate by using multiple methods to study the same activity, such as quantitative research, qualitative research, and expert review. Right: Triangulate by analyzing several different metrics related to the same activity, such as satisfaction ratings, time spent, and revenue volume.




Triangulation can take many forms; two examples of how you could triangulate UX research are shown here. Left: Triangulate by using multiple methods to study the same activity, such as quantitative research, qualitative research, and expert review. Right: Triangulate by analyzing several different metrics related to the same activity, such as satisfaction ratings, time spent, and revenue volume.


Some examples of research triangulation are:

Satisfaction metrics decline ⟶ you check revenue and time spent to see if they also changed
A quantitative usability test indicates low subscription-form success rates ⟶ you do a qualitative study to understand what features are problematic
Sales team reports that users think the software is hard to use ⟶ you do a usability study to observe problems.
Analytics data indicate a feature has high error rates ⟶ you check customer-support records to determine if problems are reported with this feature.
Interviews suggest a surprising purchase motivation ⟶ you do a survey to assess the frequency of that motivation.
One researcher notes several themes in interview transcripts ⟶ another researcher does a separate theme analysis to check if she finds the same themes  

When Should You Triangulate?
The more significant the decision, the more it pays to triangulate before making it. In fact, eventually you’re going to get another source of data whether you seek it out or not — it will be data from the market success of your product once you’ve implemented your design. Of course, at that point, it will be harder and more expensive to adjust your product.
Triangulating in advance means you’ll be less likely to be surprised by unexpected  reactions from your real users. 
The question should not be ‘how much time can we spend on research’ but rather ‘how much risk of problems or failure are we willing to accept?’
Expensive choices, like redesigning a whole product, warrant robust triangulation with a mix of qualitative and quantitative data collection and analysis, as well as an external, independent analysis.
Simple, easily reversible decisions don’t need so much investment. But experienced teams know that seeing whether other available data supports the favored approach is well worth at least a few hours.
Triangulation is much easier if you have a diverse and flexible skill set on your UX team. You need experience with a range of different methods in order to implement them quickly enough to be useful without slowing the pace of decision making and development. Ultimately, this diversity of skills is an important step towards having a mature UX practice that can rapidly deliver valuable and reliable insights.
Reference
Neili J. Salkind (ed). 2010. Encyclopedia of Research Design. Sage Publications. DOI: 10.4135/9781412961288.n469."
24,2021-02-14,"Any UX-research study aims to answer general questions about our design or about our users. What percentage of our user population will be able to subscribe to our newsletter? What major usability issues will people encounter on our site? Is design A more usable than design B for our target audience? But any time we set up a UX-research study, whether quantitative or qualitative, there is danger that it will not reflect the reality we want to capture because the study is poorly designed.  
There are two big types of study-design errors:

Internal-validity errors that bias participants towards a certain response or behavior
External-validity errors that capture behaviors or situations which are not characteristic for our target audience

We’ll talk about each of these separately. But before we do, let’s note that validity is separate from reliability. Reliability of a study simply means that you will get the same result if you repeat the study. In other words, findings are not random. There are plenty of statistical methods to calculate the degree of study reliability, and the main way to increase reliability is to test more participants. But reliability is no good without validity: a study with high reliability and low validity is one where you get a really good measurement of the wrong thing.
Internal Validity for UX Studies
Think of a study that compares two sites — site A and site B. You are trying to decide which of the two is  better and you always show the participants in your study design A first, ask them to complete some tasks on it, then move to design B and show them the same tasks. Is this study design likely to produce accurate results, that reflect the reality? In other words, will this study identify the better design?
Not necessarily. This study setup favors design B because, when they get to it, participants will be already used to the testing situation and with the task domain — if they’re testing car-rental sites, they will  already know what a LDW (loss-damage waver) is when they get to site B and they may have certain expectations regarding the steps of the rental process. They will also know what you expect them to do and how they’re supposed to perform the task. Therefore, this study is missing internal validity. (The usual fix to this problem is to alternate which site goes first, and have half of the users try site B first.)
Definition: A study has internal validity if it does not favor or encourage any particular participant response or behavior.
Internal validity is an issue in both qualitative and quantitative studies. With moderated qualitative studies, the facilitator may inadvertently bias or eliciting a certain response from the participants. For example, even a simple questions such as “Have you found the checkout difficult?” may invalidate the study results because the participants are primed  to think of difficulties, so they may identify more than normal (like with Richard Nixon’s “I am not a crook” statement).
With quantitative studies, lack of internal validity may produce results that skew in one direction, but do not reflect the reality. You may, for instance, in a benchmarking study, discover that your time on task is better on a redesigned version of the site than on the original and you may infer that you did a good job with the redesign, when in fact, the difference was due to different study protocols — the original test used the think-aloud protocol, but the test of the redesign didn’t. (And thinking aloud does take some extra time, so it can cause longer task times.)
In this example, the protocol is an example of a confounding variable — a hidden variable that can affect the results of your study, but that you didn’t take into account when you designed the study.
External Validity
External validity is about how naturalistic your study is.
If you’re designing a site for seniors and recruit study participants from the general population, will that study be valid? Will it tell you something relevant about your real audience? Possibly not, because younger participants are likely to behave differently than older ones. Or, if you’re testing a mobile design on a desktop, will your findings generalize to the use of the design in the wild? Maybe yes, maybe no — it’s impossible to know for sure (unless you do another study).  In both these situations, the studies are missing external validity.
Definition: A study has external validity if the participants and the study setup are representative for the real-world situation in which the design is used.
The concept of external validity also applies to both qualitative and quantitative studies — for obvious reasons.
Recommendations for Study Design
Here are some recommendations to help you set up studies that are both internally and externally valid.
Internal validity
Randomization is essential for ensuring internal validity.

Use random ordering of tasks. 

Task order can bias task responses. At the beginning of a study, people are usually new to both the study environment and to the system that they’re testing. It’s normal for them to take longer to perform the first tasks in a session and perhaps make more errors than normal. On the other hand, tasks that are shown at the end of the session might see the effect of participant fatigue.
That is why we strongly recommend that in any test, whether qualitative or quantitative, you randomize the order of the tasks as much as possible. (Sometimes, however, following this recommendation may not be entirely feasible — for example, if the tasks are Log in and Deposit check, it may not be possible for Deposit check to follow Log in).
Additionally, to mitigate the learning phase at the beginning of every session, we recommend that you prepare 1–2 warmup tasks (psychologists call them practice trials) that are irrelevant for your study and that are meant to get participants familiar and comfortable with the study environment and the study procedure. I like to pick easy tasks that bolster participants’ confidence and make them feel relaxed. But, if you do use warmup tasks, make sure that you do not include them in your analysis.

If your study contrasts two or more conditions (e.g., you want to compare your site with a competitor site) and each participant will be exposed to all conditions (i.e., within-subject design), you should counterbalance or randomize the order in which each participant is exposed to those conditions (for instance, the order in which they see your site and your competitor’s).  

This recommendation is related to the previous one — randomizing the task order. However, if you’re testing, say, 2 ecommerce sites, sometimes it may be unrealistic or unfeasible to ask the participant to shop on site one, then add an item to a wishlist on site 2, then go back to site 1 and subscribe to the newsletter, then shop on site 2 — this would be a detrimental and possibly confusing setup, if you want, for instance, to collect post-test questionnaires such as SUS and NPS for the two designs at the end of the session.
In that situation, we recommend that you group all the tasks for design 1 together and all the tasks for design 2 together. You should, however, randomize the order in which participants see the two designs — with some participants seeing design 1 first and others seeing design 2 first. And, within each design itself, the order of tasks should be randomized.

Control study setup from one session to the next and look for confounding variables — hidden factors that could affect your results.

For example, assume a researcher is interested in comparing two sites and uses a between-subject design. She decides to study site A with the participants in the morning sessions and site B with those participants coming for afternoon sessions. If she ends up finding that participants perform better on, say, site A, it could be because site A is better, or it could be because people are less tired in the morning.
Similarly, if a colleague helps you facilitate a study and you divide the sites — you take the sessions with site A and she takes site B, the facilitator is a hidden variable. It could be that one facilitator’s style is more biasing than the other or that one facilitator is a naturally a more pleasant person and participants feel more talkative and relaxed with her.
Thus, if you know that there will be any factors that will need to vary from one session to the next, ensure that they vary for all the conditions in your study.
When you put together a benchmarking program for your organization, planning carefully for internal validity is essential.  You have to document very carefully your study conditions (task wording, study protocol, whether think-aloud was used, and so on) so that they could be replicated in further studies that you will run in order to determine design improvements over time. Otherwise, a difference between a current version of a system and a prior installment may simply be due to study setup rather than to usability improvements.
External validity

Recruit participants who are representative of your target audience — both in terms of demographics and user goals.

In general, researchers are very careful with creating screeners that match the exact demographics of their population, yet that may not be enough to ensure external validity. It could be that your participants are in the right demographics but have very different goals than your users (or they’re simply not motivated enough).  Always strive to find participants who are likely to have the same goals as your users.

Replicate, to the best of your abilities, the natural situation in which participants will use the UI that they test.

Are your participants supposed to use your car-repair mobile application in their garage? Then don’t have them test it in a conference room. The environment — light, dirty hands, place where the phone is positioned, time available, tools available — are all likely to play a role in how usable this app is.
However, sometimes it may be impossible for a study to be externally valid.
Is External Validity Always Possible?
In some sense, any study will lack external validity — we rarely use interfaces with a stranger watching over our shoulder, sitting at a desk or in a lab. (To some extent, one could even argue that some remote studies are more externally valid than in-person ones because at least the participants may be in their natural environments.)  We also know that participants tend to behave slightly differently — more compliant and more persistent — in a usability-testing situation than by themselves.
Also, sometimes, it may be too cost-prohibitive to test a design in the natural environment. For example, we are great advocates of paper prototyping, but these types of tests will always lack external validity.  So, what should we do?
In these situations, some testing is better than no testing. With paper prototyping, it may be that your results are not externally valid and you will have to retest later on in naturalistic conditions. But the goal of paper prototyping is to identify any big hurdles so that you won’t spend money implementing something that is completely off. So, run a paper-prototyping study, identify the big issues, fix them, then move forward to a high-fidelity prototype that you could test in naturalistic conditions, on the device that participants will use to complete the task.
Another common situation that lacks external validity is mobile testing — most participants will not use mobile designs uninterrupted, sitting at a desk, and connected to wifi.  It can, however, be acceptable to test in that setup to identify those issues that will be encountered even in the best-case scenario of a great connection and no interruptions. Those are likely the first issues many mobile sites will need to address — if the site has problems even under ideal conditions, then the design needs to be fixed.  Once you’ve ironed out those issues, you still will need to retest under more realistic conditions.
Similarly, some quantitative-study professionals recommend to include only expert participants in certain quantitative studies in order to reduce variability (lack of variability translates into a lower margin of error for the study results and may allow the researchers to reduce the number of participants). The expert users will give you a best-case scenario and you should be fine as long as you don’t assume that the results will generalize to all your users.
In general, if you find yourself forced to sacrifice some external validity, it’s crucial that you always interpret your findings in context and realize that they may not stand true if the study were to be replicated in realistic conditions.
Conclusion
Poorly planned research will translate in results that are invalid. You may have potentially wasted time and money on running a study which doesn’t tell you anything about your product or your audience. Pay attention to your study’s internal and external validity — strive to recruit participants that are representative of your target audience and make sure that the study setup replicates how your users will use the system in real life and that it does not encourage any one behavior or response."
25,2020-12-06,"In our collection of UX-research methodologies, contextual inquiry is essential.  
Contextual inquiry is a type of ethnographic field study that involves in-depth observation and interviews of a small sample of users to gain a robust understanding of work practices and behaviors. Its name describes exactly what makes it valuable — inquiry in context:

Context: The research takes place in the users’ natural environment as they conduct their activities the way they normally would. The context could be in their home, office, or somewhere else entirely.
Inquiry: The researcher watches the user as she performs her task and asks for information to understand how and why users do what they do.

Contextual inquiry is useful for many domains, but it is especially well-suited for understanding users’ interactions with complex systems and in-depth processes, as well as the point of view of expert users.
Why Conduct Contextual Inquiry
Typically we conduct contextual inquiry during the early discovery stages for a new feature or product because this research data is so critical in shaping design choices such as requirements, personas, features, architecture, and content strategy.
﻿The contextual-inquiry method was developed by Hugh Beyer and Karen Holtzblatt as a way to resolve the drawbacks of other qualitative-research methodologies such as surveys and interviews. These methodologies rely on the users’ ability to recall and explain a process that they are removed from in that moment. People attempt to summarize their processes, but important details like reasoning, motivation, and underlying mental models are left out of this summary, leaving researchers with only a superficial understanding of the users’ approach to the activity.
However, users can easily talk about what they are doing and why when they are doing it. ﻿For this reason, contextual inquiry can provide richer and more relevant information about how users complete processes than self-reported or lab-based research methods do.
One of the greatest strengths of this methodology is that you get to see things you wouldn’t anticipate and uncover low-level details that have become habitual and invisible. You get to see the interruptions, superstitious behaviors, and illogical processes that directly influence UX work.
I was once redesigning the data-entry portion of a software tool used to create auto-insurance policies. First, I interviewed several specialists about how they entered vehicle data for entire fleets of commercial vehicles into the software. All three of the specialists I interviewed reported copying large batches of vehicle data from a spreadsheet and pasting it into a data table within the software interface. Later, I went to observe and interview some of these specialists as they did this process. What I discovered is that there were several other steps involved that they hadn’t mentioned. They were also crossreferencing a screen from another software tool to fetch several missing pieces of correlating information that needed to be input for each vehicle. They also hit the Save button habitually after every piece of data that was manually entered, even though the program autosaved their progress. They were either not aware or did not trust that the data was saved. These were both significant insights that influenced the design of the new tool.
When Is Contextual Inquiry Not Useful?
Contextual inquiry is designed to help us understand the in-depth thought processes of users and the underlying structure of their activities. For this reason, it’s not especially useful for targeted design tasks such as redesigning an ecommerce product page or testing a newsletter signup form on a website. These types of interfaces are fairly straightforward: they don’t typically require in-depth thought processes or an underlying body of knowledge that UX professionals must understand in order to design them.
In some of these situations, it still may be helpful to observe users go through these experiences to understand general behaviors, but you won’t likely need the added interview component and would mostly rely on direct observation.  Direct observation is similar to contextual inquiry, but the researcher silently observes behaviors most of the time, with minimal interference in the user’s process. Direct observation may also be the best field-research option if your participants cannot be interrupted or distracted while they work, such as doctors or flight traffic controllers. In these situations, you may have to follow up with clarification questions at another time.
How to Conduct Contextual Inquiry
The contextual-inquiry method uses the relationship between a master craftsman and apprentice as a model for the interaction between the participant and the researcher. Though apprenticeship is less common today than it used to be, people are still fairly familiar with the idea and able to draw inspiration from it. Just as a master craftsman teaches an apprentice a skill through doing, a researcher (“the apprentice”) learns about users' tasks by observing a user (“the craftsman”) and asking questions.
4 Grounding Principles
Contextual inquiry is based on 4 principles that help researchers adjust and apply the apprenticeship model to the context of their products and work.

Context. The researcher should observe in the natural environment. Just as craftsmen do not prepare a summary of talking points to teach technique in a classroom, researchers should conduct the research where the user typically works, avoiding labs or conference-rooms settings.
Partnership. The user and researcher are partners in the process of understanding the work. The researcher should not control the entire session and content of discussions. Both parties should be free to direct the conversation toward what needs to be considered.
Interpretation. The researcher should develop a comprehensive and shared interpretation for all important aspects of the work, aided by feedback from the user.
Focus. The researcher should understand the purpose of the research project and what information should be sought. This understanding guides the observation and the interviews during sessions.

4-Part Session Structure
Select participants that are uniquely qualified and knowledgeable in the area you need to understand. Then, use the following 4-part structure as a template to guide your approach.

 The primer 

The primer is meant to ease the participant into the session. Starting casually allows your participant to become comfortable with you and learn what to expect from the session.

Introduce yourself and take some time upfront to build rapport with your participant.
Indicate what you hope to achieve during the interview and that you expect the participant to correct any misinterpretations you may develop as you learn.
Discuss confidentiality and get approval for any filming or recording you may be doing.
Begin to broach the subject you are interested in. Ask for a summary of the work to be done during the time allotted and ask for any relevant opinions. However, because we know that recollection is not always entirely accurate, be sure to validate these opinions and explanations with your own observation


The transition 

When finished with the introduction and general interview, make an explicit and clear transition into the contextual interview portion of the meeting. Stop and explain what will happen during the rest of the session and what you need:

Let the user know that you will watch while she goes about her work and that she should expect you to interrupt whenever you see something interesting to discuss.
If it is a bad time for interruption, she should communicate this to you and continue until a better stopping point.

Don’t skip this important step. If you do, the user may carry on in interview mode. You need to shift her focus to a different type of interaction with you going forward.

The contextual interview

This phase usually goes through multiple iterations of the following 2-step pattern:

Watch and learn.
Stop and initiate discussion when the user does something you don’t immediately understand or when you want to confirm an interpretation.

The interview will begin to take on a rhythm of its own with periods of work and periods of discussion throughout. Try to understand underlying processes:

 Be aware of external resources being used.
Ask about standard steps vs. extraneous or uncommon variations in their processes and the reasons behind them.
Explain your interpretations of their tasks and workflow for the users to confirm or correct.

You should initiate discussion for 2 reasons:

If you’ve observed something you don’t understand. In this case, ask open-ended questions and let the participant give you details about why she took a certain action.
To allow the participant to validate or invalidate your understanding of the user’s mental model. One of the goals of contextual inquiry is to uncover the participant’s mental model of a process. So, when you feel you have a fairly strong hypothesis for this mental model,   ask the participant to weigh in to confirm or correct your understanding.

For example, if a user has two separate monitors and moves different windows from one to another, you may first ask the user to explain why she’s doing that. With the user’s explanation, you might form a hypothesis that provides an understandable structure behind her reasoning — such as certain windows should always go on specific monitors. To validate your hypothesis, you may ask “So, is the laptop monitor for communication only and the big screen for your work tasks?” In which case, the user could confirm your assumption or correct the inference by saying, “It’s more that I like everything that I need to monitor (email, Slack, stock tickers) to be on my laptop screen and active tasks that I am working on my big screen.”
Be judicious about how often you ask participants to validate your interpretations during this phase, as it may bias their future behaviors. You will have time in the next phase, the wrapup, to discuss all your interpretations.

The wrapup

﻿At the end:

Ask any final clarifying questions.
Review your notes and summarize what you took away from the interview by explaining your interpretation of the observed processes. This is your users’ chance to give final clarifications and correct your understanding.

The time required for a contextual-inquiry session will depend on the scope context of the work you are intending to understand. They can range from an hour or two to several days of observations and interviews.
Risks and Downsides
With every methodology, there are downsides and potential risks to be aware of. Contextual inquiry is no different. Below are some common risks to avoid when conducting contextual inquiry:

Participants default into interview mode.  Working and demonstrating while being interviewed about what you are doing is unfamiliar to most people. It can be easy for participants to begin summarizing their processes and treat the session like a tell-all, which is what contextual inquiry seeks to avoid. The transition phase of the interview needs to be carried out well to prevent this issue. If participants continue switching into interview mode, remind them that you are interested in the fine details of their work, so they should complete it as they normally would, without you there.
The session turns into an airing of grievances. Usually, when you are observing users, it’s because you are seeking to understand system issues for the purpose of a redesign. Participants may feel you are looking for their feedback on all the problems with the system, in which case the session can turn into a show-and-tell of their frustrations with the current solution. However, the purpose of contextual inquiry is to go beyond the interface and truly understand people’s thoughts and work process, regardless of the solution. In such a situation, researchers should redirect people to work as they normally would and explain the reasoning behind their actions.
You may bring your own biases to the session. With any type of user research, it’s possible that you will come into the sessions with preconceived notions or opinions about the subject matter. If you bring your opinions into these sessions, they can bias your approach to the interview and, as a result, your understanding of what you are there to observe. (Plus, since people aim to please, if users sense that you prefer certain opinions or types of answers, they will be likely to supply comments along the lines of your biases.) It’s important to approach contextual inquiry objectively. Be purposeful about leaving all your previous understanding about the work behind you and go into the research with an open mind, treating everything you learn with the same level of importance.
You may bias the user. During contextual inquiry, as you probe and ask the participant to react to your interpretations of their work, it’s possible that she may adjust her process to fit into the discussion or your interpretations. Be sure to stress that users should always work as they normally would and favor open-ended conversation that allows them to fill in the blanks for you.

Conclusion
After contextual-inquiry sessions have been completed, researchers and designers should come together to share findings and interpret the results of the interviews. Workshop exercises for finding themes in qualitative data, such as affinity mapping, help the team align on patterns and themes.  Contextual inquiry is often coupled with task analysis. In the end, teams should walk away with a shared understanding of users’ work processes, mental models, and common behaviors, so they are prepared to design solutions for their customers.
References:
Contextual Design: Designing Customer-Centered Systems, Hugh Beyer and Karen Holtzblatt, 1990."
26,2020-10-04,"Effective user researchers balance project-schedule constraints with their research budget. Efficient researchers attempt to spend only as much as needed, so the remaining money can be used for more research.
When I run research projects, I try to spend enough money on recruiting and incentivizing participants, to ensure that (1) I find the right kind of users and (2) that they are motivated enough to show up for the sessions (and are also fairly compensated for it). “No-show” participants or last-minute cancelations can thwart the project schedule, make stakeholders lose interest in the research, and waste money.
Consider the expense of each testing slot in a usability study or other UX research activity:

Amortize the planning expenses across the study participants. So, for example, 20 hours spent planning a 5-participant study amount to 4 hours of expensive UX-specialist salary for each participant slot.
Look at the costs of using the testing space. For an in-person study, renting lab space that stands empty because of a no-show user is an out-of-pocket cost. Even if you’re testing in, say, a meeting room in your own building, there’s the opportunity cost of having booked the room and prevented other employees from using it.
Include team members’ time. The biggest expense of a scheduled study slot that goes unused is often the cost of the team members (including yourself) who are waiting for the participant.

Don’t forget the reduced research insights stemming from fewer research participants than the plan called for. If you follow our recommendations for small sample sizes for most qualitative studies, each lost participant represents a fairly substantial proportion of your potential insights, and thus reduces the profitability of your design project.
When to Recruit Floaters
There are some cases when it’s appropriate to throw thriftiness to the wind and recruit extra users, also known as “floaters.” This strategy makes it likely that you will fill in each research slot. Some scenarios when this approach is beneficial include:

The project schedule allows only a small time window for research. For example, a client may need a usability test and its results within a few days.
An external factor imposes time limits. For example, equipment or the location for a lab study are rented and only available for a finite period.
The participant background requires partially screening the participants on site, at the time of the study. For example, in an eyetracking study you may need to calibrate the users’ eyes with the eyetracking system onsite.
A lot rides on observers seeing a particular research session or set of sessions. For example, if a team had to be convinced to do research and is skeptical, and it finally agrees to observe at a certain time, you need that user to be there at this critical point.

To ensure that you have enough participants when you need them, you can practice “intentional overrecruiting.” Overrecruiting can manifest in a few ways:

recruit multiple people (a participant and a floater) for each scheduled slot
recruit one participant for each slot and a floater for every two slots
add more sessions (and more participants) than needed to the research schedule

One Floater per Participant
The most expensive method, and the one that also gives the most insurance, is the one floater to one user — that is, recruiting two people per slot, even though you’re testing with only one of them.
A research day might look like this:





Sessions
Times
Participants
Floaters
Recruiting cost $80 each
Incentive cost $100 each





1


8:00AM –  9:00AM


User1


Floater1


$160


$200




2


9:30AM –10:30AM


User2


Floater2


$160


$200




3


11:00AM – 12:30PM


User3


Floater3


$160


$200




4


1:00PM – 2:00PM


User4


Floater4


$160


$200




5


2:30PM – 3:30PM


User5


Floater5


$160


$200




Totals


 


 


 


$800


$1,000




If the recruited participant shows up, I pay the floater for that time slot and let her go.  
Floaters recruited for just one session are usually paid the same honorarium as participants. So, if the participant’s incentive were $100 per participant, the single-session floater’s incentive will also be $100.
With this approach, the cost for the recruit and incentives per study is twice as much, provided all users arrive for their sessions. For example, if the recruiting and incentive cost is $180 per user and you had 5 users, then the total cost without floaters would be $900 and the cost with floaters would be $1,800.
One Floater Per Two Users
A little less expensive but also a less safe approach is to recruit one floater for every two sessions. With that pattern, a research day might look like this:





Sessions
Times
Participants
Floaters
Recruiting cost $80 each
$100 each participant and single-session floater, $200 each double-session floater





1


8:00AM – 9:00AM


User1


Floater1


$160


$300




2


9:30AM – 10:30AM


User2


Floater1


$80


$100




3


11:00AM – 12:00PM


User3


Floater2


$160


$300




4


12:30PM – 1:30PM


User4


Floater2


$80


$100




5


2:00PM – 3:00PM


User5


Floater3


$160


$200




Totals


 


 


 


$640


$1000






In this case, each floater would be available for two sessions. For example, if User1 shows up, the floater would stay available to make sure that User2 also shows up. If User2 does show up, the floater is released at the beginning of the second session. This is probably the best-case scenario for the floater since she basically needed to be available for an hour and a half and got paid for being at the full sessions. 
If User1 doesn’t show up, the floater would take that person‘s place. And, the researcher would hope that User2 is going to show up because there is no floater for that session. If User1 shows up but User2 does not, the floater would take User2’s session. This is probably the worst situation for the floater because she had to wait for one session and then also be a participant. 
Floaters who must be available for multiple time slots are usually paid more. I usually pay double the incentive. (Some people pay more than that because floaters sign up to not only be available for a longer time but participate in the study if needed. Others pay floaters more only if they end up being a test participant.) So, if $100 was the participants’ honorarium, a double-session floater would be paid $200. (Note in the example above, since there is an odd number of sessions, one floater will cover just one session and will be paid $100.) If the research was remote, one might pay floaters less because they are not asked to travel or sit in some space for a period of time; rather they can do other things in their home or office while being on call.
How to Recruit Floaters
The point of having a floater is to ensure that you cover all your research sessions. It’s essential that they show up, whether in person or for a remote study. Thus, when recruiting them, they should not feel as though they are superfluous. I’m always honest with floaters. I recruit them using the same screening criteria used for study participants. Once they are deemed the right target for the study, I explain their role:
We have recruited all the timeslots for the study, but we need to make sure we actually have a person in each session. Since sometimes people have last-minute cancellations, we need to recruit more people to be available in case an already scheduled participant can’t make it.
For one-to-one: So, we would ask you to come (or log in for a remote study) as though you are participating. If the scheduled participant arrives as planned, we will pay you your $100 honorarium and release you.
For one-to-two: So, we would ask you to come (or log in for a remote study) as though you are participating. But we would ask you to be available for two sessions, starting at 8 AM and ending after 10:30AM. If the scheduled participants arrive as planned, we will pay you $200 and release you. But if either user doesn’t arrive, we would ask you to participate in the study.
For a remote research session, I ask floaters to log into the session as they would if they were a user. For in-person sessions, I inform the floaters that there will be a waiting area and wireless and drinks available and they will be asked to stay there until released. If there is no such place, I try to find a café nearby where they can go and wait as needed.
How to Dismiss Floaters
If the situation was explained to floaters when they were recruited, there should be no problem dismissing them. Still, make sure you don’t hurt their feelings or make them feel like they did something wrong and that’s why they are being dismissed. I usually say something like this:
As you know, you were recruited to ensure we had enough participants per session. The original person we scheduled has arrived, so we are all set. Thank you so much for agreeing to be part of our study but we won’t need you this time. Here is (or I will send you) your honorarium of $100 as planned. And I hope we can keep you in mind to participate in future studies. Thank you very much.
Using Floaters to Optimize Participants
Another advantage of recruiting two users for each study time slot is that, if both show up as scheduled, you could choose the one most suited for your study. In that case, you would not designate one person as the “main” participant in advance and the other as the floater.
Picking the most suited participant can be useful in studies:

Targeting a particularly difficult recruiting profile or one that is challenging to prescreen — for example, if you are testing a high-end investment product and want to see the investors’ skills before the study
Requiring a certain demographic distribution — for example, an even age distribution

If all your early test slots are filled by younger users, you would then prioritize older users for the last slots.
(However, avoid choosing participants based on random criteria — such as which participant is more talkative or seems more pleasant or likeable— because you can easily bias your study results doing that.)
More Research Slots and Cancel as Needed
A little different from scheduling floaters is to schedule extra study sessions. If you’re not on a tight deadline and you (or your team) can pivot and do other work if a user doesn’t show up, this method can be an effective way to ensure you have enough participants, and is usually less expensive than the floater models.
A schedule using this model might look like this:





Day1 Sessions
Times
Participants
Recruiting cost $80 each
Incentive cost $100 each





1


8:00AM – 9:00AM


User1


$80


$100




2


9:30AM – 10:30AM


User2


$80


$100




3


11:00AM – 12:00PM


User3


$80


$100




4


12:30PM – 1:30PM


User4


$80


$100




5


2:00PM – 3:00PM


User5


$80


$100




 





Day2 Sessions
Times
Participants
Recruiting cost $80 each
Incentive cost $100 each





6


8:00AM – 9:00AM


User6


$80


$100




7


9:30AM – 10:30AM


User7


$80


$100




Totals for both days


 


 


$560


$700




For unmoderated (no facilitator) remote research sessions, I usually recruit one extra person for every 3 sessions. For example, if I want 5 sessions, I recruit and run the study with 7 people. Once you have enough data, you can cancel the sessions you don’t need (but, of course, still pay those users in earnest). If enough users don’t show up even with the added sessions, you can schedule more sessions.
Often, though, it makes sense to run the extra sessions even if all the needed participants did show up, for two reasons:

Unusable data in some of the sessions. After I watch the recordings, I sometimes find some issue with one of the sessions and can’t use all or most of the data. Overrecruiting thus covers not only for no-show participants, but also for sessions with poor data and saves you the cost of relaunching the study and waiting for feedback.
Additional insights. Sometimes I need to follow up on something discovered in earlier sessions — if that’s the case, I can take advantage of those later session to change tasks or interview questions and investigate the new questions.

Wouldn’t just Paying a Higher Incentive Eliminate the Need for Overrecruiting?
Research participants driven by the incentive can be more likely to show up if that incentive is larger. So, you could experiment with paying a higher incentive and not recruiting floaters. You would save the recruiting costs of an additional user and could maybe pay a higher incentive to the main participant. But this strategy works only if the factor preventing participation is related to the incentive. It doesn’t account for emergencies or last-minute schedule changes, which do happen, especially when testing users in certain professions, such as system administrators or physicians. (Also, consider whether a very high incentive could bias your study.)
Isn’t Intentional Overrecruiting Wasteful?
What I like least about intentional overrecruiting is that time and money are sacrificed by the team, users, recruiters, and researchers.
Deciding to use these overrecruiting methods is a lot like deciding whether to buy optional insurance. For example, is spending $100 per month on long-term healthcare insurance worth it so that, if you need care in your old age, you have it? Likewise, what is the insurance of having a user at each allotted time worth to your team?
The table below compares the time, recruiting, and incentive costs for a 5-user study using the three overrecruiting models.





 
1 floater to 1 user per session
1 floater per 2 users
More research slots
None





Number of Research Sessions

5   
5   
5 (+ optional 2)
5   



Days Until Research is Complete


1


1


1.5


1




Number of Participants to Recruit and Pay


5


5


7


5




Number of Floaters to Recruit


5


3


0


0




Recruiting Cost @ $80 each


$800


$640


$560


$400




Incentive Cost @ $100 each participant and single-session floater, $200 each double-session floater


$1000


$1000


$700


$500




Total Recruiting and Incentive Cost


$1800


$1640


$1260


$900




Assuming the researcher facilitates five sessions:

The recruiting and incentive costs are the most ($1800) for the 1-floater-to1-user overrecruit model, and least ($1260) for the more-research-slots model.
The risk of not having enough users in the study is the lowest with the 1-floater-to-1-user model, and higher for both the 1- floater-per-2-users and more-research-slots models.
The time before research is complete is the least (one day) with the 1-floater-to-1-user and one-floater-per-2-users models and most (one and-a-half days) with the more-research-slots model, which requires additional sessions and thus time.


In the example above consider the extra cost was $900.

Is the assurance that you will get the feedback you need from target users within the specified time worth $900 to your team?
Do you have that in your budget?
What’s the opportunity cost of that $900 — that is, what else do you need to do and spend money on where the $900 could generate a higher return?

These are the types of questions you should ask yourself when you are thinking about overrecruiting."
27,2020-09-20,"Task analysis refers to the broad practice of learning about how users work (i.e., the tasks they perform) to achieve their goals. Task analysis emerged out of instructional design (the design of training) and human factors and ergonomics (understanding how people use systems in order to improve safety, comfort, and productivity). Task analysis is crucial for user experience, because a design that solves the wrong problem (i.e., doesn’t support users’ tasks) will fail, no matter how good its UI.
In the realm of task analysis, a task refers to any activity that is usually observable and has a start and an end point. For example, if the goal is to set up a retirement fund, then the user might have to search for good deals, speak to a financial advisor, and fill in an application form — all of which are tasks. It’s important not to confuse goals with tasks. For instance, a user’s goal isn’t to fill in a form. Rather, a user might complete a form to register for a service they want to use (which would be the goal).
Task analysis is slightly different from job analysis (what an employee does in her role across a certain period of time — such as a week, month, or year) or workflow analysis (how work gets done across multiple people). In task analysis, the focus is on one user, her goal, and how she carries out tasks in order to achieve it. Thus, even though the name “task analysis” may suggest that the analysis is of just one task, task analysis may address multiple tasks, all in service of the same goal.

Task analysis focuses on how a user accomplishes her goal by completing tasks. It is different from workflow analysis — where several users are studied in order to understand the tasks they perform to achieve a shared organizational goal — and from job analysis, where one employee is studied over a period of time to understand the requirements for his role.

Studying users, their goals, and their tasks, is an important part of the design process. When designers perform task analysis, they are well equipped to create products and services that work how users expect and that help users achieve their goals easily and efficiently. Task analysis, as a method, provides a systematic way to approach this learning process. It can be flexibly applied to both existing designs (e.g., the use of an enterprise system) and system-agnostic processes (e.g., shopping for groceries).
The task-analysis process can be viewed as two discrete stages:
Stage 1: Gather information on goals and tasks by observing and speaking with users and/or subject-matter experts.
Stage 2: Analyze the tasks performed to achieve goals to understand the overall number of tasks and subtasks, their sequence, their hierarchy, and their complexity. The analyst typically produces diagrams to document this analysis.
Stage 1: Gather Information
In stage 1, typically, a combination of methods is used to learn about user goals and tasks. They include:

Contextual inquiry: The task analyst visits the user onsite and conducts a semistructured interview to understand the user’s role, typical activities, and the various tools and processes used and followed. Then the analyst watches the user work. After a period of observation, the user is asked questions about what the analyst observed.
Interviews using the critical incident technique: Users are asked to recall critical incidents, and the interviewer asks many followup questions to gather specific details about what happened. The stories provide detail on the tasks performed, the user’s goals, and where problems lie.
Record keeping: Users are asked to keep records or diary entries of the tasks they perform over a certain period of time. Additionally, tracking software can be used for monitoring user activity.
Activity sampling: Users are watched or recorded for a certain period of time in order to document which tasks are being performed, as well as their duration and frequency.
Simulations: The task analyst walks through the steps that a user might take using a given system.

When carrying out research, do not rely solely on self-reported behavior (i.e., through interviews or surveys) or simulations (remember: you are not the user!), but also observe the user at work in her own context. Otherwise, you could miss out on important nuances or details.
Stage 2: Analyze Tasks
In stage 2, the task analyst will structure the observations by certain attributes like order, hierarchy, frequency, or even cognitive demands, to analyze the complexity of the process users follow in order to achieve their goals. The result of this analysis is often a graphical representation called a task-analysis diagram.
There are many different types of diagrams that could be produced, such as standard flowcharts or operational-sequence diagrams. However, the most commonly known and used in task analysis is the hierarchical task-analysis diagram (HTA). The figure below shows an example of an HTA for the goal of creating a digital copy of a physical letter using a new home scanner.

A hierarchical task-analysis (HTA) diagram of a user trying to create a digital copy of a physical letter using a new home scanner involves several tasks (or operations): download scanner software onto a MacBook computer, launch program, scan document, then save it.

An HTA diagram starts with a goal and scenario (in the same way that a customer-journey map does) and highlights the major tasks to be completed in order to achieve it. In human factors, these tasks are referred to as ‘operations’. Each of the tasks in the top layer can be broken down into subtasks. The number of levels of subtasks depends on the complexity of the process and how granular the analyst wants the analysis to be.
Not all users accomplish goals in the same way. For example, a novice user might perform more tasks than an expert user — the latter might skip certain steps. The HTA enables these differences to be captured through ‘plans’. A plan specifies, at each level, what the order of the steps is, and which steps might be undertaken when or by whom. For example, a user who can’t remember his password has to undertake steps 1.5 (Click Reset password) and 1.6 (Enter a new app-store password) in order to accomplish the goal of downloading software for the scanner.
While a task-analysis diagram is useful to illustrate the overall steps in a process and is an excellent communication tool — especially for complex systems — it can also be used as a starting point for further analyses. For example, the following attributes could be considered for the tasks in an HTA.

The overall number of tasks: Are there too many? Perhaps there are opportunities to create a design that could streamline the process and remove some steps.
The frequency of tasks: How often are certain tasks performed? Are some tasks filled with repetition?
The cognitive complexity of the tasks: What mental processes (i.e., thoughts, judgments, and decisions) are needed to complete a given task? (A whole branch of task analysis known as cognitive task analysis is concerned with these questions and with making visible the mental schemas and processes). If there are a lot of mental operations involved, the difficulty of the overall task increases, and the analyst should consider the likelihood of user error.
The physical requirements of the task: What does the user need to physically do? Could this physical requirement affect user performance and comfort? And how could these physical requirements affect users with disabilities?
The time taken to perform each task: Activity sampling or theoretical modeling (such as GOMS) can be used to estimate how long tasks would take users to complete.

At the end of the task analysis, the analyst has a good understanding of all the different tasks users may perform to achieve their goals and the nature of those tasks. Armed with this knowledge, the analyst can design (or redesign) an efficient, intuitive, and easy-to-use product or service.
Summary
Task analysis is a systematic method of studying the tasks users perform in order to reach their goals. The method begins with research to collect tasks and goals, followed by a systematic review of the tasks observed. A task-analysis diagram or an HTA is often the product of task analysis; the HTA can be used to communicate to others the process users follow, as well as a starting point for further assessment.
References
Hackos, J. A. T., & Redish, J. (1998). User and task analysis for interface design. New York: Wiley.
Kirwan, B. (Ed.), Ainsworth, L. (Ed.). (1992). A guide to task analysis. London: CRC Press, https://doi.org/10.1201/b16826
Stanton, N. A. (January 01, 2006). Hierarchical task analysis: Developments, applications, and extensions. Applied Ergonomics, 37, 1, 55-79. https://doi.org/10.1016/j.apergo.2005.06.003."
28,2020-09-13,"UX benchmarking is the process of evaluating a product or service’s user experience by using metrics to gauge its relative performance against a meaningful standard. These metrics are usually collected using quantitative usability testing, analytics, or surveys.
Consider conducting a benchmarking study if you want to:

Track the overall progress of a product or service
Compare your UX against an earlier version, a competitor, an industry benchmark, or a stakeholder-determined goal
Demonstrate the value of UX efforts and your work

In a related article, we discuss when to benchmark. At a high level, benchmarking is a method to evaluate the overall performance of a product (and as such, is a type of summative evaluation). Thus, benchmarking studies tend to occur at the end of one design cycle, before the next cycle begins.
Benchmarking is often a program rather than a one-time activity: many organizations collect metrics repeatedly, as they go through successive releases of their designs.  Benchmarking keeps teams accountable and documents progress in a measurable way.
Process Overview
In this article, we present a high-level seven-step process for creating a benchmarking program. When first establishing this program, there will be some extra work to do in order to figure out what to measure and how. However, once you’ve determined the study structure, the process becomes fairly repetitive and a lot less work is involved.

To conduct an end-to-end UX benchmarking study, first decide what you’re going to measure and which research method you’ll use to collect those metrics. Next, collect your first measurement, redesign the product, and collect an additional measurement. Then, compare and interpret your findings, and possibly calculate ROI. Once you’ve completed the initial end-to-end process, future iterations of your study (assuming that the context remains the same) can begin at step 4 (redesign the product).

Step 1: Choose What to Measure
Focus on the key metrics that best reflect the quality of the user experience you’re interested in evaluating. Look for metrics that translate to UX and organizational goals.
That said, before you determine which metrics to collect, you must define the context of your study. In other words, consider:

What product will you focus on? (website, application, etc.)
Which user group will you target?
What tasks or features do you want to measure?

Tasks
Figure out the top tasks that users complete in your product. If your organization doesn’t have existing top tasks, you can start by documenting (most) tasks in the product. Then, prioritize the list of tasks and select approximately 5–10 that are most important to your users.

The table below outlines multiple possible product and task scenarios. It includes just one task per product, but in real life you will probably focus on more than one task.




Product


Possible task 




Smart-speaker app


Setting up a new smart speaker




Ecommerce website


Making a purchase with 1-click purchasing




Mobile-banking website


Updating contact information




B2B-agency website


Submitting a lead form




Mobile puzzle game


Solving one puzzle






The following list outlines multiple possible product and task scenarios. It includes just one task per product, but in real life you will probably focus on more than one task.
Smart-speaker app

Task: setting up a new smart speaker

E-commerce website

Task: making a purchase with 1-click purchasing

Mobile-banking website

Task: updating contact information

B2B-agency website

Task: submitting a lead form

Mobile-puzzle game

Task: solving one puzzle


Metrics

Now that you’ve focused in on a set of tasks, how can you measure them? Google’s HEART framework provides a concise overview of different types of metrics you may want to collect and track. The following table is an adaptation of the HEART framework:




 


Description


Example Metrics




Happiness


Measures of user attitudes or perceptions


Satisfaction rating
Ease-of-use rating
Net promoter score




Engagement


Level of user involvement


Average time on task
Feature usage
Conversion rate




Adoption


Initial uptake of a product, service, or feature


New accounts/visitors
Sales
Conversion rate




Retention


How existing users return, and remain active in the product


Returning users
Churn
Renewal rate




Task effectiveness and efficiency


Efficiency, effectiveness, and errors


Error count
Success rate
Time on task






Now that you’ve focused in on a set of tasks, how can you measure them? Google’s HEART framework provides a concise overview of different types of metrics you may want to collect and track. The following is an adaptation of the HEART framework:
Happiness: measure of user attitudes or perceptions

Metric examples: satisfaction rating, ease-of-use rating, Net Promoter Score

Engagement: level of user involvement

Metric examples: average time on task, feature usage, conversion rate

Adoption: initial uptake of a product, service, or feature

Metric examples: new accounts/visitors, sales, conversion rate

Retention: how existing users return and remain active in the product

Metric examples: returning users, churn, renewal rate

Task effectiveness and efficiency: efficiency, effectiveness, and errors

Metric examples: error count, success rate, time on task


Note that as an engagement metric, time on task should be high (e.g., a long time spent reading articles on a newspaper site), whereas as an efficiency metric, time on task should be low (e.g., fast to check out on an ecommerce site). In other words, the same change (say, longer time) could be either good or bad, depending on what type of use is measured.
Pick metrics that will matter for the long haul, since ideally, you’ll be collecting these metrics repeatedly over many years. Aim for 2–4 metrics that focus on different aspects of your UX (e.g., happiness and engagement).
Here are some possible metrics we may track for the tasks in our previous example.





Product


Task or Feature


Metrics




Smart-speaker app


Setting up a new smart speaker


Time on task
Success rate
Single Ease Question (SEQ)




Ecommerce website


Making a purchase with 1-click purchasing


Weekly sales with 1-click
1-click feature adoption
 




Mobile-banking website


Updating contact information


Completion rate
Errors on page
# of support calls on the same task




B2B-agency website


Submitting a lead form


Form submissions
Abandonment rate




Mobile-puzzle game


Solving one puzzle


Success Rate
Returning users






Smart-speaker app

Task: setting up a new smart speaker
Metrics: time on task, success rate, Single Ease Question (SEQ)

E-commerce website

Task: making a purchase with 1-click purchasing
Metrics: weekly sales with 1-click, 1-click feature adoption

Mobile-banking website

Task: updating contact information
Metrics: completion rate, errors on page, # of support calls on the same task

B2B-agency website

Task: submitting a lead form
Metrics: form submissions, abandonment rate

Mobile-puzzle game

Task: solving one puzzle
Metrics: success rate, returning users


Benchmarking a user experience isn’t just about tracking metrics, it’s also about demonstrating value. That’s much easier to accomplish when you select metrics that align to your organizations’ key performance indicators (KPIs). For instance, in a bank where customer-support cost is a KPI you may be able to show that a redesigned contact form contributed to decreased support costs by tracking the number of support calls before and after the redesign.
Step 2: Decide How to Measure
When it comes to determining the methodology to collect your metrics, you must consider the time commitment that the research method requires, cost of such method, skill of the researchers involved, and the research tools available to you. Don’t do something if you don’t have the right skills, since bad numbers are worse than no numbers. Also, don’t specify a measuring plan that will be too expensive to be sustained in the long term (because the entire idea of benchmarking is to repeat the measurement again and again).
Before you start planning a new study, see what existing data your organization has around the experience you want to measure. It can be extremely valuable to gain a holistic understanding of the experience and connect your UX metrics to larger organization goals. When requesting data from other sources, be sure to explain why it’s needed and how it will be used.
There are 3 research methods that work well for UX benchmarking: quantitative usability testing, analytics, and survey data.
Quantitative usability testing. Participants perform top tasks in a system and researchers collect metrics (such as time on task, success rate, and satisfaction) that measure the users’ performance on those tasks.

Analytics. System-usage data (such as abandonment rates and feature adoption) is automatically gathered. 
Surveys. Users answer questions to report their behavior, background, or opinions. Task ease, satisfaction ratings, net promoter score are all metrics collected in surveys.  

Ideally, you’ll pair a survey (to get self-reported metrics) with a behavioral, observational method (quantitative usability testing or analytics) to get a holistic view of the user experience.
In the following, we’ve charted out methodologies given our previous scenarios.





Product


Task or Feature


Metrics


Methodology




Smart-speaker app


Setting up a new smart speaker


Time on task
Success rate
Single Ease Question (SEQ)
 


Quantitative usability testing with survey




Ecommerce website


Making a purchase with 1-click purchasing


Sales
Adoption
Net promoter score
 


Analytics
Survey




Mobile-banking website


Updating contact information


Completion rate
Errors on page
# of support calls on the same task
 


Analytics
Internal customer-support data




B2B-agency website


Submitting a lead form


Form submissions
Abandonment rate
 


Analytics




Mobile-puzzle game


Solving one puzzle


Average time spent
Retention
 


Analytics






Smart-speaker app

Task: setting up a new smart speaker
Metrics: time on task, success rate, Single Ease Question (SEQ)
Methodology: quantitative usability test with survey

E-commerce website

Task: making a purchase with 1-click purchasing
Metrics: weekly sales with 1-click, 1-click feature adoption
Methodology: analytics, survey

Mobile-banking website

Task: updating contact information
Metrics: completion rate, errors on page, # of support calls on the same task
Methodology: analytics, internal customer support data

B2B-agency website

Task: submitting a lead form
Metrics: form submissions, abandonment rate
Methodology: analytics

Mobile-puzzle game

Task: solving one puzzle
Metrics: success rate, returning users
Methodology: analytics


Step 3: Collect First Measurement: Establish Baseline
Now that you’ve determined which metrics to collect and how to collect them, it’s time to gather your baseline metrics. (But not so fast — do a pilot study first to collect an initial sample of data and run a preliminary analysis to make sure your method is sound and that the data can answer your questions. Most likely, the pilot will make you revise your methodology, meaning that the initial data set should be discarded. But this is worth the investment in order to get sound results from your subsequent, bigger data-collection efforts.)
As you gather your first set of measurements, consider external factors that may affect your data and, when possible, plan around it. For instance, if you’re an ecommerce website using analytics to collect sales metrics for benchmarking, be wary of factors like extensive marketing campaigns or large-scale economic influences that can disrupt your metrics and make it difficult to correlate the design change to outcomes.
One measurement of your site is not likely to be meaningful by itself. Even if you’ve just started your benchmarking program and you don’t have prior data to compare to, you can still make comparisons with competitors, an industry benchmark, or a stakeholder-determined goal. Below we provide examples of each.

Your competitor. For example, if your product is a smart-speaker app, you could benchmark the experience of setting up your product versus setting up a competing product. (To do so, you will likely have to collect data on your product and on competitors’ products, so the prior steps will have to take that into account. That said, you could not use analytics as your methodology, since you won’t have access to your competitor’s analytics.)
Industry benchmark. You may have access to external statistics pertaining to your field. For example, if you’re a hotel website, you may want to compare your NPS to the average net promoter score (NPS) for this industry, which is 13%.
Stakeholder-determined goal. For instance, your stakeholders say they want the average time to submit a lead form to be under 3 minutes, so you may want to compare your current performance to that threshold.

As you’re considering how to interpret the outcome of these comparisons, take into account the recommendations described in step 6.
Step 4: Redesign the Product
The redesign process is outside the scope of this article, though it’s an incredibly important part: without a redesign, you won’t be able to compare multiple versions of your product.
As you redesign your product, keep the 10 usability heuristics for interaction design in mind.
Step 5: Collect Additional Measurement
After your redesign is launched, measure your design again. There is no hard and fast rule on how long to wait after a design is launched to measure again. If you’re tracking  analytics, there’s added benefit of continuous measurement. However, for task-based data collection, like quantitative usability testing and surveys, you’ll need to determine the right time to collect the data. Users often hate change, so give them a bit of time to adapt to the redesign before measuring it. The amount of time varies depending on how frequently users accesses your product. For products accessed daily, perhaps 2–3 weeks is enough time.  For a product that users access once or twice a week, 4–5 weeks before you measure is better.
As you consider the right time to measure your new design, once again document any potential external influencers that may impact your findings.
Step 6: Interpret Findings
Now that you’ve gathered at least two data points, it’s time to interpret your findings. You shouldn’t take your metrics at face value since the sample used for your study is likely much smaller than the entire population of your users. For that reason, you will need to use statistical methods to see whether any visible differences in your data are real or due to random noise. In our course, How to Interpret UX Numbers: Statistics for UX, we discuss this topic in great detail.
In general, interpreting your metrics is highly contextual to your product and the metrics you’ve chosen to collect. For example, time on task for an expense-reporting app is different than time on task for a mobile game. In the following, we outline one of the previously discussed scenarios and interpretation of the findings.
Scenario: Setting up a Smart-speaker
Assume we used quantitative usability testing paired with a survey to collect time on task, success rate, and SEQ. The following table outlines hypothetical metrics for our initial design and redesign.




 


Initial Design


Redesign




Average time on task (minutes)


6.28


6.32




Average success rate


70%


95%




Average SEQ
(1 very difficult – 7 very easy)


5.4


6.2




In summary, time on task was nearly the same, success rate increased, and average SEQ increased. Let’s assume we found statistically significant differences between these pairs of metrics. Therefore, in the redesign, users were more successful and satisfied with the setup process. In other words, the redesign was a success!
Step 7: Calculate ROI (Optional)
Benchmarking allows you to track your success and demonstrate the value of your work. One way to demonstrate the value of UX is to connect the UX metrics to the organization's goals and calculate return on investment (ROI). These calculations connect a UX metric to a key performance indicator (KPI) such as profit, cost, employee productivity, or customer satisfaction.
Calculating ROI is extremely beneficial, though not widely practiced by UX professionals (perhaps because relating your UX metric to a KPI is often convincing enough). In any case, if you struggle to prove UX impact, calculating ROI can be persuasive.
Presenting Benchmarking Findings
As you wrap up your analysis and share your findings with stakeholders, aim to tell a story with the data. Just because some members of your leadership love numbers doesn’t mean you can’t incorporate some qualitative findings or quotes from previous studies that align with your findings — this can be a great way to build empathy for your users among that data-driven audience.
Additionally, when presenting to stakeholders, be sure to have documented all of your assumptions and possible confounding variables of your study. Though you may not have to directly comment on them, having them in an appendix of your presentation shows that you have a holistic understanding of the product environment and allows you to easily reference it, should any questions arise about the validity of your measurements.
Conclusion
Benchmarking is a fantastic tool to align and correlate UX efforts to overall organizational goals and outcomes. To conduct a benchmarking study, begin by focusing on important tasks or features in your product and determine how you can measure them. Next, select a research method that allows you to collect those metrics, given your time, budget, and skills. Collect your first measurement, redesign your product, and collect those metrics again under the same methodology. Finally, interpret your findings by comparing your collected data points and using your product and organization knowledge to make sense of it all.
Then, do it all again next year! (Or after the next release.) Hopefully, your numbers will be better, and if not, you’ll know where to focus efforts during the subsequent redesign.
References
K. Rodden, H. Hutchinson, X. Fu. “Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications” (2010). Source: https://research.google/pubs/pub36299/"
29,2020-07-26,"There are two flavors of remote usability testing: moderated (using a facilitator and screensharing app) and unmoderated (tasks are administered by a testing platform, without a facilitator.
One reason why unmoderated remote usability testing tends to be more popular than moderated testing is that it’s seen as cheaper and faster. While unmoderated might save some money and time, it may not be as much as you think — and it could come at the expense of findings. We compare typical costs for each type of study.
Factors that Impact Study Costs
Two of the biggest factors that influence the cost of your study is who your participants are and how they will be recruited. Monetary costs will increase if your user population is very busy, rare, or affluent — in those cases, you'll probably need to offer higher incentives. Similarly, if you hire a recruitment agency to find and schedule your users, you might pay a much higher per-recruit fee than you would if you used a participant panel.
These estimates for time and money assume that a research practice is already established in your organization. Expect a study to take longer than our estimates if:

Stakeholders or clients must be first convinced that the research is needed.
Clear research goals have not yet been defined.
The researchers are unfamiliar with the tools used during testing and must spend time learning them.
Participants are difficult to find and schedule.

Our time estimates also do not include the time required to design and build the product or prototype to be tested.
Additionally, our high estimate for an unmoderated platform subscription is $100 per month. Organizations can spend much more than that depending on the tool. Some large platforms with lots of features and tools (like UserZoom) can cost thousands of dollars per month.
We include costs for a video-conferencing application, but you may not need to include it in your own cost calculations. Ideally, you might be running many studies over time, so that cost is amortized across projects. Also, for moderated studies, your organization is very likely to already have a video-conferencing app for other purposes, so that isn’t a new cost specific to the study. You may choose not to include that aspect when calculating your costs.
Study Estimates
These are rough estimates for a typical 5-participant qualitative usability study conducted in the United States. You’ll likely find that costs vary in different locations, particularly the required incentives.
Usability testing is an immensely flexible and versatile tool, so different variations (such a quantitative usability testing, which requires 20–40 participants), will have different costs.
Table 1: Typical Cost Estimates for a Remote Moderated Study in the US




 


Low Estimate


High Estimate




Video-Conferencing App


$15 / month


$30 / month




Recruiting Fees


$0 / participant


$80 / participant




Incentives


$80 / participant


$250 / participant




Planning & Setup


16 hours


24 hours




Conducting Sessions


90 min / participant
(60 min sessions)




Analysis


8 hours


16 hours




Costs for a 5-participant study


$415 + 32 hours


$1,680 + 48 hours




Typical costs for a remote moderated study in the United States. On the low end, a moderated study for 5 participants might cost around $415 plus 32 researcher hours. On the high end, with high recruitment fees and big incentives, it might cost around $1,680 and 48 hours.
 
Table 2: Typical Cost Estimates for a Remote Unmoderated Study in the US




 


Low Estimate


High Estimate




Platform Subscription


$0 / month


$100 / month




Recruiting Fees


$0 / participant


$80 / participant




Incentives


$50 / participant


$150 / participant




Planning & Setup


4 hours


12 hours




Conducting Sessions


30 min / participant
(20 min sessions)




Analysis


4 hours


12 hours




Costs for a 5-participant study


$250 + 11 hours


$1,250 + 27 hours




Typical costs for a remote unmoderated study in the United States. On the low end, an unmoderated study for 5 participants might cost around $250 plus 11 researcher hours. On the high end, with high recruitment fees and big incentives, it might cost around $1,250 and 27 hours.
Unmoderated Can Have Hidden Costs
Based on these rough estimates, you might expect an unmoderated study to cost anywhere from 20% to 40% less than a moderated equivalent. Your researchers might also save around 20 hours of time.
But remember, there are tradeoffs associated with using unmoderated instead of moderated testing. 
If you’re using an unmoderated-testing platform’s panel, you might be at higher risk of cheaters or “professional” participants — people who just want to get paid and move on or who participate in studies on a regular basis. You might have to remove those participants and replace them. Some platforms have good policies on replacing lackluster participants at no cost to the researcher, but others don’t. To be safe, we typically recommend overrecruiting a bit. So instead of testing 5 participants in an unmoderated study, you might want to test 6.
Researchers often prefer unmoderated testing because they feel it will save them time — no need to run the sessions. But remember, particularly in qualitative testing, you should still be watching those recordings to analyze them. You might find that whatever time you’ve saved in conducting the session  will be shifted into the analysis phase of the project.
Notice a big difference between the two estimates above — the moderated sessions are planned as 60-minute sessions, while the unmoderated sessions are 20 minutes. That’s because most testing platforms have short limits on how long sessions can run. For this reason, long, complex tasks may not be well suited to unmoderated testing.  And if you are planning to test many tasks on the same site, you may need to recruit more participants (with each participant doing only a subset of the tasks).
And finally, remember that the core difference between moderated and unmoderated testing is the presence or absence of a facilitator. A facilitated session often results in more focused sessions and more insights than an unfacilitated one, and that’s something that you might miss in unmoderated testing.
Consider All Costs
Unmoderated testing can be a great option for teams with limited resources. While moderated testing can be slightly more expensive, that cost might be worth the insights to some teams. 
Just make sure you consider all the specifics of your situation (tools, schedule, users, and recruiting) and weigh those against your research goals when you make your choice.
 
For more help planning and conducting remote studies, check out our full-day seminar, Remote Usability Testing."
30,2020-07-19,"You’ve run your research, collected your findings, now how do you present them to others? Researchers love data — facts, figures, quotes, and charts are what we thrive on. However, our stakeholders and team members are not always as drawn to this data as we are, so it’s our job to make hard facts compelling for others in our organization. Storytelling can accomplish that purpose.
Turning an overwhelming amount of data into a cohesive and engaging story can be difficult. This article discusses a 5-step process for turning data into compelling stories.
Step 1: Do the Research
We’re not in the business of making up fairytales, and, therefore, our stories should be based on real data collected from research. The research you conduct should originate in solid questions. These questions are a great starting point for your stories. For example, consider the following research questions for a hypothetical app selling cat supplies:
How do people research, purchase, and use cat furniture?
What are the pain points and areas of frustration that our customers currently experience when researching, purchasing, and using cat furniture?
What additional products are our customers using that we do not currently offer?
Once your research questions are established, you can start collecting data. Revisit your research plan, where you originally laid out the scope of your research and defined your research questions. Review any notes you took during initial meetings with stakeholders and remind yourself of the important business questions that were brought up.
The story that you’ll end up telling is not about the research itself, but about the answers to your research questions. While the methodology behind the research is interesting to us as researchers, the insights you collect are the most compelling to stakeholders and team members.
Using the cat-supply app from above, we could frame our research as a story in the following way:
We know based on analytics and user interviews that customers are looking for unique and functional ways to provide cat furniture in their homes. What we didn’t know was how they approach the buying process and ultimately end up choosing the best product to fit their needs. We decided to conduct research into this process by interviewing and observing our customers and deciphering analytics data from our app’s product pages and shopping-cart flows.
This is only the beginning of our story, but this setup will remind our stakeholders why we collected this data and what we were hoping to accomplish by gathering it. And because the data will often take us in multiple directions, revisiting the original research questions will keep us on track.
Step 2: Create a List of Research Findings
Now that you’ve conducted research, lay out all of the findings in a list. Your data will come in many forms — quotes, analytics, survey responses, and so on. It’s helpful to think of this data as bullet points. Some of your data may already be in bullet-point form, but other data may be in the form of charts and visuals. Keep these various visuals to use as evidence in your final report but translate these findings into words in this step.

Summarize the insight from visual data into a bullet point in your list. This chart will come in handy as evidence when you write your full report, but the bullet point will help translate the data into something your audience will better understand.

Step 3: Cluster Your Findings into Themes
A common mistake made by researchers is organizing their deliverable — whether it’s a report or slide deck of findings — in a chronological order that matches what happened in the research study (e.g., findings from task 1, followed by findings from task 2, followed by the results of a survey or interview). When you’re running through your data chronologically, your artifact becomes an account of what participants did in the study instead of a story of the central questions that your audience cares about. Instead, think of the themes of the story along with the research questions you set out to answer. Group the different findings into bigger themes that are coherent with each other or make the same point.

Cluster your data — a collection of user quotes, metrics, bullet points, and so on — into themes, keeping indirect findings off to the side to revisit in the next step. You’ll use these themes to tell your story.

If you come across findings that aren’t related to your original research questions, don’t throw them out. Instead, collect these in a separate category off to the side so you can revisit them later. They may not be directly related to what you set out to discover, but they may still be insightful. Using our cat-app example, you may have originally set out to learn about the furniture buying process, but during your study you find that customers spend a lot of time looking at the user-submitted cat photos. This data can inform future research questions or even take your story in a new direction.
Step 4: Think About What You Left Out
When you clustered your findings into themes in step 3, you may have collected insights that weren’t directly relevant to your research questions in a separate category. This is where you’ll revisit these findings and think about where they belong.
Findings That Invalidate Themes or Are Contradictory
When you’re crafting stories for your audience, it’s important that you don’t purposely omit facts or distort evidence. To maintain your credibility, stick to the data, even if it doesn’t fully support your recommendations or hypotheses.
If the findings contradict your recommendation, it’s likely that you’re going to spend time creating the wrong solution anyway. Moving forward, it’s best to acknowledge the discrepancy and formulate a new research question to learn what’s going on.
Research findings aren’t always straightforward and sometimes you’ll come across contradictory findings. Figure out if there are any possible reasons for that and own that there may be methodology issues behind your studies that led to these inconsistencies. Sometimes the answer may simply be that you don’t know yet or that you need to do more work to find the answer.
Unrelated, but Still Insightful Findings
We don’t have control over all of the different data we’re going to get throughout our research. A user may say something that isn’t directly related to our current research questions, but sparks interest in future research. We may observe behavior that we didn’t anticipate, but is still insightful for the overall project.
Keep track of these findings, whether in a research repository or a knowledge board, so that you can revisit them. You can add these details at the end of your story when you discuss your next steps, which will introduce the next research project to your audience and continue the cycle of research in the organization.
Step 5: Translate Themes into Words
This is where you get to formulate and write your story. Start by writing a one-sentence headline that captures each theme you clustered in the last step. This sentence should be user-focused, emphasizing what they think or do.
Here are some examples of good headlines from our recent User Experience Careers report, contrasted with less-descriptive headlines that are typically used:




Descriptive


Less Descriptive




Most UX Professionals Are Satisfied with Their Careers


Career Satisfaction




As You Gain More Years in the Field, Satisfaction Increases


Reasons for Career Satisfaction




UX Practitioners Come from Many Different Backgrounds


Backgrounds of UX Practitioners




The Most Important Set of Skills in UX Are Soft Skills


Important Skills in UX




One of the main benefits of writing descriptive headlines is that your audience — the people consuming your report or slide deck — can still gain insights from scanning a report and only reading the headlines. Good section titles cater to the needs of our audience, one of the six rules of persuasive storytelling, because we can’t assume that our audience will consume every detail (and we should expect that it won’t).
Once you have your headlines, you can write supporting statements about what the data within those headlines indicate.
Let’s look at an example. Let’s assume that one of the themes you identified in your research is the need for style and there are several findings associated with that.




Theme


Findings


Supporting evidence




Style


Several users commented that they need a cat tree that matches their décor and looks pretty.


“I want a cat tree that doesn’t look like a cat tree, does that make sense? It should match the rest of my home’s décor and be stylish.”
“So many cat trees are ugly. I wish I could find something that was modern like my furniture.”




 


78% of participants rated ‘furniture style’ as one of the top 3 reasons for shopping on our site.


Quant survey




 


Products with a high style rating have higher sales.


Style survey & analytics




You could have a headline that says:
Shoppers Need Stylish Cat Furniture 
Inside that headline, you can document the different findings. Start at the high level and add the full quotes and metrics as evidence to back it up.

An example layout of how to structure your story in writing: The headline is followed by a summary, which is then backed up by findings and data points.

The wording of this paragraph puts the audience — our stakeholders, team members, and so on — into the shoes of users and reminds them that they are not the user.
Continue wordsmithing further until you feel the story is complete. If you were to only read the headlines strung together, you should still understand the story that is being told. If your headlines together don’t tell the full story, reword and rearrange them until they appropriately communicate the top insights.
Each headline will have a summary explanation to back it up, and each paragraph could have one or more findings or data points offered as evidence.

Another example layout of how to structure your story in writing: Each headline is followed by a summary explanation, multiple statements that summarize related findings, and actual data.

Continue to reorganize sections and sentences as needed to help communicate the story to your audience. Think about what is most compelling to your audience and try to anticipate what they will take away from the findings you’ve collected. You can even think of your audience members as personas to better anticipate their needs and frustrations when it comes to data.
Conclusion
Storytelling with data is difficult to master, but when done effectively, can clearly communicate important information to the right people. Don’t just put all of the data in front of your audience and expect them to come to your conclusion. Take your audience on a journey through the data by using stories.
Learn more in our full-day course on Storytelling to Present UX Work."
31,2020-06-21,"Personas used in UX work are a quick, empathy-inducing shorthand for our users’ context, motivations, needs, and approaches to using our products. They are meant to help us focus on what matters most to our users and put ourselves in their shoes when making design decisions. Because of this, they must always be rooted in a qualitative understanding of users and reflect the what and why that drives them. They should not be based on (often dubious) correlations between different demographic or analytics variables.
Personas are not intended to be an exhaustive, scientific taxonomy of every possible user type, neatly categorized according to a multitude of psychographic, demographic, and behavioral variables — making design decisions while keeping in mind tens or hundreds of persona types would quickly become unwieldy. The whole point of personas is that they are memorable, actionable, and distinct from one another — they are there to sum up the main needs of our different audience segments so that we can recall and empathize with them easily.
There are 3 different ways that teams can create personas, depending on the research data in which they are rooted:

Proto personas, meant to quickly align the team’s existing assumptions about who their users are, but not based on (new) research
Qualitative personas, based on small-sample qualitative research, such as interviews,  usability tests, or field studies
Statistical personas, where initial qualitative research informs a survey instrument that is used to gather a large sample size, and the personas emerge from statistical analysis

All three of these have different pros and cons and will be applicable for different situations.
Proto Personas:  Meant for Quick Alignment
Proto personas are a lightweight form of ad-hoc personas created with no new research. They catalogue the team’s existing knowledge (or best guesses) of who their users are and what they want. Proto personas can be based on existing user data if your team has any, but in many cases are based solely on the team’s assumptions about who the users are, and what they need.
How Proto Personas Are Made
Typically, proto personas are created in a workshop that involves your team and key stakeholders or clients. The workshop usually takes 2–4 hours; each participant creates 2–5 proto personas of their own (using a simple template) and then shares them with the group. The group then discusses all the personas and combines, remixes, and edits various attributes into a final set of 3–6 proto personas.

Proto personas created in a workshop environment: Each participant creates 2–5 of these simple sketches, and the whole team discusses, remixes, and combines these into a few final proto personas. 

Pros of Proto Personas
Since proto personas don’t require a research project, they are well suited to teams that are working in a Lean UX framework or have low UX maturity and would otherwise not use personas at all. The other major value of proto personas is that they make your team’s implicit assumptions about your users explicit. Normally, every team member has different assumptions about typical users, and the lack of alignment means that each person makes decisions on behalf of a different intended audience. Those disorganized assumptions often chip away at the team’s focus, so cataloging the assumptions provides at least some shared direction, even if the results do not accurately capture the real users. Proto personas can also be a gateway to future research if the team considers them to be hypotheses that can be validated with research (or revised once the incorrect assumptions are brought to light).
Cons of Proto Personas
Obviously, as proto personas are not driven by research, they are often an inaccurate representation of your users and can be an echo chamber for the team’s incorrect assumptions.  Moreover, if the team finds little value in these personas, they can lead to a negative halo effect towards personas in general and towards other UX collaborative activities.
Qualitative Personas: The Best Fit for Most Teams
For most teams, the best approach for creating personas is by running solid exploratory qualitative research (such as interviewing users) with a small-to-medium sample size, and then segmenting users based on shared attitudes, goals, pain points, and expectations.
How Qualitative Personas Are Made
Start by interviewing 5–30 users (as a rolling sample of 5 users per group, until you uncover only a few new insights with each new interview). These interviews can be either fully separate sessions or tagged onto a usability test or field study. The research will uncover the main things that your users care about: their pain points, their expectations for the features and behavior of your product, the words they use for describing the tasks done with your product, how they approach key workflows, and what they’re trying to achieve. Take your transcripts and categorize the data into major themes (known as coding the data).
The analysis portion then involves looking for patterns: you are looking for interviewees that had major overlap with other interviewees in most of these key themes (but not necessarily all of them). Rather than simply looking for a perfect match between individuals you interviewed, look for broad patterns. It’s worth explaining to a colleague what the connection is as you do this work. For example, you might notice (for an ecommerce site) that multiple interviewees described inspecting many product pages before making a decision and that most of those same interviewees also said they used the shopping cart as a holding area for comparing candidate products. While those interviewees might have had many differences in how they answered other questions (such as using different devices, what they were shopping for, what their budget was, and so on), their similarities might be more important to your team than their differences, and so you might create a persona that focuses on their similarities (e.g., Researcher shoppers).
The qualitative analysis process is nuanced and detailed, and a full how-to is beyond the scope of this article. We cover this process in depth in our Personas workshop.
Pros of Qualitative Personas
Qualitatively derived personas fit the sweet spot for most teams when considering the effort involved in creating them relative to their value — they require a small time commitment and the UX team can gather necessary data in parallel with their other work. Because qualitative personas are based on user data, they are accurate and provide key insights about user motivations, expectations, and needs that are impossible to get from either analytics data, demographic info, or assumptions alone.
Cons of Qualitative Personas
The big downsides to qualitative personas are:

Since they aren’t based on large samples, there is no way to determine the proportion of your user population that each persona represents (e.g., you cannot say that Sandra the Conscientious Consumer is 60% of your user base).
There is a possibility that, due to the small-sample size, you have inadvertently omitted some users with unique characteristics or have overrepresented outliers that have uncommon viewpoints.
Especially in organizations with low UX maturity (where there is not a good understanding of qualitative-data methodologies), you may need to constantly push back on claims that qualitative personas are “not scientific.”

Statistical Personas: Mix of Qualitative and Quantitative Research
The most labor-intensive version of persona creation involves collecting data via a survey sent to a large sample of your user base and then using statistical analysis to find clusters of similar responses. While I describe these as statistical personas, really, they are mixed-method personas, because they are based on both qualitative and quantitative research.
This type of personas requires some exploratory qualitative research beforehand to identify what questions to include in the survey. There are no universally relevant persona-survey questions that will result in personas that are actionable for your team.  You must have a solid working knowledge of your specific users’ expectations and needs to create a survey that will reveal anything useful.
(While many teams have created personas based entirely on demographic or analytics data, without qualitative research, we do not recommend that approach, as it leads to personas with limited utility for UX decision making. Even with analytics data that shows behaviors at a high level, you have no information about what users were trying to accomplish, why, where, and how it felt to them. If you don’t know why someone did something, you’re going to have to make assumptions, which are often wrong. The whole point of personas is to put yourself in your users’ shoes and be able to understand what they want and why. Context is key here, and demographics and analytics data lack context.)
The big difference between qualitative personas and statistical personas is that, rather than manually clustering similar users based on their answers, you take the major themes that emerge from your qualitative research and create a survey that you send out to many people. Then, you run statistical analyses on the survey data to cluster the users into groups that are similar (because they tend to provide similar responses to most questions). It effectively takes out human bias from the clustering process — but what you gain by reducing bias you may lose in critical thinking about whether similarities between users are meaningful.
How Statistical Personas Are Made   
The first step in creating statistical personas is the same as that for qualitative personas: exploratory qualitative research to identify the main themes that come up repeatedly among users. Based on this qualitative data, create a survey that will allow you to collect quantitative data about the major themes of interest at a larger scale. Survey at least 100 (ideally 500 or more) respondents —  the statistical-analysis techniques work better with large sample sizes. Then use a statistical clustering technique such as latent class analysis (which works well on the categorical data that these surveys typically collect and also handles incomplete data well), factor analysis, or K-means clustering to find the patterns in your survey data.
Beware: the patterns that often come up in this sort of analysis may not be particularly meaningful for designers and it may be hard to put into words the criteria used for classifying users based on these analyses.
Pros of Statistical Personas
There are three big reasons why statistical personas have advantages over the other methods:

With a large sample, you can be confident that outliers are not overrepresented in your personas (i.e., that a person with an unusual mindset not shared by many others didn’t dramatically sway the results).
You can know what percentage of your total user base each persona represents, which can be helpful for tradeoff decisions that benefit one persona over another.
You can reverse-engineer the persona clustering (using discriminant analysis) to figure out which survey questions most strongly predict which persona someone was clustered into. Then you can use those questions to recruit users in future studies and thus make sure that all your personas are well represented in all your studies.

Cons of Statistical Personas
Statistical persona segmentation is expensive, time-consuming, and requires expertise in statistical analysis. Unless you have access to a statistician or data scientist, this method is not likely to be fruitful and is not recommended.
Furthermore, when done correctly, it requires running the entire qualitative-persona research AND doing all the statistical analysis too. Also, it’s not uncommon for a team to do all the statistical work and end up with personas that are very similar to purely qualitative personas based on the same qualitative research data.  
In many ways, this technique is like cracking a walnut using a hydraulic press — yes, you can be certain that the walnut’s shell will be thoroughly cracked, but it’s massive overkill in most situations and can leave a mess if not done carefully.
Summary
For most teams, a qualitative approach to personas is appropriate, as it provides a solid data-based understanding of who your users are and what they want, is cost-effective, and relatively quick. Proto personas are an option for extremely lean teams and they serve to align team members’ assumptions about users. They are a good fit for teams that would otherwise not use personas (or user research) much at all and can be a gateway into further research.  Statistical personas are an option for teams with significant resources, but they require time, effort, statistical expertise and demand that the team begin with qualitative research anyway, effectively duplicating efforts.
Learn much more about the methods for creating these different types of personas in our Personas workshop."
32,2020-06-07,"Likert and semantic differential are two types of rating scales often used in UX surveys. They often get confused because the differences between them are subtle. However, they shed light on attitudes and preferences in slightly different ways.
How UX Professionals Use Rating-Scale Questions
We often measure attitudes, perceptions, beliefs, preferences, and self-reported behavior using rating-scale questions. These types of questions allow for degrees of opinion.
Rating-scale questions appear in various research methods. The most common application of rating-scale questions is, of course, in surveys. However, rating-scale questions are also often administered in quantitative usability tests. The attitudinal data produced from rating-scale questions helps us understand how users perceive our product or service, in addition to how they performed a given task. This data provides us with a richer picture of the overall user experience.
Likert Scale
The Likert scale (pronounced Lick-urt) is named after the psychologist Rensis Likert, who created the Likert-scale method in the 1930s.
Likert scales measure agreement. In a Likert scale, respondents are asked how much they agree or disagree with a set of statements. An overall position is derived after analyzing all responses to related questions. Usability assessment questionnaires, like System Usability Scale (SUS) and Standardized User Experience Percentile Rank Questionnaire (SUPR-Q), use a Likert scale. (Technically, one question on its own is not a Likert scale, but is a question utilizing a Likert-type response format. A single question is referred to as a Likert item.)

The System Usability Scale (SUS) questionnaire uses a Likert-type scale, with 5 response points for each item. SUS asks respondents to select whether they agree with 10 different statements. Questions 1–3 from the SUS questionnaire are shown above.

Likert scales (and the Likert-type response format) are vulnerable to two response biases:
1) acquiescence bias
2) social-desirability bias

The acquiescence bias is people’s tendency to agree with others. This phenomenon is not surprising — after all, it is our nature to be agreeable. Acquiescence occurs because participants are primed by the positive (or negative) statement that they have to agree or disagree to. This type of behavior is an example of a framing effect — when a positive (or negative) aspect of a situation is emphasized, people tend to see the whole situation as positive (or negative, respectively).

One way to circumvent this issue is to alternate positively phrased statements with negatively phrased ones. For example, the SUS questionnaire alternates between positive and negative statements (as shown in the figure above). However, taking this approach is not without its own difficulties. Sauro and Lewis (2011) found that alternating between positively and negatively phrased statements can lead to confusion among both participants (who might not read the statements carefully enough to notice the alternation) and researchers (who might not realize that the responses to negative and positive questions must be coded differently).

The social-desirability bias is the desire to report views that will be regarded favorably by others. When respondents feel there is an accepted position, they will be more likely to agree with that position out of fear that disagreement may reflect badly upon themselves. (For example, if people feel that political correctness is the accepted position, they may be reluctant to report personal attitudes that go against this view.) To minimize this bias, don’t ask respondents for their name or other identifying information. Researchers have found that asking for names and other identifiers in surveys increases the social-desirability bias.

Semantic Differential
The semantic-differential question was introduced in 1957 by Osgood, Suci, and Tannenbaum in the book The Measurement of Meaning, and has since become popular.
Semantic-differential questions require respondents to rate their attitude by selecting a position on a bipolar adjectival scale. The two ends of the scale host antonym adjectives (e.g., ugly – beautiful, easy – difficult). The Single Ease Question (SEQ) is an example of a semantic differential scale. The SEQ presents a 7-point scale with the ends labeled very easy and, respectively, very difficult.

The Single Ease Question (SEQ) is an example of a 7-point semantic differential. The two bipolar ends describe the ease of interaction for a given task.

In most applications of the semantic differential, the options that can be chosen along the continuum are not labeled, as they are supposed to represent abstract points; however, variations exist where the intermediate points are either numbered (e.g., -3 – +3) or labeled with words such as very, somewhat, neither nor.
While research has found that people find it easier to comprehend word-labeled scales compared to unlabeled ones, it can be hard to come up with the right word to describe an intermediate point on a scale.
The data produced from a semantic-differential question is reliable only if two assumptions are satisfied. These are:

The pair of adjectives are truly bipolar. However, it might not always be possible to find a dichotomous pair of adjectives.
The respondents understand the dichotomy between the pair and the continuum between them. However, because the scale is not labeled, each option could be interpreted differently across multiple respondents.

Likert vs. Semantic Differential
Although both rating scales allow for degrees of opinion, there are subtle differences between them. Answering a semantic differential requires more cognitive effort than answering a Likert-scale question, as respondents must think abstractly about their attitudes in order to select an option, most notably because the points on the scale are unlabeled. However, the cognitive flexibility of the choices means respondents don’t feel trapped by a particular label, which is possible with a Likert scale.
The comparison table below highlights some of the differences between the two types of question.




 
Likert items
Semantic Differential





Information obtained

Agreement or disagreement to statements
Where the respondent’s view lies on a continuum between two contrasting adjectives


Number of options presented for selection
Typically, 5, but the number can be as many as 7 or 9
Typically, 7, but the number of points can vary


Labels for options
Each option is labeled with words. (If more levels are included, the continuum might not be fully labeled, as it’s difficult to summarize the extent of agreement when there are more than 2 agreement options.)
The two polar sides are labeled, but the options are typically not labeled, or are labeled only with numbers.


Limitations
Affected by acquiescence bias and social desirability bias

Requires higher cognitive demand to answer due to unlabeled options









Information obtained

Likert items: Agreement or disagreement to statements
Semantic Differential: Where the respondent’s view lies on a continuum between two contrasting adjectives


Number of options presented for selection

Likert items: Typically, 5, but the number can be as many as 7 or 9
Semantic Differential: Typically, 7, but the number of points can vary


Labels for scale points

Likert items: Each option is labeled with words. (If more levels are included, the continuum might not be fully labeled, as it’s difficult to summarize the extent of agreement when there are more than 2 agreement options.)
Semantic Differential: The two polar sides are labeled, but the options are typically not labeled, or are labeled only with numbers.


Limitations

Likert items: Affected by acquiescence bias and social desirability bias
Semantic Differential: Requires higher cognitive demand to answer due to unlabeled options




In some situations, it’s possible to use either a Likert item or a semantic differential for a given research question. For example, if we wanted to understand how satisfying our website is to use, we could ask how much a user agrees or disagrees with the statement: The website was satisfying to use. We could also create a semantic differential question, which asks: How satisfying was using the website? The poles of the scale contain the words ‘satisfying’ and ‘unsatisfying’. Both of these questions help us to understand the users’ perception of the ease of use of our website.
However, there are situations when it is difficult or impossible to use a semantic differential. For example, consider the statements below that were presented as a Likert scale in a survey of UX practitioners.

We don't begin thinking about solutions until we've completed our discovery.
We are given time to do enough discovery work before we begin designing new features, products or services.
The team involved in the discovery all work together collaboratively, sharing work.
Our discoveries are centered around user research with our target users.

Converting the above to semantic differentials would not be possible without changing the type of information we are trying to garner from respondents.
Thus, overall, questions that use a Likert-type response format are more flexible, and have a greater number of applications.
Tips for Using Rating Scales in UX Surveys
If you’re thinking of using rating scales in your survey, then here are some tips.

If you want to evaluate the ease of use of an interface, use standardized usability questionnaires which have been tried and tested and have undergone psychometric testing, instead of creating your own.
If you’re not sure which style of rating scale to use, test them both. Do an in-person qualitative survey to test the comprehension of the question and of the answer options. Ask your participants to think aloud while they complete the survey. You can also trial both versions of the survey and compare the responses to decide whether you should use a Likert or a semantic-differential scale. Consider the audience: will people struggle to answer a semantic differential, or will they be prone to agree too much?
Use existing scale labels when designing a question using a Likert-type response format. Stick to the typical ways of phrasing agreement and disagreement (strongly agree, agree, neither agree nor disagree, etc.), rather than reinventing the wheel and creating new response options.
Ensure your polar adjectives are true opposites when designing semantic- differential scales. For example, go for well-accepted pairs like interesting vs. boring, as opposed to odd pairings such as cool vs. strange. Test with users in person to understand if word pairings are seen as bipolar before implementing a large-scale quantitative survey.
Include optional text fields so you can get more insight. A question like Why did you choose this rating? garners the thought process behind choosing an option on the scale.
Add a Not applicable option for those questions that might not apply to all your respondents. This extra choice allows you to disambiguate any neutral respondents from respondents who don’t believe the question is relevant to them.

Conclusion
Likert and semantic differential are two types of rating scales often used in UX research. Both are tried and tested ways to measure degrees of opinion related to the experience of products and services; however, they do so in slightly different ways. Pick the right style of rating scale to suit your research aims and be aware of the limitations and nuances of both.
References
Likert, R. (1932). A Technique for the Measurement of Attitudes. Archives of Psychology, 140, 1–55.
Osgood, C.E., Suci, G.J., and Tannenbaum, P.H. (1957). The Measurement of Meaning. University of Illinois Press, Urbana, Illinois.
Sauro, J., Lewis, J.R. (2011). When designing usability questionnaires, does it hurt to be positive?. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '11). 2215–2224."
33,2020-05-17,"This article reports the results of a survey, in which 436 UX practitioners completed (or attempted to complete) an online survey in October 2019.
The discovery phase is an important part of the UX-design process, so we aimed to understand how companies perform discoveries on real-world design projects and the barriers to carrying out discoveries effectively.
A reminder from our full article on discovery in UX: A discovery is a preliminary phase in the UX-design process that involves researching the problem space, framing the problems to be solved, and gathering enough evidence and initial direction on what to do next.
Survey Findings
Most UX Practitioners Carry Out Discoveries for Some Projects
75% of the respondents claimed that their organization carries out discoveries. Out of those, only 20% said that their organization performs discoveries for every project, while the majority (60%) claimed that their organization carries out roughly 1 discovery phase for every 2–4 projects.
Many Organizations Forgo Discovery Due to Lack of Time
The top reason for not carrying out discoveries at all in an organization was time pressure to deliver, followed by no management buy-in. Only 7% of the 69 respondents whose organizations did not carry out discoveries believed lack of resources was the biggest factor.

“[The] focus is on delivery — they want to deliver ‘something’, and then fix later.”

It’s worth noting that the desire to deliver quickly may also affect UX professionals’ work in other stages of the design process, particularly in carrying out usability testing or having UX insights incorporated into product development. Rushing into implementation with too little time for UX activities is a sign of an organization’s low UX maturity.

75 respondents shared the one top reason why their organization doesn’t perform discoveries at all as part of its design process. The graph indicates the distribution of their answers.

Of course, in reality, several factors dissuade organizations from implementing discoveries, and our respondents explained that it was often a combination of both management buy-in and time pressure.

“In addition to time pressure, there is little CEO buy-in. He tends to feel that we already know the problem that needs to be solved.”
“I think there's also a lack of understanding of their value — the company considers itself very innovative, and there's a race to come up with the next amazing idea.”

Interestingly, respondents that work for organizations that do run some discovery phases in their design projects reported (in a free-text field) the same reasons for why certain projects did not get a discovery phase. Both these findings suggest that lack of buy-in and time are top challenges for running discoveries.
UX Practitioners Who Conduct Discoveries Are More Likely to Report that Their Projects Are Successful
Respondents were asked about the last project they worked on at their organization. Of those who did not carry out a discovery, 52% of respondents said that their last project was successful — barely better than chance! In contrast, 83% of respondents who had carried out a discovery in their last project said that the project was successful. A chi-square test revealed this difference to be highly significant (p<.00001).
Although we can’t be certain if these projects were actually successful and we don’t know whether it was the discovery phase which contributed to the success of the project rather than correlated factors, this result does lend credence to the widely held belief that discoveries lead to better outcomes, because they frame the problem to be solved, set teams in the right direction, and provide them with relevant knowledge to design successful solutions. Further evidence that this is the case was provided when respondents rated their agreement to the following statement: Our discoveries positively impact the products and services we build. 84% of 240 practitioners with experience of carrying out discovery phases agreed with the statement, and only 5% disagreed.

The bar chart shows the percentage of respondents who reported their last project to be successful. People who worked on a project with a discovery phase were more likely to report success than those whose projects did not have a discovery phase. Error bars indicate the 95% confidence interval.

There are two ways of interpreting this chart:

Performing a discovery cuts the risk of failure by 75% (4 vs. 16 failures).
Performing a discovery increases the chance of success by 59% (83 vs. 52 successes).

Of course, these two ways of stating the results are two sides of the same coin, but it is remarkable that the most striking impact of discoveries comes from risk reduction. This is actually true of most UX work: doing good UX will often increase product success, but it will almost always insure you against true disasters.
The Average Team Size Is 4
We asked respondents to tell us who was on the team carrying out the discovery. The average number of individuals involved full time on a discovery project is 4 (SD=3.5), with a range from 1 to 36 (which is far too many). The most common roles present on a discovery project were UX or product designer (74%), researcher (60%), and product or project manager (52%). Only 36% of discoveries had a product owner.

The bar chart shows the percentage of respondents who indicated that these roles were present on their discovery team.

Most Discoveries Are Carried Out in Under 2 Weeks
We asked respondents to tell us how long the last discovery they ran was. Obviously, all discoveries are different, but we wanted to know roughly how long the average discovery phase is, across all projects.

The chart shows distribution of discovery lengths, as given by 242 UX practitioners. Each bar indicates the percentage of practitioners reporting the corresponding length for their last discovery. A majority of respondents (58%) reported that their last discovery lasted at most 2 weeks.

I would have expected the data to form more of a bell curve, with few participants running very short or very long discoveries, but instead the chart above is somewhat bimodal. The majority (58%) of discoveries undertaken by respondents were on the short side — at most two weeks. Unless users are easy to access and the problem space is relatively small, two weeks is too short. Just under a third perform longer discoveries (over 7 weeks).
There are many possible reasons why the majority of discoveries are short. One may be related to budget and time constraints, which result in time-limited discoveries. Another reasons might be the popularity of Google Ventures’ Design Sprint, as well as the adoption of continuous product discovery, which means some organizations favor leaner initial discovery phases, while carrying out more discovery-style research throughout the design process.
The problem with slimming down the discovery phase or replacing it with a design sprint is that some teams may miss out on a time of crucial learning, beginning the design process with solution ideas that might not be based on real problems. Survey respondents commented on the brevity of discoveries in their organization:

“In my opinion, the time spent on discoveries in my company is not enough and sometimes discoveries are carried out not to know the problem better, but to confirm [the] initial idea.”
“The UX team believes discovery work can be continuous with 5 or less participants and always presenting a solution first without researching the problem space first, just going off anecdotal evidence on problems. 2–3 people's response to a proposed solution that doesn't address the problems they've actually expressed doesn't seem like best practice.”

Most Common Activities
In the discovery phase, it’s possible to perform many different activities: stakeholder interviews and workshops, analyzing analytics data, field studies. Respondents were asked which activities they had performed in their last discovery. When carrying out exploratory research, it’s best if data is triangulated from numerous sources and through different methods, to ensure that teams are getting a full picture of the problem.

The bar chart shows the percentage of respondents who reported performing each of 13 discovery-phase activities.

Across all discoveries, the average number of activities performed was 4.6 (SD=2.5). The most common activities carried out were user interviews and stakeholder interviews. Both methods are quick, which would explain their popularity, given that many discoveries are run in less than 2 weeks. The short duration of discoveries might also explain why diary studies are the most unpopular activity.
Biggest Challenges in Carrying Out Discoveries
Many discoveries are far from perfect and are hampered by real-world constraints. Respondents were asked what their biggest challenges were in conducting discoveries effectively. 215 responses were provided via a free-text field; these responses were coded and analyzed. There were many different challenges, so we simply report on the most frequent challenges mentioned by respondents.
Lack of Time and Budget
The most frequent blocker mentioned was limited time (84 comments). Budget (27 comments) was also a factor; note that budget also affects allocated time. Even if the organization plans to do a discovery, the discovery itself is often the first thing to be reduced or removed.

“The intent is there to allow time for discovery but then deadlines and bottom-line revenue is pushed by old mindsets and sadly everything falls back 50 years to system development, and we try our best to [do] some form of discovery.”

In some cases, UX is simply brought in too late, which means UX practitioners have less time to carry out essential discovery work.

“It isn't often built into the project process, or teams come to us too late to do anything. Even if we discover the solution doesn't address a problem, too much development work has been put into it to stop…”

In other cases, respondents spoke about not having enough budget to perform longer discoveries, because time is money, especially for agencies pitching work to clients.

“Demonstrating the value of comprehensive discoveries to [the] public sector. This sector is very accustomed to penny pinching and it can seem like a ‘luxury item’ when they already believe they have identified the problem(s).”

Budget constraints can also affect things like how big the team is and whether there is enough money for incentives or recruiting for user research.
Lack of Buy-in and Hard to Prove Value
Another big theme was lack of buy-in (61 comments). This means UX practitioners are constantly trying to explain the value of the discovery phase, and are sometimes not successful in convincing stakeholders or clients to do a proper discovery due to “legacy ways of thinking,” beliefs that product managers already know their users, that solutions are obvious, or that shipping a Minimum Viable Product (MVP) will provide all required learning.

“Again, selling in a discovery-based project is tough. Whilst we advocate for it and show benefits, some projects still believe [that] shipping [an] MVP will give them the learning they require. Discovery can provide good direction, reducing risk and increasing confidence of the product.”

Many respondents spoke about stakeholders and other team members not understanding what the discovery is. Some specifically mentioned the difficulty in quantifying the ROI of a discovery, which is often needed to gain buy-in from those holding the purse strings.
The Right People Aren’t Always Involved
16 respondents mentioned challenges related to getting the right people on the team or to lead the team.

“Even though every project starts with discovery, it is still a largely inconsistent process and it is not currently led by product managers or designers. Instead, it is led by project managers, engineers, or account managers. We're working to change this, but product managers and designers are still largely left out of discoveries leading to pain later on in the process.”
Some respondents were a team of one, and, thus, responsible for doing discovery alone. Others mentioned that the people with the right skills weren’t involved, which made running the discovery difficult. For example, some teams lacked a person able to competently carry out user research.
“Putting together a multidisciplinary team is a tad difficult when we have limited staff with the right skill sets.”

Lastly, other respondents claimed that people with a lack of understanding were also on the team, and it was hard to collaborate with them or prevent them from jumping to solutions and succumbing to their own biases.
Access to Users
30 respondents mentioned getting access to users was difficult in their discoveries. Several explained that they worked for B2B organizations and were denied access to users or the process to recruit them was very difficult. This problem was exacerbated by lack of time to recruit specialized users. This problem is not unique to the discovery phase, and we hear about it often from our attendees at our UX conferences.
Survey Limitations
We invited UX practitioners to complete the survey through LinkedIn, Twitter, our Alertbox newsletter, and our website. Since we recruited a convenience sample, we cannot claim that it is representative. It’s likely that in our sample, UX practitioners with experience of carrying out discoveries are overrepresented. We tried to minimize this effect by calling for all UX practitioners to complete the survey, regardless of whether their organization does or doesn’t run discovery phases.
Conclusion
While many UX practitioners have succeeded in implementing discovery phases in their organization, most discoveries are still too short, underresourced, and don’t have the full backing of the organization. This is often the case when clients and stakeholders don’t understand their value. This attitude leads to cost-cutting and trimming of what are deemed unnecessary activities. As a result, too many design projects timidly explore already familiar corners of the design space, missing out on building features that satisfy previously unmet user needs. The good news for those who do perform discoveries is that they possess a ‘secret weapon’ that will help them establish a sustained competitive advantage.
As a profession, we’ve done well to find our place in the development process, convincing others that we should test with users. But we need to become better at educating our organization as to why it makes business sense to spend time upfront in discovery, and why getting UX practitioners involved early is important. Imagine how much more successful projects would be if we could make that happen!
Learn More: Discoveries: Building the Right Thing, a full-day course at the UX Conference."
34,2020-05-03,"A redesign is usually intended to produce a change in the user experience. We want to make the experience better (faster, easier, or more enjoyable). But better compared to what? Establishing a UX benchmarking practice is a great way to make sure you’re moving in the right direction and that you have a clear reference point for any improvements.

UX benchmarking refers to evaluating a product or service’s user experience by using metrics to gauge its relative performance against a meaningful standard.

How Benchmarking Works
We refer to benchmarking as a practice because, ideally, once you begin benchmarking, you can track your progress over time, again and again, redesign through redesign — it’s an ongoing process.
Essentially, benchmarking involves collecting quantitative data that describes the experience.  For example, we might collect any of the following UX metrics:

Average time to make a purchase
Numbers of clicks on a Submit button
Success rate for an application completion
Average ease-of-use rating for creating an account
Eight-week retention rate for an app (percentage of users continuing to use the app after eight weeks)

You can collect those UX metrics using potentially any quantitative methodology, but analytics, surveys, and quantitative usability testing are the three methods that often work best for benchmarking. You can also use customer-service data (for example, the number of support emails about a specific task).
Once you have those numbers, you have to compare them to something. A single number is meaningless, since you have no idea whether it’s good or bad. With benchmarking, you can compare your UX metrics against four different possible reference points.




Compare Against


Example




An earlier version of the product or service


In 2019, the average time to make a purchase was 58 seconds. After our recent redesign, the average time to make a purchase is now 43 seconds.




A competitor


Our success rate for application completion is 86%, while our competitor’s is 62%.




An industry standard


The average ease-of-use rating for creating an account on our hotel’s website is 5.3/7. The average ease-of-use rating for that task in a study of the top 6 hotel websites was 6.5/7. 




A stakeholder-determined goal


Our eight-week retention rate is 8%, but we’re aiming for at least 15%.




There’s no reason you couldn’t compare against several of these reference points. When you take your first benchmarking metrics (often called the baseline), you won’t have data from earlier versions to compare against, so it’s good to compare against a competitor or industry standard at that point.
If you work in a very niche or private industry, it may be challenging to find the exact industry standards you want. Sometimes academic institutions will publish useful data. In addition, Jeff Sauro’s team at MeasuringU.com conducts industry-standard studies for major business sectors (for example, banking, airlines, hotels, etc.).
When to Benchmark
You might be familiar with a type of UX research that helps us learn what  works or doesn’t work about a design and figure out how to fix those problems. That type of research is called formative evaluation — it helps us decide how to form or shape the design. Qualitative interviews and usability testing are frequently used for this purpose. 
Benchmarking is not formative; it is a summative evaluation: in other words, it helps us assess the overall performance of a (sort of) complete design (a summary of its performance). Our designs are never really complete, we’re always improving them. But you can think of a benchmarking study as a kind of snapshot in time, capturing the experience of a specific version of the product or service. Benchmarking can happen at the end of one design cycle and before the next cycle begins.


Your design process may not look exactly like this. Your version might have different labels for the components or a different number of steps, but you’re probably doing some variation of these activities. 




Benchmarking can happen at the end of one design cycle (for example, after the evaluate phase) and before the next cycle begins (for example, before the define phase).


It’s up to your team to decide how often you conduct benchmarking studies. Their frequency may also depend on the methodologies you’re using. Quantitative usability testing can be very expensive and time-intensive, so maybe you’ll decide to perform a benchmarking study once per year. Analytics can be much quicker for collecting benchmarking metrics, so maybe you’ll benchmark after each major product redesign.
Why Benchmark
Benchmarking allows us to assess our impact and improvement. It’s helpful for reflecting on our process and design choices. There’s a lot of value that benchmarking can provide to product or service teams.
But benchmarking’s real power comes in when you show those results externally, for example, to stakeholders or clients. You can demonstrate the impact of your UX work in a concrete, unambiguous way. And you can even take that a step further by using those metrics to calculate return on investment (ROI) — show stakeholders and clients exactly how much more they’re getting in return for what they paid. That way, you’ll have no problem arguing for more funding and bigger projects in the future.
 
For guidance and hands-on practice creating your own UX-benchmarking plan, check out our full-day course, Measuring UX and ROI.
For help analyzing and interpreting quantitative data, check our full-day course, How to Interpret UX Numbers: Statistics for UX."
35,2020-04-26,"Remote moderated usability testing has a lot of advantages. Compared to in-person studies, it’s often less expensive, less time-consuming, and more convenient for participants. In cases where participants can’t travel to a testing location, remote moderated usability testing is an excellent alternative.
Perhaps the biggest downside to remote moderated usability testing is that it can require lots of advance preparation, particularly in setting up the tool you use. If you’ve never done a remote moderated study before, it might be tough to know where to begin.
Follow these 7 steps to ensure your remote moderated sessions run smoothly and yield rich, deep insights.
Study Planning
1. Choose a tool for communicating with the participant.
Choose a screen- and audio-sharing tool that will be easy to access and install for testers, but also for observers and facilitator.
You don’t have to use the same tool for all test participants. For example, those joining from their office computer may need to use a company-sanctioned meeting tool. Others may not be able to install any software — for example, due to an aggressive firewall.
There are many screen-sharing tools available, but some of the tools we use for remote testing include:

Zoom
GoToMeeting
Join.me
Skype
Lookback.io

When you choose your tool, pay special attention to the installation requirements. Are there any operating systems or browsers on which the tool doesn’t work?
For example, Lookback offers remote moderated usability testing on mobile, but  doesn’t support all versions of iOS or Android. Figure out those details early, as they may play a role in your recruitment process. If possible, you’ll want to screen participants to make sure they can use the tool. Also, be aware that many remote moderated usability-testing tools for mobile can have stability issues — it’s tough to perfectly support all the different types of mobile devices out there.



These screenshots depict scenes from a remote moderated usability study on mobile using Lookback.io. Left: The facilitator introduces the session and explains the instructions to the participant (in the thumbnail image in the lower right corner, strangely shown as a duplicate during the intro. Her face has been blurred for privacy.) Right: The participant begins her session on Google. (You might notice that this participant was sitting in a stairwell during the session – not an ideal location for a study. It’s important to emphasize to participants that they need to be in a quiet, private location with a good internet connection.)






This screenshot is from a remote moderated usability study on mobile using Lookback.io. The facilitator introduces the session and explains the instructions to the participant (in the thumbnail image in the lower right corner, strangely shown as a duplicate during the intro. Her face has been blurred for privacy.)




In this screenshot, the participant begins her session on Google. (You might notice that this participant was sitting in a stairwell during the session – not an ideal location for a study. It’s important to emphasize to participants that they need to be in a quiet, private location with a good internet connection.)



Also, think about how you’ll handle observers in this tool. Make sure that you can mute them during the session to prevent any unnecessary noise. If the tool has a chat feature, it should be possible to keep team conversations private so that participants don’t get distracted. You can also use a separate tool (e.g., a messaging app like Slack) to allow observers to communicate among themselves or with the facilitator.
Decide whether you’ll want to share webcam video. If your system and connection can handle it, it’s almost always best to share the participant and facilitator’s faces, as it improves the communication during the session.
2. Plan how to administer the tasks.
With in-person testing, you can just hand the participants a sheet of paper with the task printed on it, and ask them to read it out loud. Administering tasks is a bit more complicated with remote moderated tests, and it will likely depend on the tool you’re using.
The ideal way to deliver tasks is one at a time, and by asking participants to read the task out loud themselves. That way:

You know they’ve read the entire task.
They won’t look ahead to later tasks.
They will practice speaking out loud.

In addition, you want participants to have the instructions easily accessible as they are doing the task, to be able to refer to it in case they forget them.
There are three options for delivering the task for remote moderated testing:

Send participants a document (e.g., pdf) with all the tasks.
Use the screen-sharing tool’s chat window to send the text of the current task to the participant, during the session.
Have the moderator read the task out loud to the participant.

Send participants a document (e.g., pdf) with all the tasks (or with links to the tasks).
In this case, participants must be instructed to not read through the tasks before the session. If the document contains the task text, make sure you start with a blank page and that each task is presented on a separate page. Include page numbers to be able to guide participants to a specific task. Participants can print the document in advance or refer to it electronically during the test session.
If a digital document can be accessed during the session, you can also include links to the task text instead of the text itself. In that case, each task’s instructions can live as a webpage (e.g., a Google Doc) on the web and the participant can be asked to click on the corresponding link to read that task.

One option for delivering tasks is to use a PDF list of links, each one opening to that task’s instructions in a Google Doc.

Use the screen-sharing tool’s chat window to send the text of the current task to the participant, during the session.
For example, Zoom has a chat feature that allows attendees to send each other links and text. The chat window is a convenient way to deliver task instructions; ask the participant to read the instructions out loud before beginning.
Have the moderator read the task out loud to the participant.
Although this method is the least advisable, it may be the only option sometimes (for example, because of tool or device limitations). If you choose this option, prepare to reread the task instructions multiple times, as needed.
3. If possible, schedule technology-practice sessions.
For each participant, try to schedule a 15-minute online meeting the day before her session. Also invite one or two practice observers.
This is the time to get participants to set up the technology needed for the test. Have them install any necessary applications, and work through any hiccups with screen sharing, audio, and reaching websites.
Choose a website or app completely unrelated to the one you plan to test, and ask participants to use it, briefly. In this practice session, don’t show users the actual website or prototype that you plan to test, and don’t have them attempt test tasks. If participants need to physically sign consent or NDA forms, now it is the time to discuss the logistics and ensure that they can access the documents and know how to send them back to you before the session.
If practice sessions are not possible due to scheduling constraints, make sure you leave some extra time for setup during the actual session.
Be aware that even if you have a technology-practice session, you may still encounter tech problems during the session. We’ve had situations where a participant’s setup worked perfectly in a practice session, and then didn’t work at all the next day (for example, due to her computer automatically updating its operating system overnight). It’s always best to add a little extra time to each session to ensure you can deal with minor problems without ruining the session.
On the Day of Testing
4. Send out reminders.
As with any type of testing, send reminder emails to participants and observers, either the night before or the morning of testing.
For participants, remind them when their session begins, and of anything they need to bring or do during the session. For example, you might remind them to:

Have their laptop connected to a charging cable, or their mobile device fully charged.
Have a headset or pair of headphones to minimize feedback and improve audio quality.
Open the remote software tool they practiced using during the technology-test session.
Be connected to a strong WiFi or wired connection.
Be in a quiet place where they won’t be interrupted.

For observers:

Remind them of the testing schedule.
Tell them any rules for observing (for example, join five minutes early, or keep yourself on mute).
Give them tips for observing the study, a copy of the tasks that users will attempt, and specific issues you want them to watch out for.

During Each Session
5. Invite the team to join the session.
The exact time when observers should join the session varies depending on:

the remote-software tool
whether the technology-practice session with the user was successful
your relationship with the observers

Joining early, a few minutes before the session starts, has the benefit of being least disruptive to the participant, especially if the remote tool beeps whenever a new person has joined the meeting. But the disadvantage is that it may potentially waste observers’ time and diminish their engagement.
If the latter is a concern, it’s ok to have observers join the session once all the logistics have been cleared (usually, a few minutes after the scheduled start time). Alternatively, you may be able to keep observers busy during the down time by asking them to discuss the previous session’s findings or review the observer tips and think about specific issues.
6.  Start the session with the participant.
Once your participant has joined the meeting at the agreed time, run through some variation of the following script:

Hi [name], thanks for joining.
I am [XXX]; I work for company [YYY]
Before we begin, is [name] the correct way to pronounce your name?
OK, thanks.
I have some colleagues here with me. They’ll just be quietly observing the session, looking for ways we can improve the [site/app].
Is it ok if we begin recording?
[Wait for confirmation, then begin recording.]
[If the participant has not already signed the consent form, perform the following steps.]
	
[Share your screen.]
[Open the consent form you’d like your user to read out loud.]
This is a consent form that explains what we’re doing here today and how we will use the information. Please read this out loud, and then if you agree, please say your full name out loud.
[Wait for reading and consent.]


[Turn presentation control over to the user, for them to show their screen.]

7. End the session

Thank participants for their help.
Stop and save the recording.
Check in with your observers to discuss the major observations for that session, and any changes to tasks or procedure you’d like to make going forward.

Repeat steps 5 through 7 until your study is complete. (Step 8: Pat yourself on the back for a study well done!)

Summary of the steps involved in remote moderated usability testing

More Tips for Good Remote Moderated Testing

If you haven’t already obtained a signed consent form in advance, ask participants to read your consent form out loud, and then verbally give their consent by saying their full name out loud.
Don’t neglect your own tech setup while you’re thinking about others! Use a headset and a strong WiFi connection, just like the participants should.
Whenever in doubt, ask the user how to pronounce her first name. In in-person studies you can get the user’s attention with a gesture or sound. But if you need to interrupt the participant during a remote session, saying the person’s name can feel the most natural. But you want to make sure you know how to pronounce it.
Keep track of what time the session is supposed to end, confirm it with the user at the beginning of the session, and finish on-time.
Accept that things will go wrong and have a backup plan. Software gets updated at the last minute, firewalls interfere, apps have bugs. When will you release the observers? The user? What if you need to reschedule? Just as with in-person testing, it’s a good idea to build in a little extra time to your sessions and recruit more participants than you really need.
Do a pilot test to help refine your process, facilitation, and tasks. A pilot session is a test of the test itself, and not a test of the design, so your participant can be either a representative user or someone else.

Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote moderated user testing (including sample scripts, research plan, and tasks), check out our full-day seminar: Remote Usability Testing."
36,2020-04-19,"We recently published the 2nd edition of our User Experience Careers report, 7 years after the 1st edition was published. Our report is free and a gift to the UX community.
The 2nd edition is based on several research studies carried out in 2019 and is based on responses from over 700 UX professionals. They included an online survey completed by 693 people, 2 focus groups, and 17 remote, semi-structured interviews, all carried out with UX professionals around the world.
What Hasn’t Changed?
UX practitioners are just as satisfied with their careers as they were in 2013; career satisfaction got an identical rating of 5.4 on a 1–7 scale (where 1 was completely dissatisfied, and 7 was completely satisfied). (The 95% confidence interval was from 5.27 to 5.46.) The comments around career satisfaction were grouped into these main reasons why people love their career in UX:

Enjoying the process and the work
Seeing the impact of their work
Receiving recognition for their work
Having opportunities to grow and excel

Job titles are almost as varied as they were in our original study, with our 2019 survey respondents reporting 134 unique job titles related to UX (slightly down from the 210 job titles reported in 2013, but we’re still far from any agreement on job titles). Likewise, the backgrounds of UX practitioners are still as diverse as they were in 2013, although one thing was common: the majority had a degree. This finding was also present in the 2013 study.
Industries hiring UX practitioners and the kinds of things UX practitioners work on are also much the same. For example, IT and software is still the largest hiring industry, followed by the finance and insurance industry, and consulting agencies. We did see some increase in the amount of work being done on mobile applications since 2013, and some UX practitioners are now working on AI products.
Finally, soft skills are still the most important skill set in gaining a job in the field and succeeding in it. While hard skills are desirable, hiring managers and UX practitioners alike see soft skills as a core requirement for a career in the field.
(For more information about our research with UX careers in 2013, see the article we published then.)
What’s New?
Since our first edition was published in 2013, UX has emerged as a field in many more countries around the world. In our most recent study, we received responses from 65 different countries, compared to 38 countries in our original study. We also had a larger representation of non-English speaking countries, which could be an indicator of how the UX field has grown outside of the Western world over the 6-year period between studies.
Our new report also analyzes the difference between UX roles, particularly between researchers, designers, and generalists. Although specialization is not new to the field, we had not performed this analysis in 2013. Some highlights of the differences between researchers and designers are given below.
The Designer Role in a Snapshot
Those with a UX designer role often have a broad range of responsibilities: from designing prototypes to collaborating with subject-matter experts or carrying out qualitative usability tests. Designers tend to have a design-related education and have skills in using prototyping tools and performing visual design. Many have front-end–coding skills.
One of our survey respondents had this to say about working as UX designer:

“The role of UX designer is a dream job for creative people who love to invent and get products into people’s hands. In the best environments, it’s pure creativity and invention, which is mostly unattainable with any other role.”



 
The Researcher Role in a Snapshot
Those with a UX researcher role are likely to have fewer responsibilities than designers, focusing on UX research, as opposed to information architecture, content strategy, or any kind of design work. Researchers tend to have fairly strong communication skills, used for either writing or public speaking — much more so than designers. They’re less likely to have front-end coding skills in HTML and CSS.
A UX researcher had this to say about the role:

“I really love what I do as a UX researcher. I really love the process of understanding the problem space, learning about the different people we design products and services for, and synthesizing those findings to help guide designers and product managers.”



Changing Careers
Our research also explored the experience of people transitioning into a career in UX from different fields. Since the field is still growing, anyone who is interested in and suited to the field can get a job in UX. Some of our survey, focus-group, and interview participants had previous careers before transitioning to UX. Their experience and knowledge of other fields, they believed, were helpful in getting into the UX field and in being capable and effective in their roles, as well as excelling quickly.
In some cases, it’s possible to slowly start doing UX at your organization without having to leave and find a new job. Several of our participants had this type of transition, and it’s probably a wise strategy as there is less of a requirement to prove your experience for the role. A participant in our interviews — a product designer from Nigeria — explained that he learned UX while working as a freelance graphic designer.

“Actually, I even learnt on the job, so I was freelancing prior to joining [name of employer], and so I had to convince clients, let's do user research! Let's not dive straight into design! And when I was able to do that, I was able to learn on the job.” 

He was then able to apply for a role at a different company as a product designer with evidence in his portfolio that he had done user-centered design.
Advice for New UX Practitioners
We asked all of our participants if they had any advice for people new to the field. The top pieces of advice were:

Keep learning and stay curious. Several participants mentioned continuous learning, which can be built up through reading, asking questions, and applying concepts to your own work.
Respect others. There’s no room in UX for an overinflated ego and lack of respect. Being able to listen, collaborate, and appreciate others and their roles were listed as incredibly important traits of a successful UX practitioner.
Be confident. Imposter syndrome and low confidence can get in the way of voicing your point of view, and may impact the ability to advocate best for users. But our participants advised to speak up and not to worry about asking silly questions, because these will pay off.
Be in charge of your own career. Having a career in UX is not always perfectly defined. Our respondents advised UX practitioners to consider their strengths and weaknesses and be motivated to take their own career progression into their own hands.

The Full Report
The full report, User Experience Careers: What a Career in UX Looks Like Today, is free and available for download."
37,2020-04-12,"Under normal circumstances, few teams have enough time and resources to perform as much in-person usability testing as they’d like. Due to recent quarantine and travel restrictions, almost all UX teams are suddenly finding that in-person methodologies are not currently possible.
Acting under the (correct) assumption that any user data is better than no data, many teams are turning to remote usability testing. One of the most popular variations of remote testing is unmoderated usability testing.
Unmoderated usability testing (also known as asynchronous testing) is a popular way to get a product tested by users without traveling or breaking the bank.  It usually involves using one of the many available services (such as What Users Do), setting up some tasks, and waiting for the data to be collected. 
This method has some substantial benefits: 

No recruiting (if you’re using the built-in panels of users that the remote-testing services provide)
No moderation skills needed
Easy test setup 
Fast results
Low cost

However, the data obtained from such remote unmoderated sessions is often less detailed than that from in-person testing. This difference is accounted by the serious drawbacks of unmoderated remote testing:

Short test sessions (the length is limited by the testing service’s rules, but usually is around 20 minutes, compared with 1–2 hours for in-person studies)
No way to clarify slightly ambiguous or unclear instructions
Inability to ask users to elaborate on a comment or continue using an area of the design
Unrepresentative testers
Variability in participants’ motivation and commitment to the test 
High chance that testers will multitask or get distracted by their environment

In contrast, remote moderated usability testing combines the advantages of both methods (in-person and remote unmoderated): it can deliver high-quality findings (comparable to in-person testing), but is convenient and inexpensive (like remote unmoderated). The benefits of remote moderated usability testing include: 

The facilitator may change, skip, and reorder tasks as needed.
The facilitator may ask follow-up questions or for clarification if needed.
The participant is less likely to spend time on activities unrelated to the test.
The situation may feel more natural than talking out loud to oneself.
The test sessions can be longer (usually about an hour) and leave room for in-depth exploration of a design.
The team can watch the test at the same time and discuss the findings immediately after the session. 

We acknowledge the two most resource-intensive parts of moderated remote testing, which are:

Recruiting users: You can’t take advantage of a built-in panel. However, you can also recruit users to match your target audience. Use a recruitment agency, your own user database if you have one, your website, social networks, and friends and family.
Setting up the meeting software: It can take extra time and effort to ensure that it works for your test participants, your observers, and you.

Even with these hurdles, remote moderated testing is easier than most people think. Check out our checklist with descriptions for each step.
Three Usability-Testing Formats Compared





 


In-person moderated usability testing


Remote moderated usability testing


Remote unmoderated usability testing




Facilitator required


Yes


Yes


No (the testing platform acts as the facilitator)




Ability to ask specific questions


Yes


Yes


No (though you can set up generic followup questions after each task)




Ability to reorder or modify tasks


Yes


Yes


No




Ability to clarify instructions for participants


Yes


Yes


No




Ability to coach or encourage participant to think out loud


Yes


Yes


No (though you can instruct them at the beginning of the session)




Testing location required (for example, a usability-testing lab or focus-group facility)


Yes


No


No




Typical cost


High


Low


Low




Session scheduling


Fixed session dates and times


Flexible session dates and times (can be modified depending on participant availability)


No scheduling needed (users participate on their own time)




Typical session length


Can be short (30 min) or long (2-3 hours)


Can be short (30 min) or long (2-3 hours)


Most platforms require short sessions (around 30 min)




Risk of “cheating” or unmotivated participants


Low


Low


High (depending on how participants are recruited)










Facilitator required




In-person moderated


Yes




Remote moderated


Yes




Remote unmoderated


No




Ability to ask specific questions, reorder/modify tasks, clarify instructions, and coach participants




In-person moderated


Yes




Remote moderated


Yes




Remote unmoderated


No




Testing location required




In-person moderated


Yes




Remote moderated


No




Remote unmoderated


No




Typical cost




In-person moderated


High




Remote moderated


Low




Remote unmoderated


Low




Session scheduling




In-person moderated


Fixed session dates and times




Remote moderated


Flexible session dates and times




Remote unmoderated


No scheduling needed




Typical session length




In-person moderated


Can be short (30 min) or long (2-3 hours)




Remote moderated


Can be short (30 min) or long (2-3 hours)




Remote unmoderated


Most platforms require short sessions (around 30 min)




Risk of “cheating” or unmotivated participants




In-person moderated


Low




Remote moderated


Low




Remote unmoderated


High (depending on how participants are recruited)





When to Use Remote Moderated Usability Testing
If your team’s UX-research resources are severely limited, remote unmoderated usability testing may be your only option. This is often the case for quantitative usability studies, which often require more than 30 participants. In such situations, remote unmoderated usability testing is certainly better than no usability testing.
However, remote moderated usability testing may be a better fit than remote unmoderated testing or in-person moderated testing if:

You want deep insights and rich data
Your participants are busy, geographically distributed, or otherwise cannot travel to a testing location
Your researchers have enough time to meet with each participant individually

Give Remote Moderated Testing a Try
Remote unmoderated testing has the benefit or being fast, inexpensive, and easy. It can get some great insights, and should be part of every UX researcher’s toolbox. 
However, remote moderated testing can give you significantly more useful, interesting, detailed findings than you’ll get from remote unmoderated tests. It takes a bit more coordination, but the small amount of extra effort is well worth the beneficial impact this methodology will have on your research. If you’ve never tried it, there’s no better time than now.
Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote moderated user testing, check out our full-day seminar: Remote Usability Testing."
38,2020-03-15,"Definition: A discovery is a preliminary phase in the UX-design process that involves researching the problem space, framing the problem(s) to be solved, and gathering enough evidence and initial direction on what to do next. Discoveries do not involve testing hypotheses or solutions.
Discoveries are crucial to setting design projects off in the right direction by focusing on the right problems and, consequently, building the right thing. They are often referred to as ‘product discoveries’ (although I’m not keen on this name because it can set the expectation that this phase is about discovering requirements for a given product).
In order to be effective, a discovery should be broad and technology- or solution-agnostic. When teams carry out a discovery on a product they have already decided to build, it no longer is a discovery, but, instead, it becomes a requirements-gathering exercise or a validation exercise where teams seek to confirm that their solution is the best. The discovery is off track when teams are asked “How do we make [insert name of solution] work for users?” or told to “Go find out what the user needs are for [insert name of solution]”.
A discovery should start with a broad objective such as like: “Go find out about this problem, just how big it is, and what the opportunities might be.”
Well-done discoveries ensure that any solutions proposed later are desirable to users, viable for the organization, and feasible with the technology made available.
A discovery should result in the following:

Understanding of users: Through user research, the project team achieves an understanding of who the users are and how they are affected by a particular problem, as well as what they need, desire, and value from a solution (and why).
Understanding of the problems to be solved and of the opportunities: Through investigative work, the team understands how and why the problem(s) occur, what effect the problem has on users, as well as on the organization. It understands the magnitude of the problem and opportunities for the organization, product, or service.
Shared vision: During discovery, the team works with stakeholders to understand overarching business objectives and desired outcomes and get answers to questions such as ‘what do we want to achieve?’, or ‘what does success look like?’. This approach, in turn, focuses the team on the problems (and later the solutions) that will have the greatest impact on that outcome. The team should also have an idea of what to measure going forward, to understand whether the solution is working towards the desired outcome.

A discovery starts broad and requires team members to investigate the context of the problem. The double-diamond diagram introduced by the UK Design Council — and reproduced below — illustrates the high-level process of a discovery: first, the team expands its understanding of the problem by researching its full context; armed with this knowledge, the team agrees on what the problem is, before moving to the next phase of ideating and testing in the Develop stage.


Discovery covers the Discover and Define stage of the double-diamond model. In the Discover stage, lines of inquiry diverge as a team explores the problem space. In the Define stage, the team aligns on an evidence-based problem statement and on a vision for the future. (The second diamond is about the creation of an actual design to solve the problem we’ve identified. We don’t discuss these stages here since they do not use discovery methods, but rather design and implementation methods.)




Discovery covers the Discover and Define stage of the double-diamond model. In the Discover stage, lines of inquiry diverge as a team explores the problem space. In the Define stage, the team aligns on an evidence-based problem statement and on a vision for the future. (The second diamond is about the creation of an actual design to solve the problem we’ve identified. We don’t discuss these stages here since they do not use discovery methods, but rather design and implementation methods.)


When Is a Discovery Needed?
A discovery is needed anytime when there are many unknowns that stop a team from moving forward. Moving forward only on assumptions can be risky, as the team may end up solving a problem that doesn’t really matter — wasting time, money, and effort.
A discovery might also be needed when the team is not aligned in what it wants to achieve.
Discoveries are often carried out differently depending on the type of problem the team needs to investigate. Below are some examples of instigators:

New-market opportunities. If an organization is looking to explore where to expand its product or service offerings, a discovery is often needed. The discovery might involve researching a new audience, performing competitive reviews, and investigating whether the size of the opportunity warrants entering the market.
Acquisitions or mergers. When organizations merge, it’s likely that systems, processes, and tools will also need to be consolidated. A discovery could focus on common problems faced by each organization, in order to find a common solution.
New policy or regulation. This instigator is especially relevant for government organizations or organizations that operate in an environment affected by regularly changing regulation. Such a discovery would involve studying the populations affected by the change, reviewing the regulation to understand it, and assessing how business operations must change to support the new regulation.
New organization strategy. This driver of change comes internally from the organization (unlike new regulation, which often originates externally). For example, during my time in the UK Government, one government-wide strategy was to become ‘digital by default’, which meant moving away from expensive, paper-based processes to efficient (digital) ones. Discoveries in numerous government departments focused on understanding the needs of their users, as well as the extent of paper-based processing, in order to ensure that a shift to digital was, in fact, efficient and user-centered. Another common strategy is to provide common platforms for those areas of an organization that do essentially the same thing, in order to help the organization become more consistent in what it does, and efficient. Discoveries in these situations would focus on identifying common needs and backstage processes across multiple products and services in order to potentially consolidate them.
Chronic organizational problems. Perhaps sales have been low this year, or satisfaction has been low for several quarters. Often organizations find themselves simply focusing on symptoms (e.g., adding webchat), rather than on causes. A discovery involves inward- as well as outward-facing research to understand why these problems occur and examination into causes to identify the greatest opportunities for improvement.

Common Activities in Discoveries
There are many different types of activities that could be carried out in a discovery. I won’t cover them all, but here are a few that are performed in most discovery phases.
Exploratory Research
Research helps us learn new things about a domain. This type of research is known as generative or exploratory because it generates new, open-ended insights. By carrying out this research, we learn about the problem space (or the opportunity space). A discovery phase does not involve testing a hypothesis or evaluating a potential solution.
At the beginning of a discovery, the research topic might be extremely broad, whereas later it narrows in on those aspects of the problem space that have the most unknowns or present the greatest opportunities.
Common exploratory-research methods include user interviews, diary studies, and field studies with a representative group of users. Surveys can also be used to gather data from a larger group of users; the data can be triangulated with qualitative insights from other methods. Finally, focus groups can sometimes be used in a discovery (although with this method, the findings are frequently contaminated by groupthink).
Stakeholder Interviews
Stakeholders often hold unique knowledge, insight, and data about internal, backstage processes and about the users who interact with them. Interviewing stakeholders provides an additional layer of insight that helps the team understand the scale of the problem and also the viability of later solutions.
Interviewing key people in the organization can provide you with an understanding of:

Key business objectives of the organization, individuals, or teams (These are helpful to determine if and how these broader goals tie-in to the goals of the project.)
Data and insights about how problems affecting users impact backstage work (such as inquiry type and volume, additional processing)
Solutions they’ve tried before that have or haven’t worked, how they implemented them, what other problems they caused, as well as why they were removed (if applicable)

In addition to interviewing stakeholders, including key stakeholders in the discovery process or having them weigh in throughout not only facilitates further buy-in, it also provides more insights.
Workshops
Workshops align team members and stakeholders and are a useful tactic for discovery. Some workshops commonly used in discoveries include:
Kickoff workshops, A kickoff workshop occurs at the beginning of the discovery and aims to create alignment on the objective of the discovery, and when it will be complete. It is normally attended by the client or key stakeholders who are invested in the discovery, as well as by the discovery team itself. It can also include agreement on the roles and responsibilities of each team member during the discovery.
Assumption-mapping workshops. Many teams bring in experts and do data gathering activities in a workshop. They question the validity of certain ‘facts’ and identify the deep-rooted assumptions that need further exploration. Part of this workshop can also include prioritizing assumptions in terms of risk to the outcome of the project. The riskiest assumptions should be prioritized in terms of research activities.
Research-question­–generation workshops. This workshop is similar to the assumption-mapping workshop, and the two are often combined; the team discusses what the unknowns are and drafts research questions. The research questions can be prioritized in terms of their importance and how well they will work to gather the knowledge needed to move forward.
Affinity-diagramming workshops. After performing exploratory user research — such as user interviews, contextual inquiry, and diary studies — insights and observations are transferred to sticky notes and the team works to affinity-diagram them to uncover themes around problems, causes, symptoms, and needs.
Service-blueprinting workshops. Using a large map of the overarching service, the team plots insights from user research and business analysis in one place. They use the map to identify gaps that need further research and major opportunities.
Problem-framing workshops. The team defines the problem as a simple statement that will focus the team going forward. It may also compose ideation statements like How-Might-We’s based on that problem statement.
People Involved
Discoveries are best performed with multidisciplinary teams, where team members are dedicated full-time to the project and are collocated. Depending on the scale of the problem and the discovery activities, the number of people involved and the type of roles they play may vary.
Key roles include:
Someone who can do research: A UX researcher or UX designer needs to plan and carry out user research.
Someone who can facilitate or lead the team: Although self-organizing teams are always best, sometimes team members are new to discovery and may need some direction, or perhaps the team is large and needs some managing. There are many titles that could fill this role, including product manager, project manager, delivery manager, service designer, UX strategist. This role often involves facilitating workshops, ensuring that the team communicates well, and maintaining alignment throughout the discovery process.
A sponsor or owner: Someone from the organization needs to own the project. This person often has a lot of domain and subject-matter expertise, as well as knowledge about who needs to be consulted. The owner should be influential enough to get the discovery team access to other people, teams, or data.
Someone technical: A developer or a technical architect who understands enough technical detail to be able to speak to engineers is needed in order to explore available technologies, their capabilities, and constraints.
In addition to these roles, there could be many others, including business analysts who research business processes, visual designers who explore branding, or interaction designers who work on developing appropriate design principles. It’s best if the team agrees to specific roles and responsibilities at the beginning of the discovery phase.
The Outcome of a Discovery
At the end of the discovery, the team has a detailed understanding of the problem and what outcomes to aim for, as well as where to focus its efforts. They may also have some high-level ideas for solutions that they can take forward and test. In some cases, the end of a discovery might be a decision not to move forward with the project because, for example, there isn’t a user need.
Discovery isn’t about producing outputs for their own sake. However, the following might be produced to help the team organize learnings about the problem space and users:
● A finalized problem statement: a description of the problem, backed up with evidence that details how big it is and why it’s important
● A service blueprint
● User-journey maps
● User-needs statements
● Personas
● High-level concepts or wireframes (for exploring in the next phase)
Summary
A discovery is a preliminary phase of a design project. It can be initiated by many different kinds of problems, involve different size teams, and many research or workshop activities. However, all discoveries strive to gain insight about a problem space, as well as to achieve consensus on desired outcomes.
Reference
UK Design Council’s Double Diamond Model: What is the framework for innovation? Design Council's evolved Double Diamond.
Learn More: Discoveries: Building the Right Thing, a full-day course at the UX Conference."
39,2020-02-09,"When writing usability-testing tasks, you must walk the thin line between telling users too much and too little. Too often usability tasks direct users straight to the site area in which the team is interested, whether it’s a redesigned website or a new piece of content. This approach will usually reap some information about the feature’s usability, but it leaves on the table the potential to learn about the important topics of discoverability and findability. It’s also the reason why some companies are doing lots of testing but still producing unhelpful designs. You can learn more if you start users off with broad instructions before directing them to what you are interested in. Prepare directed tasks that target your points of interest, but give them to participants only if the broad tasks don’t give you the insights that you need.
Stepped Tasks
Definition: A stepped task in a usability test is a task that is part of a set of related tasks (the “steps”) that present progressively more specific and elaborate instructions for the same activity.  The first step in a set of stepped tasks is the vaguest, but observing users trying it may deliver all the information needed by the researchers, if the user performs the task fully.
Think of the second stepped task as a followup question that provides a micro hint intended to steer participants in the right direction if they didn’t interact with the UI areas of interest during the first attempt. The third step (if needed) would give an additional micro hint — and so forth, for as many steps as needed.
An easy way to keep track of subtasks is to add a letter after the task number, or number them as decimal points of the main task; thus, for task 4, you would have steps 4, 4A, 4B (or 4, 4.1, 4.2, 4.3), and so on.  When I am facilitating a usability test, even small things like this help me streamline the process and keep my team in the loop about what we are trying to accomplish in the research. (Note: I never show the users the task numbers because I may give tasks out of order or skip some, which is disconcerting to some people. Also, seeing task numbers can cause users to wonder how many tasks they will be asked to do, which can be distracting.)
Set of Stepped Tasks: Example
Let’s look at an example of a set of stepped tasks in a usability test. Imagine that you want users to discover, find, and use the calendar feature on the corporate intranet.





Research goals: How easy it is to discover, find, and use the calendar?  
			 




Task 4: Your colleague Richard Smith asked you to hold January 12 for a meeting from 2:00PM to 3:00. What might you do to make sure you don’t forget?


What the task accomplishes: Suggests the available functionality without specifically stating that it’s available in the UI, saying what it’s called, where it is, or how to use it




 


User action: Goes to the intranet’s calendar and makes an entry
Facilitator action: Stops the multistep task here because all the relevant information has been collected


What we’ve learned: The user realized there is a calendar without being directly told there is one and used the features we were interested in.




 


User action: Picks up his phone and adds a to-do item
Facilitator action:  Gives the user the next step in the multistep task because the goal of the study has not been reached yet


What we’ve learned: The user decided to use a different strategy to accomplish the broad task, and we’ve got no information about the intranet calendar.




Task 4-A Can you find another way to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?


What the task accomplishes: Suggests the available functionality without directly pointing the user to the calendar feature, naming it, or indicating how to use it




 


User action: Goes to the intranet’s calendar and makes an entry
Facilitator action: Stops the multistep task here because all the relevant information has been collected


What we’ve learned: The user discovered the calendar without being specifically told there is one, and made an entry. (But we also know that our calendar was not the first thing to come to mind, so we do have some work to do.)




 


User action: Says, “I wonder if there is a calendar,” looks for one, but doesn’t find it, and gives up on looking for one the intranet; instead, he says he’d write a physical sticky note and put it on his desk
Facilitator action: Gives the user the next step in the multistep task because the goal of the study has not been completely reached yet (specifically, it’s unclear if the user can find and use the calendar)


What we’ve learned: He realized there could be a calendar without being specifically told there is one, but could not find the calendar. We don’t know if he can use the calendar.




Task 4-B: There is a way to schedule a meeting on the intranet. Can you use the intranet to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?


What the task accomplishes: Tells the user there is a calendar-like feature without specifically saying what it’s called, where it is, or how to use it




 


User action: Goes to the intranet’s calendar and makes an entry
Facilitator action: Stops the multistep task here because all the relevant information has been collected


What we’ve learned: Once told that a calendar-like feature was available, the user found it and made an entry.




 


User action: Says, “Okay, I guess there is a calendar,” looks all over for one, but doesn’t find it and gives up on looking for one on the intranet
Facilitator action: Gives the user the next step in the multistep task because the goal of the study has not been completely reached yet (specifically, it’s unclear if the user can use the calendar)


What we’ve learned:  Even when told that a calendar-like feature exists on the intranet, the user could not find it. We don’t know if he can use the calendar.




Task 4-C: At this point the facilitator takes the person to the intranet calendar and gives him the same task: Can you use this intranet tool to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?


What the task accomplishes: Brings the user to the calendar feature without explaining how to use it




 


User action: Makes an entry in intranet calendar
Facilitator action: Stops the multistep task here because all the relevant information has been collected


What we’ve learned: The user could not find the calendar; once taken to the calendar, he was able to use it to make an entry.




 


User action: Cannot create an entry in intranet calendar or has issues doing so
Facilitator action: Depending on the behavior observed, stops the multistep task here or continues giving additional subtasks to see the use of individual features within the calendar


What we’ve learned:  The user did not realize there was an intranet calendar and could not find it after being told there is one.  Once he was taken to the calendar, he had issues trying to make an entry. (Thus, the design has problems relating to findability, navigability, and to the interaction design of the calendar feature itself.)










Research goals: How easy it is to discover, find, and use the calendar? 




Task 4: Your colleague Richard Smith asked you to hold January 12 for a meeting from 2:00PM to 3:00. What might you do to make sure you don’t forget?




What the task accomplishes: Suggests the available functionality without specifically stating that it’s available in the UI, saying what it’s called, where it is, or how to use it




User action: Goes to the intranet’s calendar and makes an entry




What we’ve learned: The user realized there is a calendar without being directly told there is one and used the features we were interested in.




Facilitator action: Stops the multistep task here because all the relevant information has been collected




Alternate user action: Picks up his phone and adds a to-do item




What we’ve learned: The user decided to use a different strategy to accomplish the broad task, and we’ve got no information about the intranet calendar.




Facilitator action:  Gives the user the next step in the multistep task because the goal of the study has not been reached yet




Task 4-A Can you find another way to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?




What the task accomplishes: Suggests the available functionality without directly pointing the user to the calendar feature, naming it, or indicating how to use it




User action: Goes to the intranet’s calendar and makes an entry




What we’ve learned: The user discovered the calendar without being specifically told there is one, and made an entry. (But we also know that our calendar was not the first thing to come to mind, so we do have some work to do.)




Facilitator action: Stops the multistep task here because all the relevant information has been collected




Alternate user action: Says, “I wonder if there is a calendar,” looks for one, but doesn’t find it, and gives up on looking for one the intranet; instead, he says he’d write a physical sticky note and put it on his desk




What we’ve learned: He realized there could be a calendar without being specifically told there is one, but could not find the calendar. We don’t know if he can use the calendar.




Facilitator action: Gives the user the next step in the multistep task because the goal of the study has not been completely reached yet (specifically, it’s unclear if the user can find and use the calendar)




Task 4-B: There is a way to schedule a meeting on the intranet. Can you use the intranet to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?




What the task accomplishes: Tells the user there is a calendar-like feature without specifically saying what it’s called, where it is, or how to use it




User action: Goes to the intranet’s calendar and makes an entry




What we’ve learned: Once told that a calendar-like feature was available, the user found it and made an entry.




Facilitator action: Stops the multistep task here because all the relevant information has been collected




Alternate user action: Says, “Okay, I guess there is a calendar,” looks all over for one, but doesn’t find it and gives up on looking for one on the intranet




What we’ve learned:  Even when told that a calendar-like feature exists on the intranet, the user could not find it. We don’t know if he can use the calendar.




Facilitator action: Gives the user the next step in the multistep task because the goal of the study has not been completely reached yet (specifically, it’s unclear if the user can use the calendar)




Task 4-C: At this point the facilitator takes the person to the intranet calendar and gives him the same task:  Can you use this intranet tool to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?




What the task accomplishes: Brings the user to the calendar feature without explaining how to use it




User action: Makes an entry in intranet calendar




What we’ve learned: The user could not find the calendar; once taken to the calendar, he was able to use it to make an entry.




Facilitator action: Stops the multistep task here because all the relevant information has been collected.




Alternate user action: Cannot create an entry in intranet calendar or has issues doing so




What we’ve learned:  The user did not realize there was an intranet calendar and could not find it after being told there is one.  Once he was taken to the calendar, he had issues trying to make an entry. (Thus, the design has problems relating to findability, navigability, and to the interaction design of the calendar feature itself.)




Facilitator action: Depending on the behavior observed, stops the multistep task here or continues giving additional subtasks to see the use of individual features within the calendar.





To seamlessly move participants to the next task in a set of stepped tasks, give them verbal reassurance. For example, you might say:

Thank you for doing that [as I write some notes to demonstrate that I am capturing what I saw]. I am getting a lot of very helpful notes. There is a way to schedule a meeting on the intranet. Knowing that…
Thank you. That was very helpful. Do you have any comments about doing that? [Wait for comments.] I learned a lot from watching you do that. Now I am going ask you to try this. [Take them to the calendar area.]

Then you could gesture to the print task:
Can you use the intranet to make sure you keep January 12 from 2:00PM to 3:00 for a meeting with Richard Smith?
Stepped Tasks to Test Features with Ubiquitous Labels
Stepped tasks are helpful in many cases, but the main situations when they are useful are when you want to avoid using a feature name in the task or you are testing discoverability or findability.
Discoverability of a UI feature or content refers to how easily users realize that the feature or content exists in the interface. Telling test participants upfront that a feature exists in the task instruction means that you miss the opportunity to learn whether they could discover it on their own. That’s why, with stepped tasks, the first task is the broadest. But if participants don’t discover the feature, you can slowly direct them to it in subsequent steps. At that point, you will have learned that the feature is not discoverable.
Findability of a UI feature or content refers to how easy it is for users to locate and access it, once they know it exists. Telling test participants upfront how to find a feature means that you miss the opportunity to learn whether they could find it on their own.
Avoiding Words in the UI
In general, stay away from using the same terms in the tasks as are used in the UI. This guideline can be difficult to follow because ideally you have already done some user research and chose terminology that matches the user’s mental model. Say for example, users think of the feature as a graph, so you named the button in your app Create Graph. It’s hard to come up with a different word for the graph concept in order to test it. But it’s worth doing it — because using those same words in the task description will prime the user to look for that name in the UI. Sometimes test participants will read a task and then type in the search box one or more words that appear in the instruction. Or, they will scan the UI for that exact word in the task description. Because of that, they may find the answer faster and the task may seem easier than it is, giving a false sense of confidence and satisfaction and misleading both the user and the researchers.
It can also be difficult to find a descriptive activity or phrase that replaces a commonplace word without generating an overly convoluted and abstracted task description. You may try it anyway, but also prepare a stepped task that either tries a different synonym or describes what the feature may do instead of mentioning its name. For example, to get participants to use a graph feature, you may give users numbers in a table and ask them to convey the data to a team, or show them a picture of a graph and ask them to reproduce it.
In our calendar example above, a calendar feature will help save time slots — hence the task instruction. The task doesn’t mention the words calendar or schedule (a term used in the UI), so we can see if users would realize they have a calendar, think to use it, and find the command to create a meeting.


As stepped tasks become more specific, the opportunity to learn about discoverability and findability decreases.




As stepped tasks become more specific, the opportunity to learn about discoverability and findability decreases.


Why Not Just Make Up a New Task During the Test Session?
You may be asking: Why not write broad task descriptions, and, then just wait and see what happens during the test? If users cannot find or discover the feature, the facilitator could react in the moment by adding a task as needed. That is an option, and sometimes, in fact, you will be forced to take this route because you didn’t predict that there could be an issue until you saw it. But I prefer to avoid this approach when possible, for several reasons:

It’s hard to make up a good task under pressure. Crafting test tasks can take multiple tries and reviews from others. Doing it in seconds with the user watching and waiting, when you are also focused on all the other responsibilities you have as the usability-test facilitator, is difficult and can result in a task that is too leading, ineffective, or otherwise poor. And while the stepped task examples in this article seem simple, part of what makes them so is that the facilitator predicted them and is prepared for them, versus being surprised in the test and needing to react then.  
A written task reinforces that a usability test is an exercise in observation, not a conversation or an interview.  It’s difficult to watch people in a usability test struggle. But we usually need to do it in order to learn about the issues in the UI and later fix them so our many users don’t also encounter them. Still, some test facilitators tend to help test participants too soon because they feel uncomfortable watching them struggle.

Reinforce Observation, Not Conversation
The more the facilitator talks to the test participant, the more the participant gets used to it and expects the usability test to be a conversation or interview.  A good usability test has a rhythm; it goes like this:

The facilitator hands the user a task.
The user reads it aloud.
The user works on it (usually while thinking aloud)
The facilitator stays mostly quiet.
The task ends.
The facilitator asks followup questions.
The facilitator hands the user the next task.

This rhythm is broken when the facilitator needs to make up a new task.
Handing users a written task reinforces to them that the usability test is mostly an exercise in observation by the facilitator, not an interview or conversation.
To a lesser degree, reading a prompt that you have prepared as a stepped talk will keep you from saying too much, or getting into the habit of assisting because you are uncomfortable.
Conclusion
Some companies spend a lot of money on research, including user testing, but still produce poor designs. There are many reasons for this state of affairs. One is that the tasks given in user testing are too focused — an issue that I have observed countless times in my many years of coaching and guiding other researchers. By “too focused tasks” I don’t mean leading tasks, — that is, priming users, giving instructions, or divulging something about the UI that jades the task.  Rather, I mean sound tasks that don’t take into account factors such as the tested UI feature’s discoverability and findability.
Testing discoverability and findability isn’t easy, but we should still strive to do it. Broad tasks give the users the opportunity to discover and access UI features by themselves, make the testing less directed or leading, and give you more insights about how the user naturally interacts with the UI. But, if users don’t get to the feature of interest on their own, a stepped task can bring the user there so you can also learn about the feature’s usability. Since each test participant is expensive, we want to wring as much insight as we can out of every session."
40,2020-01-26,"The critical incident technique (CIT) is a systematic procedure for obtaining rich, qualitative information about significant incidents from observers with firsthand experience, which in turn helps researchers understand the critical requirements for individuals, processes or systems.
Definition: The critical incident technique (CIT) is a research method in which the research participant is asked to recall and describe a time when a behavior, action, or occurrence impacted (either positively or negatively) a specified outcome (for example, the accomplishment of a given task).
The instance reported by the participant is known as an ‘incident’. In order for the incident to be critical, the participant must be confident that the event had a causal relationship with the outcome (which is the focus of the study). An example of a critical incident question is given below:
Please think of an Agile project that you worked on that was successful. Please describe a specific time when someone did something (or something happened) that positively contributed to the success of that project.
This method was formally introduced to the social sciences by John Flanagan, in a seminal paper published in the Psychological Bulletin in 1954. The technique was developed and finessed in numerous studies undertaken by Flanagan and fellow psychologists from the US Aviation Psychology Program during the second World War, and later by the American Institute for Research and the University of Pittsburgh. Since the publication of Flanagan’s paper, the CIT has become popular in the social sciences and has found applications in human–computer interaction research, because it facilitates the gathering of many detailed ‘incidents’ (behaviors or events). These are useful in understanding critical requirements for roles, systems, and processes. For example, the CIT has been used to highlight characteristics of successful personnel (such as leaders, nurses, doctors, air-traffic controllers), as well as critical requirements for processes (such as training programs and services) and interfaces.
In UX, the CIT is often used in user interviews. However, critical incidents can also be captured using questionnaires, focus groups, or structured diary studies. Flanagan believed that researchers carrying out ethnographic-style research could also document critical incidents, although little has been said on how this approach should be implemented.
Using the critical incident technique is slightly different from asking a standard, example-style question in a user interview. The table below shows some questions that could be given to employees to learn about their experience using an enterprise tool. A critical incident question is compared to other example-style questions.



The participant is asked:
Type of question




Tell me about a time where you used the tool in your job.
Example question: the participant is asked to provide an example; there is no direction from the researcher as to what kind of example. The answer could be anything that happened to come to the participant’s mind.


Tell me about the last time you used the tool in your job.
Specific example question: The participant is asked to describe the most recent time. This is not necessarily a critical incident, just the most recent.


Tell me about a particular time when you used the tool in your job where it helped you to be effective in your work.
Critical incident question: the participant is asked to think of a specific incident that was critical to the accomplishment of a task.



Usually, in a critical incident interview, the participant is given time to think of each incident before describing it, as recall can often take time. The interviewer also has carefully scripted followup questions meant to elicit enough factual information about the incident. The interview could look something like this:



For the next series of questions, I’d like you to focus on how you use the tool in your work.
Interviewer introduces the focus of the study




What are some things you do with the tool?
How often do you use the tool?
When do you use the tool?



Check tool-use criteria



Please think of, and tell me about, a particular time when you used the tool, and it made you effective in your work.

Critical incidents (positive)





What task were you doing at the time?
Why did you choose to use the tool?
In what way did the tool make you effective?


Clarification questions


Is there another time you can think of where you used the tool and it helped you to be effective in your work?

Seek out further incidents



Now conversely, please think of, and tell me about, a particular time when you used the tool, and it made you ineffective in your work.
Critical incidents (negative)




What task were you doing at the time?
Why did you choose to use the tool?
In what way did the tool make you ineffective?



Clarification questions



Is there another time you can think of where you used the tool, and it made you ineffective in your work?

Seek out further incidents




Generally speaking, when the researcher seeks critical incidents, the participant is asked for events that demonstrate both positive and negative effects on the outcome. These questions are usually separate, as in the example interview above. However, in some cases, the researcher may ask for a positive or negative case at the same time and allow the participant to choose which incident to begin with. When asked separately, it is typical to begin with asking for positive incidents in order to begin constructively.
When the critical incident technique is used in research, each participant could contribute many incidents. It is quite possible that hundreds of incidents (sometimes thousands) are collected through numerous interviews which then need to be coded. When codes are well saturated (e.g., there are many incidents for each code), researchers can be fairly confident that they have documented the core requirements for the object of study. In the enterprise-tool example, these requirements could include ease of access (users need to be able to find and open the tool quickly), responsiveness (the tool needs to respond quickly — for example, autosave shouldn’t slow users down), or nonintrusive updates (updates should not interrupt the user at work).
Pros and Cons of Using the Critical Incident Technique
This method has some advantages, as well as some key disadvantages for usability research.
Pros

Quickly uncovers system issues
Captures incidents over a long timeframe: Participants can go back as long as they can remember. As a result, incidents could span years. This is an advantage over observational research, which is often time restricted.
Captures information about rare or uncommon incidents: When observing users in their domain, key incidents are not always witnessed because they are rare or uncommon. The CIT makes discovery of these incidents possible.
Emphasis on more-important issues rather than less-important issues. Most other methods usually collect a preponderance of low-importance issues, simply because they tend to be more numerous. Of course, there’s no guarantee every reported critical incident is actually important, but significant events will likely be easier to recall than minor incidents.
Flexible: The CIT can be applied in interviews, focus groups, and surveys.

Cons

Relies on memory and pure recall: Memory is fallible, and so details can often be lost, or critical incidents can be forgotten. Recall is also challenging and even stressful for some participants, particularly in a face-to-face setting.
Doesn’t represent typical usage: Often, participants recall extreme events, but small usability issues and typical usage are rarely mentioned in CIT interviews.

When deciding whether to use the critical incident technique, consider what your research goal is and whether a usability test or observation in the field will be better suited for achieving it. If using the critical incident technique in your research, ensure that you know what type of incidents you want to learn about. Take the time to compose an interview script and pilot it to check that your questions aren’t too prescriptive, vague, or ambiguous. (Anecdotally, researchers have found that varying the types of words used in the critical incident question can affect the type of incidents recalled, so think carefully on how not to lead participants). Lastly, you will always learn more about how people use existing interfaces and what their pain points are if you carry out observational research, such as contextual inquiry or usability testing.
Summary
The critical incident technique (CIT) is a useful methodology to uncover critical requirements for people, systems, and processes. When using the CIT, ensure that you are clear on the kind of incidents you want to study, prepare an interview script, and pilot it. Complement CIT interviews, focus groups, or surveys with observational research (like contextual inquiry and usability tests) to get an accurate picture of the usability of systems, products, or services.
Reference
Flanagan, J.C. (1954). The Critical Incident Technique. Psychological Bulletin, 51(4), 327-357."
41,2019-12-29,"Many organizations have flourishing design teams who conduct user research on a regular basis. This fact is largely positive, but the popularity of lean UX methods means that many of those doing user research have no formal research background. Some teams may cut corners or follow shoddy research practices which can harm participants, causing unnecessary distress and suspicion during research sessions. In some cases, participants are willfully or unintentionally deceived about the research purposes or how their data will be used. These tendencies usually occur when the team carrying out user research lacks a solid foundation of research ethics.
Definition: Research ethics is the careful consideration of the rights, well-being, and dignity of people involved in research activities.
Research ethics applies to how teams design studies and conduct them, as well as to how they use participant data after the study concludes. Establishing research-ethics expertise is paramount, especially for organizations which:

Research sensitive topics — for example, bereavement, mental health, chronic illness, addiction
Research with vulnerable people — such as children, prisoners, people with cognitive impairments, people with low literacy

Regardless of the type of research you conduct or whether a specific UX-research role exists in your organization, anyone doing research with people should have a good grasp of research ethics. As organizations scale their user research practices, research ethics usually becomes part of the organization’s ResearchOps team. These teams offer support and services such as research training, guidance, and oversight.
Elements of an Ethically Mature User-Research Practice
Ethically mature organizations are likely to have:

A code of conduct: Some organizations write their own code of conduct so that it is as relevant as possible to their user-research context. Other organizations might adopt a professional body’s code of conduct, and direct user researchers to this document. They also cite adherence to the code of conduct in all participant communication.
Research-ethics training for all people who carry out user research: This type of training should be included in onboarding, elearning, or ad-hoc training courses.
Guidance documents: Often, organizations have guidelines on how to collect consent, how to write good consent forms and information sheets, as well as how to approach researching certain topics or user types.
Standardized consent forms and information sheets: Mature organizations have standardized study documents which contain areas where researchers can fill in the details about the study, while keeping the core language consistent.
Ethics experts: These could be people on the ResearchOps team or volunteers who deliver training, provide advice, or share knowledge with the team.
Data policies for user research: Organizations have a specific data policy for UX teams carrying out user research; this policy covers relevant data protection laws and how the organization complies with it. It includes what constitutes personal data produced during or from user research activities, where it gets stored, and how it is handled.

If your organization lacks these elements, start by standardizing your consent documents and processes for collecting and storing research data. Look at a few example codes of conduct from professional bodies (such as the American Psychological Association) and communicate the code of conduct to everyone doing user research. Improving research ethics at your organization means you’ll protect your participants’ interests while ensuring your user research practice flourishes.
The basics of research ethics are fairly easy and simply require an awareness of the issues and an agreement to do no evil. For example, if you know that it’s best to keep participants anonymous, then it’s common sense that you shouldn’t report participants’ names when quoting them. But what about if the research is internal and your stakeholders are likely to know the participants (for example, because they helped to recruit them in the study or because they could identify the employee quoted by job title). What should you do? Those are the kinds of issues that benefit from documented guidance and experts.
Assess the Maturity of Your Organizations’ Research Ethics
Answer the following questions to assess the ethical maturity of your user-research practices. If a question isn’t relevant to you, skip it. If you answer yes to any of the following questions, give yourself 1 point. If you answer no or not sure, you get 0 points.
Knowledge and Education

Do people carrying out user research receive ethics training at your organization?
Is there a research code of conduct (whether your own or one adopted from a professional body)?
Is there written guidance on research ethics accessible to those doing user research?

Informed Consent

Do you have standardized consent forms to use in your organization?
Do you give participants a consent form to complete before any research takes place?
Do you provide an information sheet (which contains details about the study) to the participant in advance of the research?
When concealment or deception are not necessary conditions of the study design, do you provide a clear and understandable explanation of why you are doing the research and what is involved?
Do your consent forms contain an accurate account of what will happen to the participant’s data?
Are your consent forms written in plain language, that your participants can understand?
Do you provide a means of contact, which allows participants to follow up with you to ask questions about the research?
Do you tell participants at the beginning of the session that the research is voluntary, and that they can withdraw at any time?
When conducting in-person research, do you give participants their incentive at the beginning of the session, so they can withdraw without being penalized?
Do you have alternate versions of consent forms, appropriate to your specific user types? (e.g. low literacy, visually impaired, and so on)
If your participants are children, do you obtain the guardian’s consent prior to engaging with the child?
If your participants are children or people with cognitive impairments, do you collect assent from them in addition to obtaining a responsible adult’s consent?
If you research in the European Union or with EU citizens, do you have GDPR-friendly consent forms?
If you use deception or concealment, do you provide debriefs after the research activity concludes?

Participant Welfare

Do users leave your sessions the same or better off after attending the research?
If conducting research on sensitive subjects, do you have resources for support available for participants, should they need it (e.g., phone number of a charity that can help or of a free counselor)?

Recordings and Photography

Do you make participants aware of what kinds of photos or recordings will be made (e.g., their vocal feedback, their face, photos of them or of their computer), and how they will be used?
When audio/video recording or photography is not a requirement for study participation, can people opt out of being recorded or photographed?
When carrying out data collection in public spaces, do you put up signs and cordon off areas where recordings or photography will happen?

Observers

Do you tell participants in advance of their session if there will be observers?
Do you give participants a choice of whether they are observed before the observation begins?
Do you tell participants the identity of any observers that they may know, so they can make an informed decision about whether to take part in the study, or allow observation?

Data Handling

Do you have a data-protection policy for handling research data?
Do you have a data-retention policy (that specifies for how long you will store participants’ data)?
Do you delete personal data (such as recruitment information and video recordings) as soon as you no longer need it?
Do you store participants’ personal data in a secure place, where only the relevant people who are supposed to access the data have access?
Do you collect research data on encrypted, password-protected devices?

Sensitive Subject Matter

Before carrying out research on sensitive subjects, is the research design reviewed by someone else to determine if it is beneficial and won’t harm participants?
Do you (or your researchers) receive special training on how to research sensitive topics or run studies with vulnerable participants?
Before studying a sensitive subject, do you conduct desk research or speak to subject-matter experts on how to protect the interests of your participants?
Is research appropriately designed so as to mitigate negative impacts and distress on the participants?

Researcher Welfare

After conducting research on sensitive topics, are researchers provided with access to free counseling, if they need it?
Are researchers provided with basic training on how to remain safe when conducting research in the field?

How Did You Do?
Divide the total number of points you received by the total number of questions that you considered relevant to you and did not skip. Multiply that number by 100 to get a percentage.



90% and over

Excellent ethical maturity
			Your organization demonstrates an excellent level of maturity when it comes to ethical research practices. If there are areas you were unsure about, or if you answered no to certain questions, consider how you can make small improvements in these areas. Do this especially if you are currently trying to expand research in your practice, or if you’re going to start researching new domains, with vulnerable populations, or topics that could cause distress.


75-89%

Good ethical maturity; some improvements could be made
			It seems like you’re covering a lot of the basics of ethical-research practices, but perhaps there’s room for improvements to your consent-gathering processes? Or, maybe you need to start investing in training and establishing visible standards of accountability in your organization.


<75%

Poor research ethics; problems need addressing
			Gaps in research ethics exist and need to be addressed. Perhaps new teams carrying out user research aren’t using appropriate consent forms or perhaps teams researching with vulnerable populations are in need of supervision. Whatever the gaps, it’s time to start addressing them as you continue to do more research or scale up your user-research practices.



Learn more about research ethics in our one-hour seminar: Ethics for User Research."
42,2019-12-15,"Are you an introvert who hates meetings and derives pleasure from thinking in solitude? Or do you get excited when you can discuss ideas with others and work side by side with them toward a common goal? Working alone and working with others have each their own advantages and disadvantages. In this article, we define a collaborative technique that combines the two in order to maximize the work quality and efficiency: diverge and converge.

Definition: The diverge-and-converge collaboration method has two stages: (1) a diverge stage, during which team members work independently to produce individual insights and (2) a converge stage, where they discuss the results of the diverge phase as a group in order to decide on some collective output.

When team members diverge, they research, analyze, generate, or design independently, without discussing thoughts or findings with others.  When team members converge, they share their findings, insights, ideas, or designs with one another.
How Diverge-and-Converge Works
The technique can be incorporated into any collaborative activity which involves consolidating the input of multiple people into a single collective output. A facilitator should introduce the diverge-and-converge method before or at the beginning of the meeting, to keep participants on track.
Depending on the goal of the activity, the diverge part can happen before the workshop, with participants working by themselves at their own pace, or during the workshop, with all participants spending a dedicated period of time to do the individual work in parallel.
For example, if the goal is to speed up data collection for a large research study, several UX researchers may run study sessions separately, using a shared research guide. After the data has been gathered, team members will converge in a meeting to discuss research findings. Alternatively, if the goal of the activity is to generate design ideas for a particular problem, participants in an ideation workshop may all spend the first part of the meeting sketching ideas individually and then they may converge and consolidate those ideas. 
The amount of time dedicated to each phase depends on the complexity of both the independently generated input and of the collective output. For example, running usability studies can take hours, sometimes days. Analyzing research findings for a large data set can also take days, sometimes weeks. Consequently, both the diverge and converge portions for research can extend over the course of a month. On the other hand, an ideation session is best run when timeboxed — that is, within a designated amount of time, like 5 to 10 minutes — in order to emphasize quantity over quality of ideas.  
The most important piece of running diverge-and-converge exercises is to set expectations and ground rules before running these activities. Doing this early can help ease tension between group members, increase contributions, and avoid groupthink. Here are some rules and expectations you should share with your group before running a workshop.

Strictly enforce divergent and convergent time as “quiet” and “talking” time.
	Some people are naturally chattier than others or like to bounce ideas off of others or “think aloud” before solidifying their own ideas. Not only does this disrupt the quiet time needed for individual thinking, but it can also inadvertently sway the output of the diverge phase (or even that of the converge phase) by adding bias.
Ban the words “no” or “but.”
	People often become defensive and reluctant to share when they hear others openly rejecting their ideas or contributions with words like  “no” or “but.” Banning these words reduces inherent tension, increases willingness to discuss, and, paradoxically, and enables dissenting contributions to be heard (because they won’t sound so offensive).

Why Use Diverge-and-Converge
Many of the issues that emerge in collaborative workshops happen precisely because team members are not given the time to first sit with their ideas and insights before subjecting them to the opinions of others. When converging first, the input is still very “malleable” rather than being substantial enough to have a nuanced discussion.
Diverging first enables team members to:

focus on the task at hand without distractions
quickly gather data or organize information
tackle multiple parts of a project
interpret or analyze information at their own pace (which is crucial for people who speak a different native language, or people with disabilities or impairments)
generate creative ideas in a safe space
counteract groupthink (i.e., being influenced by others’ ideas)


When groupthink happens, an influential group member’s opinion (in the comic above, the yellow lightbulb, A) may directly or indirectly squelch dissenting opinions (the green lightbulb, B) of others.

Converging afterward enables teams to:

identify patterns and uncover new perspectives
frame problems differently
align and build a shared understanding of the problem to be solved
create strong team relationships for future collaborative efforts
build empathy among team members
increase the odds of solving the problem or finding objective truth (More and varied solutions will have a higher chance of solving the problem than one idea will.)

Diverge-and-converge will ultimately produce a higher-quality result than would have resulted from simply generating one good idea and running with it. Several years ago, we conducted a measurement study of an example of parallel design (which was equivalent to the “diverge” stage for a user-interface design project). Four different designers independently designed solutions to the same user-interface problem. The measured usability of the best of these 4 designs was 56% higher than the average usability of all designs. (This number can be interpreted as an estimate of the gain to be had from having 4 different ideas, instead of just an average one.) Even better, when the ideas from the 4 designs were merged into a single design, the result gained an additional 14% in measured usability compared to the single-best of the original designs. This outcome shows that even the best design wasn’t perfect. The 3 designs that scored lower in overall usability still contained some good ideas.
In the case study, the final, converged design was 70% better (56%+14%) better than the average of the individual (diverged) designs. Obviously, this was just a case study, and your mileage will vary. However, the example shows that the gain from this method can be quite substantial. It does cost a little more time to employ two steps instead of proceeding directly to the solution, but the ROI usually makes it worthwhile.

Each person’s perspective, in isolation, is rarely a complete understanding of the problem. In this adaptation of the parable of the blind men and the elephant, each of the three painters sees a different view of the animal, and all three of these perspectives are incomplete if they do not incorporate the perspectives of others.

When to Incorporate Diverge-and-Converge
Diverge-and-converge activities are most useful during collaborative efforts which are subject to skewed results due to personal bias, groupthink, or the HiPPO effect (highest paid person’s opinion). Here are some common use cases:
Collecting and Analyzing Research Findings
Research is just as subject to bias and groupthink as any other activity. Diverge-and-converge can reduce bias and increase empathy with both participants and coworkers.
A diverge-and-converge technique like affinity diagramming can be used in a variety of circumstances (e.g., analyze findings from sessions, build cognitive maps and empathy maps) to cluster research findings and analyze themes. Or, if time does not allow a full affinity-diagramming workshop, a quicker version may simply involve verbally sharing with team members individual notes made while observing a user session or reading an interview transcript.

Researchers can avoid projecting their own biases into their research and speed the process along by gathering data independently and converging to comprehend their insights together.

Mapping Workshops
Diverge-and-converge methods can be used to build any types of maps — customer-journey maps, experience maps, service blueprints, and so on. The diverge stage usually involves participants individually reflecting upon and labeling the data used to build the map, and the converge stage consists of aggregating individual insights into a final artifact.

Teams can diverge to jot down their insights at their own desks and transfer those insights onto sticky notes on a journey map or service blueprint.

Design-Thinking Workshops
Diverge-and-converge activities are a common practice in design-thinking workshops in order to maximize contributions and ensure alignment. For example, this method can be used to create need statements or for ideating on new design ideas to prototype next.

Teams can diverge to ideate or create prototypes, then converge to prioritize which ideas or prototypes get implemented or tested.

Conclusion
Diverge-and-converge is a powerful tactic that eliminates bias, broadens insight, adds depth to conversations. When incorporated at the right moments, it can also help you get many team members’ hands in the user data, code data quickly, and increase the likelihood of creating the right solution to the right problem. After all, many hands (and brains, when given the chance to think) make light work."
43,2019-12-15,"User research can be difficult to conduct in Agile environments and, due to time or budget constraints, teams tend to make assumptions about their users in order to move forward in the development process. In an ideal world we want to make decisions based on actual data, but sometimes we have to make assumptions first and test them later.
An assumption is something we take on faith or as a best guess despite a lack of proof. We can’t know everything about users or their environment before starting a project, so some assumptions are usually necessary. The danger with assumptions is that they are often treated the same as facts later in the project, when team members forget that the assumptions have a shaky base. But decisions based on incorrect assumptions can have serious consequences, so teams need to be aware of their assumptions by properly documenting them and creating action plans to turn them into real data.
With all of the moving parts in an Agile process, formally tracking questions and assumptions is commonly neglected. In this article, we show how Agile teams can track and test assumptions.
Creating a Knowledge Board
There are four types of user-related statements and activities that typically inform Agile development:

Questions about user behaviors, attitudes, or motivations
Assumptions about these behaviors
Research to test assumptions
Facts based on collected user data

The same statement can start as a question, become an assumption, and then become a documented fact based on user research. (During the progression through these phases, the information rarely stays constant: usually, as the evidence solidifies, the conclusion needs to be modified.) Let’s discuss each of these individually.

1. Questions: What We Don’t Know
Agile teams have a product backlog that they pull from in order to determine what will be committed in the upcoming sprint. This product backlog is comprised of new features and technical debt in the form of user stories and epics. The backlog is where we want to formulate research questions within our teams to set ourselves up for user research.
Brainstorm these research questions together with your team so that everyone has a shared understanding of your knowledge gaps. You won’t know the answers to these questions yet, but they will help you formulate a plan for uncovering the answers. Do this consistently as part of your backlog-refinement meetings so that you uncover gaps throughout the entire product lifecycle.
Let’s look at a couple of examples:




 


Example 1


Example 2




Feature from Backlog


Provide offline functionality


Build a user profile




Research Question


Will our users want to use our application offline?


How much personal information will users provide?




Next Steps


Do additional research before building functionality that users may not need


Determine how much information to collect and gauge how much users trust our product




Once you can guess the answer to a question or have an idea of what users might do, it’s ready for the second phase.
2. Assumptions: What We Think We Know
Phase two of this process is making a guess — or an assumption — as to the answer of one of your research questions. You may have some data to back up your guess — whether it’s a persona, an insight from early user research, or something your client or stakeholder passes along to you. This data does not directly answer the research question, but it allows you to make a semi-informed guess and move forward. Wild guesses are the weakest assumptions, whereas estimates derived from empirical information or insights are better, though still not the same as proven facts.
Document the assumptions you’re making and add them to the corresponding research question, whether you’re doing so in a text editor or on a Kanban board. Aim to include the following information for each research question:

The assumption(s) you’re making
Where each assumption came from (client, persona, interview, etc.)
Next steps for research

The next steps for research will list whatever research you need to test your assumptions. They could include usability testing, field studies, website analytics, or additional user interviews.
Using our examples from phase one, here’s how we might document what we think we know:




 


Example 1


Example 2




Research Question


Will our users want to use our application offline?


How much personal information will users provide?




Assumptions


Users may be in a basement without access to internet

Data collected from stakeholder on October 31st



Our target market will likely provide name, email address, phone number, and current location

Based on case study from our closest competitor





 


Users don’t want to use their personal cellular data at work

Insight from user interviews during discovery phase



 




Next Steps for Research


Conduct field studies to observe users who may use the application in a basement and determine what features will be useful for them


Run A/B tests of two different account-creation forms to determine the most applicable account fields that users will complete when signing up




3. Research: Testing Assumptions
Once you’ve documented assumptions and you know the next steps for gathering data, it’s time to conduct research and test your assumptions. After the designated research has been conducted, you will move forward in one of two directions:

If your assumptions were wrong and you need additional information, refine them and continue documenting them until you feel confident in your data.
If your assumptions are validated, you’re ready to move on to phase four.

4. Facts: What We Know
Now that you can confidently answer the research questions documented in phase one, you can turn them into factual statements. It’s important to keep these documented and continue to revisit them throughout the product lifecycle, so that your team is aligned.
Looking at our same research questions as above, we can now provide answers based on the data we collected from our studies:




 


Example 1


Example 2




Research Question


Will our users want to use our application offline?


How much personal information will users provide?




Findings from Research


In our field studies, participants matching our primary persona spent half of their workday in a basement without stable access to a network connection.


Participants who encountered the layout with the phone-number field were more likely to abandon the task altogether, whereas participants who encountered the layout without the phone number were more likely to complete the form.




Factual Statement


Since our application allows these users to optimize a large portion of their workflow, we have concluded that users will want to use our application offline.


We’ve concluded that users will provide a name, email address, and current location, but not a phone number when creating a profile.




Documentation Methods
Just like there is no definitive rulebook when it comes to Agile backlogs, there is no one right way to document the difference between assumptions and facts throughout your process. Choose a method that will mesh well with your team’s current process and is easily accessible to the team.
Kanban Board
This is my recommended method, as it’s easy to see exactly where knowledge is in its journey. Create a column for each phase of the process. A digital Kanban board is ideal for easy access by all team members, but you may also consider a physical Kanban board for colocated teams for constant visibility.

Trello.com: A Kanban board is ideal for tracking at which stage each research question sits. Cards are movable from one column to the next; you can also specify due dates, owners, and tags for each question.

Project-Management Software
Some teams find it easiest to document knowledge in the same way that they document their product backlog. They create a separate backlog of research questions and document their progress in the same way they would in the product backlog. The details of each question will live within each item and can be transferred over to the product backlog easily if applicable.

Jira Software: The knowledge board can live within your existing project-management software.

Text Document
If your team is more comfortable using Google Drive, Dropbox, or Sharepoint as a repository of information, you may consider keeping a running document tracking the assumption information. If you choose this route, however, make sure to use the outline feature of your text editor so it’s scannable and easily contributable.
Conclusion
There are many instances in Agile where we have to make assumptions, whether it’s because we’re short on time or we don’t have instant access to users. The goal is to turn these assumptions into validated facts through user research.
A second goal is risk management, based on recognizing your “known unknowns.” Even if you can’t turn all unknowns into facts, you’re not going to be completely blindsided if one of them turns out to be different than what you had guessed (because you knew that your guess could be wrong). Much worse to be hit by the infamous “unknown unknowns,” which are the things you never even thought of. Such surprises can truly doom a project, because you can’t manage a risk that you aren’t aware of. Reducing this risk is a key reason to spend time documenting what you don’t know.
Reference
Kirkland, Kieron. 2017. How a knowledge Kanban board can help your user research. Retrieved from https://userresearch.blog.gov.uk/2017/02/16/how-a-knowledge-kanban-board-can-help-your-user-research/"
44,2019-12-01,"Usability testing is a popular UX research methodology.

In a usability-testing session, a researcher (called a “facilitator” or a “moderator”) asks a participant to perform tasks, usually using one or more specific user interfaces. While the participant completes each task, the researcher observes the participant’s behavior and listens for feedback.

The phrase “usability testing” is often used interchangeably with “user testing.”
(One objection sometimes raised against the phrase “user testing” is that it sounds like researchers are testing the participant — we never test the user, only the interface. However, the term is intended to mean testing with users, which is exactly the point of empirical studies.)
Why Usability Test?
The goals of usability testing vary by study, but they usually include:

Identifying problems in the design of the product or service
Uncovering opportunities to improve
Learning about the target user’s behavior and preferences



Usability testing helps us to uncover problems, discover opportunities, and learn about users.


Why do we need to do usability testing? Won’t a good professional UX designer know how to design a great user interface? Even the best UX designers can’t design a perfect — or even good enough — user experience without iterative design driven by observations of real users and of their interactions with the design.
There are many variables in designing a modern user interface and there are even more variables in the human brain. The total number of combinations is huge. The only way to get UX design right is to test it.
Elements of Usability Testing
There are many different types of usability testing, but the core elements in most usability tests are the facilitator, the tasks, and the participant.


A usability-testing session involves a participant and a facilitator who gives tasks to the participant and observes the participant’s behavior.


The facilitator administers tasks to the participant. As the participant performs these tasks, the facilitator observes the participant’s behavior and listens for feedback. The facilitator may also ask followup questions to elicit detail from the participant.


In a usability test, the facilitator gives instructions and task scenarios to the participant. The participant provides behavioral and verbal feedback about the interface while he performs those tasks.


Facilitator
The facilitator guides the participant through the test process. She gives instructions, answers the participant’s questions, and asks followup questions.
The facilitator works to ensure that the test results in high-quality, valid data, without accidentally influencing the participant’s behavior. Achieving this balance is difficult and requires training.
(In one form of remote usability testing, called remote unmoderated testing, an application may perform some of the facilitator’s roles.)
Tasks
The tasks in a usability test are realistic activities that the participant might perform in real life. They can be very specific or very open-ended, depending on the research questions and the type of usability testing.
Examples of tasks from real usability studies:

Your printer is showing “Error 5200”. How can you get rid of the error message?
You're considering opening a new credit card with Wells Fargo. Please visit wellsfargo.com and decide which credit card you might want to open, if any.
You’ve been told you need to speak to Tyler Smith from the Project Management department. Use the intranet to find out where they are located. Tell the researcher your answer.

Task wording is very important in usability testing. Small errors in the phrasing of a task can cause the participant to misunderstand what they’re asked to do or can influence how participants perform the task (a psychological phenomenon called priming).
Task instructions can be delivered to the participant verbally (the facilitator might read them) or can be handed to a participant written on task sheets. We often ask participants to read the task instructions out loud. This helps ensure that the participant reads the instructions completely, and helps the researchers with their notetaking, because they always know which task the user is performing.
Participant
The participant should be a realistic user of the product or service being studied. That might mean that the user is already using the product or service in real life. Alternatively, in some cases, the participant might just have a similar background to the target user group, or might have the same needs, even if he isn’t already a user of the product.
Participants are often asked to think out loud during usability testing (called the “think-aloud method”). The facilitator might ask the participants to narrate their actions and thoughts as they perform tasks. The goal of this approach is to understand participants’ behaviors, goals, thoughts, and motivations.


In this usability-test session, the participant sits on the left, and the facilitator sits on the right. The participant uses a special testing laptop, which is running screen-recording software. The laptop has a webcam to capture the participant’s facial expressions and is connected to an external monitor for the facilitator. The facilitator listens to his feedback, administers tasks, and takes notes. The photo captures the moment after the participant’s task, when the facilitator is asking him followup questions.


Types of Usability Testing
Qualitative vs. Quantitative
Usability testing can be either qualitative or quantitative.

Qualitative usability testing focuses on collecting insights, findings, and anecdotes about how people use the product or service. Qualitative usability testing is best for discovering problems in the user experience. This form of usability testing is more common than quantitative usability testing.
Quantitative usability testing focuses on collecting metrics that describe the user experience. Two of the metrics most commonly collected in quantitative usability testing are task success and time on task. Quantitative usability testing is best for collecting benchmarks.

The number of participants needed for a usability test varies depending on the type of study. For a typical qualitative usability study of a single user group, we recommend using five participants to uncover the majority of the most common problems in the product.
Remote vs. In-Person Testing
Remote usability tests are popular because they often require less time and money than in-person studies. There are two types of remote usability testing: moderated and unmoderated.

Remote moderated usability tests work very similarly to in-person studies. The facilitator still interacts with the participant and asks her to perform tasks. However, the facilitator and participant are in different physical locations. Usually, moderated tests can be performed using screen-sharing software like Skype or GoToMeeting.
Remote unmoderated remote usability tests do not have the same facilitator–participant interaction as an in-person or moderated tests. The researcher uses a dedicated online remote-testing tool to set up written tasks for the participant. Then, the participant completes those tasks alone on her own time. The testing tool delivers the task instructions and any followup questions. After the participant completes her test, the researcher receives a recording of the session, along with metrics like task success.



In remote unmoderated usability testing, the flow of information changes because the facilitator does not interact with the participant in the same way as an in a moderated test. The testing platform takes on the role of the facilitator, administering tasks to the participant. The researcher designs the study and upload task instructions on the platform, and then reviews the data after it’s collected, usually by observing video recordings of the tasks.


Cost of Usability Testing
Simple, “discount” usability studies can be inexpensive, though you usually must pay a few hundred dollars as incentives to participants. The testing session can take place in a conference room, and the simplest study will take 3 days of your time (assuming that you have already learned how to do it, and you have access to participants):

Day 1: Plan the study
Day 2: Test the 5 users
Day 3: Analyze the findings and convert them into redesign recommendations for the next iteration

On the other hand, more-expensive research is sometimes required, and the cost can run into several hundred thousand dollars for the most elaborate studies.
Things that add cost include:

competitive testing of multiple designs
international testing in multiple countries
testing with multiple user groups (or personas)
quantitative studies
use of fancy equipment like eyetrackers
needing a true usability lab or focus group room to allow others to observe
wanting a detailed analysis and report about the findings.

The return on investment (ROI) for advanced studies can still be high, though usually not as high as that for simple studies.
NN/g Resources for Usability Testing

Qualitative Usability Testing (Study Guide)
User Testing: Why & How (Video)
How to Conduct Usability Studies (Report)
How to Set Up a Desktop Usability Test (Video)
How to Set Up a Mobile Usability Test (Video)
Turning User Goals into Task Scenarios for Usability Testing (Article)
Usability Testing for Mobile Is Easy (Article)

Facilitating a Usability Test
For hands-on training and help honing your facilitation skills, check out our full-day course on usability testing.

Talking with Participants During a Usability Test (Article)
User Testing Facilitation Techniques (Video)
Team Members Behaving Badly During Usability Tests (Article)
Thinking Aloud: The #1 Usability Tool (Article)

Recruiting Participants

Recruiting Test Participants for Usability Studies (Article)
Why You Only Need to Test with 5 Users (Article)
How Many Test Users in a Usability Study? (Article)
Usability Testing with 5 Users: Design Process (Video)
Usability Testing with 5 Users: ROI Criteria (Video)
Usability Testing with 5 Users: Information Foraging (Video)
Employees as Usability-Test Participants (Article)
Using Usability-Test Participants Multiple Times (Video)
Obtaining Consent for User Research (Article)

Remote Usability Testing
For detailed help planning, conducting, and analyzing remote user testing, check out our full-day seminar: Remote Usability Testing.

Remote Usability Tests: Moderated and Unmoderated (Article)
Remote Moderated Usability Tests: How and Why to Do Them (Article)
Remote Unmoderated User Tests: How and Why to Do Them (Article)
Tools for Unmoderated Usability Testing (Article)

Special Usability Testing Studies or User Groups

Quantitative vs. Qualitative Usability Tests (Article)
Conducting Usability Testing with Real Users’ Real Data (Article)
How to Conduct Usability Studies for Accessibility (Report)
Paper Prototyping: Getting User Data Before You Code (Article)
Paper Prototyping 101 (Video)
Beyond the NPS: Measuring Perceived Usability (Article)
International Usability Testing (Article)
Usability Testing with Minors (Article)

Printable Usability Testing Poster
You can download and print a poster that explains usability testing (available below for your preferred size printer paper: A4 size or US letter size, or you can scale the printout for bigger sheets)."
45,2019-10-27,"Many UX teams rely on remote usability testing to efficiently get design feedback from users. There are two types of remote user testing:

Moderated remote testing involves a researcher meeting with a participant via remote screen-sharing software, which allows the researcher to provide instructions, observe the user’s interaction with the design in real time, and ask followup questions specific to that participant’s session.
Unmoderated remote testing does not require a researcher to attend each test session; instead, a software application provides instructions to users, records their actions, and may ask them predetermined followup questions.

Is Unmoderated Testing Right for Your Project?
Unmoderated studies do not include any direct interaction between the researcher and the study participants, which is both their biggest benefit and their greatest drawback.
Because there’s no need to schedule an individual meeting with each participant, unmoderated testing is usually much faster than a moderated study. It may be possible to launch a study and receive results within just a few hours. Unmoderated studies also allow you to collect feedback from dozens or even hundreds of users simultaneously. And for international studies, you don’t have to get up at an ungodly hour to match users’ time zone.
However, there are important limitations of unmoderated usability testing: 

Early-prototype testing is difficult without a moderator to explain and help participants recover from errors or limitations of the prototype.
Without a moderator, participants tend to be less engaged and behave less realistically in tasks that depend on imagination, decision making, or emotional responses.

To better understand this second limitation, think, for example, about the difference between shopping and purchasing. Shopping can include many different types of research and comparison — there’s no single ‘right’ way to do it. In order to shop realistically, participants must first imagine themselves needing that product, then pay attention to details and make comparisons. A participant who just pretends to shop and isn’t very motivated often will glance only at a few products and quickly select one that seems reasonable. But, in real life, consumers who are spending their own money on a product they actually need behave very differently. Of course, a moderated study is not totally realistic either, but because participants are aware that the moderator is observing them, they will be socially motivated to fully engage with the task. Thus, this social pressure compensates for the lack of personal motivation in moderated studies — and even more so in in-person studies.
Unmoderated studies work best for evaluating live websites and apps or highly functional prototypes. They are appropriate for studying activities that don’t require a lot of imagination or emotion from participants.
Unmoderated research requires even more meticulous planning that a moderated study, since you can’t rely on human judgment to adapt the study procedures on the fly. For an unmoderated usability study, you’ll need to go through all the steps below:

1. Define Study Goals and Participant-Recruitment Criteria
Choosing software should not be the first step in unmoderated research. Before you decide which testing software to use, you should get a clear idea of what you hope to accomplish by doing the study. Then you can select a tool which has the capabilities best suited for your research goals, rather than limiting your study to fit within the technical constraints of a particular tool. Clearly articulated study goals allow you to identify must-have requirements for the testing software.




If the study goal is to…


The study tool must be able to:




Compare how long it takes people to complete a signup and checkout process on your site vs. your competitors



Measure time on task
Aggregate time on task data
Display individual times to inspect outliers
Export time data to a standard spreadsheet format (e.g., .cvs)





Help a large team understand why users struggle to complete the checkout process within a mobile application



Record screen and audio of native mobile applications
High-quality video capture, to see detailed interactions
Easily create clips and compilations of videos to share with the team




Examples of how study goals may affect tool selection 

Goals vary from study to study, and a tool that is well suited to one study might be not at all effective for another. Knowing the study objectives is essential in order to make a good tool choice.
In this stage, you will also need to think about the types of participants you want to include in your study. What should their demographic be? Where do they need to be located? Will they be users new to your system or experts? Will they match a particular persona or user group in your target audience? These questions usually are determined by your study objectives and can also inform your tool selection.
2. Select Testing Software
For unmoderated studies, the software that administers the test is absolutely crucial to getting useful results. The software must guide the participants through the session and record what happens. It may also control the selection of study participants.
Fortunately, there are many different unmoderated testing services available, with varying combinations of functionality at different price points. The plethora of choices means that you can now be more critical in selecting a tool that suits the requirements of your project. (Features and pricing change frequently for many of the unmoderated testing tools, so be sure to compare them to your needs at the time of your study.)
It’s definitely worth your time to thoroughly investigate and pilot test tools, because migrating a study to a different system due to a technical limitation discovered after you launch a study is not fun! (Neither is trying to integrate data collected by two different tools.)
3. Write Task Instructions and Followup Questions
Many unmoderated testing services include study templates with generic example tasks. Don’t blindly copy them. The tasks that you give participants to do on your site or application should be highly specific to your situation. Generic tasks, such as “What is the purpose of this site,” are unlikely to give you good insights: to really assess the usability of your system you will need to write your own tasks.
In our experience training other companies to run their own remote usability tests, writing tasks is where most researchers fail in getting the results they need from their studies.
To write good task instructions for an unmoderated study, first articulate what it is you want the user to accomplish (such as: use the help section to answer a question, upgrade an account, or save an article to read later). Then, describe that objective with instructions that are specific, realistic, and actionable — without including hints that make the task too easy. You’ll need different types of task instructions depending on whether you’re doing a qualitative or quantitative study.
In unmoderated studies, the activities that you want the participants to conduct have to be even more carefully written than the tasks for moderated sessions. Participants cannot ask for clarification if they don’t understand the instructions and you can’t ask them to try again if they do the wrong thing. If users misinterpret your instructions and perform the wrong task, your test is wasted. Unmoderated task instructions should also explicitly tell users when they should stop; remember, the moderator won’t be there to ask them to move to a different task.
You should also meticulously plan any followup questions. These can include quantitative questions, in which participants rate the subjective difficulty or satisfaction of an activity. Or you may ask open-ended questions which prompt users to describe specific parts of the experience. Carefully choose how you phrase your questions; broad wording such as “How would you describe this brand?” may lead unmoderated participants to talk about their past experiences instead of the system they just used.
4. Pilot Test
A pilot test is a trial session that you run before your actual study begins, in order to discover any problems with your study design or procedure. Pilot testing is a good idea for all user studies but it’s especially important for unmoderated studies, because there won’t be a moderator available to fix problems while the study is running. Even the most thorough pilot testing can’t catch everything, but you can often detect and fix problems with:

Task instructions that people misinterpret because the wording is ambiguous


Tasks that are missing, or presented in the wrong order (especially in complex studies with many tasks)
Prototypes that are missing functionality or content necessary for the study
Incompatibilities or technical limitations which prevent your testing software from capturing the data you want to record — especially if you are trying to test an intranet or native mobile application
	(If your testing software records data via a web-browser extension, then browser or website restrictions may prevent it from capturing entire pages or sites or lead to low-quality recordings that are difficult to analyze. If you’re using a testing service for the first time, do a quick test of the recording process before you even bother setting up your tasks.)

You can discover some of these problems by going through the study yourself as though you are a participant, but others will not become apparent until you have real participants using their own equipment. Make sure to analyze the data collected in your pilot study!
5. Recruit Participants
There’s no point in watching people use your system if they don’t match your target audience. Make sure you have some control over who participates in the test, either through screening questions, or by recruiting your own participants.
Some tools only offer unmoderated user testing with participants from their panel, while others will provide you with a URL to distribute to your own pool of testers.
Using a provided participant panel is fast and easy (especially if your product is something that is relevant to a broad consumer audience). Panel participants are familiar with the study software, too, and can participate in your test as soon as they have time. Almost every panel includes some basic demographic filtering, but most studies get better results if you screen participants using questions about their behavior, not just about their age or gender. If the experience you’re testing is relevant only to people who meet specific criteria such as driving a car or shopping online regularly, make sure to use a tool which allows you to write your own custom-screening questions.
The downside of using panel participants is that many do these studies so frequently that they’ve learned to focus on certain aspects of the design and look for things to critique. To compensate for possible “professional testers,” recruit extra participants and exclude data from people who didn’t seem honestly engaged with the activities.
Being online means that you can test users on the other side of the globe as easily as people on the other side of the street. Many unmoderated testing services now have panels which include participants from all around the world. If your site targets international customers, unmoderated testing is a great way to reach a wide variety of locations. Just remember that if you recruit participants to complete the test in other languages, you’ll also need a researcher or translator fluent in that language to interpret the results.
In summary:

B2B sites, sites that target elite or rich customers, and other services with narrow target audiences usually can’t use panels and must recruit the test participants themselves.
B2C sites and other services that target a broad audience usually get faster and cheaper results by using a panel.
You don’t always need the biggest panel, but if you require users from a particular region some panels may take a very long time to fulfill your study. (If you really need a specific audience, discuss your requirements with the testing service or panel manager in advance, and if they’re at all vague about whether they can do it, consider using a different service.)

6. Analyze Results
Unmoderated studies can quickly accumulate a LOT of data, so you’ll need an organized, analytical approach to turn this data into actionable insights about your design.
If you collect qualitative data, such as video and audio recordings of participant actions and comments, you’ll need to review each session recording. Users’ verbal and written comments can be misleading, so you have to watch their behavior in order to understand what works or doesn’t. In a moderated study you can follow along with the participants as they conduct activities, but during unmoderated studies you need to be able to watch a recording afterwards.
Screen recordings are helpful, but, in the absence of an audio recording, it is easy to miss why certain behavior occurred. If nothing is happening on the screen, is it because the participant is reading, or is she thinking about where to click next? The audio recording of participants verbalizing their actions is essential. (Recordings of the participants’ webcam to capture their facial expressions are also nice to have, but not essential. It can be more difficult to recruit participants who have a webcam and are willing to be recorded, so make it a secondary requirement.)
In each recording you’ll want to identify problems, questions, and both positive and negative reactions to the design. This process can be relatively quick if you have only a few recordings to review, but for large studies with dozens of participants, video analysis becomes extremely time-consuming. If you expect to analyze large studies (or to carry out several different smaller studies), look for an unmoderated testing tool which offers robust features video analysis, particularly:

Tagging videos with timestamped notes as you watch them
Aggregating, exporting, sharing, and visualizing the notes you’ve added to your recordings
Producing short clips or highlight compilations of important moments in your recordings

Of course, you can make notes and video clips even if this functionality is not built in to the unmoderated testing tool — but unmoderated testing services which include these features don’t necessarily cost much more than barebones tools that lack them. If you will analyze more than a few hours of recordings, it’s well worth it to pay slightly more for a tool that speeds up your data analysis.
For extremely large qualitative studies, consider tools which can collect some quantitative measures or which offer automatic transcription. These features don’t eliminate the need to carefully review your recordings, but they can certainly expedite the process by directing your attention to specific recordings that are likely to be significant (such as recordings where users had low satisfaction ratings or recordings where particular keywords were mentioned).
If your study is primarily quantitative, your analysis will be quite different. Metrics such as success rate, task time, and subjective ratings will be automatically collected by your study tool. But to ensure your conclusions are accurate you’ll need to review your data and:

Clean the data by identifying and excluding inaccurate values. For example, if a few task times are much shorter or longer than the others, investigate why, and exclude the outliers from your analysis if the values are inaccurate because the participants didn’t fully complete the task, or did the wrong task.
Perform statistical tests to assess the significance of your results (especially when your goal is to compare multiple designs or tasks).
Generate data visualizations to help communicate your findings to others.

Some unmoderated testing tools can automate the process of excluding outlier data points and many have built-in data visualizations charts. But, since cookie-cutter charts don’t always show the most important results, if you plan to do quantitative analysis, make sure your tool includes the ability to export your data so you can perform your own analysis using Excel or specialized statistics software.
Summary
Unmoderated research requires less work than moderated testing during the session, but it requires meticulous advance planning before the study begins. You can learn more about remote user testing and the relation between this method and in-person testing in the full-day Usability Testing training course, which includes hands-on details on writing tasks, facilitating sessions, and more."
46,2019-10-06,"Survey questions are delicate things. Even small details in wording can affect how your respondents interpret and answer them. A carelessly written question can ruin a study, so it’s worth a little extra time to perfect your survey. ​
Case Study: Survey on How People Use the Web
Recently, we decided to replicate a study conducted 21 years ago by researchers at Xerox PARC. The original study investigated how the information found online affects people’s decision making. The study consisted of a large-scale survey in which 3,292 respondents described in detail a situation where online content impacted their decisions or actions.
Today, people rely even more heavily on information found on the web than 20 years ago. From purchasing a house to deciding where to eat for dinner, the web helps users make a huge variety of decisions. Thus, we replicated the study to see if the important online information-seeking behaviors have changed over two decades.
In the Xerox survey, the researchers asked the following single question:

Please try to recall a recent instance in which you found important information on the World Wide Web, information that led to a significant action or decision. Please describe that incident in enough detail so that we can visualize the situation.

While we wanted the responses to be comparable to the 1998 study, we realized that we would likely need to tweak the question’s wording to ensure we’d collect information that reflects today’s use of online services. Through 4 rounds of pilot testing the survey, we were able to refine the question.
1st Round of Testing
We wanted to keep the question as close to the previous version as possible to make a valid comparison.
For the first version of the question, we only changed “World Wide Web” to “online” to reflect current terminology. Google Ngram showed that in 1998, the word “online” only appeared about 1.5 times more than the phrase “World Wide Web” in the Google Books corpus, but in 2008, the frequency of the word “online” was already more than 100 times higher than that of “World Wide Web.” Also, Google Trends showed that the related queries of the word “online” included “online films” and “online games”, while the ones of the phrase “World Wide Web” included “World Wide Web Wikipedia” and “who created World Wide Web,” suggesting that today people use the word “online” to refer to the things they can do on the World Wide Web.
Thus, we rephrased the survey question as follows. 

Please try to recall a recent instance in which you found important information online, information that led to a significant action or decision. Please describe that incident in enough detail so that we can visualize the situation.

For this phase, we recruited 11 participants who filled out a written survey and we collected their verbal feedback at the end of the survey.
Four of these pilot participants reported that this question was too general, and they were not sure what we wanted. This was probably not a problem 21 years ago, but now it is because of the pervasiveness of the internet. According to a report by USC, the time spent online increased from 9.4 hours per week in 2000 to 23.6 hours per week in 2016.  An article by Clickz showed that, on average, in 2019, people spent 6 hours and 42 minutes online per day. Gathering information online has become such a frequent and mundane task for many people that they struggled to pick out a specific instance to report.
To address this problem, we added an explanatory sentence in the second design.
2nd Round of Testing

Please try to recall a recent instance in which you found important information online, information that led to a significant action or decision. Please describe that incident in enough detail so that we can visualize the situation.
A significant action or decision can be any change in your plans, thoughts, or actions that you consider to be meaningful.

We thought that giving a bit more explanation on “significant” could ease people’s concerns that their actions might not meet our standards. This version of the question was tested with 5 users; the survey was remote and unmoderated.
In this second pilot, people were constrained by the explanatory text and talked only about the changes they made because of the online information. For example, a participant wrote, “I looked at the weather on the app on my phone before I left for work in the morning. It said the temperature was colder than I expected it to be. So I put on a warmer coat and a hat”. Another job-seeking participant talked about how online information “changed her applying strategy” and made her focus on certain types of companies. Almost all the responses were related to some specific changes, but change should not be a necessary aspect of a significant decision or action. We realized that adding explanations to ""significant"" could bias the respondents’ answers. We decided to remove the clarifying sentence and try another approach.
3rd Round of Testing
For the third round of testing, we tried adding a multiselect question before the main question about the respondents’ significant activity.

Which of the following online activities have you done in the past month? (Please select all that apply)

□ Bought something 
□ Watched a TV show or movie
□ Planned a vacation
□ Sent an email
□ Posted on social media (for example, Facebook or Instagram)
□ Researched a topic


We hoped that this question could help users reflect on their recent online activities, and that this process may help them answer the following question. We carefully balanced different kinds of activities — from entertaining to serious ones. We invited 4 users to fill out the revised version of the survey, and also conducted a cognitive walkthrough with 3 participants to gain insights on the language of the survey.
Unfortunately, all of the participants in this group ended up reporting activities that sounded too similar to our multiselect responses. Seven users were all talking about the research they did online, like “researching information about tax base transfer in California” and “looking up information regarding weight-loss surgery.” Not all significant decisions or actions have to be related to research, so we realized that the multiselect responses were priming our respondents. Namely, the last response option in the first question, “researched a topic"",  primed the participants to come up with research-related answers in response to the second question. We decided to remove the priming question from the survey.
4th Round of Testing
At this point, we were quite confident that the biggest problem was that people had too many online activities to choose from. They needed reassurance that they could choose just one to report. That could help explain why pilot participants were confused when presented with the original question and why they were easily influenced by the changes we’d tested: people weren’t sure which decisions counted as “significant” and which ones didn’t, so they tried to find clues from other information researchers provided. This was probably not a problem during the original PARC study because, at that time, the Internet was not pervasive and didn’t impact people’s lives as much.
Based on this insight, we revised the question again, to include a clarification that could help respondents if many instances came to mind.

Please try to recall a recent instance in which you found important information online, information that led to a significant action or decision. Please describe that incident in enough detail so that we can visualize the situation.
If you can recall several such instances, please describe the one that was the most important to you.

With this addition, we reassured users that they could reply to the question with the one example they believe is the most significant to them.
We piloted the survey online and collected 50 responses.
The 50-person pilot survey went well; we got a diverse set of responses. Besides researching to make decisions, a participant mentioned that “Buying my current phone, Google Pixel 2 XL. Kept seeing commercials on Hulu about it”, which showed that an ad influenced her decision. Another response described how she got a ticket for one of her favorite-band’s concerts because of a notification she received on her phone.
Satisfied with the detail and variety of the data we had collected, we decided to run the full study based on this version of the main question. We collected 700 responses that we analyzed both quantitatively and qualitatively to better understand the current profile of online information-seeking behavior. (Our findings from the final study will be reported in a subsequent article.)
Tips for Survey Design

Make sure that your research questions can be investigated with your survey methodology.

Surveys cannot answer all research questions. They are good at helping us capture attitudinal data, but not behavioral data. The details and the contextual information they can provide are also limited. In our case, we wanted to identify online information-seeking behaviors that could lead to significant decisions and actions. A survey can address this goal. But if we want to understand why people choose certain types of information-seeking behaviors instead of others or when and where they engage in these behaviors, surveys are not appropriate. Instead, user interviews or field studies can work better in these situations.

Avoid priming or asking leading questions.

Keep the language of the survey questions neutral. People are social animals who can interpret subtle clues and try to behave as (they assume) researchers want them to, even subconsciously. As we saw in this case study, minor changes in phrasing the same question or adding another question before it can result in dramatically different responses.

Run pilot studies. You can test several versions at the same time.

Sometimes, you may not be able to tell if your survey language is neutral enough until you run it with real people. For your very first pilot, your colleagues or people in a coffee shop could act as testers. However, conduct at least one round of pilot testing with respondents from your demographic of interest — don’t rely just on your coworkers. Ask your participants to think aloud as they are completing the survey, to help you identify any interpretation issues or potential leading questions. Testing 5–10 users for each version of your pilot should work fine.

Pay attention to the timing of collecting the responses.

Sometimes the time when you send out an online survey can impact the number and quality of your answers. In our study, half of the participants were sent the survey on a weekend and half on weekdays. We did that to avoid biased results related to the timing of response collecting. If your users are likely to be busy during the daytime, sending out a survey at 9:00 A.M. may prevent you from collecting high-quality data.
Poor phrasing, ambiguity, or the wrong sequence of questions can easily result in skewed survey results. Iron out any such issues before you spend the money to collect your data. Like user-interface designs, surveys need to be tested. In fact, a survey instrument is a design, so treat it as such.
References
Morrison, J.B., Pirolli, P. and Card, S.K., 2001, March. A taxonomic analysis of what World Wide Web activities significantly impact people's decisions and actions. In CHI'01 extended abstracts on Human factors in computing systems (pp. 163–164). ACM.
USC Annenberg Center for the Digital Future. The 2017 Digital Future Report.
ClickZ. Internet growth + usage stats 2019: time online, devices, users."
47,2019-09-22,"These days, we’re spoiled for choice when it comes to remote user research. The vast array of tools available at many different price points can be overwhelming ⁠— especially since many of the descriptions of tools are virtually indistinguishable.
Every remote-research tool promises to deliver user insights, but they do so in very different ways. If you’re trying to choose a tool, this list can help you understand exactly what you’re getting and make sure the service you pick is a good fit for your research needs.
All of the user-research tools compared in this article allow you to do studies that are:

Remote: Participants can be located anywhere, the entire study is completed online.
Unmoderated: Participants complete the study on their own, without a researcher guiding the session.
Task-based: Participants receive instructions to complete specific tasks.
Behavioral: Users’ actions are recorded by the tool so you can tell what people did and whether they successfully completed the tasks.
Interactive: Participants can test on live sites or interactive prototypes rather than just seeing a static image.
Do-it-yourself: You can plan and carry out your own studies, without using the tool’s research-consultancy services.

Together, these qualities allow you to conduct studies that are similar to in-person usability testing, but without the moderator meeting individually with each participant. Unmoderated testing is often a good option when you have limited time or budget or when users are geographically dispersed.
2 Types of Data in Unmoderated Usability Testing
It’s important to understand that different types of data that can be collected by various tools. Some tools record unstructured qualitative data in the form of video recordings; some tools collect highly structured quantitative data about tasks, and some tools can gather both of these types of data.



Type of Data
Qualitative
Quantiative




How it is collected
Video recordings capture sceen activity and think-out-loud narration by the test participant
Metrics are recorded for dimensions such as time spent, success rate, satisfaction, and perceived difficulty


What it reveals
What participants did and why they did it
How common certain problems, behaviors, and opinions are among participants


Challenges
Unstructured video recordings are time-consuming to watch and analyze.
Metrics do not reveal causes of behavior; low participant motivation or inaccurate self-reported data can cause misleading metrics


Useful when you need to...


Understand why a problem is happening and get ideas for how to fix it
Evaluate new designs when you have no idea what problems users might encounter
Inspire empathy for users within your team or organization




Track usability over time
Quickly and accurately assess precise frequency of problems
Quickly assess subjective participant reactions of a large group of participants
Persuade stakeholders who prefer quantitative data





Make sure you have a clear idea of what you hope to achieve through your research. Then you’ll be able to decide whether you need qualitative recordings, quantitative data, or both.
Tools and Data Types
The chart below lists 15 different tools which can be used to conduct unmoderated usability testing. The position of each tool in this chart indicates both the type of data collected by the tool and how long the tool has been in existence. Generally speaking, tools which have been available longer are more mature, with more robust features. Also, though there is never any guarantee, a longer-lasting service is less likely to go under in the middle of your study and make any already-collected data evaporate into a lost corner of the cloud.

The diagram above indicates several unmoderated-testing tools which combine both types of data collection. The features listed for these tools are quite similar, so it can be difficult to distinguish between tools by reading their descriptions. Despite these surface similarities, there are important differences between these services, which are easier to understand if you’re aware of the history of each system. UserZoom and Loop11 initially focused on quantitative metrics, and later added qualitative recordings; while UserTesting, Userlytics, and Userfeel initially focused on video recordings and later added quantitative metrics. As you might expect, the tools’ original functionality tends to be more robust, while the newer features are more limited. (These distinctions are represented in the chart above by the placement of each tool’s name, which is positioned closer to its original data type.)
It’s also worth noting that the metrics-only tools included in this diagram, Maze and KonceptApp, are both designed to be used for testing prototypes and are not suitable for testing live websites or applications. Although they can simulate interactions, such as letting test participants click a link and move to another screen, this behavior requires you to actually build or import an interactive prototype.
Feature Comparison
Once you’ve determined the type of data you want to collect, review the precise capabilities of the tools you are considering. Some features which may be important to the design of your study are listed in the table below.



Recruiting
Study Design & Setup
Qualitative Data
Quantitative Data






Participant panel
Set quotas for multiple types of users
Custom screening question(s)
Multiple Languages
Bring your own users
External panel integration




Test websites (desktop, mobile, and prototype)
Test native mobile apps
Test static wireframes or screens
Separate instructions for each task
Persistent access to task instructions
Custom welcome & final screens
Copy a past study
Branching (skip logic) to personalize tasks
Randomize task order
Professional research services available
Shared projects for team collaboration
Supports moderated testing




Record screen & audio
Record face
Timestamped notes
Export individual session notes
Export all project notes
Download individual recordings
Download entire project
Share recordings via url
Produce video highlights compilation
Automatic transcription
Browse video thumbnails




Simple rating questions
Custom ratings and written questions
Task time
Filter out speeders and cheaters
Rate of task abandonment
Data export (csv or xls)
Data-visualization charts
Success rate by url or click location
Click heatmaps
Clickpath across screens





Features that you may need to consider when selecting remote unmoderated usability-testing software
As a starting point for your comparison, we’ve prepared a list of tools and features we were able to confirm for each tool. This spreadsheet provides a detailed feature comparison for 15 tools for unmoderated user testing. 
When to Use NONE of These Tools
This article has focused on tools for unmoderated usability testing, but that's not always the right research method. For example, moderated usability testing (whether in-person or remote) is more appropriate for evaluating an early-stage prototype or to identify usability issues in interface or tasks that are so complex that it’s necessary to provide personalized directions and ask followup questions to fully understand users’ behavior. Also, all participants in unmoderated studies are people who were willing and able to install a browser extension or application and carry out a fairly complicated online interaction. If your target audience includes a lot of users who wouldn’t opt to participate in this type of research, you’ll need to use other methods to find and observe these people.
Finally, some research questions are better answered with a completely different type of study, such as an A/B test, 5-second test, interview, field study, card sort, or tree test. (Some services — most notably UserZoom — support a wide range of such research methods.) You should always figure out which research method best addresses your question before choosing any tool.
Full-Day Seminar on Remote Studies
For detailed help planning, conducting, and analyzing remote unmoderated user testing, check out our full-day seminar: Remote Usability Testing."
48,2019-08-18,"Eyetracking Research
Eyetracking equipment can track and show where a person is looking. To do so, it uses a special light to create a reflection in the person’s eyes. Cameras in the tracker capture those reflections and use them to estimate the position and movement of the eyes. That data is then projected onto the UI, resulting in a visualization of where the participant looked.
This research can produce three types of visualizations:

Gazeplots (qualitative)
Gaze replays (qualitative)
Heatmaps (quantitative)



This gaze plot shows how one participant processed a web page in a few minutes. The bubbles represent fixations – spots where the eyes stopped and looked at; the size of the bubble is proportional with the duration of the fixation.



Sorry, your browser does not support the video tag.

This video clip is a gaze replay — it shows how one participant’s eye processed a page on Bose.com.


This heatmap is an aggregate from many participants performing the same task. The colored areas indicate where people looked, with red areas signifying the most amount of time, followed by yellow and green, respectively. To get this type of visualization, we recommend having at least 39 participants perform the same task on the same page.


We use this eyetracking data to understand how people read online and how they process webpages. Our eyetracking research has yielded major findings such as:

Banner blindness: People avoid elements (like banners) that they perceive as ads.
Uncertainty in the processing of flat UI elements: Extremely flat UIs with weak signifiers require more user effort than strong ones do.
Gaze patterns: Users tend to process different content in different ways. Two of the most common patterns are the F-pattern and the layer-cake pattern.

In an eyetracking study, the tracker has to be calibrated for each participant. Every individual has a different eye shape, face shape, and height. As a consequence, the tracker has to “learn” each participant before it can follow their gaze. Once the machine is calibrated, the participant has to stay roughly in the same position — moving too far side to side or leaning in or out can cause the tracker to lose calibration.
Materials List
In this desktop eyetracking study of how people read online, we used the following materials:

Desktop eyetracker with built-in monitor (Tobii Spectrum)
Powerful PC desktop tower
Large monitor for facilitator and observer
Two keyboards
Two computer mice
External speakers
External microphone
Printed task sheets
Printed facilitator script
Printed consent forms
External hard drive for backing up data
Two tables, side-by-side
Two chairs
Envelopes with incentives for participants (cash)

Lab Setup
Room
For this specific study, we rented out a 4-person office space in a WeWork coworking facility. This office provided enough space for a participant, a researcher, and 1–2 observers, without getting too crowded. 
PC, Monitors, & Eyetracker
We used a powerful PC desktop tower, connected to two monitors:

Participant’s monitor (with the eyetracking cameras attached)
Facilitator’s monitor (showing the participant’s gaze in real time)

The participant and facilitator each had a separate mouse and keyboard, so they shared control of the PC. The facilitator controlled the PC only for setup, calibration, and to stop and start the recording.


The facilitator’s monitor, keyboard, and mouse are set up to the left of the participant’s monitor, keyboard, and mouse. In this room, we chose to place the eyetracker in the corner because it was out of the range of direct overhead lights (which can sometimes cause problems with the tracking). The facilitator’s monitor was angled away from the participant, to prevent her from seeing it.




During each session, the participant (right) completed tasks using what looked to her to be a normal monitor. Meanwhile, the screen was shared on the facilitator’s screen with real-time gaze data. The facilitator (me, left) monitored the gaze calibration, watched user behavior, and administered tasks and instructions as needed. I also took some notes, but as eyetracking facilitation requires multitasking through many activities, those notes were very light. Primarily, I used my notes to record any issues I saw in the gaze data or to remind myself to go back and rewatch particularly interesting incidents. Human eyes move fast, so the bulk of eyetracking analysis work has to happen by slowing down the videos and watching them several times.


Using a separate monitor for the facilitator was optional, but had two major benefits:

Space: Having a separate monitor allowed the facilitator to observe the task without sitting too close to the participant.
Real-time gaze data: The facilitator’s monitor showed a red dot and line representing the participant’s gaze; these were useful for monitoring the participant’s calibration. (If the participant shifts in her seat, the tracker can lose her eyes. Lost calibration means that the gaze visualization won’t show what the participant was looking at — making the data unusable. By monitoring the gaze data in real time, the facilitator can catch the problem and recalibrate as needed.)

I’d recommend using a large, high-definition screen for the facilitator’s monitor, in order to easily see which words the participants were (and weren’t) reading on the screen.


This screenshot shows the facilitator’s view during a session. The white dots in the upper right corner represent the position participant’s eyes as seen by the eyetracker. If the dots disappear or move too far from the center, the facilitator knows she needs to intervene to save the calibration. The real-time gaze data is shown on the screen as red dots and lines (center). This provides another piece of information for monitoring calibration. For example, if the participant seems to be reading a headline, but the red dots are appearing a half-inch below that headline, that could be an indication that the calibration is off.


Tables and Chairs
The monitors, keyboard, mice, and tasks sheets were spread across two tables that we pushed together. The facilitator sat in a rolling chair, so she could easily move closer to the participant to adjust the eyetracking equipment as needed or to hand him a task sheet. The participant sat in a fixed (not rolling) chair. This little detail won’t necessarily matter in a normal usability test, but matters a lot in eyetracking — you don’t want to give participants any reason to move out of range and ruin the calibration.
Task Sheets
Task sheets are another detail that can sometimes cause problems in eyetracking studies. When participants look down at a task sheet, they’re turning away from the eyetracker. When possible, it’s nice to have the task instructions delivered either verbally or through the eyetracking software itself. 
In the past, we’ve found that referencing task sheets can break the calibration, but we did not have a problem with it in this study: when people looked back up at the screen to perform their task, the tracker was able to refind and track their eyes. Be aware that this capability may differ depending on the tracker you use.
Eyetracking Now vs. 2006
The setup for a desktop eyetracking study hasn’t changed very much in the past 13 years. Compared to a photo of our setup in a 2006 eyetracking study, our 2019 version looks quite similar —  two monitors, an eyetracker, and a PC tower.
However, even though the structure of the system may be similar, the technology has definitely changed from 2006 (check out those little low-resolution monitors!). Compared to 2006, eyetracking tools have certainly improved the calibration process and they’ve gotten better at hiding the eyetracking mechanisms in the eyetracker (thanks largely to smaller cameras).


In 2006 Kara Pernice (right) facilitated an eyetracking study with a very similar setup to our 2019 study.


Tips for Your Eyetracking Study
Think through your goals for the study. What data are you looking to gather?

Gaze replays and anecdotes: If you’re looking for video clips and qualitative insights, a lightweight tool might work for you. Instead of the complex setup we used for this study, you could consider using lightweight USB-connected eyetracker systems or special eyetracking goggles (particularly for testing mobile designs). Those types of studies can be much easier to run than full-fledged quantitative eyetracking studies. Be aware, though, that those products are often not capable of producing gazeplots or heatmaps. Lightweight systems also tend to be less precise —  instead of a little dot showing you which word someone is reading, you might get a big bubble that just shows you which paragraph he’s looking at.
Gazeplots: If you want static visualizations of where individuals looked on a page, you could use a setup similar to ours, but you wouldn’t need as many users. You could collect data from 8-12 participants. (For regular qualitative usability testing, it’s usually best to test with around 5 users, but for a qualitative eyetracking study you’ll want to recruit a few extra test users to account for calibration problems and other technical issues.)
Heatmaps: If you want static visualizations that summarize where many people looked at a page on average, you’ll need to run a quantitative study like we did. We usually recommend having 39 participants complete the task you want to use for a heatmap.

If you’re planning an eyetracking study, it’s important to think through all the little logistics details. Running a day or two of pilot testing is a good way to work through all the potential hurdles you’ll encounter. Based on our experiences, you should absolutely expect technical difficulties.
I also highly recommend dedicating 1–2 days just to set up your equipment, before your pilot testing. Traditional eyetracking tools are complex, delicate systems. You’ll want plenty of time to think through and experiment with your study setup.
 
For more details, check out our free report on how to run eyetracking studies."
49,2019-08-11,"Cognitive mapping is a mapping method used to create a visual representation of a person’s (or a group’s) mental model for a process or concept. It can be a useful tool throughout user research, from gathering data to analyzing findings and articulating similarities and patterns. A cognitive map helps break down complex research questions, establish priorities for followup research, and add clarity to abstract concepts.
In cognitive mapping sessions, users are asked to create a map of a process, concept, or problem. The cognitive map is a representation of users’ mental models. The representations obtained from interviews can further be used in guiding the design process.   
This article outlines how to use cognitive mapping in user interviews:

When to Use Cognitive Mapping 
Running a Cognitive Mapping Interview
Variations on the Basic Cognitive Mapping Method
Benefits of Cognitive Mapping in User Research
Disadvantages of Cognitive Mapping 
 


When to Use Cognitive Mapping 
Cognitive mapping as a means of data collection is best suited for: 
Exploratory and discovery-based research. Data from a cognitive mapping session is unpredictable—no two maps will look alike. This is for two reasons: first, each user holds a different mental model, and second, the cognitive mapping user interview is unstructured, free-form (i.e., if two different researchers interviewed the same user in two different sessions, the resulting cognitive maps would have some differences even though they would both represent the same mental model). Conversation will jump around and participants will expand on ideas unpredictably. This unstructured format can help uncover new themes and insights because participants have the freedom to contribute in a natural, stream-of-consciousness way. Thus, cognitive mapping interviews will benefit discovery research, when a theme, pattern, or hypothesis is still evolving.
Research about specific topics with complex relationships or process components. Cognitive mapping is helpful for exploring research questions with ambiguous aspects that are tough to communicate verbally.  
PAR (Participatory Action Research). PAR is a collaborative research method used to gather information to use for future change (usually, within social and environmental issues). Traditionally, it used to empower people (who are directly affected by an issue) by offering them a primary, active role towards change. In the context of UX, cognitive mapping can help participants uncover new ideas by externalizing and visualizing their existing knowledge. For example, when we used a cognitive mapping approach for our design-operations research, several participants felt that the sessions clarified their understanding of the subject. One participant said “this was personally so helpful for me to do this. I’m leaving with several ideas of things to go do.""
 
Running a Cognitive-Mapping Interview
Presession Checklist 

Prime (but don’t overprime) the participant. Before the interview day and at the start of the actual interview explain the reason for the interview and how the data from it will be used. Warn participants that you will invite them to write, draw, and move sticky notes around throughout the session. However, do not use the phrase “cognitive mapping,” so people don’t start researching the topic and study for the interview.  
Prepare and practice.   

Get used to the method. Compared to traditional user interviews, cognitive mapping sessions are more unpredictable and require flexibility and improvisation. Your first 2-3 cognitive mapping sessions will likely offer more learnings on the method than on the topic you want to study. Consider running a practice session with a team member before the actual sessions.
Decide on your approach. There are two primary things you should decide ahead of time: 
		
Introduction. A good rule of thumb is to begin with a word-association exercise to generate ideas, then move to explore those terms’ meanings and relationships to other words, forming either an organizational structure or groups of categories. 
Overall Flow. Plan ahead of time if you want your session to be free-form (participants create their own maps) or structured (participants fill in a given skeleton structure — e.g., a mind map, concept map, Venn diagram). 




Figure out logistics.  An observer can help you with setting up the technology, recording the sessions, and notetaking. If you have multiple team members involved in the sessions, assign distinct roles (and have researchers rotate through accordingly depending on experience and expertise):  
	
Facilitator. The primary role of the facilitator is to conduct the interview. The facilitator is responsible for introducing the team, instructing the participant on the method, and asking the questions.  
In-room notetaker. The in-room note-taker should focus on taking notes regarding not only what participants say, but also how they do it: 
		
How and where do participants place items on the map? 
What physical gestures do they make throughout the interview? What content do these gestures relate to?
When are the participants unsure or ambiguous? 





It helps to have the in-room notetaker also be responsible for the setup of the technology (and any technology-driven logistics between sessions). This approach frees the primary facilitator to focus on making the participant feel comfortable and on creating the cognitive map.  

Remote observer(s). If more than two team members participate in the research, don’t place them in the same room as the participant. Instead, live-stream the session and have them observe remotely, taking notes in the same collaborative spreadsheet as the in-room notetaker (see below). If you have back-to-back sessions scheduled with participants and your observers are colocated, assign one observer to handle logistics like welcoming the next participant and preparing each batch of paperwork.  


Four researchers were involved in our cognitive-mapping sessions. We set up a remote monitor and had two people observe the session remotely. 


Choose your materials and room. Cognitive-mapping interviews can be conducted in many different locations — at the participant’s office, in a third-party meeting room, or in a neutral, casual environment like a library or coworking space. When choosing a location, keep in mind that you will need large table (and potentially whiteboard space) for the mapping to take place. It is best to bring and set up the materials ahead of time, so that when the participant walks into the space everything is ready to go. 


Common cognitive-mapping materials include a variety of multicolored sticky notes and markers, large-format paper, and potentially a whiteboard and dry-erase markers. Set up the materials so that you visually prime the participant to use them according to your plan. (For example, post a few sticky notes on the large paper to insinuate that participants can and should do the same as they begin mapping.)  


Record the session. Ideally you should videorecord the sessions so that you will be able to review them later. The benefits of recording are: (1) rewatching the creation of the map may uncover insights not caught in the moment, (2) you can revisit sessions if you find that you need clarification during the analysis and (3) you can educate others on the method.  
Test your technology and logistics. You’ll also want to set up and test your notetaking method and documentation plans ahead of time. In our research, we set up a collaborative document (see below) where all three observers could take notes simultaneously. As each notetaker documented an observation, she coded it (using her own schema). This method allowed us to collect data and analyze it simultaneously in an iterative process. (Note: this approach does not replace formalized coding during more rigorous research analysis, but it’s intended to help researchers quickly understand themes and build out hypotheses from session to session.) 


During our research, we set up a collaborative spreadsheet where the observers could document and code (in real time) the participants’ verbal remarks, physical gestures, or written words. The Topic columns held notetaker-specific tags for each note, created as the session notes were generated (for example, Culture, Behavior, and Facilitation). Each session’s notes were documented in a new sheet, as seen across the bottom of the window.

 
Interview Process 

Opening. The opening sets the stage for the rest of the interview. Don’t rush, invest some extra time in explaining the purpose of the session. The goal of the opening is to make the participant feel valued and aware that speaking openly will have no negative consequences. The opening should put the participant at ease, ultimately making for a more relaxed interview.

a. Introduce yourself. Begin the opening by introducing yourself and stating the purpose of the research. For example, 
“I’m Sarah, a researcher at Nielsen Norman Group. This is my colleague Maria, who will be taking notes throughout our session. Thank you for taking the time to join us today. The purpose of our research is to learn more about how companies are organizing their design teams. We are really interested in your experience working as a design leader. We would like your honest answers. There is no wrong way of doing things.”
b. Introduce the method. Cognitive-mapping sessions are different from traditional interviews, so you’ll need to introduce the method as part of your opening. State the duration of the interview, run through what you’ll be asking the participants to do, and leave some time for questions. 
“Today’s session will last an hour. I’m going to be asking you to use the markers, stickies, and paper to visualize the topics (and relationships between them) that we ask you about today. There are no rules, no wrong way to do it, and don’t worry about spelling things perfectly. Nothing is off limits and you can use any of the materials you like. Do what you feel most comfortable with. Do you have any questions for us before we begin?”
c. Ask an initial trigger question. Prepare an initial warm-up question to start the conversation. It should be easy to answer an open-ended — for example, a free-association question: 
“When I say the words ‘design operations,’ what are the first terms that come to mind? Think out loud and write each word on a separate sticky.”  

Facilitating the session. After the initial trigger question, the facilitator should prompt the participant to continue to build out the map. The goal of the facilitator is to make the conversation flow and feel natural, rather than disjointed.

• “If these terms were to be grouped, how would you group them? Go ahead and do this.” 
• “Is there a relationship between these groupings? If so, do you mind showing us by drawing it?”
• “Let's focus on this one (point at a sticky note with a term on it). What makes up [read term on sticky].”
• “When you say [term on sticky], what comes to mind?”
It is helpful to prepare some topics and probing questions ahead of time to use as needed. (However, due to the unstructured nature of the interview, the sequence of the topics will vary from interview to interview.) Pivot through your topic list using phrases like: 
• ""Let's circle back and give some attention to this area of the map, but through a new lens of [new topic].""
• “This feels really fleshed out. Is there anything else you want to include? (Give time for the participant to add any last thoughts.) If not, let’s flip to a new page and tackle the topic of [new topic].”
In some cases, you’ll have participants who are reluctant to write (and will revert to speaking). Aside from physical cues (like placing new stickies in front of them as they think aloud or turning to a new sheet of paper as the current one gets too crowded), you may want to offer additional verbal prompts until they are comfortable drawing or writing on their own: 
• “That is interesting. Let’s write that term down.” 
• “Go ahead and note for what you mean by that.” 
• “Where does that topic fit on the map?” 
• “At what point does what you’re referencing verbally occur? Can you show me on your map?”

Closing. Make sure to ask if the participant has anything else to share or add to the completed map(s). Thank the participant and make sure that the participant leaves feeling valued. If appropriate, the closing is also a good time to gather the participant’s thoughts on the method used. This input can inform your future use of cognitive mapping. 

“Do you have anything else you’d like to add about the topic or map? (Give the participant time to wrap up their closing thoughts or ideas.) What did you think about the method we used? Thank you very much for your time. What you’ve told us has been so useful and we really appreciate it.”
 
Analysis
A cognitive mapping session will have 3 data outputs: (1) the transcript (and the video) of the interview, (2) the map created by the participant during the session, and (3) the notes taken throughout the session by various note takers. 
Cognitive mapping is a qualitative research method and, thus, the mapping output will require qualitative analysis. Data will be sorted and coded in order to uncover the overarching themes. 
Single-session analysis. Analyzing one mapping session is similar to analyzing a traditional interview. Follow the steps below (or adapt them to your context and research needs):

Establish a few descriptive codes (for example, for physical gesture, behavior, challenge) ahead of time to guide note takers. These should be based on your research goals. 
After the sessions, use grounded theory (traditional methodology used for analyzing ethnographic or interview-generated data) to code what was said throughout the session. 
Analyze the map using a mix of descriptive (describe what was said) and interpretive (explain what was said) codes. Just as you would highlight specific phrases or sentences in a transcript, add your notes (codes) on a new-color sticky to the map. 
Combine the above three data outputs into themes by grouping things by subject or topic, then looking at similarities in actions, thoughts, emotions, or motivations. Subgroups should appear within the high-level groups. The names of these smaller clusters become your patterns or themes. Logistically, this process can be done in many ways — the key is to get all your data types into one place (a spreadsheet or a whiteboard are the most common). 

Multisession analysis. After you have analyzed each map individually, you’ll combine the data across multiple sessions. The goal of this phase is to compare patterns and themes across multiple participants to identify similarities and draw conclusions and insights. Begin by assessing what themes persist across several participants, and, conversely, what themes are unique to only one or a few participants. 
Consolidate the ideas and thoughts for themes that appear across participants. For example, in our DesignOps research, the analysis of the individual maps revealed that 9 out of 12 participants mentioned ideas around standardization, so we concluded that design standardization across an organization is an important component of DesignOps Sometimes you can go beyond identifying common themes and find relationships or “bridges” across ideas. For example, out of the 9 maps where standardization emerged, 7 connected it to processes and tools, whereas 2 connected it to career path and personal development.   
The main possible outcomes of multi-session analysis are:

Rough agreement between users: Most of the users have approximately the same mental model and generate approximately the same cognitive map. (You’ll never have 100% agreement, given the open-ended nature of this exercise.) In this case, you can produce a stereotypical map as a deliverable from your research.
Bimodal (or multimodal) maps: Users fall into two (or a small number) of groups; within each group there is a with a rough agreement on the map. In addition to generating stereotypical maps for each group, you should also aim to discover what causes users to belong to one or the other group (for example, the organization size may impact group membership). Because cognitive mapping is a qualitative method, you usually won’t have enough data points to reliably estimate the relative size of the groups in the real world, but you can try to match them up with personas or other existing classifications of users.
Strong variability: There is no agreement on the overall map, because most users generate very different maps. Even in this case, there may be segments or regions of the map where you can identify rough agreement or bimodal submaps.

Analysis is a messy process, especially when dealing with cognitive maps. Consider involving others (notetakers or project peers) to discuss the themes and patterns as they emerge from the data. When reporting your findings, you have two primary goals: 

Communicate clear and concise findings. Aim to share only what is most relevant for your initial research questions, in order to keep insights digestible and relevant for stakeholders, developers, and other project members.  
Support your findings with proof. Backup your findings with tangible proof of the research you did; show a picture of a map and pull a quote from the transcript. Sharing research is as much about the findings as it is about evangelizing the power of the method (and of UX, in general). 
	 


Variations on the Basic Cognitive Mapping Method
Over time, you should adapt the process to your facilitation style and context. Take the liberty to alter the method to suit your needs. Here are some ideas: 
Supply an Existing Framework or Set of Terms 
Instead of allowing participants to use their own approaches to mapping, supply them with a list of terms (think open card sorting meets mapping) or with a preexisting framework (often a mind map, concept map, or Venn diagram) for mapping. This approach works well in two specific cases. First, it can give a novice or quiet participant somewhere to start. Second, it can validate a preexisting model from past research (and thus you supply the terms and framework from that model, and see if the participant generates the same). 
Create a Group Map
Cognitive maps created individually can be adapted to represent a group in two ways: 1) by combine the results of individual interviews into a collective map or 2) by creating the map directly during a group discussion (think about the participants of a focus group cocreating a map together). This adaptation works well when the research priority is to surface patterns (whether openly amongst a team during PAR or during analysis of several consecutive sessions). 
Change the Medium
Don’t be afraid to experiment with your medium. A cognitive map can be created on a whiteboard, digitally (using a tool like Mural.co or Miro), or using a combination of tools (e.g., paper, sharpies, sticky notes, and white board) throughout the same session. 

In this participant session we began by using paper, stickies, and sharpies, then placed the stickies on the white board into a preexisting structure drawn by the researcher. 

Generate a Map as an Analysis Technique 
Though the focus of this article is on using cognitive mapping with participants in an interview, cognitive mapping can also be used to articulate the interviewer’s or observer’s understanding of the participants’ mental models afterward. In some cases, the derived map could then be validated, edited, and revised as a followup by the participant. 
 
Benefits of Cognitive Mapping in User Research
Cognitive mapping produces a visual representation of how a participant thinks about a particular issue or situation. The below characteristics make it a valuable technique in discovery-based, “messy” user research: 

Flexible format. Cognitive mapping is far more unstructured than other research methods such as traditional interviews, qualitative surveys, and usability tests. This open-ended format encourages participants to provide any information they feel is related to the topic at hand. Because this approach allows for jumping around, it can uncover participants’ natural instincts and thought processes. 
Paper trail of the session. In cognitive mapping sessions, all ideas are documented on paper. This paper trail provides a visual stimulus throughout the interview for both the facilitator and participant. The participant can point back and link current discussion to earlier ideas, often expanding further. The facilitator has a visual ‘map’ of previously mentioned concepts and can allude back in conversation reusing the exact language of the participant (and without relying on notes or memory). Additionally, the facilitator can use nonverbal cues (facial expressions, gestures,) from the participant to garner further insight. 
Value to the participant. Participants often leave the session with a new understanding of the domain because they had to externalize existing knowledge in a visual, tangible way. By questioning themselves on how topics link together and determining where random chunks of ideas fit in, they gain a better grasp of the concept at hand. Additionally, having participants use their hands during the session can release some of the stress and anxiety often associated with interview situations.  
Rich, multimedia data. Using cognitive mapping in an interview results in a rich set of data sources for analysis:  
	
The transcript and recording 
The artifact created by the participant 
A video of the creation of the artifact (which can capture hesitations, motions, gestures, etc.)  
		 




Disadvantages of Cognitive Mapping 
Cognitive mapping, like any research method or skill, takes time to learn and perfect. Often the first attempts feel unproductive and messy. Discussed below are disadvantages to the method, all of which can be overcome with time and practice.  

The map is only one input. The cognitive maps are an addition to a user-interview transcript, not a replacement. Do not lean on maps alone, but combine them with semistructured interviews. The maps will vary largely depending on the participants’ comfort with the method, experience, and personality. In some cases, participants’ maps can be directly compared. However, that will not be always easy because fidelity and formality will change from participant to participant. 


These maps were created in four consecutive cognitive mapping interview sessions conducted with design leaders.


The facilitator or interviewer will affect the quality of the data. A cognitive mapping session is a nuanced balancing act: the facilitator must listen and understand what the participant is saying, and, at the same time, offer prompts for mapping. The facilitator must read the participant, read the map, and get the participant to contribute in a productive way. Thus, each session likely requires a high level of improvisation and could be intimidating for novice researchers. Like with traditional user interviews, practice in low-stress scenarios (where the output is not a priority) and train new facilitators to the method by having them observe a series of sessions before taking on the role of lead facilitator. 
There is an increased likelihood to roam from the research topic. This method is unstructured and unpredictable. It is easy for participants to stray from the question or prompt. It is the interviewer’s responsibility to help the participant circle back to the topic at hand (for example, by pointing back to the map, or giving a redirectional, linking prompt). 
Some participants may feel discomfort with the method. Due to the nature of writing and ‘drawing’, some individuals may not be comfortable with cognitive mapping. This situation often manifests through cues such as minimal elaboration, constantly asking repeated questions around completing the task “correctly”, and spelling woes. The interviewer can help the participant feel at ease by reiterating that “there are no right or wrong ways” and to “spelling doesn’t matter.” However, in some cases, regardless of perfect facilitation, the participant will revert to a traditional interview. 

 
Conclusion
Cognitive mapping in user interviews can be a useful resource when researching a user’s perception of a problem, process, or organization. By producing a tangible representation of a user’s mental model, the researcher (and often the participants themselves) can better understand the problem space and its relationship to other people, topics, or processes.  
 
References 
Blanchflower, T. J. (2018) ""Implications for the Design of Technology in Students’ Use of Tools and Signs in Notetaking from Texts"" (Doctoral dissertation). Retreived from https://repository.lib.ncsu.edu/handle/1840.20/35235
Eden, C., Ackermann, F., (1998) “Making Strategy: the journey of strategic management”. London: Sage Publications Ltd."
50,2019-07-28,"In the user-experience profession, we preach iteration and evaluation. There are two types of evaluation, formative and summative, and where you are in the design process determines what type of evaluation you should conduct.
Formative evaluations focus on determining which aspects of the design work well or not, and why. These evaluations occur throughout a redesign and provide information to incrementally improve the interface.
Let’s say we’re designing the onboarding experience for a new, completely redesigned version of our mobile app. In the design process, we prototype a solution and then test it with (usually a few) users to see how usable it is. The study identifies several issues with our prototype, which are then fixed by a new design. This test is an example of formative evaluation — it helps designers identify what needs to be changed to improve the interface.
Formative evaluations of interfaces involve testing and changing the product, usually multiple times, and therefore are well-suited for the redesign process or while creating a new product.
In both cases, you iterate through the prototyping and testing steps until you are as ready for production as you’ll get (even more iterations would form an even better design, but you have to ship at some point). Thus, formative evaluations are meant to steer the design on the right path.
Summative evaluations describe how well a design performs, often compared to a benchmark such as a prior version of the design or a competitor. Unlike formative evaluations, whose goals is to inform the design process, summative evaluations involve getting the big picture and assessing the overall experience of a finished product. Summative evaluations occur less frequently than formative evaluations, usually right before or right after a redesign.
Let’s go back to our mobile-app example. Now that we’ve shipped the new mobile app, it is time to run a study and see how our app stands in comparison to the previous version of the app. We can gather the time on task and the success rates for the core app functionalities. Then we can compare these metrics against those obtained with the previous version of the app to see if there was any improvement. We will also save the results of this study to evaluate subsequent major versions of the app. This type of study is a summative evaluation since it assesses the shipped product with the goal of tracking performance over time and ultimately calculating our return on investment. However, during this study, we might uncover some usability issues. We should make note of those issues and address them during our next design iteration.
Alternatively, another type of summative evaluations could compare our results with those obtained with one or more competitor apps or with known industry-wide data.
All summative evaluations paint an overview picture of the usability of a system. They are intended to serve as reference points so that you can determine whether you’re improving your own designs over time or beating out a competitor.
The ultimate summative evaluation is the go/no-go decision of whether to release a product. After all is said and done, is your design good enough to be inflicted on the public, or do we think that it will harm our brand so badly that it should never see the light of day? It’s actually rare for companies to have a formal process to kill off bad design, which may be why we encounter many releases that do more harm than good for a brand. If you truly embrace our proposition that brand is experience in the digital age, then consider a final summative evaluation before release.
Origin of the Terms
The terms ‘formative’ and ‘summative’ evaluation were coined by Michael Scriven in 1967. These terms were presented in the context of instructional design and education theory, but are just as valuable for any sort of evaluation-based industry.
In the educational context, formative evaluations are ongoing and occur throughout the development of the course, while summative evaluations occur less frequently and are used to determine whether the program met its intended goals. The formative evaluations are used to steer the teaching, by testing whether content was understood or needs to be revisited, while summative evaluations assess the student’s mastery of the material.
When Each Type of Evaluation Is Used
Recall that formative and summative evaluations align with your place in the design process. Formative evaluations go with prototype and testing iterations throughout a redesign project, while summative evaluations are best for right before or right after a major redesign.
Great researchers begin their study by determining what question they’re trying to answer. Essentially, your research question is the same as the type of evaluation. Below is a list of possible research questions you might have and the corresponding evaluation. For that reason, this table is descriptive, not prescriptive.




Questions you might ask


Type of evaluation




How is our interface performing compared to our competitors?


Summative




What usability issues exist in our interface?


Formative, Summative




How does our interface compare to the industry benchmark?


Summative




Do users understand our navigation?


Formative




How has our overall experience changed over time?


Summative




Does our interface comply with recognized usability principles?


Formative




Is this product good enough to launch? (Go/no-go decision)


Summative




Research Methods for Formative vs. Summative Evaluations
After it is clear which type of evaluation you will conduct, you have to determine which research method you should use. There is a common misconception that summative equals quantitative and formative equals qualitative ­­— this is not the case.  
Summative evaluations can be either qualitative or quantitative. The same is true for formative evaluations.
Although summative evaluations are often quantitative, they can be qualitative studies, too. For example, you might like to know where your product stands compared with your competition. You could hire a UX expert to do an expert review of your interface and a competitor’s. The expert review would use the 10 usability heuristics as well as the reviewer’s knowledge of UI and human behavior to produce a list of strength and weaknesses for both your interface and your competitor’s. The study is summative because the overall interface is being evaluated with the goal of understanding whether the UX of your product stands up to the competition and whether a major redesign is warranted.
Additionally, formative evaluations aren’t always qualitative, although that is often the case. (Since it’s recommended to run an extended series of formative evaluations, it makes financial sense to use a cheaper qualitative study for each of them.) But sometimes big companies with large UX budgets and high level of UX maturity might use quantitative studies for formative purposes in order to ensure that a change to one of their essential features will perform satisfactorily.  For instance, before launching a new homepage design, a large company may want to run a quantitative test on the prototype to make sure that the number of people who will scroll below the fold is high enough. 
Conclusion
Formative and summative evaluations correspond to different research goals. Formative evaluations are meant to steer the design on the correct path so that the final product has satisfactory user experience. They are a natural part of any iterative user-centered design process. Summative evaluations assess the overall usability of a product and are instrumental in tracking its usability over time and in comparing it with competitors.
For more on formative and summative evaluations, check out our courses like Usability Testing or Measuring UX and ROI.
References
Greenstein, Laura. What Teachers Really Need to Know About Formative Assessment. ASCD, 2010."
51,2019-07-14,"Finding the right participants for a user-research study can be challenging, even if you have clearly defined your target audience. You can learn something from almost any user, but you’ll get more meaningful insights if your study participants have the same behaviors, interests, and knowledge as your actual users.
Using Screening Questions to Select Participants
To recruit study participants, you should ask screening questions which assess their background and characteristics. For example, if you’re designing a website about online games, you might need people who are interested in gaming. To assess this, you could simply ask them, “Do you play online games?”
But screening questions that make the purpose of the study obvious can backfire. If people can easily guess what the study is about, some will be tempted to exaggerate their responses just so they can participate (and receive the incentive, if the study offers one). When part of the screening process for a research study, the question “Do you play online games?” will clearly signal to respondents that the study is about gaming. It will be easy for people to guess that, if they admit that they do not play online games, they’re unlikely to be invited to participate in the study.
Therefore, screening questions have two conflicting goals:

They must elicit specific information about users.
They should also avoid revealing specific information about the study.

Achieving both goals is tricky, but it can be done by carefully preparing screening questions using two main techniques: open-ended questions and distractors.
Open-Ended Screening Questions
An open-ended question asks people to answer in their own words instead of choosing from a list of predefined answers. Since there are no answer choices provided, it’s difficult for people to guess which answer is ‘right.’
Open-ended questions can be used to elicit details about past experiences. Rather than asking whether someone plays online games, you can ask “What activities do you do online?” and select participants who mention games.
Open-ended questions are also good for assessing:

Disqualifying occupations: If there are specific categories of people you need to exclude from the study, an open-ended question will be better than a multiple-choice one with answers that list the excluded categories. For example, unless your design is for expert users, it’s best to exclude those whose job relates to the design you are evaluating, because their professional experience makes them too different from a normal user. If you’re evaluating a website about travel planning, people who work in the tourism industry would likely have a completely different perspective from the average traveler. The open-ended question, “What is your occupation?” is more likely to get accurate responses than a question which lists all the excluded occupations and relies on people to self-identify whether any of those describes their job.
Level of experience or interest: If you need to recruit people who are highly familiar with a certain topic, open-ended questions can elicit authentic details that tell you whether respondents have the relevant experience. If you ask people how often they play online games, it’s easy for someone who plays a few times a year to exaggerate and claim that she plays several times a week. But if you ask her to describe some of her favorite games and why she likes them, you will quickly be able to distinguish the hard-core gamer who can immediately list the names and details of many games from the person who can barely remember any. (If you are writing screener questions that someone else will be asking in an interview, request that respondents’ answers to these open-ended questions be written down word for word.)

However, open-ended questions alone are not enough to ensure effective participant screening, because they have some critical limitations. If you are recruiting for a very specific niche behavior, an open-ended question may fail to elicit relevant information because some people who do engage in the behavior may not mention it in their response to a general question. Also, in purely practical terms, responses to open-ended questions require more time to produce and also more time to collect and to analyze. Because the answers are free text, you need to read through each respondent’s statement and evaluate its meaning. This extra step may not be possible in some contexts (such as unmoderated usability studies, which are often designed to allow people to instantly proceed to the study after answering the screening questions, with no time allowed for a researcher to review the screener responses).




 DON'T rely on 'yes or no' questions:
""Would you consider using a short-term scooter rental service again in the future?""


 DO ask open-ended questions to elicit authentic experiences:
""Please describe the last time you rented a scooter.""




Distractor Answer Choices
Multiple-choice questions can be instantly evaluated, making them suitable for unmoderated recruiting; they also allow you to assess specific behaviors that people might not think to mention in an open-ended response. But, it’s still important to avoid revealing the purpose of the study. To do so, borrow a technique used by teachers for multiple-choice tests: include distractors among the answer choices. Distractors are incorrect answer choices which camouflage the right answer by surrounding it with incorrect responses. Good distractors look like they could be correct responses — they help distinguish between people who truly know the correct answer, and those who are just guessing and are likely to pick one of the appealing distractor answers.
You can incorporate distractors into multiple-choice screening questions by providing answer choices that include both your target response and several equally plausible responses. The distractor answers should be realistic both as activities that people would do and as research topics for an organization. For example, if you’re recruiting people who are interested using a scooter-rental app, you might ask about scooters and include walking, ride sharing, and taxis as distractors. But hang gliding would not be a good distractor, because it’s obviously not a reasonable transit option in an urban area, and also not a behavior so common that companies would conduct research about it.
For some questions, it will be appropriate to allow people to select more than one answer. Make sure to exclude respondents who select all the answers to one question — (especially if they do so repeatedly for several questions). Selecting all choices is a warning sign that the person may be too eager to participate, and it would be safer to choose someone who selected fewer choices.
If you’re using a testing platform which allows asking only one or two screening questions, even a single, well-written screening question with good distractors can be effective at identifying which potential participants match your target audience.




 DON'T ask easy-to-guess multiple choice questions:
If you needed to get to a meeting on the other side of downtown, about 2 miles away, which of the following would you consider doing to get to your meeting?
			A. Walk
			B. Hang glide
			C. Rent a scooter


 DO use plausible distractor answers to conceal the subject of the study:
If you needed to get to a meeting on the other side of downtown, about 2 miles away, which of the following would you consider doing to get to your meeting?
			A. Walk
			B. Rent a bike
			C. Rent a scooter
			D. Take an Uber




Conclusion
Finding the right test participants is important for any user-research project, but it becomes essential when you need to identify a specific type of user from within a large pool of general consumers. Screening is especially important if you’re looking for:

Potential future users, who aren’t yet customers, but could realistically become customers in the future
Highly motivated users, whose interest in a particular topic or activity is so strong that their knowledge and behavior significantly differ from those of the  ‘average’ person

These types of users are often desirable target audiences, but the attitudes and experiences that make these audiences valuable are often difficult to assess accurately with multiple-choice questions. Carefully planned screening questions discourage exaggeration and guessing, and identify those users who truly fit your target audience.
Learn more about writing screening questions and other tips for recruiting user-research participants in our free report How to Recruit Participants for Usability Studies. This is also one of the topics covered in the full-day course on Usability Testing."
52,2019-07-14,"Cognitive mapping, mind mapping, and concept mapping are three powerful visual-mapping strategies for organizing, communicating, and retaining knowledge. They help us lay out complex ideas, processes, and recognize patterns and relationships. 
Cognitive maps, mind maps, and concept maps look and feel similar; this similarity causes confusion. They are three different ways of visualizing a mental model — whether it belongs to the designer, the researcher, or the user. Each has its strengths and benefits. This article is a comparison of these three popular types of diagramming and their uses in UX.
Cognitive Maps
Cognitive maps are the umbrella term for all visual representations of mental models. All mapping techniques described in this article are instances of cognitive maps. 
Definition: A cognitive map is any visual representation of a person’s (or a group’s) mental model for a given process or concept.
Cognitive maps have no visual rules that they need to obey: there is no restriction on how the concepts and the relationships between them are visually represented.
History
The idea of cognitive map originates from the work of the psychologist Edward Tolman, who is famous for his studies of how rats learned to navigate mazes. In psychology, it has a strong spatial connotation — cognitive maps usually refer to the representation of a space (e.g., a maze) in the brain. Cognitive maps have since been used in a range of fields; Colin Eden, an operations researcher, used the term in a broader sense to refer to a mental model representation of any type of process or concept (whether spatial or not). 


Cognitive mapping is free-form and can include numerous visualization methods, including bulleted lists, flowcharts, concept diagramming, or affinity mapping. Though the above example is digital (and thus high-fidelity), cognitive maps are often low-fidelity and created with paper, pen, and sticky notes.  


Characteristics 

Diverse in nature and purpose. Cognitive mapping is used in a broad range of disciplines for a variety of purposes. Cognitive maps are the most general type of mental-model visualization. 
No restrictions on structure or form. Cognitive maps do not have to adhere to a specific format. Thus, they are often abstract and have no consistent hierarchy. They are flexible and can accommodate a wide set of concepts or situations that need to be represented. 

Uses in UX

Externalize knowledge. Visualizations (of any kind) aid in cognitive processing; they can help us refine our thinking, breakdown ideas, and capture thoughts. For example, a visualization becomes a helpful tool for describing where a new feature is accessible or when a new team member is onboarded to  a new complex system.  
Identify themes across different concepts. Presenting concepts in a visual format can surface new patterns and connections. In our DesignOps research, we asked participants to create a cognitive map of the organizational structures in their companies. After mapping, they were able to identify similarities (like team makeup, pain points, or bottlenecks) across siloed teams.  
Mental-model elicitation. Cognitive mapping can help UX researchers understand users’ mental models of a system or of a process. This understanding can be crucial when researching complex systems or even when embarking in the design of a new product.  Mental-model elicitation is usually carried out through individual interviews in which the participant builds a visual representation of her mental model of the research topic.  The resulting cognitive map represents a tangible representation of the participant’s thoughts and can serve as a conversation prompt for the facilitator. Several such maps can be clustered based on their characteristics; these categorizations can guide the design process. 

The next two sections describe two more-constrained types of cognitive maps: mind maps and concept maps.
Mind Maps 
Mind maps are the most simplistic, and thus straightforward type of cognitive maps. They have a clear hierarchy and format, and they are relatively quick to create and consume. 
Definition: A mind map is a tree that represents a central topic and its subtopics. 
History
The core characteristics of mind maps are rooted in the development of semantic networks, a 1950s’ technique for representing knowledge. In 1974, British author Tony Buzan popularized the term 'mind mapping.' 


Mind maps have a central parent topic, with nodes that branch outwards towards the peripheral. 


Characteristics

Clear organization and structure. Mind maps are restricted to tree structures. They have clear, directed flows outward from the tree root to its leaves. 
One central topic. In mind maps, all nodes (except the tree root) have only one parent node. Each node can have children corresponding to that concept’s subtopics. Every concept in a mind map can be traced directly back to the root topic.
No definition of relationships. There is no distinction between different types of relationships among nodes — all the edges in the tree are represented in the same way and are unlabeled. 

Uses in UX 
Mind maps help organize a collection of information connected to a single topic and structure it in a systematic, meaningful way. In UX, they are helpful when doing categorical ideation work, such as: 

Breaking-down components on a specific webpage — for example, in order to determine the mini-IA of the page
Planning subject topics within a website (for example, our article topics on nngroup.com)
Mapping information covered in an FAQ (Frequently Asked Questions) or privacy policy

Concept Maps
Concept maps are a more complex version of mind maps. They place an emphasis on identifying the relationships between topics. Additionally, a node in a concept map can have several parents (whereas a node in a mind map will have just one).
Definition: A concept map is a graph in which nodes represent concepts and are related through labeled, directed edges that illustrate relationships between them. 
History 
Concept mapping was developed in the 1970s by American professor Joseph Novak to help teachers explain complex topics in order to facilitate  learning, retaining, and relating these new topics to existing knowledge. 


Concepts maps are read top down; unlike with mind maps, a node can have multiple parents and the edges are labeled to indicate relationships between nodes.


Characteristics: 

Each node can have more than one parent (i.e., one node pointing to it). While each mind map node has only one parent, a node in a concept map can have several parents. Thus, nodes in a concept map are often more interconnected than nodes in mind maps, which makes concept maps well suited for describing complex interconcept relationships. 
Graph edges are directed and labeled with the names of the relationships between the nodes they connect. Each edge illustrates a specific relationship (and usually is labeled with a verb or preposition that captures it).

Uses in UX
Concept maps help visualize complex concepts that are interconnected in various ways. They can support multiple perspectives and ways of looking at the same problem and can be used to:

Develop a holistic picture of a set of concepts and their interconcept relationships, such as:
	
Data  
Organizational operations


Connect concepts with action. Concept mapping emphasizes relationships by linking one idea to another with verbs. This characteristic is useful when analyzing a problem (the maps often surface undiscovered causes and effects). By visualizing the content as a web, it becomes easy to follow a ‘trail’ of relationships and thus identify systemic solutions. 

The maps can be created individually or in a group (if the purpose is to create a shared understanding of an internal process, for example.) 
Methods Compared 
When it comes to representing physical space, there are many types of possible maps:  topographic maps, geologic maps, pedestrian maps, street maps, and so on. They are all flat representations of the surface of the earth, but highlight different properties of this surface. Cartographers apply different guidelines for designing a hiking map of a park, a highway map of a state, or a political map  of a continent. 
Like the different maps of the earth, all types of cognitive maps are in some ways the same. 
UX practitioners must be cartographers of UX maps — adapting format and structure to best suit the needs and context of the map they are creating. Use this table as a quick guide to compare mind maps, concept maps, and cognitive maps.  



 
Mind Map
Concept Map
Cognitive Map




Purpose
Expansion of a single topic
Explore relationships among several concepts 
Capture a process or dynamic ecosystem in free-form


Defining Characteristic
One primary, single center; one parent per node 

Labeled relationships between nodes; multiple parents per node

Lack of any consistent structure; mixed forms (list, diagram, graph, flowchart)


Adaptability 
Low
Low to Medium
High



 
To simplify when to use what, imagine introducing a new My Account view into a website. 
A mind map could be used to map the different sections, and corresponding sub-sections of content that would be within the My Account page. 
A concept map could map the larger website as a whole, and the different entry points that the user could use to access My Account, combined with what data could be exported or shared where. 
A cognitive map could be constructed in a user interview (by the participant) to uncover a participant’s current processes, mental models, and considerations for accessing and sharing personal account information.
Conclusion
The three maps above are not the same as flowcharts, so an enumeration of steps should not be fit for a map. However, there are diverse benefits from visualizing a concept, idea, or process, whether it be through a cognitive map, mind map, or concept map, and both individually or amongst a team: 

Provides a tangible visual abstract thoughts 
Communicates relationships or patterns between concepts  
Deepens our knowledge and understanding of a specific topic or concept 
Helps us integrate new ideas with existing systems
Synthesizes a complex ecosystem into a single visualization that can be shared

Taking fuzzy, abstract concepts and making them tangible enhance team communication and creates common ground. It’s also make it easy for a team member to immediately spot something on the map and say, “that’s not right.” Maybe your colleague is right, and something wasn’t captured correctly. Or maybe your colleague is wrong, and the map uncovers a misconception that would have otherwise lead to friction later in the project. Either way, the mapping exercise pinpointed something that required further discussion, which is far more efficient in the long run than proceeding on a project with a misaligned understanding.
Cognitive maps, mind maps, and concept maps ultimately enhance our cognitive understanding. Using one technique over another will not make or break a project. Ideally, a combination of all three will be used as needed at different points in your process, depending on your needs.  
 
References 
Buzan, T. (1993) The Mind Map Book. London: BBC Books
Eden, C. (1988) “Cognitive mapping”, European Journal of Operational Research, 36:1-13
Eppler, Martin J. (2006) “A Comparison between Concept Maps, Mind Maps, Conceptual Diagrams, and Visual Metaphors as Complementary Tools for Knowledge Construction and Sharing.” Information Visualization. Palgrave Journals. 
Kelly, G.A., (1995) The psychology of personal constructs. New York: Norton.
Novak, J.D., Gowin, D. B. (1984). Learning How to Learn. New York: Cambridge University Press.
Tolman, Edward C. (July 1948). ""Cognitive maps in rats and men"". Psychological Review."
53,2019-06-09,"User interviews are an important generative method for UX. Generative methods (like interviews and focus groups) produce knowledge. In contrast, evaluative methods (like a usability test of a draft design) test hypotheses. When interviews are done well, we learn about our users’:

thoughts
beliefs
mental models
experiences

This knowledge helps us to build good products and services and address real user needs rather than imaginary ones. Taking the time to speak to people who might use your product or service (before you start designing) can teach you things you might otherwise never uncover, even if you run regular usability tests.
However, when done poorly, interviews lead to bad design decisions that can ultimately hurt the organization and its customers. As we have said many times, a great user interface built with the wrong features will fail. This article outlines four common issues with user interviews, along with strategies for avoiding them.
Issue #1: Wrong Purpose
Some UX teams run user interviews to glean information that cannot, in fact, be learned from interviews. Some examples of research questions which cannot be answered satisfactorily by interviewing people include:

Which color most improves the user’s impression of the brand or product?
Will people buy or use our product in the future?
Which features do people need in the product?

While all of the above are reasonable research questions, they will not be answered reliably by interviewing people because they are about peoples' behaviors. Interviews do not produce reliable data about user behavior. Rather, observing people is a much better way to learn about their behavior.
Let’s illustrate this with an example. Imagine a designer wants to learn what the best page-background color should be for a given design. She conducts interviews in which she shows people mockups and asks them which they prefer. There are multiple assumptions that make this study unacceptable.

The participant prefers a color. Asking participants which color they prefer suggests that they should think about and have an opinion about color. They may not, but simply being asked may make them form an opinion. (This is known as the query effect.)
A user's response in the interview is an accurate representation of their real-life feelings about page color. True, some participants might have a preference in the moment for a blue background rather than a purple one, but that doesn’t mean that they would have the same preference when using the product in a real-life setting outside of the usability lab.
The color preference affects the user's overall perception of the product. Even if users actually do like, dislike, or feel some emotions related to the color, it doesn’t mean it will affect their overall perception of the design or organization to any meaningful degree. A background color is just one of many factors (such as usability, utility, informational value, and so on) that contribute to the overall user experience.

To meet the research goal in this example, the designer should instead run a desirability study; the 5-second test is a common example of this.
Solution: Choose the Research Method Based on Your Research Question
Your research question should always dictate which research method you use. For example, if you want to know whether users would be able to use a design, then you should watch them interacting with it instead of interviewing them. On the other hand, if you want to understand people’s thoughts, impressions, or perceptions of an experience, then there’s no better method than performing a well-run interview.
Few UX teams take the time to spell out their research questions. Doing so can focus the research and help teams understand which methods are the most appropriate to answer the said questions.
Issue #2: No Stakeholder Buy-in for Interviews and Their Findings
Often our stakeholders don’t fully understand the differences among various research methods. To them, it’s all research. This attitude can result in two issues:

Teams are not given the time to do user interviews at the outset of a project because stakeholders think they already know enough from analytics or anecdotes from the sales department.
Following an extensive discovery phase, interview-based insights are often received with skepticism and elicit dreaded responses such as “We already knew this” or “That can’t be right. My sister-in-law had a different experience.”

Solution: Educate and Involve Stakeholders in Understanding and Prioritizing Research Questions
Research is successful when stakeholders (i) understand the research goals, (ii) understand why using one research method as opposed to another is important, and (iii) are bought in from the start.
If you’ve identified stakeholders who could be opponents of your work, then a great way to convert them into supporters is to invite them to shape the research.

Invite your stakeholders to a workshop where you explain the research method and the importance of research questions. Then ask your stakeholders for their input in prioritizing your research questions. Stakeholders can also help by offering their own research questions.
Use dot-voting or a prioritization matrix to work with your team and stakeholders to rank the research questions in order of importance.

There are 4 reasons why including stakeholders in this way is beneficial:

It’s more likely that the findings will be accepted as stakeholders have had a hand in shaping the research, and therefore feel some ownership.
It brokers a strong relationship between the business and UX, as including them shows you want their expertise on the business perspective.
Your stakeholders can offer ideas about other research questions you hadn’t thought about.
It’s another way to educate people outside your team about the value that UX brings to the organization. A prioritization workshop can be an educational experience. By describing how useful different methods are — and when it’s okay to use them — your stakeholders will begin to understand that there’s more to UX than they initially thought.

Issue #3: Poor Planning
Too many people launch straight into interviews hoping they’ll get clarity as they conduct interviews with users. That’s an expensive way to waste participants’ (and your) time and collect unhelpful user data.
Without planning, interviews can often:

meander and produce findings which are superficial at best,
include poor questions, leading questions, or closed questions that never allow participants to give their honest thoughts or opinions about a topic, or to tell their story.

As a result, the data produced from those interviews is misleading or not informative for decision making.
Solution: Plan and Pilot the Interview Guide
Make sure you understand what you want to find out. After you spell out your research questions, put together an interview guide that will steer the conversation during the interview. The guide should have a few well-constructed questions. Aim for a few broad and open-ended questions to facilitate exploration, as opposed to a long list of closed questions.
Common questions used in UX interviews ask users to talk about problem spaces and experiences we want to learn more about. Such questions include:

Describe a typical day where...
Tell me about a time where…
If there was one thing you could change, what would it be?

Prepare nonleading probing questions. Examples of good probing questions include:

Tell me more about that.
Can you give me an example of that?
Why do you think that?
Why is that important to you?

Pilot your guide. First, pilot your interview guide on yourself to learn how you would answer the questions in your interview guide. Even asking yourself your own questions might make it clear that some of your questions don’t work. Second, before you run your interviews with lots of users, recruit one user for a pilot interview. Don’t tell that person that the session will be a pilot; rather, conduct the interview as though it wasn’t a pilot. And, just like a usability test, you’ll see quickly how you can improve the design of your interview guide before you interview again.
Plan a realistic schedule. Too many people schedule all their interview sessions back-to-back, with no break between them. Interviewing is tiring (much more so than moderating usability tests), and if you have no breaks in a day, it’s likely that your later interviews will be of poor quality. If you have the time, space your interviews out over a week (say 2–3 per day) instead of doing them all in one day.
Issue #4: Poor Analysis
Few teams take the time to analyze the results of the interviews properly, especially if they work in Lean or Agile. Unfortunately, when many interviews are done and not analyzed properly, it’s common to experience the following issues:

Only the memorable insights are reported; some important insights or nuances are lost.
The reported findings are colored by personal biases, as it’s easy to favor or recall insights that support pre-existing beliefs about your users.

Solution: Systematically Analyze the Transcribed Results
Record sessions.
Few teams record their interviews, and even fewer teams transcribe the sessions. If you interview a lot of people, it’s worth recording them. Even a good notetaker can miss things, as it’s virtually impossible to capture everything that was said verbatim. Most participants have no problem with audio recording the conversation, but always do get consent before audio or video recording.
Transcribe your interviews.
Analyzing transcripts is much better than relying on patchy notes or memory. You’ll stay closer to the data for longer, and you’ll be less likely to leap to conclusions that aren’t supported by the data. There are plenty of inexpensive AI-based transcription tools which will transcribe the audio recording for you in a matter of minutes if you're willing to do some manual work to correct transcription errors. Alternatively, you can pay a professional transcriber to obtain more accurate transcriptions.
Include the team. 
Invite your team to help you analyze the transcripts. This approach is a great way for your team members to build empathy for the user while cementing what they learned from carrying out interviews.

Block a half-day workshop where you give your team the transcripts to read and highlight.
Remind your team of your overarching research questions.
Rotate the transcripts between your team members, ensuring everyone reads each transcript and actively engages with the text.
Following this activity, work with your team (or alone) to go back through the highlighted text and cluster quotes into meaningful groups, which will eventually become your themes.

Improve Your Interview Skills
Interviewing is a skill and requires practice and reflection to finesse. When you find a spare hour, review the interviews you’ve conducted. Which of your questions were not effective? Think about how you could improve the questions you asked and your overall interview style.
Invite a colleague you trust (or your mentor) to review your recordings or transcripts with you and to give you constructive feedback. This process takes time but is worth it because of the higher value you will gain from future studies.
Those who invest the time to rewatch or relisten to recordings and analyze transcripts of interviews they’ve performed will become better practitioners.
Learn more: User Interviews, Advanced techniques to uncover values, motivations, and desires, a full-day course at the UX Conference."
54,2019-05-05,"The Disgust for Hush: A Universal Pattern
Silence.
Dead air.
Crickets.
Even the simple act of reading these words might cause a prickly, uneasy feeling.
The fact is, lack of back-and-forth chatter makes us uncomfortable. Research by Koudenburg, Postmes, and Gordijn has shown that, in the United States, it takes only four seconds before an extended period of silence becomes uncomfortable during conversation. Four seconds! Why the disgust for hush? Long story short, humans equate silence with rejection. We have an evolution-driven desire for conversation because it makes us feel connected and accepted. So why would we want to intentionally create periods of “awkward” silence with participants in workshops or research activities?
The power of intentional silence is well-known and utilized among many professional groups: Sales people pause after their pitches for dramatic effect. Counselors practice waiting five seconds after a patient stops speaking before responding. Nurses and physicians employ intentional silence in order to demonstrate compassion and respect. And negotiators adhere to the saying: “He who speaks first, loses.”
As UX professionals, we, too, can harness the power of intentional silence. If we can just become comfortable with that brief period of unsettling silence during our user interview sessions, usability tests, and workshops, we’ll get more out of our participants. Intentional silence, used strategically, can create space, invite response, and signal interest. And it is in periods of silence where participants often offer crucial and most-poignant information.
How to Be Intentionally Silent: Count to Seven
I use a personal guideline when practicing intentional silence: When I find myself in the depths of a poignant pause, I count to seven before speaking. It is an attempt to coax the participant into filling the silence first. Let the first four seconds tick away — remember, this is where the awkwardness will spike — and then allow a few more seconds to pass. The additional three seconds provide space for the participant to collect his or her thoughts and continue speaking after the awkwardness has peaked. The longer you keep quiet, the more the other person will want to fill the void.
When to Be Intentionally Silent: User Interviews, Usability Testing, and Workshop Facilitation
Here are some situations relevant to UX professionals where intentional silence is particularly powerful, and some examples of appropriate and inappropriate usage.
User Interviews
When interviewing users, apply intentional silence to create space for participants to think and respond in a thoughtful, unhurried manner. Show you are interested in what they have to say by allowing them time to articulate their thoughts. Don’t finish interviewee’s sentences in an attempt to read their minds in order to fill silence. Participants are likely to simply agree with you and discontinue their thought.
 
Poor Example: Interjecting During Pauses Shuts Participants Down
In this audio clip of a user interview, the interviewer jumps in to finish the interviewee’s thought when there is a brief pause in conversation. Effectively, the participant discontinues his train of thought and potentially valuable insight is lost.

 

Transcript:
Interviewer: “Tell me a little about your design process.”
Participant: “Ok, so, there’s a process we have here that we’re trying to use to timebox our design process. It’s like a way to…”
Interviewer: “...to scale Agile?”
Participant: “Oh, yeah, SAFE. You know about it then.”
 
Good Example: Intentional Silence Allows Participants to Process Their Thoughts
In the next clip, rather than fill the silence when the participant struggles for a few seconds to find the right words, the interviewer uses intentional silence to allow the participant time to gather his thoughts. In this example, the use of intentional silence results in a valuable insight regarding the difficulty of including UX in the company’s Agile design process.

 

Transcript:
Interviewer: “Tell me a little about your design process.”
Participant: “So, there’s a process we have here that we’re trying to use to timebox our design process. Like...it’s like a way to…”
[Interviewer uses intentional silence]
Participant: “...well, it’s a way to make sure our designers are working with different roles. But there’s really no clear way to insert UX into the process. I mean, it’s been a challenge for our team.”
By the way, how long did that moment of intentional silence feel to you as you were listening to this clip? Did you start to feel uncomfortable? The silence was only five seconds (though it might have seemed longer due to our innate discomfort with that length of pause in conversation)!
 
Poor Example: Interrupting Silence Cuts Off Participants’ Thoughts
Even when interviewees appear to have finished what they had planned to say, it’s a good practice to wait a few seconds. Often, they will add on interesting information, as their first utterance is often simply thinking initial thoughts out loud.
In the following clip, the interviewer rushes to fill the silence after the participant appears to have made a complete statement, moving on to a different topic.

 

Transcript:
Participant: “We’re really good at going fast. Like, the teams are just clipping along and it really makes it so we hit our milestones...”
Interviewer: “Ok, then. Let’s talk about how your team uses tools.”
 
Good Example: Intentional Silence Helps Participants Dig Deeper
Compare the above clip with the following example: The interviewer uses intentional silence to encourage the participant to elaborate, and the participant continues his thought to reveal poignant information that actually conflicts with his first statement.

 

Transcript:
Participant: “We are really good at going fast. Like, the teams are just clipping along and it really makes it so we hit our milestones.”
[Interviewer uses intentional silence]
Participant: “...I mean, well, we maybe miss one milestone per month, and that’s always because marketing slows down the process, so it doesn’t really count. It’s like it always feels like they’re operating on a different set of information than the rest of us. I don’t know if they need to be in the standups or what, but ... it’s bad.”
Qualitative Usability Testing
When conversing with participants during qualitative usability testing, employ intentional silence to allow participants time to read, make judgements and generally reflect on their experience. Filling in periods of silence by barraging the user with questions or making inappropriate commentary about the design is often a sign of a nervous facilitator.
 
Poor Example: Filling Silence with Useless Commentary Distracts Users from the Task at Hand
Falling into general back-and-forth chatter with the usability-test participants invites side conversation and bias. In the audio clip below, the moderator cannot resist filling in the break in user speech with biased commentary. An interjection such as this one would influence the user’s behavior and distract him from the task he is trying to complete.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
Moderator: “Yeah, that’s pretty bad.”
 
Good Example: Intentional Silence Encourages Natural Progression in Task Flow
Instead, resist the urge to eat up silence with comments or a barrage of questions. Don’t answer rhetorical questions or questions where the participant is simply thinking out loud, and don’t interrupt the thought process. Allow participants the time and space they need to complete the task at hand as they would normally.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
[Moderator uses intentional silence]
Participant: “I just want to see when my headphones will get here.”
[Moderator uses intentional silence]
Participant: “I’m just going to click on the cart and see if there’s a way to get to my past orders.”
 
Poor Example: Too Much Intentional Silence Can Backfire
Beware of letting intentional silence linger for too long. Users might conclude that they are being unhelpful or that you are disinterested in the session. In the following clip, the moderator’s prolonged silence frustrates the user, who is lost in the task.

 

Transcript:
Participant: “This is crazy. I have no idea how to get to my account.”
[Moderator uses intentional silence]
Participant: “Uh...If I click here, will it take me to my profile?”
[Moderator uses intentional silence]
Participant: “I’m lost. I have no idea what to do.”
[Moderator uses intentional silence]
Staying completely silent throughout a test session is often a sign of a nervous facilitator. When the participant grows agitated, such as in the above clip, intercede with rhetorical questions (e.g., “What do you think?”) or neutral responses, such as “What are you looking for?”
Workshop Facilitation
Finally, intentional silence can be a powerful facilitation technique in group meetings such as workshops. Don’t be afraid of silence in a group setting! Participants are more likely to share if there is space in the room for commentary. Remember, your role as a facilitator is not to share your knowledge with the group; it is to collect knowledge from others and align the many different perspectives in the room. Participants are more likely to speak up if there is a pause in the room.
 
Good Examples: Use Intentional Silence to Invite Contributions of Varied Perspectives
Precede your periods of silence with a question that invites response. Ask an open-ended question, then wait. Use the count-to-seven rule to ensure anyone with an idea has had ample time to contribute before moving on. Here are some examples:

“I’m going to pause here so we can hear some additional perspectives.” [Count to seven.]
“What are some reactions to that?” [Count to seven.]
“Anyone want to play devil’s advocate on that?” [Count to seven.]
“Has anyone experienced something similar?” [Count to seven.]

 
Poor Usage: Intentional Silence After a Participant Contribution Can Feel Exclusionary
While using intentional silence to encourage contributions from workshop participants is a useful technique, be careful not to give the impression of a cold shoulder with misplaced intentional silence. Do not follow participant contributions with intentional silence if they have completed their thought. This signals rejection or disagreement to the contributor. Instead, acknowledge the comment and ask for reactions or additional thoughts.
What to Do Instead of Talking
You may be wondering, “What do I do during these silence periods instead of speaking?” Use your body language to allow time for people to articulate their thoughts. Maintain eye contact and focus. Do not say anything or even nod your head. Wait patiently and relaxed, giving the person time to speak. If you can’t resist filling the silence, use soft encouragers to invite the participant to elaborate, such as, “Tell me more about that,” or, “What did you think about that?”  Another trick: If you must do something, take a sip of water, continuing your count as you do.
Cultural Influence on Comfort Level with Silence
It is interesting to note that research suggests a higher degree of discomfort with prolonged silences in Anglophone cultures, such as the United States. For example, the same study by Koudenburg, Postmes, and Gordijn mentioned in the beginning of this article found that, while English speakers became uncomfortable with silence after just 4 seconds, Japanese speakers were comfortable with a silence lasting twice that long (8.2 seconds). Additionally, research by Petkova has found that Finnish speakers are also more comfortable than Americans with extended periods of silence in conversation due to the high value attributed to privacy in that culture. However, regardless of the exact number of seconds that leads to discomfort in conversation, one fact remains true: Being heard is one of the deepest human needs, across all cultures. And being heard requires silence.
References
Koudenburg, N., Postmes, T. & Gordijn, E. H. (2010). Disrupting the flow: How brief silences in group conversations affect social needs. Journal of Experimental Psychology.
Petkova, Diana P. (2015). Beyond Silence. A cross-cultural comparison between Finnish “Quietude” and Japanese “Tranquility”: Eastern Academic Journal. 4. 1-14."
55,2019-04-28,"Recently we published the 4th edition of the UX Design for Children report and the 3rd edition of the UX Design for Teenagers report, which covered the usability of sites and apps for minors aged 3–12 and 13–17, respectively.
Conducting user research with young users requires additional considerations compared with studies with adult participants. For example, from a legal perspective, children and teens cannot give their own permission to participate in the study and therefore we must work with their parents or legal guardians to get formal permission. Additionally, children and teenagers tend to get distracted more easily than adults, they often read at a lower proficiency level, and their research capabilities are inferior. With these problems in mind, we share some tips for recruiting, study preparation, and facilitation with minors.
Recruiting
Effective participant recruiting is always essential to a successful usability process. But when working with youths, we need to give special care to the number and type of participants we recruit, as well as to the compensation that they receive. The differences between a 7- and 17- year old are much more pronounced than, for example, a 37- and 47-year old. For this reason, segmenting an accurate user group is particularly important.
1. Recruit a few more participants than you need.
Approximately one in nine recruited users fail to show up to a study. When we’re working with minors, there are now at least two people that can affect the attendance of the participant: the parent and the child. (Sometimes, there could be even four, since some sessions involve friendship dyads.)  
In addition to no-shows, working with youths may also result in more interruptions throughout the study. More interruptions mean less time working on tasks and perhaps fewer research findings in the long run. Interruptions can be due to environmental distractions (discussed later), water or restroom breaks, or simply the participants’ need to get comfortable in their chairs.
Absent participants and study interruptions are unavoidable. However, you can protect yourself from rework and lost time by recruiting a few more participants than what you’d normally need in a similar adult-based study.
2. Don’t group all minors together. Segment users based on varying levels of maturity.
The power law of learning shows that task time decreases with the number of repetitions of that task. This law is as true for youths as for other users. Each year, youths get more experience with interfaces, so they get a little faster and a little better at using the web. In addition to the skill-level spectrum, there is also variation in content interest across ages.
When designing for kids, there are certain cognitive and physical considerations to keep in mind. For example, children aged 3 to 5 may be comfortable only with touchscreen interfaces; however, by the time they’re 6, they will likely be able to use a trackpad. This difference is important to the way we set up our labs and prepare tasks.
In the case of teenagers, a 13- or 14-year old likely has different interests than a 16- or -17-year old — presumably because 13- and 14-year-olds have yet to reach high school, a pivotal change in perceived maturity.
To get relevant study findings and design for the right audience, recruit appropriate age and interest groups.
3. Determine an age-appropriate incentive.
Cash is always appreciated as an incentive. However, to ensure that minors are content with their compensation, consider alternative or additional incentives.
Gift cards to frequently visited vendors can be an appropriate substitute to cash incentives. For example, if you’re testing a site specifically aimed at children or teenagers, a gift card to the site or a free membership would make sense. However, this gift is best for older minors that can shop online on their own.
When testing with young children, we’ve offered a toy bin that contains small toys, erasers and stickers; participants enjoyed selecting a gift from the bin. (Note: this incentive was paired with a gift card. Although, if the toy being offered was comparable to that of a cash or gift card incentive, it could be appropriate on its own.)
 Remember that testing with minors requires effort from the parents, too. Monetary incentives may be controlled by a parent, so it’s nice to supplement a cash incentive with a toy or another tangible item.
4. Emphasize the need for sociable participants.
Youths are likely to feel uncomfortable talking with an unfamiliar adult in a lab setting. It’s a unique and strange experience for them.
As researchers, there are things we can do in task preparation, environment setup, and facilitation to make youths feel more at ease, but the first step is getting the right participants. Reserved participants are difficult to facilitate. This is another reason we recommend recruiting more participants than you need — just in case you get several shy participants.
Task Writing and Study Prep
Writing good usability tasks is an art form. When it comes to working with youths, task writing needs to be varied, age-appropriate, and simple, without providing too many hints. 
5. Use age-appropriate language in written tasks. Aim for simple tasks.
If you’re working with an age range comfortable with reading, be sure to keep the language simple. Avoid using complex terms and write at an age-appropriate language.  Alternatively, if you’re testing with kids at a prereading age, provide instructions verbally.
In addition to writing clearly, don’t create tasks of unrealistically high complexity. For example, asking a 7-year-old to create an account and go through an ecommerce checkout workflow is not realistic and might make participants feel like they’re being tested (even though they’re not).
Although task difficulty and wording might be different for a child or teen compared to an adult, it is important to make sure the task is still realistic and actionable.
6. Avoid providing clues.
Be careful not to use UI-specific language in task writing. Tasks that include terms used in the interface bias users.
For example, in our work with teenagers, they often wrote words or phrases from the task directly into search bars. This habit, though present in all age groups, tends to be more frequent in younger groups due to participants’ less developed research abilities. 
7. Prepare a plethora of varied tasks because participant interests vary.
When testing any interface, it is important to prepare different types of tasks. If you only have one or two types of tasks prepared and a user gets bored and unengaged, it’s almost a waste of the session. Children and teens are even more likely than adults to get bored and stop providing valuable insights.
For example, in our teen research, although we wanted to test games, not every participant was interested in playing. In this case, we had plenty of other types of activities (ecommerce, entertainment, school, etc.) that the user could engage in.
Additionally, children and teens might work very quickly through tasks. In our studies, youths tended to be more likely to stop researching once they’ve found an answer rather than continuing research to confirm they’ve found the right answer.
Prepare various tasks to keep minors engaged and interested so that they will continue to provide valuable responses and interactions with the system. Likewise, formulating extra tasks can guarantee you don’t end a session early because your participant ran out of things to do.
8. Schedule sessions no longer than 60–90 minutes and leave enough time for breaks in between.
Kids can get tired quickly and lose motivation if they’re working for too long.  In our study of children aged 3–12, we conducted 60-minute sessions. This was a good amount of time to give an introduction to the study, conduct a brief interview, and work through multiple tasks.
With teenage participants, we conducted 90-minute sessions. This lengthened time accounted for tasks of a higher complexity, more talkative teenagers, and more device switching (laptop, tablet, mobile).
9. Consider conducting sessions in friendship dyads with children 6- to 8-years old.
Paired usability testing is more effective for kids between ages 6–8. With children younger than 6-years-old, pairs didn’t work as well because one child would often get bored watching the other child control the device. On the other hand, children older than 8 tended to be comfortable enough with a device to not need someone else there.
10. Design a lab that’s child friendly.
As mentioned earlier, usability labs can be intimidating to youths.  The testing environment should be as pleasant as possible, while also devoid of potential distractions.
For example, a large window overlooking a busy street might seem appealing to you (as someone stuck inside all day), but to a child, it’s a distraction waiting to happen. Either choose a different windowless room or ensure that the participant is facing away from the window (while also avoiding screen glare).
Other considerations for a child-friendly lab include having tissues on hand to wipe runny noses, a chair without wheels to prevent rolling around, and perhaps a booster seat (if you’re testing with users that may not be able to see the device from their seat easily).
Facilitating
One of our challenges in testing youths was encouraging them to think aloud as they worked on the tasks. Children and teens tended to be much less talkative than adults, mainly because they were shy or self-conscious. Facilitating minors requires extra work to get the insights you need. For example, facilitating children and teens often requires being slightly friendlier and praising than in an adult usability test, still without biasing the participant.
11. Confirm that parents signed the consent forms.
Working with minors requires parental approval; minors cannot sign consent forms on their own behalf. In our usability testing with older teenagers (16–17-year-olds), it was not uncommon for them to show up alone. Providing the consent form ahead of time allowed them to have their parent complete the form beforehand. (Note: In theory, a participant could forge a signature. In our process, a recruiter spoke with a parent first and then with a child.) 
12. Be prepared for siblings and parents to come along.
Youths frequently needed someone to bring them to the study. In many cases, the participant arrived with a parent and a sibling. In these circumstances, it is important to note that the participant may not be the only nervous one. Parents may also be uneasy about usability testing if they’ve never participated in research themselves. For this reason, we offer parents the option to wait outside of the usability lab (this could be behind glass or just outside of the room) or sit in the room (in that order).
If parents decide to wait in the room, place them in a seat outside of the participant’s view and remind them to be quiet as the session begins to avoid distracting the participant throughout the study.
13. Be thoughtful when asking minors to think aloud.
Getting youths to think aloud is often challenging. For some participants, it might feel unnatural; others may simply forget to do it after a few minutes.
In our work with children, we told them that they were the experts, and that we wanted them to teach us how kids use websites and apps and what they think about them. This method encouraged most children to share, though its effectiveness still varied depending on the child.
During our research with teenagers, we asked them to read a think-aloud document out loud. This document is an alternative to a demo video. Reading the document served as a warmup to the actual study and helped teens understand what was being asked of them. Although this method was generally successful, some participants were still quieter than others and needed periodic reminders to think aloud.
14. Dress casually.
Suits, lab coats, or any other authoritative apparel can make young participants uncomfortable and less willing to share. Dress casually but professional. For example, nice jeans and a sweater, button-up shirt, or blouse can work well.
15. Be friendly and establish a relationship, but remain neutral.
When conducting usability tests with minors, the facilitator should not look or act too authoritative. This presence could alienate minors and make them feel uncomfortable and hesitant about sharing their thoughts and feelings. In addition to being friendly and approachable, remind users that they’re not being tested, and there is no right or wrong answer.
For example, young children (under 5), tend to be disrupted by the “poker face” technique that’s appropriate for adult participants. If they see that the facilitator isn’t reacting to what they’re doing, the child tends to become quieter and less responsive.
Effective usability testing with children requires being cognizant of all your responses and keeping them neutral and meaningless. Generic praises like “you’re doing a great job at teaching me about apps” or “all of this is really helpful to see” make the child more confident and willing to speak.
16. Start off easy.
It is common for usability participants to feel pressure to perform well, no matter how many times you tell them that you’re not testing them. By starting with something simple, you build confidence.
In our research with youths, we often began by asking them to talk about themselves, show a website they like, or talk about their online experiences. This initial sharing, where they were in control, served as a warmup and got them comfortable talking. Once they were relaxed, we moved on to a simple, age-appropriate task.
Conclusion
Usability testing with minors requires special considerations to ensure effective results. Begin by recruiting participants based on their level of maturity and interest- or age-group to obtain relevant findings. Thoughtfully consider incentives that your child or teen participants would enjoy. Be sure to prepare an abundance of age-appropriate tasks. Design a child-friendly lab and anticipate parents or siblings staying in the room. Finally, on the day of the study, dress casually and present yourself as a friendly yet professional figure."
56,2019-04-21,"As UX professionals, it is our job to advocate on behalf of our users. To do it, we must understand them. Understanding our users means building empathy for human beings who experience the product or service we create. 
In an effort to practice empathy, many teams mistakenly practice sympathy. Although these words have different meanings, teams often incorrectly use sympathy and empathy interchangeably. This confusion results in a large gap in their understanding and in an inability to address real human needs. 
The goal of this article is to help you reflect on your current UX practice — are you unknowingly practicing pity and sympathy instead of empathy?
What Is Sympathy?
Definition: Sympathy is the acknowledgement of the suffering of others.
Sympathy is often the reaction (in the form of sorrow or pity) to the hardship or plight of another person. However, in contrast to empathy, there’s still a feeling of distance between you and the other person, and their adversity is not something you personally relate to or expect to share. You don’t envision yourself suffering the user's problems (and you definitely don't share those problems now), much as you do recognize that some users have problems.
In UX, sympathy is limited to acknowledging that users are going through a difficult scenario, task, or journey. If we are sympathetic to our users, it does not mean that we put ourselves in their shoes and feel their pain or annoyance. For example, when we’re designing an accessible website for people who are blind, we may express sympathy by acknowledging their potential challenges: 

“It will be hard to consume the content if you can’t see the infographics.”
“This font is kind of small and light. It may be hard for someone older to read.”
“It would be tough to navigate this website with a screen reader.” 

It is true that some sympathy is better than none. For example, it is better to have sympathy for users with limited technical skills than despise them for their shortcomings. However, the true goal of design is not to be nice to users, but to empower them. For example, this is why we don’t recommend lengthy error messages about how sorry we are about an error. Rather, we suggest that error messages allow users to quickly correct the problem and move on.
What Is Empathy? 
Empathy is a step beyond sympathy, and more complex in nature. 
Definition: Empathy is the ability to fully understand, mirror, then share another person’s expressions, needs, and motivations. 
In UX, empathy enables us to understand not only our users’ immediate frustrations, but also their hopes, fears, abilities, limitations, reasoning, and goals. It allows us to dig deep into our understanding of the user and create solutions that will not only solve a need, but effectively improve our users’ lives by removing unnecessary pain or friction. Instead of just designing an accessible website, practicing empathy is using a screen reader, blindfolded, in order to complete a task on your own website. 

“I am struggling to find my way around the site.”
“This is much tougher than I thought it would be.”
“I will advocate for the changes that need to be made.”

The Spectrum of Empathy 
There is no firm threshold that marks one’s transition from sympathy to empathy. Rather, the relation between the two is best represented on a spectrum with pity (the most disconnected and abstracted version of sympathy) on one end and compassion (the more connected and embodied version of empathy) on the other. 

The spectrum of empathy includes pity, sympathy, empathy, and compassion. Pity and sympathy require little to no effort or understanding, while empathy and compassion require effort to understand and engagement to produce a positive change. 
Pity is simply when you feel sorry for somebody else. You don’t like their unfortunate situation, and maybe you’ll even do something to rectify the situation, but mostly to make your own unpleasant feelings go away.
On the other end of the spectrum is compassion, the feeling where you relate the most to the users as independent actors, as opposed to objects. (""Actors"" in this context means that we recognize that the users have their own purposes, wants, and needs, and that they are acting to satisfy what they want to accomplish, not what we think they ought to do or want. Thus, we don't impose our priorities or preferences upon the users, which would be an act of objectifying them that's more characteristic of sympathy.) Compassion is a call to action derived from empathy — when our understanding of another’s thoughts or feelings give us the compulsion, duty, or desire to help change that person’s situation for the better.  
How to Practice Empathy in UX 


Use Qualitative Research Methods  


Practicing empathy in UX must begin with user research. We must set aside ego and assumptions and immerse ourselves in research. Qualitative methods, such as user interviews, cognitive mapping, and diary studies, allow us to dig into user behaviors, motivations, and concerns. 
Remember to use open-ended questions. When you ask users to explain things to you, they often reveal surprising mental models, problem-solving strategies, hopes, and fears. For example: 
What makes you happy? instead of Are you happy? 
How has your family affected you? instead of Are you close to your family?
What would make you stronger? instead of Tell me your weaknesses.
Practice empathy as you conduct research. Be aware that you don’t know what people are going through and what will trigger a memory or be difficult for them. 


Recruit Diverse Users 


Make accessibility a part of your research plan. This approach allows you to test your assumptions and explore potential opportunities for improvement with actual end users. Use well-known organizations, state chapters, or local training centers to help you recruit participants with disabilities.


Have Your Team Watch Research Sessions and See Real Users


When conducting research, invite all team members, and key stakeholders to observe the sessions. Doing this vastly increases the potential of empathy, and the corresponding acceptance of research findings. Seeing is believing. And seeing and interacting with the user live is even more powerful.
Prior to inviting your coworkers to user testing, you may need to spend some time evangelizing UX research within your team or organization. Make sure everyone knows how deep user-experience design needs to be and what you can accomplish: saved time, minimal rework, and a product that solves a real user need.
Make sure that you post recordings from user sessions in a place where people can access them and watch them on their own, in case they cannot observe live.


Use Videos of Users Whenever Presenting Research Findings to Stakeholders


Supplement your findings and recommendations with video clips showing how users actually perform that task. Not only will your findings be more compelling, but you’ll also be building general empathy towards your audience. Make sure you show a variety of people in your videos (it shouldn’t be hard if you followed our first guideline and recruited a diverse participant group) — with different backgrounds, different demographics, and different abilities.


Make an Empathy Map


Empathy maps capture users’ emotions, hopes, and fears and distill your knowledge of the users into one place. An empathy map can help you discover gaps in your current knowledge and identify the types of research needed to address it.
Even more importantly, an empathy map can help others become empathic towards users because it can act as a source of truth throughout a project and protect it from bias or unfounded assumptions. Empathy is a complex skill, and empathy maps reduce the risk of misalignment because everyone has the same visual baseline. 


Invest in a Diverse Team


“You are not the user” may sound like a cliché, but it’s part of the human psychology to think that others think and behave in the same way that we do. If everybody in your team is male, younger than 30, and with a tech background, you’ll end up with designs that will implicitly favor that user group.
Recruit team members with a variety of backgrounds and demographics. That will not guarantee empathy for users, but will at least be a first step in the right direction. Diversity must go wide to include experiences, skills, and attitudes acquired and evolved over each employee’s lifetime — known as ‘acquired diversity.’ In broad terms, as defined by Chloe Heath, acquired diversity is a person’s experience of the world and the cultural quirks accumulated over a long period. 


Build Empathy into Your Design Guidelines


Within this diverse team, create protocols that encourage empathy. For example, Caroline Jarrett’s Question Protocol uses consistent intention and prioritization behind each and every question, rather than asking all possible questions to the user (some of which may make the user feel inadequate or uncomfortable, bring up bad memories, etc.).
If you’ve been with your team for a while, you may have a good idea of the types of erroneous assumptions it tends to make. Create specific guidelines that can act as a check point against your team’s bad habits, especially when it comes to empathy. For example, if your team is prone to not changing designs that don’t work for specific users, consider introducing a guideline that can correct this behavior. The guideline could be Each design must be tested with a diverse user group that maps onto our target demographics. Instead of Let’s validate this design, say Let’s learn what works and what doesn’t work well for users and why.
Conclusion 
Empathy in UX is essential. It is a bridge into our user’s minds and our greatest asset as UX professionals. Empathy allows us to design with intent, introduce focus and clarity, advocate on behalf of our users, and challenge our assumptions.
(For more on the difference between sympathy and empathy, see also Neel Burton's Empathy vs Sympathy.)"
57,2019-02-24,"Usability testing with a small number of participants is an incredibly efficient way to improve an interface. By observing real people attempt to use a design, we see the interface from the users’ perspective. Since average users lack insider knowledge about how the system is “supposed” to work, they often encounter problems that the creators of the design didn’t foresee.
Discovering these unexpected problems is the point of doing usability testing. But the very fact that they are surprising, and not problems that stakeholders anticipated, often leads to doubts about the whether the issues observed are actually representative of what ‘real’ users would encounter.
Stakeholders who have doubts about whether they should trust findings from usability- tests often assert that test participants are not representative or that there aren’t enough test participants to take the results seriously.

When you encounter these objections, here are some techniques you can use to help teams understand why they should take qualitative usability testing seriously. In the simplest cases, skepticism may be simply a consequence of a lack of experience with the methodology, and can be successfully addressed by clearly explaining the method, and communicating findings in a compelling format.
Prepare Stakeholders with Proactive Explanation
First and foremost, make sure to explain your research method before conducting the usability sessions. Describing the process in advance is much more persuasive than trying to defend it after people have already become skeptical. Prepare your own explanation, or share videos and articles that cover what usability testing is and why you need only a few participants. Acknowledge the limitations of this method upfront: mention that some questions, like whether people believe a certain brand is reliable, could not be accurately answered by talking to just 5 users. But usability testing isn’t about beliefs or preferences — it’s about discovering how normal users behave  in the specific context of using a particular design for a particular task.
Prepare stakeholders for surprising findings by explaining:
“We’ll probably see people encounter problems that we don’t expect, because our familiarity with the design makes it hard for us to spot some issues.”
“Since all users lack our team’s insider knowledge about the design, it’s likely that problems caused by this difference of perspective will affect many users and will show up within the first few test sessions.”
“Rather than trying to prove exactly how bad each problem is by repeating tests with a lot of users, we will focus on quickly discovering common issues, so we can fix them and improve the design.”
Share this context with members of the team in the early planning stages, especially if they are not familiar with usability testing. Even teams that are familiar with usability testing benefit from reminders that they are not like their users.
Hearing Is Believing
Sometimes stakeholders dismiss test results because they view them as just one more opinion — the opinion of the researcher who wrote the report.
The whole point of usability testing is to hear from users. Insights and analysis from a skilled researcher can add great value to a report, but always make sure that users’ direct feedback, in their own words, comes through loud and clear. You can  prominently incorporate direct quotes from your study participants; video clips are even more compelling (if you are able to record sessions and have time to edit the footage).
The most persuasive method of all is to arrange for teams to actually directly observe the usability-testing sessions. This practice allows them to see for themselves that the test participant is a real person and develop empathy for the user’s struggles with the system. When team members directly observe sessions, they can also see that the researcher is not leading or influencing users.
Addressing Objections that Participants Aren’t Representative
To avoid objections that test participants are not representative of “real” users, make sure to test with users who are actually representative — and share how you recruited them in advance with your stakeholders.
Test with Real Users
If you are testing a live website or application, recruiting actual visitors or users is an effective way of finding representative participants. Today’s sophisticated analytics tools (such as HotJar or Ethnio) make it easy to target specific types of users, such as people who are using a particular section of a website or a certain feature of an application. Lists of past or prospective customers are another great source for realistic test participants.
Get Buy-In on User Profiles Before Recruiting
If you can’t recruit participants directly from the actual system, you may need to recruit participants from other sources. But you can still ensure that your participants are representative by carefully defining which traits are important characteristics of your target audience. If you have personas, refer to them. Ask the stakeholders who will be receiving the research findings for their input about who the target audience is. Then screen potential test participants by asking them questions which selectively identify people with all the essential characteristics of the target users.
Whichever method you use to find participants, describe it to your stakeholders every step of the way: during the planning stages of the study, by distributing participant profiles to stakeholders who observe test sessions directly, and as part of the final report.
Objections that There Aren’t Enough Participants to Prove a Problem
Proactive explanations of the usability-testing methods go a long way towards building credibility and confidence in test findings. But there may still be concerns that a small number of people experiencing a problem doesn’t necessarily mean it’s a common issue — especially if only a single participant in the study encounters the problem. That participant could be an “outlier,” who has unusual expectations or habits that are not shared by many other users.
The truth is, seeing at least 2 people struggle with the same issue does inspire more confidence in the validity of that problem. But that doesn’t mean you should discount issues that only one participant experienced. (Statistically, there’s not much difference between 1-of-5 vs. 2-of-5 in terms of confidence intervals.) Instead, gather more information to better explain the context of that single instance.
Analyzing Findings from A Single User
When only a single test participant experiences a problem, first consider the following:

How much would it cost to fix it? If there’s an easy fix, it may be less expensive to just fix it rather than invest time in trying to document the problem, let alone spending time in team meetings debating the issue endlessly. (This option works best when you already plan to test of the next design iteration, where you can check whether the ‘fix’ introduces any new problems.)
How serious was the problem for that one person? Serious issues are typically those that prevent user from completing a task, cause great frustration, or relate directly to key design goals. If any of these are true, but it’s not an easy problem to fix, then further investigation is warranted before committing to a design change.

Gathering more information about single-participant usability findings does not necessarily require more user testing. One starting point to consider is competitive analysis: if a review of competitor designs reveals that most do not have this problem, then fixing it may be important in order to meet user expectations.
Check Independent Data Sources to Estimate Frequency
Existing data sources such as usage analytics or support requests are excellent for estimating the frequency and potential impact of a problem.
For example, review support requests and compare how many of them relate to the ‘outlier’ issue, compared to other issues observed in testing. This information can provide a relative estimate of the real prevalence.
If you don’t have specific data about how many users encounter a problem, you may be able to determine what proportion of users interact with that feature and how valuable those users are to the organization’s goals. For example, imagine one participant in a study misunderstood how a sorting feature worked, and because of that she failed to find a product. If you have analytics with event tracking set up for that sort function, a quick check of that data can reveal what percentage of real customers use the sort function. (Since not all users will experience the problem, the percentage of people affected will be somewhat lower than the percentage who use the feature.)
Compare Test Observations with Usability Theory
Consider the extent to which your single-user usability issue can be explained by existing knowledge about user behavior and what makes user interfaces easy or difficult to use. For example, check the 10 usability heuristics which have proven themselves over decades. Or look at published usability guidelines based on substantial user research with a broad range of other designs. While such broad UX theory doesn’t specifically address your individual design, you can generalize from previous research.
If the issue you observed with a single test participant can be easily explained by existing usability theory, you have a stronger case for believing that it’s a real usability problem. If, on the other hand, the theory predicts that your design ought to be easy and not cause problems, then it’s possible that your unlucky test participant was indeed an outlier and the issue would be unlikely to repeat with any frequency.
Unfortunately, the state of UX theory is such that it can’t conclusively decide the matter one way or the other. Human behavior is so variable, and the usability of a design is further dependent on a myriad of small contextual details, that we can never know with 100% certainty whether something is good or bad. But this doesn’t mean that we know nothing about UX. We have substantial amounts of conceptual insights from decades of prior research about what makes user interfaces easy or difficult for certain categories of users and certain types of tasks, and you can use this knowledge to interpret individual empirical observations.
Also, relating your test observations to the body of externally validated UX knowledge can be a useful way for you to explain your findings to your team. Make it clear that it’s not just you who think that the design has a problem, but that many others have found similar problems in testing other designs. And put a name to the problem. These tactics help you rise about the “tiny sample” objection.
Conduct Quantitative Usability Testing
For potentially serious problems that cannot be placed into context with existing data sources, testing with just 5 users may truly not be adequate to confidently recommend a course of action. In this case a quantitative usability study can provide more confidence in what proportion of users are likely to encounter a problem. If the problem can be tested in an unmoderated remote test, collecting data from 20 users needed for an accurate quantitative study can be reasonably affordable. 
Quantitative research may also be worthwhile if an organization has deeply rooted skepticism about qualitative methods with small groups. Measuring the incidence of qualitative findings with a larger sample of users can prove the reliability of qualitative methods within the specific organization’s designs and users. To capitalize on the investment in qualitative research, thoroughly document and publicize the project within the organization, with particular attention to the correspondence between qualitative and quantitative findings.
Conclusion
It’s pointless to discover design problems if the team doesn’t believe the findings and therefore doesn’t do anything to fix the issues. Effectively addressing doubts about usability test findings is an essential part of user research, and should be incorporated into every research plan, from start to finish."
58,2019-02-10,"Introduction
Customer-journey maps visualize the steps that a person goes through in order to accomplish a goal. To be convincing and compelling, journey maps must be based in truth, rather than a fairy-tale–like depiction of how we would like users to interact with our products.
This article is a discussion of which research methods are appropriate for collecting data to create a customer-journey map. Additional articles discuss when to create customer-journey maps, the 5-step process for creating journey maps, and journey mapping in real life.
Why Conduct Research for Customer-Journey Maps?
Research can be expensive and time-consuming, so what’s wrong with creating and using an assumption map based on stakeholder input and cutting out the research phase? While stakeholders do hold valuable knowledge about different areas of the customer journey, most of them do not have a broad enough perspective of the customer journey, nor a deep enough perspective of users needs at each stage, to be able to piece together a realistic, comprehensive view.
A journey map based on assumptions alone carries two risks:

It carries less weight and is more likely to be written off as “anecdotal” than seen as a compelling tool to drive change.
Team members may end up using what is actually an inaccurate map to make decisions that alter the experience (for better or worse).

Step 1: Look for Existing Data First
Before beginning research for your journey-mapping initiative, spend some time looking for existing, relevant data within your organization. There is often existing (though disparate) information about the journey buried throughout various past internal efforts. This data, both qualitative (e.g., data from past focus groups, customer-support call logs, etc.) and quantitative (analytics, customer-satisfaction scores, etc.) can give you clues about how to shape and focus the content of your research efforts.
Step 2: Conduct Qualitative Research
You may be tempted to use existing quantitative data as the basis for your journey map. While quantitative data can give you a high-level understanding of customers’ general attitudes and levels of satisfaction for specific interactions (think: NPS), it is less appropriate for understanding emotions, mindsets, and motivations at the level required for effectively depicting the entire journey.
For this type of insight, qualitative research methods that allow you to directly observe or converse with customers are a better use of your time. Consider the following qualitative research methods that will allow you to understand users’ thoughts, feelings, and actions at each phase of the customer journey:
Customer or User Interviews
Interviews allow you to hear first-hand stories about customers’ experiences, mindsets, and actions. If you have been able to use existing data to create an overarching hypothesis of the phases in your customer journey, you can ask direct questions about each phase. Broad questions, such as, “Tell me how you feel about [product or service]?” are less helpful than specific ones, such as ""What was particularly challenging or easy about the sign-up process?""
Interviews can be conducted in-person or over the phone. One technique for in-person interviews is to encourage participants to use sticky notes to visually capture their steps from the moment they discovered the need for a product or service through usage. This process helps users recall steps and rearrange them accurately throughout the interview. Subsequent phone interviewees can be sent strawman templates created from the in-person process, and invited to review and revise to as needed to reflect their experiences.
Field Studies
Interviews are a valuable research method for journey maps; however, because what people say they do is not always what they actually do, it’s best to couple interviews with additional qualitative methods, such as field studies. Field studies can take many forms, from in-home visits utilizing contextual inquiry to “shop-alongs” for retail experiences.
Observing customers in their own element is critical for uncovering blind spots and verifying what customers tell you during the interview process. Take note of any differences in findings from interviews and field studies. For example, during one journey-mapping research initiative, customer-service representatives reported the “correct” protocol for finding answers to customers’ questions who called in with an issue. But during a field study, however, those same customer-services representatives were observed using convoluted workarounds to find answers to customers’ questions.
Diary Studies
Because customer journeys happen over time and across many different channels, diary studies are a particularly useful method for understanding users’ thoughts, feelings, and actions over time. Diary studies are long-term studies: Users are asked to log each and every action they take related to a specific goal (e.g., buying a refrigerator or signing up for a new mobile plan), as well as how they felt during those interactions over many days, weeks, or months. Because the participant’s actions, feelings, and thoughts are captured as close to real-time as possible, the (fallible) memory that interviews rely on is eliminated. Data is also captured from participants at all stages of the journey, rather than just from one phase. Diary studies are inexpensive to set up and can be running in the background while you conduct additional types of research.
Competitive Analysis
Competitive analysis can be especially helpful if you are designing a future-state journey map for a product or service that does not exist yet. You can take a virtual approach by using a remote-usability testing platform to record customers’ use of competitor sites and have them comment on their thoughts, feelings, and motivations at specific points within the session. This data allows for research input even when there is no existing user base.
Qualitative Research Methods Appropriate for Customer-Journey Mapping




Research Method


Why It’s Used for Customer-Journey Mapping




Customer or user interviews

One-on-one conversations with customers uncover first-hand stories, frustrations, and needs.



Direct observation

Observing users perform actions in their natural environments ensures you understand the actual flow of user interactions and uncover mindsets interviewees were not able to recall.



Contextual inquiry

Observing users perform tasks while you have the ability to ask questions allows you to clarify your observations and provoke open-ended conversation.



Diary studies

Long-term studies allow customers to document their behaviors, thoughts, and emotions over time so that you can understand a variety of journeys.



Competitive analysis


Competitive evaluations allow you to benchmark competitive experiences and identify their strengths and weaknesses.




Plan a Multipronged, Qualitative Research Study
When time and budget allow, it’s best to use a multipronged approach for customer-journey mapping research. That is, combine several different qualitative methods from above into a research study in order to explore the journey from multiple angles. Here is a sample research plan that makes use of multiple qualitative methods. (This plan should be adjusted based on contextual factors such as project goals, timeline, and budget.)
Sample Customer-Journey Research Plan



1
User Interviews


1a
Conduct in-person user interviews to uncover first-hand stories specific to all relevant phases of the customer journey; use sticky notes to allow participants to map their steps as they talk.


1b
Provide a rough journey template from phase 1a to a new set of participants and conduct phone interviews with a digital whiteboard tool, allowing them to review and revise your template.


2
Field Study


2a
Perform contextual inquiry to observe participants using your product in their actual environment (e.g., their home, the office, etc.) and clarify what you heard during user interviews.


3
Diary Study (Running in background while other methods are conducted)


3a
Conduct a diary study to better understand longitudinal actions, thoughts and emotions.


3b
Conduct user interviews with diary study participants at key milestones throughout the study.


4
Competitive Analysis


4a
Conduct a competitive analysis to compare your findings to user relationships with other similar companies or products.



Finally, when revisiting and revising your assumption map with your newly collected insights, consider taking the opportunity to bring users into your workshop as part of the process!
Complement Qualitative Research with Quantitative Data
Aside from highlighting potential problem areas to help shape qualitative research efforts at the onset of a customer-journey mapping initiative, quantitative data can also add another layer of evidence to your insights to make your narrative even more compelling.
For example, after the qualitative research study has been completed, you may choose to supplement or reinforce findings in the following ways:

Follow up customer interviews with a survey to understand the frequency and magnitude of any of the behaviors you uncovered in your conversations
Use digital analytics (e.g., page views or exit rates for relevant web pages) to add credibility to your claim that certain points in the journey are frustrating to users
Supplement high or low areas depicted in the journey map with satisfaction metrics that align to specific interactions




Quantitative Data
How to Apply:


Survey data
Quantify frequency and magnitude of uncovered behaviors or follow up on unclear findings.


Customer feedback
Tally up feature requests to uncover missed opportunities and make a case for new ideas.


Digital analytics
Quantify pain points with details about frequency and impact to better understand customer frustrations.


Social sentiment analysis
Reveal customer emotions about each stage in the journey to help focus journey-mapping efforts.


Customer-loyalty or satisfaction scores
Align numerical scores to each customer-journey phase in order to bolster qualitative data such as quotes.



Conclusion
As you begin planning your customer-journey–mapping research initiative, use the following steps as a guide:

Before investing in external customer research, take a look around your organization: Is there any existing data that might prove useful, or at least help you shape your research plan?
Plan your research study by selecting a combination of qualitative research methods that allow you direct interaction with or observation of users. User interviews, field studies, and diary studies are all useful, appropriate methods.
Use quantitative data from sources such as analytics, customer-satisfaction or loyalty scores, and surveys to reinforce and supplement findings from qualitative research in your map.
Bonus tip: Always remember to keep your core team of stakeholders involved in the research process and aware of new insights and developments throughout your study. Involvement increases buy-in and lessens stakeholder attachments to assumptions!

To learn more, check out our course, Journey Mapping to Understand Customer Needs."
59,2019-01-27,"While presenting a recent training seminar on quantitative research for UX, I was asked an interesting question:

“I’m leading a team tasked with overhauling a complex enterprise product. We have a high-fidelity prototype, and we’ve been conducting extensive research to get it ready for launch. The stakes are high — even minor improvements could lead to big productivity gains, but conversely, minor issues in the design could cause big problems for our users.
In our quantitative usability testing, we’ve seen substantial reductions in the amount of time it takes people to do important tasks in the prototype as compared to the old product. But here’s the problem — in qualitative interviews, our users hate the prototype. The feedback is so negative. How do we reconcile this contradiction? What should we do?”

The situation that this UX lead described is a common one for many research-focused digital product teams.
The ideal way to conduct UX research is to use multiple methodologies, mixing both quantitative and qualitative research.  Using multiple approaches to answer our research questions and to see our product’s performance in different ways is a sophisticated triangulation strategy,. But what happens when those different research methods tell different — even contradictory — stories?
In this article, I’ll consider some possible problems, explanations, and solutions for this UX lead’s situation. Unfortunately, I have very few details about her product and her research, but I’ll generate some theories based on other questions I’ve been asked and teams I’ve worked with.
Check the Methodology
In situations like the one described above, the first step is to examine how each study was conducted. Since UX research involves studying human beings, there are a huge number of potential mistakes that could’ve resulted in an incorrect or misleading finding. 
Before we consider what these contradictory findings might mean, we need to check some critical components in each study. We should look for potential problems in four areas:

Participants
Tasks
Logistics
Analysis

Participants
Who was involved in each study? 
How many people participated in each of the studies? Were there any outliers — people who behaved very differently than the rest of the group?
Was the same user group involved in both the quantitative and the qualitative research? How were the participants in the two studies recruited? Answering these questions may point to the reason behind the contradictory findings.
For example, maybe the researchers decided to recruit users with different levels of expertise in using the product. If novices participated in the quantitative research, but experienced users provided the qualitative feedback, then the differences between the participant groups could influence the results.
Tasks
Which tasks did the quantitative study include?
It’s possible that users may be more efficient when performing a handful of tasks that the company cares about. However, if users regularly engage in a broad set of tasks, that increased efficiency in one area of the product may not exist throughout the system.
How much exposure preceded the qualitative interviews?
If the researchers just pulled up the new version on a laptop, pointed to it, and asked participants what they thought without giving them the chance to actually complete any tasks, that could explain some negative responses. Users could’ve just been reacting to the fact that the UI looked new and different, if they didn’t have adequate time to explore its features.
How much exposure preceded the quantitative study?
Were users given any practice tasks before they tested the system? Did they receive any type of training from the researchers? If the quantitative participants had more exposure to the new system than the qualitative participants, they have had time to get over their initial negative reactions and learn to be efficient with the new product.
Logistics
How were the studies run?
We’ll need to verify that the studies were conducted in a reasonably realistic manner — that the studies have external validity.
For example, imagine that this product is always used on a factory floor, where users are exposed to a lot of environmental noise and distractions. If the study was conducted in a quiet conference room, the users may have performed better with the new version. But there could be some aspect of the design that would make it perform worse in realistic conditions.
Additionally, we’ll also need to check that there wasn’t some accidental problem with how the quantitative study was run that could have biased the result — that the study has internal validity. We can ask: who was moderating those tests? How much experience did the moderators have?
Even small confounding variables could produce an invalid result. For example, imagine if all of the participants who tested the new version of the product did so in the morning on a Monday, and all of the participants who tested the old version did so in the evening on a Friday. There could easily be something about the timing of the tests that influenced the participants to perform better or worse.
Analysis
Do we have statistical significance?
For the quantitative research, was the difference between the two designs statistically significant? In order words, were the faster task times in the new version reliable and not likely due to random change?
How was time on task analyzed?
In many studies, the time on task includes only successful attempts. The new design was faster than the old one, but were the success rates comparable?  If the average time on task increased by 2 minutes, but the proportion of users who could successfully complete the task decreased by 40%, that would still be bad for the company and the users!
What the types of errors did people run into? 
We should look not just at time on task, but other metrics that were collected during the quantitative study, to see if they all suggest that the new product is better. Even if there were fewer errors with the new design, it’s possible that they were more severe than the errors made with the old system and that they influenced users’ attitude in the qualitative study.
Interpreting the Findings: Not Only What Users Say, But Why?
If we find no substantial faults or explanations in the methodologies, it’s time to consider what the conflict between these two standards of quality (quantitative efficiency and qualitative satisfaction) might mean.
As UX professionals, it’s our job to listen to users. But as any experienced UX professional will tell you, that sounds easier to do than it really is. That’s because we can’t just listen to users and follow their verbatim requests. People usually don’t know what they really want — your users aren’t the designers of the system, they can’t see the big picture the way you can. What’s worse, their feedback is often influenced by other factors (faulty memories, social pressure, psychological biases, etc.)
This is part of the reason why a triangulation strategy is so necessary. We can’t just ask people what they want and do what they tell us. We have to collect a mix of data (quantitative, qualitative, self-reported, and observed) to really see what’s going on.  Then we can use that information to interpret what our users say.
So, in this UX team lead’s example, how should we make sense of the user feedback, which seems to contradict the quantitative performance data? We need to look at why these people might be responding so negatively to an objectively better product, while the task times in the quantitative study seem to be better.
Perceived Usability Can Differ from Objective Usability
Unfortunately, we don’t know exactly how much this particular team reduced time on task in the quant studies. The UX lead said that the reduction was “substantial,” but that could mean a matter of seconds or minutes. From the company’s perspective, even a reduction of seconds could be hugely beneficial. Imagine that thousands of employees perform this task thousands of times per year — at the company level, those efficiency gains add up quickly, and could result in cost savings.
However, from an individual user’s perspective, those gains might not matter so much. If it’s an improvement of seconds, an individual user may not even realize that the new system is actually faster, as she doesn’t see her own time on task or that of other participants. Or maybe they do realize the new system is faster, but those small gains may not seem worth the difficulty of a new workflow.
People Don’t Like Change
The users of this complex enterprise product have been using it almost every day for work. Some of them have been using essentially the same version of the application for many years. Even if it isn’t the most efficient it could possibly be, they’re used to it. They know how it works. By changing things, the design team is asking the end users to invest effort to become proficient with the new version. (It’s a common finding that users hate change — which is a reason to do research before release so that subsequent changes can be minimized.)
If users in the quantitative study received training or practice with the new system before the test (as described above in “Check the Methodology”), there may have been an initial lag in performance that was not captured by the measured task time. When a new interface is introduced, there will sometimes be an initial loss of productivity. Learning a new interface for a complex task takes time and is less efficient than simply doing the task with the old, familiar interface. Even though in the end the new interface may prove better, (1) people have no way of knowing that when they first start using it; (2) in the beginning, the experience can be worse. 
It’s also possible that there was one negative reaction to some (presumably minor) feature of the new system — for example, a change in color that people did not like, a change in the visibility of the teams’ contributions in an Intranet — that did not necessarily affect UI performance, but dominated their reaction and created a peak-end effect.
Next Steps
My advice to this team lead was to first consider these reasons behind the user feedback, and then step back and look at the larger picture. Of course, in UX, quantitative data should never automatically overrule qualitative information or designers’ instincts (taking that approach leads to comical design mistakes.)
When weighing conflicting findings, we have to consider the tradeoffs. We always want users to be effective, efficiency, and happy with the products they use. However, in this context, the potential efficiency gains are probably much more attractive to stakeholders than the employees’ happiness. This new version of the product is very likely to be implemented, regardless of how users feel about it. That could be a potential problem, though — if users hate this new version enough, it could lead to decreased job satisfaction or employee turnover. It’s worth this company’s time to try to make its users both efficient and happy.
As we’ve discussed, this negative feedback may be a temporary negative reaction to change. Since the stakes are high, and so is this team’s research budget, my recommendation would be more investigation to see if that hypothesis is correct. The team could try qualitative beta testing with new hires, who had minimal exposure to the previous system, and see if their feedback differs. New hires will not have the same attachment to the old system as more experienced employees and may be less susceptible to affective reactions to change. (On the other hand, new hires are also less likely to have as much domain knowledge as people who have been using the system for a while, so they may ignore some important aspect.) Positive feedback from new hires might indicate that the experienced employees’ responses were caused by an initial aversion to change.
Or, the team could conduct a systematic learnability study, with multiple rounds of quantitative usability testing that track task time, task completion, and satisfaction over time. This study will give an accurate and complete picture of how user performance and satisfaction changes as people gain experience with the new product. If the new design is truly better than the old one, the team should expect both the satisfaction and the performance measures (task time and task completion) increase over time and eventually reach comparable or better numbers than the current design. The study will give a good idea of how much exposure to the new design people need in order to overcome their initial negative reaction.
(We did one such study for a consulting client. While the details have to remain confidential, I can say that it took a full year before users performed better with the new design than with the old, which they had used daily for a decade. In the long run, the new design was indeed much better, but the decision to change over required long-term commitment from management.)
If those studies show that the initial negative reactions will be replaced by long-term satisfaction and productivity gains, then the team can be confident that it is moving in the right direction. From there, they can plan an incremental rollout of the new system. Allowing current users to opt in to the new product when they’re ready (and not under pressing deadlines) can reduce the short-term frustration.
Alternatively, another possible outcome of research could be that the new design is mostly good, but that there’s some good aspect in the old design that should be retained in the new version.
The Challenge of UX Research
Making sense of contradictory findings is part of the challenge (and the fun) of conducting UX research. Each methodology is just one piece of information, a way of looking at our users or our products from a different perspective. The data should always inform our decisions, but at the end of the day, it’s up to us to make sense of that information and make the best choice.
 
For more strategies to help you incorporate quantitative data into your decision-making process, check out our full-day course, Measuring UX and ROI.
For more on how to correctly analyze quantitative data, check out our full-day course, How to Interpret UX Numbers."
60,2018-10-07,"A user interview is a UX research method during which a researcher asks one user questions about a topic of interest (e.g., use of a system, behaviors and habits) with the goal of learning about that topic. Unlike focus groups, which involve multiple users at the same time, user interviews are one-on-one sessions (although occasionally several facilitators may take turns asking questions).
UX Interviews tend to be a quick and easy way to collect user data, so they are often used, especially in Lean and Agile environments. They are closely related to journalistic interviews and to the somewhat narrower and more formal HCI method called the critical incident technique, which was introduced in 1954 by John Flanagan.
Although you may feel that doing a UX user interview is simple and straightforward, there is more to a good interview than many people realize. Here, I distill some of the best practices.
Why Do User Interviews?
Interviews give insights into what users think about a site, an application, a product, or a process. They can point out what site content is memorable, what people feel is important on the site, and what ideas for improvement they may have. They can be done in a variety of situations:

before you have a design, to inform personas, journey maps, feature ideas, workflow ideas
to enrich a contextual inquiry study by supplementing observation with descriptions of tools, processes, bottlenecks, and how users perceive them
at the end of a usability test, to collect verbal responses related to observed behaviors
	
(Do defer the interview until after the behavioral observation segment of the usability study: if you ask questions before the participant tries to perform tasks with your design, you will have primed the user to pay special attention to whatever features or issues you asked about.)



How to Do a User Interview
First and foremost, think of an interview as a type of research study, not a sales session or an informal conversation. Then, use the following tips to make your interviews most effective.
Set a goal for the interview.
Ask product stakeholders what they want to learn. From their desires, determine the main goal, ensuring that it’s realistic. Too broad of a goal, like learn about users, is a likely to make interviews fail, because it will not focus your questions in a direction relevant to your design needs. A concise, concrete goal related to a specific aspect of the users’ behavior or attitudes can bring the team to consensus, and direct how you’ll construct the interview.
Examples of good interview goals:
How do nurses feel about logging medical data, and what are the processes they believe they use?
Learn how architects share CAD drawings with engineers, and where they feel there are challenges and opportunities.
Find out how bicycle couriers get the best route directions, and what they feel works well, where they think there are issues, and how they think things could be improved.
Make the user feel as comfortable as possible. Create a rapport with the user.
People are more likely to remember, talk, and let their guard down if they feel relaxed and trust the interviewer and the process. Here are some tips for an effective interview.

Have a video call or phone call (or at least some interaction) with the user before the interview itself.
Before the interview day, and also at the start of the actual interview, explain the reason for the interview, and how the data from it will be used.
Make the user feel heard by taking notes, nodding, frequent eye contact, offering acknowledgments like “I see,” and repeating the words the user said.
Let users finish their thoughts. Do not interrupt them.
Don’t rush the user. Pause. Slow down your pace of speech. Talking slowly has a calming effect and indicates that you are not anxious and that you have time to listen.
Start with questions that are easy to answer and that are unlikely to be interpreted as personal or judgmental. For example, instead of “What was the last book you read?” try “What do you like to do in your spare time?” The latter is open-ended, while the former assumes the user read a book recently; those who did not may feel stupid.
Show some empathy by asking related questions. But recall that it is difficult to act sympathetic without also being leading or making assumptions. For example, imagine that a user said he could not reach the customer-support team. You can show some concern by asking the user to elaborate: “You couldn’t reach support. Can you tell me more about that?” You could even try a question like “How did that make you feel?” but only if the user did not already indicate how he felt. For example, if the user already verbally or even nonverbally expressed frustration when recalling the event, then asking how he felt would seem as though the interviewer had not been listening. As an empathetic human being, you might want to say, “That must have been frustrating,” or “I’m sorry your time was wasted like that.” But those would be leading points. Instead, asking a question that relates to the users’ feelings can show that you are listening and feel for their plight. At the absolute end of the interview, you can express some of these more apologetic sentiments.
Be authentic, and don’t fake empathy. Acting can make you appear disingenuous. It is better to be yourself; don’t say something if you don’t genuinely feel it.

Keep in mind that there’s a big difference between rapport and friendship. The user does not have to really like you, think you’re funny, or want to invite you out for a cup of coffee in order to trust you enough to be interviewed.
Prepare questions before the interview.
While you will likely think of questions while sitting with the user, do bring to the interview a list of questions you aim to have answered. A question list ensures that you will:

be able to get your team’s feedback about your questions before the interview
remember everything that you wanted to know and ask users about as many of the right topics as possible during the interview
construct clear, nonleading questions better than you would in the moment
overcome your stress or fatigue by having questions on hand to refer to

Anticipate different responses, and construct followup questions based on your research goals.
Of course, the whole reason you are doing interviews is because you don’t already know or feel completely confident about what people will say. Yet, anticipating answers to the best of your ability can help you better prepare for the interview.
Think about what you would do if you hit a dead end — in other words, if the user did not have a response for your question. Are there ways in which you can help the user to find an answer? For example, imagine you are working on a new travel website, and that a participant was recruited because she has booked travel online within the last 6 months. Let’s pretend that some of the research goals of the interview are:

Do people remember how they chose vacation destinations?
What’s memorable about vacations?
What do users feel is easy about booking travel now?

To begin, ask users if they can recall a time when they booked travel. Prepare additional questions in case they can’t remember a relevant event right away. See the image below for a possible flow addressing that situation.

Examples of how two different people might respond to the same question followup questions (in grey boxes) that the interviewer may ask to get to the same place.

Write dialog-provoking interview questions.

In each question, ask for just one thing. Instead of “Do you use a navigation system, and if so, which one?” try “How often do you use a navigation system?” then follow up with “Which one or ones do you use?”
Jog the memory by asking about specific events rather than about general processes. Remembering an incident will nudge the user’s memory and enable them to talk about precise occurrences.

For example, imagine the interviewer is a doctor who wants to know the last time a patient had an asthma attack. She reviewed the patent’s history and anticipated some questions. An interview might go like the one in the image below.

Example questions (in grey boxes) that a doctor might ask to learn about how a patient’s asthma attacks were triggered.


After you ask about an event (e.g., an asthma attack), wait a few moments to give the user and opportunity to think about that event. Then begin asking questions about the event, such as, “When did that happen?” or “What were you doing before that happened?”

Avoid leading, closed, or vague questions.
Ideally, your questions should elicit rich, unbiased answers from the interviewee.

Leading questions prime the user by inadvertently suggesting a response. For example, a question like “Why do you enjoy using the Acme product so much?” suggests that the user uses the product and enjoys using it. A better question might be “Why do you use the Acme product?”
Closed questions elicit “yes” or “no” answers. For example, if an interviewer asks, “So, you use the Acme product each morning?” then the participant could sincerely respond with just a, “yes”, and not elaborate. A better question might be “Can you tell me about how you use Acme?”

A caveat: while closed questions are less likely to elicit wordy answers, they are easier for users than open-ended questions. Sometimes, you can precede an open-ended question with a closed one to ease the user into a topic or protect users from feeling stupid when they don’t remember an event.
For example:

“Do you remember when that happened?”
“Yes.”
“When was it?”

(This type of question sequence is okay during a user interview, but is less appropriate in a usability test, where we want to limit interaction with the user as much as possible.) Vague, ambiguous questions are difficult to understand and often confuse participants. They can also make people feel uncomfortable or guilty for not understanding what you mean. To figure out if a question is too vague, consider informally testing it with random people to see if they understand what you mean.
Prepare more questions than you believe you will have time to ask.
Some participants like to talk and give very long answers to questions. Others need prompting in the form of followup questions to deliver the same amount of information. Be ready to address both situations.
Practice your go-to followup questions.
Have at the ready some clear phrases to prompt users to elaborate an answer. I have used:

These questions can be used in virtually any situation.

Locations for Interviews
User interviews can be conducted in many different locations — at the user’s site, in a controlled environment like a lab, or remotely, using online-meeting tools.
Consider these factors when choosing locations:

User convenience and comfort: Which location will be most comfortable and easiest for the users? Will it be more likely that they will not cancel if the session is in their office or at their homes?
Team convenience: Do you want your team to observe the interviews?
Context and examples: Is it important that users have their own tools and other environmental elements at the interview? Artifacts can nudge the interviewee’s memory and can also paint a better picture of the users’ processes for the interviewer. However, sometimes getting people out of their usual environments can help them think freely and creatively.
Bias: Is the location likely to sway the users’ stories? If you brought people to your Acme office and asked about Acme usage, will they say more nice things about Acme than if they were in a different location? (Spoiler alert: the answer is yes.)

Interviews vs. Usability Tests

 
Some researchers confuse the user-interview method with the usability-testing method. While the methods do have some commonalities and a user-testing session may include an interview at the end, the differences are many and important. Some of these differences are summarized in the table below.




Differences Between User Interviews and Usability Tests




 


Interview


Usability Test




A design (early sketch, prototype, or working software) is necessary for the study.


No
It’s possible to ask questions in the absence of any design.

			 


Yes
In a usability test, users interact with the design.

			 




User data is behavioral. 


No
			Users report their beliefs and perceptions in an interview.


Yes
			Researchers observe what the users do.




(Some) data is self-reported. 


Yes


Yes
In a user test, researchers base their findings not only on what people do, but also on what people say.




The participant must talk a lot to for the research to be effective.


Yes
			Interviews rely on the user giving opinions, recalling events, and discussing them.


No
			A usability tests can be informative even if the user doesn’t talk much.




Facilitators/interviewers maintain normal eye contact with the user, as they would in any conversation.


Yes
			The interviewer often faces the user or sits by her, and looks at her as though they are having a conversation.


No
			Usability-test facilitators avoid the user’s direct line of vision and sit next to and a bit behind the user: ideally users suspend disbelief and act as if they were on their own.




Facilitator creates a somewhat strong rapport with the participant.


Yes
			Interviewers usually need to bond at least slightly with the user to elicit information.


No
			Usability tests facilitators should be warm, polite, straightforward, and trustworthy in the session setup. But, during the session, they should fade into the background of the test as much as possible.




What You’ll Learn
Before you do a user interview, consider what it is that you want to learn, then choose your research method. To help decide between an interview and a usability test, refer to the table below.




Types of Things Learned in Interviews vs. Usability Tests




 


Interview


Usability Test




Whether a design is easy to use


No
			 


Yes




What makes a design easy or difficult


No


Yes




Whether people believe they would use a design


Yes


Yes




Whether people would use a design


Maybe


Maybe




Note that neither user interviews nor usability tests are guaranteed to tell you whether people will actually use a design. Asking users “Would you use this?” prompts them to rationalize their answer and potentially ignore certain aspects of the reality that are likely to affect their behavior but may go against their response. And a usability test encourages participants to engage with a design more than they might normally do (as they complete different tasks); in doing so, they may discover features or qualities that can ultimately affect their willingness to use the design.
A word of advice: don’t choose to do an interview just because you don’t know how to do a usability test or because you can’t stay silent while a participant uses a design. Almost anyone can [learn how to] do a usability test.
Limitations of Interviews
Unlike behavioral data that captures how participants interact with a design, data from interviews is self-reported — it reflects users’ perceptions and feelings about a process, a site, or an interaction. Like any self-reported data (included that from focus groups and surveys), interview data is tenuous because:

Human memory is flawed, so people don’t recall events fully or accurately.
Participants don’t know exactly what is relevant for the interviewer, so sometimes leave out details. They usually don’t think minor interactions are important enough to bring up.
Some people are proud or private, others are shy and easy to embarrass. Thus, not everybody will share every detail with a stranger.

Conclusion
Interviews are a quick and easy way to get a sense of how users feel, think, and what they perceive to be true. Do them, but complement them with observation-based research to attain an accurate and thorough sense of what users really do and a higher feeling of confidence with the information you collect.
Learn more: User Interviews, Advanced techniques to uncover values, motivations, and desires, a full-day course at the UX Conference.
Reference
Flanagan, John C. (1954). The Critical Incident Technique. Psychological Bulletin. 51(4), 327-358. http://dx.doi.org/10.1037/h0061470"
61,2018-09-02,"UX is sometimes perceived as a “soft” science. Often, that’s due to our field’s reliance on qualitative research and observations. To investigate how digital product teams use quantitative research to get “hard” data (or why they don’t) we surveyed 429 UX professionals.
How Often Teams Use Quantitative Research
We asked respondents to roughly estimate how frequently they, or someone else on their team, perform quantitative studies.


The majority of respondents said their teams perform quant research at least occasionally: 44 % report doing it “sometimes” and 27% say they perform one or more quantitative studies per design project.


When interpreting these results, bear in mind that there may be some sample bias at play. We recruited our respondents from Twitter and LinkedIn outreach, and offered a chance at a free report or online seminar as an incentive. We also mentioned that the survey was about quant practices. As a result, our sample consisted of NN/g-fan UX practitioners who potentially were interested or had heard of quantitative research methods. I’d bet that sample (and probably you, since you’re reading this) likely performs more quant research on average than the wider UX community as a whole.
(Although, since you are reading this article, that likely means the results of this survey are probably fairly representative for the kinds of projects you work on, even though they may not be representative for all design projects in the world.)
We were somewhat surprised to hear that the majority of our respondents (71%) said they performed quantitative research either “sometimes” or “at least one study per project.” This result makes sense when we look at the methodologies that respondents report using.
Which Methodologies Teams Use
We asked respondents to tell us how frequently their teams were using 11 popular UX research methodologies — 7 quantitative and 4 qualitative. For each methodology, respondents told us if they were using it “often,” “sometimes,” “rarely,” or “never.”
 


This stacked bar chart shows the proportion of respondents who reported using each methodology often or sometimes. Analytics, qualitative usability testing, and interviews were tied for the most popular methodology, each with 67% of respondents reporting that they use it “Often” or “Sometimes.” These three were the only methods used by a majority of respondents, with everything else clocking in at 48% at the most.


The most frequently used methodologies, as reported by our respondents, were, in order:

Analytics (Quant)
Qualitative usability testing (Qual)
Interviews (Qual)
Large-sample surveys (Quant)
Small-sample surveys (Qual)
A/B or multivariate testing (Quant)
Card sorting (Quant/Qual)
Quantitative usability testing (Quant)
Focus groups (Qual)
Tree testing (Quant)
Eyetracking (Qual/Quant)

Predictably, the relatively lower-cost methodologies rank near the top (analytics, qualitative usability testing, interviews) while more expensive methodologies are towards the bottom (quantitative usability testing and eyetracking, which can be both prohibitively costly).
These results give some context to the surprisingly high frequency of quantitative studies in design projects. 74% of respondents who reported using quantitative research at least once per project also reported using analytics “often” (86 out of 117).
Analytics data can play a significant role in UX design. Unfortunately, in this survey, we don’t have an indication of exactly howrespondents were using analytics in their projects. 
It’s possible that many of the analytics-heavy respondents, who reported using quantitative research at least once per project or sometimes, used that analytics data in a meaningful way to guide their design projects — for example, to help them identify problem areas in the product. 
However, it’s also possible that those simply reporting that their teams were just collecting analytics data in every project, not that it had any real significance for their work. This possibility is supported by some of the open text-field comments from our respondents.

“We don’t have funding for more advanced quantitative research methods (beyond click tests, surveys, etc.)”
“My company started as an A/B testing and CRO company and hasn't evolved their thinking beyond conversion rates.”
“My manager has access to the analytics and measures success on traffic within our products/sites rather than interpreting the numbers to extract meaningful insights.”
“They're the only metrics we have access to. We don't collect any usage data, so sales and revenue exclusively drive product decisions.”

Success Criteria
User research (not just quantitative, but qualitative too) can help us determine whether our designs work as we want them to, and whether we’re meeting our goals.
We asked respondents in our survey how they know when a design project is successful. In a multiselect, they could choose from 5 options:

Based on calculated improvements using metrics and quantitative research (like quantitative usability testing, NPS, and analytics)
Based on observed improvements through qualitative research (like qualitative usability testing and interviews)
As long as the leadership/executives are happy, the design changes are counted as a success
We don’t really know
Other



Respondents told us how they and their colleagues judge the success of their design projects.Based on quant research, based on qual research, and happy stakeholders each received selections from more than 40% of the respondents. (These percentages add up to more than 100% because people could select multiple options.)


In an ideal world, success would be evaluated by both qualitative and quantitative data — observations and measured results. That would tell us if we’re hitting our goals and making our users happy, which would make the leadership or stakeholders happy as a natural consequence.
Unfortunately, those aren’t the results we found: only 24% of respondents checked bothof the options for looking at quant and qual research. While quant and qual data were each prioritized by over 40% our respondents, so were happy executives/stakeholders. Additionally, a disheartening 18% of respondents admitted that they “don’t really know” whether or not their design changes are actually improvements.
Quant Research Challenges
We often hear complaints from UX teams that they want to do more quant research, but too many insurmountable obstacles get in the way. To capture this in our survey, we asked respondents to choose their quant research challenges from a multiselect list of nine of the concerns we hear most frequently, plus an “other” write-in field.

Quantitative research is too expensive
Quantitative research is too time-consuming
Difficulty recruiting enough participants for large sample sizes
Lack of knowledge on the team about how to conduct or analyze quantitative research
Lack of knowledge on the team about what quantitative research is, when to use it, or what the methodologies are
Lack of understanding of the value of quantitative research
Lack of understanding of the value of research in general — not just quantitative research
Difficulty interpreting or reporting quantitative research findings
Another group in the organization is responsible for quantitative research, and UX isn’t included
Other
I’m not sure



Respondents told us about their issues with quantitative research. (These percentages add up to more than 100% because people could select multiple options.)


Difficulty recruiting large samples was the most popular response (37%). Some respondents reported struggling to collect large samples because their end users were blocked by gatekeepers. For example, one respondent who works on an enterprise product explained, “We rely on Product Management to decide when we can or cannot contact customers who may not want to offer their employees' time.”
After difficulty recruiting, the rest of the options had fairly similar rates of selection (16–29%). Only 2 respondents out of 429 reported performing at least one quantitative study per project and having no significant concerns. The primary takeaway here seems to be that almost everyone struggles with quantitative research in some way — even those who reported doing quantitative research frequently. 
Ignorance as a Roadblock
Notably, lack of knowledge about quant methods and analyzing quant data ranked towards the top of this list. Quant research can be intimidating to UX professionals, their teams, or their stakeholders, and ignorance is a substantial roadblock. 

“[Our challenge is,] in particular, the advanced math behind A/B testing”
“Lack of data scientists (1 at the moment) and limited quant skills (or training) for qualitative researchers. Can be off-putting […] when there are better people who correct you too ;)”
Several respondents working in consultancies or agencies complained that they struggled to “get client buy-in” on quant research. 
“Our UX team understands the value of it, and how/when to use it. The product teams in the rest of the organization, however, is a different story...”
“Owner does not see the value in quantitative research. Crazy, I know.”

Despite these challenges, there’s some good news in these results: only 16% of respondents found quant UX research to be too expensive. Obviously, whether or not something is considered “expensive” depends on three factors: the cost, the cost–benefit ratio, and the available budget. (Even research with a favorable cost–benefit ratio will be too expensive if the cost exceeds the available budget.)
A decade ago, it was the common understanding in the UX field that quantitative studies were expensive and reserved for extravagant, well-funded projects in big companies. We’ve always advocated discount usability instead of deluxe usability methods, in order to get user research more widely used. 
However, while cheap methods are still great and should account for the majority of research on a design project, quantitative methods are less of an unaffordable luxury than they used to be, for three reasons:

The cost is down, due to improvements like remote research services (e.g., UserZoom) and automated data collection.
The cost–benefit ratio is more favorable, because quantitative findings are used as more than just vanity statistics — instead, they’re often used for longitudinal tracking, demonstrating ROI, and triangulation with qualitative findings.
User-research budgets in general are growing, as more companies move to higher levels of UX maturity.

Your Quant To-Do List
Based on these findings, we have the following recommendations for you to consider on your next project.

You can probably afford to do some quant research, so plan for it. Even if that means starting with the cheaper or more lightweight methods like analytics, that’s ok. Getting your team or your client started with quant research is the important thing, and you can work to expand your quant methodologies as you build up expertise.
As early as possible, consider how you can get the necessary sample size, since this is the problem holding most teams back and shouldn’t be left to the last moment. If you struggle with getting around gatekeepers who block your access to users, it might take some networking to convince them of the importance of your research. If recruiting is a constant obstacle for your team (and you have the resources), consider dedicating an in-house part-time or full-time recruiter to the job.
Before even starting the project, educate yourself and your team on the available quant methods, the appropriate uses for each, and how to interpret findings and turn them into action items. Don’t let “lack of knowledge” stand in your way.
Don’t just use the most popular methods, but consider when some of the more specialized methods will be more valuable to answer specific design questions.
Combine methods. In particular, use a combination of quant and qual studies to inform each other, increasing the effectiveness of both.
Plan how you will judge the success of your design at the very beginning,preferably using a combination of quantitative data and qualitative observation. That way you’ll join the elite 24% of UX practitioners that do this right.

To learn about the value of quantitative research, quantitative methodologies, and how to choose between them, check out our full-day seminar, Measuring UX and ROI."
62,2018-07-08,"Intranet teams are often faced with shrinking budgets and team sizes, which increase workloads and make it difficult to include observational user research in redesign projects. Instead, many teams survey their employees to inform their redesigns. Any research is better than none, and product suggestions and complaints about intranet issues communicated through feedback surveys can be helpful. However, self-reported data is not enough for a good redesign, and can be misleading: survey responses often represent only a small proportion of employees and may paint an unrealistic picture of the existing strengths and weaknesses in the current design. For example, some of the usable elements of the intranet may go unnoticed and may even end up being changed in favor of fixing issues for the few “squeaky wheels” who complained.
Observational research (including in-person and remote testing and user interviews) is commonly used and recommended to inform intranet redesign. However, in this article, we point out the advantages of a less used but informative research method: field studies.
Some of the main benefits of doing intranet field studies include:

seeing realistic context and user data
understanding user journeys (and correcting inaccurate theories)
building empathy with users

Seeing Realistic Context  and User Data
Lab studies isolate participants in artificial settings. Despite the researchers’ best efforts to simulate real-world conditions, sometimes that is not possible.  For example, a participant reaching a dead end in one task might say, “When I do this at my workstation, I pull up this document that’s on my desktop and upload that file.” However, you won’t see how she retrieves the file (Does she remember the name? Is it written on a piece of paper by her desk?) and how easy it is to use it. Or, an employee accessing the intranet on the phone, in a busy environment, may have a lot more competing stimuli to deal with than the same employee completing the same task in a lab setting.
A field study provides data about how users divide their time across tasks, physical or contextual limitations that may influence task completion, as well as props and flows that employees may develop in order to optimize their efficiency.   
Not only do field studies enable rich observations in realistic contexts, but they also show you how people work with their actual data. In the field or a lab, getting clearance to observe their data largely depends on the sensitivity of the information being processed and on the authorization levels of the research team. If the user’s real data can be observed, in the lab or field, the observations will be more accurate and more insightful than if the research team were to come up with “nearly real” data.
Understanding User Journeys (and Correcting Inaccurate Theories)
Field studies not only help us understand the details of the different activities as people perform them in the wild, but also provide us with a holistic picture of these activities and how they fit in the users’ workflow. They tell us:  

What the actual journey is
How the employees transition across different channels (digital or physical) to complete their task
What sources of information the employee relies on
Where in the journey the employee is having a hard time
Why the employee is having a hard time and what specific need is not addressed at these points

In a typical work setting, users may engage in activities that the researchers did not anticipate, or they may complete tasks in unexpected ways. Thus, field studies may ultimately build a more accurate representation of workflows within the organization. They can also help identify workflow pain points and needs that are not met due to a mismatch between the design of the existing system, the assumptions of the team building the system, and the mental model that users have of the system.
Users may be able to tell us about some of these pain points. However, their rendition of the problem may be incomplete or inaccurate. For instance, while serving in the Army Reserve, I once volunteered to run a field study intended to improve communication with service members who resided outside a unit’s geographic region. I had initially conducted interviews and surveys with these service members and identified a common complaint:  their forms were often rejected due to improper formatting or inaccurate information. Many service members suffered monetary consequences such as paying out of pocket for their own travel expenses; failing to get paid for duty; and getting flagged as delinquent for training when, in fact, they had submitted the paperwork months before. Further, many headquarters personnel grew resentful about these issues and started to hold assumptions that these geographically separated service members “were lazy, didn’t read the forms, and didn’t sign into the shared drive like they were supposed to.”
However, the field study revealed system and form issues, which were the true causes of the problem. These issues were experienced in different ways by multiple users:

System administrators were forced to deny access to parts of the share drive, to protect against accidental data removal: people would often copy files to their own local or personal folder to “save” them; however, some users inadvertently moved the file out of the shared drive in the process.
HR administrators at higher echelons wanted to protect themselves against liability, so each level of bureaucracy that processed the form had its own followup document required. This practice resulted in up to three versions of a form, each needed by different people at different levels.
HR administrators at lower echelons needed to maintain an up-to-date version of the form, but they had no power over the higher-echelon’s digital forms. So, they ended up forwarding any updated forms to service members via email, and otherwise collecting and printing the forms and putting them together in stacks outside each unit’s HR office — that was the only place where you could find a completely updated version of the all the documents. 
Service members assumed that this stack of forms was composed of the same documents on the shared drive (but they were not). Even when using the correct forms, the users rarely had a good frame of reference for how “right” was supposed to look, because of a lack of information governance or “right” example.

The list of issues discovered was very long for this specific study, but it all boiled down to misunderstanding the needs of each type of user, and a lack of visibility of those users.
The solution was ultimately an overhaul of the workflow, stricter information governance, and modifying the documents so they could be used by multiple groups and accessed via a less restrictive system without compromising data security. We probably never would have come up with these solutions if we had not done field studies to truly understand the reality of the users.
Workflow mismatches are frustrating and counterproductive in a typical workspace. In the case above, they affected pay, travel, and personnel-strength metrics. Although in peacetime this information can seem relatively low-stake, in a deployed setting this could result in potentially uninformed or dangerous military decision making. In workspaces such as military, first responder, or healthcare, the mismatch in expectations can result in a difference of life or death.
Many intranet users often do tasks that incorporate inputs from a number of sources. Sometimes a source may be another system. Other times, it may be information found outside of any digital system. Many users will often keep information handy on paper, posted in their workspace, or written in their phone. Information silos may not be visible in a digital system but become more tangible when observed in a workplace. Field studies provide you with a glimpse into the workarounds they’ve created to make their lives easier, and how those workarounds might influence others.
Other research methods also do not as readily reveal human relationships. A spoken request from a colleague is untraceable in a digital system. Participants in our field studies often verbally asked a supervisor or a peer nearby for information as opposed to looking it up online or writing an email. Observing these cases enables researchers not only to understand how relationships among employees affect task performance, but also to gain more insight into workplace dynamics and culture, such as:

How some employees prefer to interact with each other
How some words may have different interpretations for different people and user types
Whether some employees feel isolated from other groups based on the nature of their workspace (e.g., they work alone in the field) or their responsibilities.

These insights can offer ideas for improvements or new functionalities and can ultimately lead to better workplaces.

Field studies allow researchers to understand how workflows might be influenced by external informational inputs, such as sticky notes with special codes, paper “cheat sheets”, and even helpful cubicle neighbors (all of which are likely symptoms of a design problem — or to be positive, strong hints at opportunities for productivity improvement through redesign).

Building Empathy for Users
Like other observational methods, field studies allow teams to build empathy for users and to step into their shoes—well, at least their workspaces—for a moment. Intranet designers are often intranet users themselves, so the temptation to make unfounded assumptions based on their own use is big. Yet, even in this circumstance, the tenet “You are not your user!” applies. Field studies provide an understanding of the motivations behind certain user behaviors and help teams move from blaming the user to blaming the system.
During a client field study which we ran last year, a nonprofit providing job placements and professional certifications to young adults (“students”) in low-income communities, planned to redesign its intranet portal. One of the biggest pain points noted by the staff was the lack of engagement within one user group: the students. Some students would not respond to job opportunities, and others would not communicate with the organization after they had started their new job. Initially, the perception was that students needed more training on how to use the intranet portal, or that the students were not upholding their agreement to communicate with the organization after they joined the workforce.
However, as the field study progressed, it became clear the students were very responsive and eager to engage, but they did not engage in the intended way – they would call and email rather than using the tool. It also became clear the tool was difficult for many students, because it was built primarily with the staff in mind and then retrofitted for the students to use. Other critical pain points emerged for other roles as well, not just the students, including productivity-halting workflow issues, multiple redundant systems, and a lack of a unified language within the organization. While the problem was initially perceived to be one of educating and persuading the students, it was ultimately much larger than that, influencing other roles in the organization as well.
Identifying the real problem is one of the main reasons to conduct field research. After all, if you solve the wrong problem, it doesn’t matter how well you solve it. A great design of the wrong thing? It’ll still be the wrong thing.
Unlike usability testing, which makes it relatively easy to accommodate observers, field studies often require constraining the number of live observers, due to physical space as well as study effectiveness. Using web conferencing or remote live streaming services can enable more people to watch a field study live. But these tools may require users to install software, can interfere with their activities, and limit observations to on-screen actions.  Plus, some users may not be willing to have their activities shared online. (If you do stream the session, ensure that only your team can view it and that the user’s privacy is protected.) In practice, building empathy through field studies is often done indirectly — not through direct observation of users by a large number of team members, but rather by understanding behavior patterns, workflows, motivations, and pain points and sharing those with the rest of the team.
Summary
It can be challenging to run a field study: obtaining the authorization to view real data can be a whole ordeal on its own; and observing people in their real work environment and taking notes is challenging and time-consuming. However, this method can reveal a plethora of undiscovered issues and risks and is well worth the time, money, and effort. No matter what is at stake within the intranets on which you work, a field study with about 10 users is often enough to reveal pain points and to shine a light on big-picture issues. These pain points are incredible insights and opportunities just waiting to be discovered. Once we identify them, addressing them can make a world of difference to our designs.
For more information on intranets, take a look at our Intranet reports."
63,2018-06-17,"Visual details like fonts, colors, alignment, and images are increasingly expected to not just create a usable experience, but also to express the complex brand traits such as friendliness, reliability, or innovation.
Many teams begin by defining the target brand traits; then designers and stakeholders select the visual details they believe will best convey those brand traits.  This approach assumes that the opinions of designers and stakeholders will accurately predict users’ reactions. It’s a great first step, but it does not guarantee that what the designers thought looked ‘friendly’ will be perceived as such by users.
When organizations have a lot to gain from effective branding, aesthetic choices and their impact on users’ attitudes should be assessed through a rigorous, data-driven approach.
How to Test Perceptions of Visual Design
As with any form of UX research, recruit test participants who are representative of your target audience. They  don’t need to have any design expertise — people don’t need training in visual design to know whether they like something; in fact, users can reliably rate how much they like visuals in less than a tenth of a second (according to one study by Gitte Lindgaard and her colleagues). However, knowing whether someone likes a design doesn’t indicate whether the design conveys the right brand qualities.
(And for good measure, let’s emphasize that it’s also not a valid criterion whether you like the design or whether you think it expresses the targeted traits. You are not the user, and neither are the other members of your team nor your management.)
To measure brand perceptions, instead of just asking people whether they like a design, use a more structured approach including 2 main parts:

Exposure to the visual stimulus: Show study participants the visual design, which could be a static image, a prototype, or a live interactive website or application.
Assessment of user reactions to the stimulus: Measure users’ reactions to the design using either open-ended or strictly controlled questions.

Presenting the Visual-Design Test Stimulus
The “test stimulus” (that is, the visual representation of the design) you use can be easily adapted to work with several different types of research studies. When conducting in-person visual-design evaluations, you can simply show people a static image, either printed on paper or displayed on a screen. Printed pages should be a realistic size, and pages that are longer than 2 screens are typically better evaluated in a digital form, since printing them out would show users far more content at once than they would ever actually see on a screen. Use static images if you want to ensure that you get feedback about immediate first impressions of a specific visual design.
Aesthetic and brand impressions can also be assessed using remote, unmoderated methods, which allow testing with users who are difficult to meet in person, or with large groups of users (useful when you need a high degree of certainty in the findings). Any survey tool that can display images works for remote assessments.
If you’re interested in first impressions, present the visual stimulus to the user for a short amount of time. There are two ways in which you can achieve this goal:

5-second test:  With this type of test, you show the stimulus for 5 seconds (or for another short period of time). This approach is best for accurately capturing people’s ‘gut reaction.’ 5 seconds of viewing time is too short for reading copy or for noticing details like specific fonts or colors, but it is enough for forming an impression which accurately reflects the visual style.
First-click test:  You give participants a specific instruction (such as “Find out more about this organization”) before they are exposed to the design and stop them after they click the location on the screen where they could complete that task. Most users will still spend only a few seconds on this type of test, but instead of intentionally looking at the whole page, they will search for a specific task-related feature or link, and only view the rest of the design peripherally. This test is best suited if you expect your users to already have a specific goal in mind the first time they encounter your site.

These two tests are easiest to administer remotely, using services such as 5 Second Test and Userzoom (for 5-second tests) or Chalkmark (for first-click tests).
Keep in mind that, with a first-click test, the exact task instructions you provide will certainly influence what participants notice and remember about the visual design. If your users are likely to have a variety of goals on your site, randomly assign users to one of several different task instructions, or stick to the more neutral 5-second test.
Comparing Multiple Design Variations
Frequently, showing users more than one possible visual designs helps them identify what they like (or dislike) about each variation. If you ask participants to assess more than one design, be sure to vary the order in which they see the alternatives, since part of people’s response may be influenced by which version they see first. (For example, if one version is easier to understand, those who see that one first will have learned about the content and will be less confused by the other variation.) Keep track of which version each person sees first, so you can take it into account when analyzing responses.
Also, when asking a user to evaluate different versions of the same design, the differences must be significant enough to be immediately detectable to a lay person. Small changes such as minor variations in font sizes or substitutions of similar fonts may be obvious to a visual designer but are often undetectable to the average user. Asking people to consciously identify and evaluate these subtle details will most likely just confuse them and waste your time. (Even worse, you may fall prey to the query effect, where users make up an answer simply to satisfy the question, even though they don’t really feel differently about the two overly-similar versions.)
Assessing User Reactions: Open-Ended vs. Structured 
Once participants have been exposed to the design, the next step is to measure their responses. People’s aesthetic impressions can be very idiosyncratic and will need to be systematically analyzed to identify meaningful trends. This can be done with open-ended feedback, but using a slightly more structured approach makes it easier to understand overall patterns. Here are a few techniques that can be used, ranging from completely open-ended to highly structured:

Open-ended preference explanation: Ask users to explain why they like a design
Open word choice: Ask users to list 3 to 5 words that describe the design
Closed word choice (desirability testing): Provide users with a list of terms and ask them to pick the words which best describe the design
Numerical ratings: Collect numerical ratings about how much the design exhibits specific brand qualities

Open-Ended Preference Explanation
The first method, simply asking people to explain why they like (or don’t like) a design, can work well for in-person sessions with highly motivated and articulate users. This question casts the broadest net and can be useful if you don’t know much about your audience’s expectations, and want to discover what matters to them. It can also help identify opinions that based on personal idiosyncrasies (such as “I like purple”), that can be screened out so you can focus on more substantive factors. The drawback of this approach is that you may get only brief or irrelevant responses if the participant is not motivated or just not very articulate.  This method is especially risky in an unmoderated remote setting (such as a survey), since you won’t be able to ask for more detail in followup questions if someone gives a vague response such as ‘It’s nice.’
Open Word Choice
A slightly more structured approach to assessing user perceptions is to ask test participants to list several words which describe the design. This format ensures you get at least some specific feedback, while still keeping the question open-ended to discover factors you may not have considered, but which are significant to your audience. You may get a wide range of descriptors back, and will need to analyze them carefully to identify meaningful themes. A good approach for this analysis is to categorize terms as generally positive, negative, or neutral, then group terms with similar meanings, and evaluate whether they match your target brand attributes. For example, the table below shows descriptors provided about a business-to-business website whose brand goal was to be trustworthy, contemporary, and helpful. None of these terms were specifically named by the study participants as descriptors, but many users described the design as simple (with both positive and negative connotations).




Positive
Neutral
Negative





Simple
Simple, Bold
Professional, Neat
Corporate
Elegant
Human


Sober
Tripartite
3 parts


Bland
Bland, Typical, Safe
Too Simple
Simple, Generic
Plain
Basic
Dated
Too Much Information
Clinical





Open-ended word-choice questions elicit a broad range of descriptors, which must be analyzed to determine whether they effectively express the desired brand traits.
Structured Word Choice
Requiring users to choose descriptors from a list of terms you provide is a controlled variation of the word-choice method. By supplying users with a limited set of words, this method focuses specifically on whether the target brand attributes are perceived by participants. The brand traits you hope to convey should be included in your list of terms, along with other choices which describe contradictory or divergent qualities. Structured word choice, (also known as “desirability testing”) is less sensitive than open word choice to discovering new points of view, but makes it easier to compare different versions of a design, or the reactions of different audience groups to the same design. This technique works well in an in-person study, where you can ask users follow up questions and let them refer to the design as they explain their reasoning for selecting each term. It can also be used in a remote study, but it's not a good idea to combine this with a '5-second' test format because looking through a long list of words may take so much time that by users get to the end they don't recall much about a design they only saw for 5 seconds. Instead, use a survey tool which allows people to see the design as they are choosing words from the list. 
Numerical Ratings of Brand Perceptions
Finally, the most controlled approach is to collect numerical ratings of how well each brand trait is expressed by the design. To avoid prohibitively long test sessions, pick the 3–5 most important brand qualities and ask people to rate how well each of them is captured by the design. (The more questions you have, the more difficult the questionnaire, and the higher the chance of random answers.) Because this paradigm limits the ability to discover different perspectives and reactions, numerical ratings are appropriate only if you’ve figured out the most common perceptions in previous research and simply want to assess the relative strength of each quality.
Finally, a word about focus groups: although they can be used to capture user preferences, this method is risky if you don’t have a talented, experienced focus-group facilitator available. Capturing detailed feedback about a visual design from each participant in a group conversation is difficult. One tactic that can help is to ask participants write down their own perceptions before discussing them as a group, and collect these written comments for later analysis. Also focus groups don’t capture any behavioral information.
Assessing Visual-Design Aesthetics Within Usability Testing
All the methods described above focus specifically on visual impressions; but the reality is that people do not encounter visual design in isolation, but rather as part of a holistic experience which also includes content and interaction. Each dimension of the user experience affects the other dimensions: more aesthetically appealing designs are often perceived as more usable. Likewise, users’ perceptions about brand traits are influenced by interaction-design choices: a design that appears simple and welcoming at first glance can quickly become confusing and frustrating if people can’t understand how to use it.
Although first impressions are important, they don’t tell the whole story. You should also assess how the visual design affects users’ behavior and task success when they actually interact with the system. In fact the effects of subtle changes, such as slightly increased header sizes, may only be apparent when people actually use a system; at first glance they may not even notice a difference, but when skimming an article, larger headers may make it easier to jump quickly to a specific section. This changed behavior may again improve users’ ability to find relevant information and make them like the site much more. People may even say that the writing is better (because they read more information of interest) even when the actual copy remained constant and the only change was to the typography.
Luckily, typical usability-test protocols can be easily modified to incorporate assessments of visual design. You can include specific questions about visual impressions and even word-choice exercises into a regular usability-testing session. However, instead of trying to capture the users’ first impression, these aesthetic assessments should happen after the behavioral usability portion of the study is complete.
The sequence is important because if you ask someone’s opinion about the visual design at the beginning of the session, you run the risk of biasing the behavioral portion of the study. Especially if users have seen multiple versions and picked a ‘favorite,’ they are likely to ignore or minimize any problems they experience with their ‘favorite’ version throughout the rest of the session.
Instead of asking users about their visual-design perceptions at the beginning of the session, have people complete the behavioral tasks first, and pay attention to actions or spontaneous comments that relate to the visual design. For example, in a recent test of a prototype of our company’s website, we asked users to complete normal usability tasks such as finding content. While attempting a task, one user casually commented that the new navigation menu at the top of the page was helpful. This menu was not actually a new feature of the design — it was the same menu present on the website that this user had visited regularly in the past, now displayed in a lighter font and without uppercase styling.


A study which compared two different fonts for the global navigation using normal usability-testing methods discovered that the original navigation menu (top) which was presented in a heavy, all-caps font became more discoverable and was perceived as a ‘new’ addition to the site when presented in the lighter, title-case font (bottom.)

Once the task-based portion of the study is complete, you can shift to assessing users’ perceptions of the brand traits. Their answers won’t be based exclusively on visual impressions like they would be in a 5-second test, but the impression formed from the combination of visuals, content, and interaction is actually closer to how users react in the real world.
Should Visual Impressions Be Tested in Isolation, or as Part of Usability Testing?
For interactive systems, assessing visual preferences should never be done instead of usability testing. If you only have time and resources to do one test, make it a usability test with added techniques to assess the effects of visual design.
Consider using the standalone methods described in this article when:

Time and resources permit multiple types of testing
Visual and brand perceptions may significantly influence the success of the product
Before testing an interactive prototype, to compare divergent visual approaches
After testing an interactive prototype, to confirm findings with a larger sample of users

 
References
Lindgaard, G., Fernandes, G., Dudek, C. and Brown, J. “Attention Web Designers: You Have 50 Milliseconds to Make a Good First Impression!” Behavior and Information Technology, 25(2), 2006. https://www.tandfonline.com/doi/abs/10.1080/01449290500330448
Rohrer, Christian. “Desirability Studies: Measuring Aesthetic Response to Visual Designs.” xdStrategy.com, October 28, 2008. http://www.xdstrategy.com/2008/10/28/desirability_studies/"
64,2018-05-13,"When you want to compare several user interfaces in a single study, there are two ways of assigning your test participants to these multiple conditions:

Between-subjects (or between-groups) study design: different people test each condition, so that each person is only exposed to a single user interface.
Within-subjects (or repeated-measures) study design: the same person tests all the conditions (i.e., all the user interfaces).

(Note that here we use the word “design” to refer to the design of the experiment, and not to website design.)
For example, if we wanted to compare two car-rental sites A and B by looking at how participants book cars on each site, our study could be designed in two different ways, both perfectly legitimate:

Between-subjects: Each participant could test a single car-rental site and book a car only on that site.
Within-subjects: Each participant could test both car-rental sites and book a car on each.

Any type of user research that involves more than a single test condition has to determine whether to be between-subjects or within-subjects. However, the distinction is particularly important for quantitative studies.
Experimental Design in Quantitative Studies
Unlike qualitative studies, quantitative usability studies aim to result in findings that are statistically likely to generalize to the whole user population. How the data from such studies is analyzed depends on the way in which the study was designed (that is, on the study’s experimental design).
Often, the main goal of quantitative usability studies is to compare — a site with its competitors, two different iterations of a design, or two different groups of users (such as experts vs. novices).  Like in any scientific experiment in which we want to detect causal relationships, a quantitative study involves two types of variables:

Independent variables, which are directly manipulated by the researcher
Dependent variables, which are measured (and expected to vary as a result of the independent-variable manipulation)

(If the study produces statistically significant results, then we can say that a change in the independent variable caused a change in the dependent variable.)
Let’s go back to our original car-rental example. If we wanted to measure which of the two sites, A or B, is better for the task of reserving a car, we could choose Site (with two possible values or levels — A and B) as the independent variable, and the time on task and the accuracy for booking a car could be the dependent variables. The goal of the study would be to see whether the dependent variables (time and accuracy) change when we vary the site or they stay the same. (If they stay the same, then none of the sites is better than the other.)
To plan our study, the next question that we need to answer is whether the study design should be between-subjects or within-subjects — that is, whether a participant in the study should be exposed to all the different conditions for the independent variable in our study (within-subjects) or only to one condition (between-subjects). The choice of experimental design will affect the type of statistical analysis that should be used on your data.

It is possible that an experiment design is both within-subjects and between-subjects. For example, assume that, in the case of our car-rental study, we were also interested in knowing how participants younger than 30 perform compared with older participants. In this case we would have two independent variables:

Age, with 2 levels: under 30, over 30
Site, with 2 levels: A and B

For the study, we will recruit an equal number of participants in each age group. Let’s assume that we decide that each participant, whether under or over 30, will make a car-rental reservation on both site A and on site B. In this case, the study is within-subjects with respect to the independent variable Site (because each person sees both levels of this variable — that is, both site A and site B). However, the study is between-subjects with respect to Age: one person can only be in a single age group (either under or over 30, not both). (Well, technically, you could pick a group of under-30-year olds and wait until they turn 30 to have them test the sites again, but this setup is highly impractical for most real-world situations.)
Some independent variables may impose the choice of design. Age is one of them, as seen above. Others are Expertise (if we want to compare experts and novices), User Type (if we want to compare different user groups or personas — for example, business traveler vs. leisure traveler), or Gender (assuming that a person cannot be of several genders at the same time). Outside usability, drug trials are one common case of between-subject design: participants are exposed to only one treatment: either the drug being tested or a placebo, not both. And sometimes the manipulation itself changes the state of the participant: for example, if you want to see which of two curricula is more effective for teaching reading, you could not have the same student be exposed to both, because once she’s learned how to read, she cannot unlearn it.
Which Is Better: Between-Subjects or Within-Subjects?
Unfortunately, there is no easy answer to this question. As seen above, sometimes your independent variables will dictate the experimental design. But in many situations, both designs may be possible.

Between-subjects minimizes the learning and transfer across conditions. After a person has completed a series of tasks on a car-rental site, she is more knowledgeable about the domain than she was before. For example, she may now know that car-rental sites charge an extra fee for drivers under 21, or what a collision-damage waiver is. That knowledge will likely help her become more efficient on a second car-rental site, even though that second site may be very different from the first.

	With between-subject design, this transfer of knowledge is not an issue — participants are never exposed to several levels of the same independent variable.

Between-subjects studies have shorter sessions than within-subject ones. A participant who tests a single car-rental site will have a shorter session than one who tests two. Shorter sessions are less tiring (or boring) for users, and can also be more appropriate for remote unmoderated testing (especially since tools like UserZoom usually require a fairly short session length).
Between-subject experiments are easier to set up, especially when you have multiple independent variables. When the study is within-subjects, you will have to use randomization of your stimuli to make sure that there are no order effects. For example, in our car-rental study, we need to make sure that participants don’t always start with site A and then move on to site B. The order of the sites needs to be random for each participant. This is easy with just two sites: randomly assign 50% of users to start with each site. But as you increase the number of independent variables and of levels for an independent variable, randomization becomes more difficult to implement within some of the existing platforms for quantitative usability testing.
Within-subject designs require fewer participants and are cheaper to run. To detect a statistically significant difference between two conditions, you’ll often need a fair number of a data points (often above 30) in each condition. If you have a within-subject design, each participant will provide a data point for each level of the independent variable. For our car-rental study, 30 participants will provide data points for both sites. But if the study is between-subjects you will need twice as many to get the same number of data points. That means twice the cost.
Within-subjects design minimize the random noise. Perhaps the most important advantage of within-subject designs is that they make it less likely that a real difference that exists between your conditions will stay undetected or be covered by random noise.
	Individual participants bring in to the test their own history, background knowledge, and context. One may be tired after a long night of partying, another one may be bored, yet another one may have received a great news just before the study and be happy. If the same participant interacts with all levels of a variable, she will affect them in the same way. The happy person will be happy on both sites, the tired one will be tired on both. But if the study is between-subjects, the happy participant will only interact with one site and may affect the final results. You’ll have to make sure you get a similar happy participant in the other group to counteract her effects.
In practice, researchers won’t be able to assess such differences between participants — although they may match the gender, the experience, and the age across groups, it will be difficult to predict or detect other factors specific to each participant.


Randomization: Essential for Both Types of Design
Whether your experimental design is within-subjects or between-subjects, you will have to be concerned with randomization, although in slightly different ways.
Above, we discussed why randomization is important in within-subject designs: it counteracts the possible order effects and minimizes transfer and learning across conditions.
For between-subject designs, you must make sure that participants are allotted randomly to conditions, because you want to ensure that your participant assignment does not affect your study results. Thus, if a researcher decides that all the participants that he likes should interact with site A and then he finds that site A performed better than site B, he won’t know whether he’s discovered a true difference between the sites or whether the result simply reflects his assignment (for example, because people who sense that they are liked tend to return the favor, and may be more patient or have a positive mindset during the test).
Even without such an obvious bias as your personal preferences, it’s easy to get randomization wrong. Say that you run a study across four days, Saturday through Tuesday. You might decide to have the first half of the test users start with site A and have the second half of the users start with site B. However, this is not a true randomization, because it’s very likely that certain types of people are more likely to agree to a study during the weekend and other types of people are more likely to sign up for your weekday testing slots.
Conclusion
User research can be between-subjects or within-subjects (or both), depending on whether each participant is exposed to only one condition or to all conditions that are varied within a study. Each of these types of experimental design has its own advantages and disadvantages; within-subjects design requires fewer participants and increases the chance of discovering a true difference among your conditions; between-subjects designs minimize the learning effects across conditions, lead to shorter sessions, and may be easier to set up and analyze."
65,2018-04-22,"Many UX professionals gravitate towards qualitative (qual) methodologies, which are widely perceived as being easier and cheaper than quantitative (quant) research. They shy away from the intimidating prospect of larger sample sizes and statistics associated with quant.
If that sounds like you, you’re missing out! Quant methodologies are an important part of any experienced UX researcher’s toolkit. Quant methods allow you to:

Put a number on the usability of your product. Numbers are sometimes more persuasive than findings and videos from qual testing (particularly when you’re trying to convince folks like executives).
Compare different designs (for example, your new version vs. your old version, or your product vs. your competitor’s product), and determine whether the differences you observe are statistically significant, and not due to random chance.
Improve UX trade-off decisions. For example, if a proposed design improvement is expected to be expensive to implement, is it worth doing? If you have an estimate of how much the change will improve the usability, a quant method may help you decide whether the redesign is worth it.
Tie UX improvements back to organizational goals and key performance indicators (thus demonstrating your return on investment and justifying your UX team’s existence).

This article can help you get started — the first step is determining which quant UX research method you need. We’ll cover some of the most popular types of quant research:

Quantitative Usability Testing (Benchmarking)
Web Analytics (or App Analytics)
A/B Testing or Multivariate Testing
Card Sorting
Tree Testing
Surveys or Questionnaires
Clustering Qualitative Comments
Desirability Studies
Eyetracking Testing

Each of these methods yields valuable quantitative data, but the techniques vary widely in the type of data collected, as well as the amount of resources and effort required.
This article lists the most common use cases for these methods, and estimates cost and difficulty for each. As with any research method, each of these can be adapted to fit a variety of needs. Depending on your specific circumstances, your costs and difficulty may be different from our rough estimates. Additionally, you should be aware that each of these methods will require different minimum sample sizes to determine statistical significance.
Quantitative Usability Testing (Benchmarking)

Use:

Tracking usability over time
Comparing with competitors


Cost: Medium
Difficulty of Collection: Medium
Difficulty of Analysis: Medium
Type of Method: Behavioral (what people do)
Context of Use: Task-based

Although not used as often, quantitative usability testing (sometimes referred to as usability benchmarking) is a lot like qualitative usability testing — users are asked to perform realistic tasks using a product. The primary difference between the two is that qual usability testing prioritizes observations, like identifying usability issues. In contrast, quant usability testing is focused on collecting metrics like time on task or success.
Once you’ve collected those metrics with a relatively large sample size (around 35 participants or more), you can use them to track the progress of your product’s usability over time, or compare it to the usability of your competitors’ products.


When you track a usability metric over time, across many different iterations of a product, you can create charts like this one. This type of information can help you keep an eye on your product’s UX, and make sure it improves over time.


The type of usability testing you choose (in-person, remote moderated, or remote unmoderated) will impact the cost and difficultly associated with this method. Since the goals of quant and qual usability studies are different, the structure of the test and the tasks used will need to be different as well.
For all the skills you need to run a basic quantitative usability-testing study, see our full-day course Measuring User Experience.
Web Analytics (or App Analytics)

Uses: 

Detecting or prioritizing problems
Monitoring performance


Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: High
Type of Method: Behavioral (what people do)
Context of Use: Live

Analytics data describe what people are doing with your live product — where they go, what they click on, what features they use, where they come from, and on which pages they decide to leave the site or app. This information can support a wide variety of UX activities. In particular, it can help you monitor the performance of various content, UIs, or features in your product, and identify what doesn’t work.
For an explanation of the differences between analytics and quant usability testing, watch this 2-minute video.
For more on analytics with a special focus on how these methods fit within UX, see our full-day course Analytics and User Experience.
A/B Testing or Multivariate Testing

Use: Comparing two design options
Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: Low
Type of Method: Behavioral (what people do)
Context of Use: Live

While you can use analytics metrics to monitor your product’s performance (as described above), you can also create experiments that detect how different UI designs change those metrics — either through A/B testing or multivariate testing.
In A/B testing, teams create two different live versions of the same UI, and then show each version to different users to see which version performs best. For example, you might create two versions of the same call-to-action button label: Get Pricing vs. Learn More. Then you could track the number of clicks that the button receives in the two versions. Multivariate testing is similar, but involves testing several design elements at once (for example, the test could involve different button labels, typography, and placement on the page.)
Both of these analytics-based experiments are great for deciding among different variations of the same design — and can put an end to team disputes about which version is best.


A/B testing splits your incoming site traffic (users), and directs some users to one version of the UI, and others to the other version. 


A major downside to this methodology is that it’s often abused. Some teams fail to run the tests as long as they should, and make risky decisions based on small numbers. 
For more on A/B and multivariate testing for UX, see our full-day course Analytics and User Experience.
Card Sorting

Use: Determining information-architecture labels and structures
Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: Medium
Type of Method: Attitudinal (what people say)
Context of Use: Not using product

In a card-sorting study, participants are given content items (sometimes literally written on index cards) and asked to group and label those items in a way that makes sense to them. This test can either be conducted in person, using physical cards, or remotely using a card-sorting platform like OptimalSort.


When card sort tests are conducted in person, the user sorts and categorizes physical cards. Each card contains a description of the content it represents.


This method gives you the opportunity to get into users’ mental models of the information space. What terminology do they use? How do they logically group these concepts together?
Quantitative analysis of the percentage of participants who created similar groupings can help establish which categorization approach would be understandable to most users.
Tree Testing

Use: Evaluating information-architecture hierarchies
Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: Medium
Type of Method: Behavioral (what people do)
Context of Use: Task-based, not using product

In a tree test, participants attempt to complete tasks using only the category structure of your site. It’s essentially a way to evaluate your information architecture, by isolating it away from all other aspects of your UI.
For example, imagine your product is a pet-supplies website, and this is your top-level hierarchy.


A visualization of your hierarchy might look something like this. Participants in a tree test are asked to find a specific item in your hierarchy (e.g., collars). They first see only the top-level categories (for example, dog, cat, bird, etc.) Once they make a selection (dog), they see that selection’s child categories.


You might ask your participants in one task to find the dog collars. Quantitative analysis of the tree-test results will show whether people were able to find the right path to this item in the information hierarchy. How many participants picked the wrong category?
This method is useful in identifying if an IA structure, labels, and placements agree with people’s expectations.
For more information about how to design and evaluate information architecture, see our full-day course Information Architecture.
Surveys and Questionnaires

Use: Gather information about your users, their attitudes, and behaviors 
Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: Low
Type of Method: Attitudinal (what people say)
Context of Use: Any

Surveys are a flexible user-research tool. You can administer them in a variety of contexts — as short intercept surveys on a live website, in emails, or after a usability test. 
They can produce combination of quantitative and qualitative data — ratings, proportions of answers for each choice in a multiple-choice question, as well as open-ended responses. You can even turn qualitative responses to a survey into numerical data (see the following section on coding qualitative comments).


With semantic differential rating scales like this one, each radio button stands for a numerical value. Respondents can choose Easy to Use (1), Difficult to Use (5), or a value in between. The average response to this question measures the perceived difficulty of your app.


You can create your own custom surveys, or you can use one of the many established questionnaires available (for example, the System Usability Scale or Net Promoter Score). An advantage of one of those questionnaires is that you can often compare your result to industry or competitor scores, to see how you’re doing. Even if you create your own custom questionnaire, you can still track your average scores over time, to monitor product improvements.
For more on designing surveys, as well as many qualitative user research methods, see our full-day course User Research Methods: From Strategy to Requirements to Design.
Clustering Qualitative Data

Use: Identifying important themes in qualitative data
Cost: Low
Difficulty of Collection: Medium
Difficulty of Analysis: Medium
Type of Method: Attitudinal (what people say)
Context of Use: Any

This technique is less of a data-collection methodology, and more of an analysis approach for qualitative data. It involves grouping observations from a qualitative study (for example, a diary study, survey, focus group, or interviews) based on common themes. If you have a lot of observations, you can count the number of instances when a particular theme is mentioned.
For example, imagine you run a diary study asking participants to report every time they use your product in their daily lives for a week, with the goal of understanding in what context they use your products. You may count the instances when people used the product at work, in their homes, or on the go.
This method can identify the prevalence or frequency of a specific theme or situation — for example, the frequency of a user complaint or of a UI problem. 
This approach is a good way to mine numerical data from large amounts of qualitative information, but it can be quite time consuming.
Desirability Studies 

Use: Identifying attributes associated to your product or brand
Cost: Low
Difficulty of Collection: Low
Difficulty of Analysis: Low
Type of Method: Attitudinal (what people say)
Context of Use: Task-based

Quantitative desirability studies attempt to quantify and measure some quality of a product — such as aesthetic appeal, brand strength, tone of voice. These studies can be customized depending on your research questions, but they generally involve first exposing participants to your product (either by showing them a still image or by asking them to use the live product or a prototype). Then you’ll ask them to describe the design by  selecting options from a list of descriptive words. With a large sample size that is representative of your population, trends start to emerge. For example, you may that 84% of respondents describe the design as “fresh.”
Eyetracking Testing

Use: Determining which UI elements are distracting, findable, or discoverable
Cost: High
Difficulty of Collection: High
Difficulty of Analysis: High
Type of Method: Behavioral (what people do)
Context of Use: Task-based

Eyetracking studies require special equipment that tracks users’ eyes as they move across an interface. When many participants (30 or more) perform the same task on the same interface, meaningful trends start to emerge and you can tell, with some reliability, which elements of the page will attract people’s attention. Eyetracking can help you identify which interface and content elements need to be emphasized or deemphasized, to enable users to reach their goals. 


Eyetracking software can create a variety of visualizations using the aggregated gaze data (where users looked on the interface, represented here by the green dots).


A major obstacle to running eyetracking studies is the highly specialized, prohibitively expensive, and somewhat unstable equipment that requires lots of training to use.
If you’re considering running an eyetracking study, check out our free report on How to Conduct Eyetracking Studies.
Choosing a Method




Method


Typically Used for


Cost


Difficulty of Collection


Difficulty of Analysis


Type


Context of Use




Quantitative Usability Testing


Tracking usability over time
Comparing competitors


Medium


Medium


Medium


Behavioral


Task-Based




Web Analytics (or App Analytics)


Detecting or prioritizing problems
Monitoring performance 


Low


Low


High


Behavioral


Live




A/B Testing


Comparing two specific design options


Low


Low


Low


Behavioral


Live




Card Sorting


Determining IA labels and structures


Low


Low


Medium


Attitudinal


Not Using Product




Tree Testing


Evaluating IA hierarchies


Low


Low


Medium


Behavioral


Not Using Product




Surveys and Questionnaires


Gather information about your users, their attitudes, and behaviors


Low


Low


Low


Attitudinal


Any




Clustering Qualitative Comments


Identifying important themes in qualitative data


Low


Medium


Medium


Attitudinal


Any




Desirability Studies


Identifying attributes associated to your product or brand


Low


Low


Low


Attitudinal


Task-Based




Eyetracking Testing


Determining which UI elements are distracting, findable, or discoverable


High


High


High


Behavioral


Task-Based




This table provides a summary of the methods discussed above.
Start with Your Research Question
When trying to determine which quant method to use, lead with your research question. What do you need to know? Some of these methodologies are best suited to very general research questions. For example:

How did our product usability change over time?
How are we doing compared to our competitor?
Which of our problems have the biggest impact? How should we prioritize?

For these types of questions, you’ll likely want to use quant usability testing, web analytics, or surveys.
Other methodologies work well when you have a more specific question you want to answer. For example:

How should we fix our global-navigation categories?
What do most of our users think about our visual design?
Which of these two design alternatives should we use for the dashboard?

For these research questions, you’ll probably want to use A/B testing, card sorting, tree testing, coding qualitative comments, desirability studies, or eyetracking.
There are some grey areas within those recommendations, however. For example, an A/B test may not be an option for your company, for security or technical reasons. If that’s the case, and you can afford it, you could do an in-person quant usability study to compare two prototypes. However, that isn’t the typical use for quant usability testing, so I did not discuss it here.
Consider the Cost
After the research question, the second most influential factor in choosing a methodology is cost. These methodologies will vary a lot in cost depending on how your implement the study. The tools you use, the number of participants you have, and the amount of time spent by your researchers will all impact the final cost. To make this even more complicated, many teams have widely different research budgets. Again, the cost estimates here are relative.
Lower-budget teams will rely on digital methods — remote usability testing, online card-sorting platforms like OptimalSort, A/B testing, and web or app analytics. As a rule of thumb, the in-person methodologies (such as in-person usability testing, in-person card sorts) tend to be more expensive because they require so much more of researcher’s time. Additionally, they can require travel and equipment rentals. Eyetracking is the most expensive methodology listed here, and should be employed only by teams with big budgets and research questions that warrant using it. 


This chart shows where the quant methods discussed in this article sit in terms of their suitability for different levels of granularity of research questions (general to specific).


Next Steps
Once you’ve selected a method, learn about it! Do your homework to make sure you’ll be able to plan and conduct the study the way you’d like to, and to ensure you’ll get useful results. I’ve included links throughout this article to point you towards more resources for each method, as well as a Resourcessection at the end.
Be warned: You can’t just collect metrics and start making decisions without doing any statistical analysis. It isn’t enough to just collect rating-scale responses from 5 users, take an average, and move on. 
For each method discussed here, there are different recommended minimum sample sizes — the number of data points you’ll likely need to collect in order to have reliable data and determine statistical significance. You’ll need to hit those minimum sample sizes. If you don’t, you have no assurance that your findings aren’t just a fluke.
Be sure to factor in the time you’ll need to research relevant statistical concepts for whichever method you select, as well as the cost of obtaining the correct minimum sample size. I promise, it isn’t quite as hard as it looks, and your quant data will be well worth the trouble.
Resources
Measuring UX and ROI (Full-day course)
“Understanding Statistical Significance” (Article)
“What Does Statistically Significant Mean?” (Article)
“Quantitative Studies: How Many Users to Test?” (Article)
“How to Compute a Confidence Interval in 5 Easy Steps” (Article)
“Return on Investment for Usability” (Article)
Return on Investment (ROI) for Usability, 4th Edition (Report)
“When to Use Which User-Experience Research Methods” (Article)
“UX Research Cheat Sheet” (Article)
Measuring U’s sample size and confidence interval calculators (Tool)
Quantitative Usability Testing
“Quantitative vs. Qualitative Usability Testing” (Article)
“Accuracy vs. Insights in Quantitative Usability” (Article)
“Writing Tasks for Quantitative and Qualitative Usability Studies” (Article)
“Remote Usability Tests: Moderated and Unmoderated” (Article)
“Remote Moderated Usability Tests: How and Why to Do Them” (Article)
“Success Rate: The Simplest Usability Metric” (Article)
Analytics
Analytics and User Experience (Full-day course)
“Analytics vs. Quantitative Usability Testing” (Video)
“Three Uses for Analytics in User-Experience Practice” (Article)
“Five Essential Analytics Reports for UX Strategists” (Article)
A/B Testing or Multivariate Testing
Analytics and User Experience (Full-day course)
“Putting A/B Testing in Its Place” (Article)
“Define Stronger A/B Test Variations Through UX Research” (Article)
“10 Things to Know About A/B Testing” (Article)
“Multivariate vs. A/B Testing: Incremental vs. Radical Changes” (Article)
Card Sorting
Information Architecture (Full-day course)
“Card Sorting: Uncover Users’ Mental Models for Better Information Architecture” (Article)
“Card Sorting: Pushing Users Beyond Terminology Matches” (Article)
“Card Sorting: How to Best Organize Product Offerings” (Video)
“How to Avoid Bias in Card Sorting” (Video)
Tree Testing
Information Architecture (Full-day course)
“Tree Testing: Fast, Iterative Evaluation of Menu Labels and Categories” (Article)
“Tree Testing Part 2: Interpreting the Results” (Article)
“Using Tree-Testing to Test Information Architecture” (Article)
Surveys and Questionnaires
User Research Methods: From Strategy to Requirements to Design (Full-day course)
“Beyond the NPS: Measuring Perceived Usability with the SUS, NASA-TLX, and the Single Ease Question After Tasks and Usability Tests” (Article)
“12 Tips for Writing Better Survey Questions”(Article)
“Cleaning Data from Surveys and Online Research” (Article)
Clustering Qualitative Data
“5 Examples of Quantifying Qualitative Data” (Article)
“How to Code and Analyze Verbatim Comments” (Article)
“Diary Studies: Understanding Long-Term User Behavior and Experiences” (Article)
Desirability Studies
“Desirability Studies: Measuring Aesthetic Response to Visual Design” (Article)
“Using the Microsoft Desirability Toolkit to Test Visual Appeal” (Article)
“Microsoft Desirability Toolkit Product Reaction Words” (Article)
Eyetracking Testing
How to Conduct Eyetracking Studies (Free report)
“Eyetracking Shows How Task Scenarios Influence Where People Look” (Video)"
66,2018-04-08,"In the world of design-optimization methods, A/B testing gets all the attention. Multivariate testing is its less understood alternative, often deemed too time-consuming to be worth the wait. While this method has its limitations, they are counterbalanced by its benefits, which cannot be easily achieved using A/B testing alone.
Multivariate Testing (MVT)
Let’s assume you wanted to optimize the design of a live product-detail page in order to maximize conversions for adding the item to the user’s cart. You are considering several possible changes:

Using a video of a product instead of an image
Changing the label of the main call-to-action button from Buy Now to Add to Cart

A multivariate test can help you decide which combination of these design choices would optimize conversions.
Let’s first clarify some terminology:


Variable: A UI element (such as an image or headline) with multiple possible design versions
In our ecommerce example, the variables are the visual representation of the product and the label of the call-to-action button.


Variant: Each design version of a variable
The product image and the product video represent the two variants for the visual-representation variable; the Add to Cart and the Buy Now labels are the variants of the call-to-action.


Variation: The resulting design containing a variant of each variable, to be compared with other variations
In our example, there would be 4 design variations, corresponding to all the possible combinations of the variables’ variants: image × Add to Cart, image × Buy Now, video × Add to Cart, video × Buy Now.



Definition: A multivariate test (MVT) is a design-optimization method in which multiple variants of specified variables are tested in a user interface, with the goal of maximizing conversions (either major conversions like completing an order, or micro conversions like interacting with a feature on a page). This method determines which combination of the variants results in the highest performing design (in terms of the conversion goal specified).


In a multivariate test, 2 or more design elements (the variables) are tested. Each of these variables could have multiple variants. For example, in the above page we could test 2 variables: visual representation for a product (with 2 variants: an image or a video) and label for the main call-to-action (with 2 variants: Buy Now or Add to Cart).

Difference Between Multivariate Testing and A/B Testing
Multivariate testing is often regarded as a type of A/B testing, although its setup and strengths are somewhat different. Here are the similarities and differences between them:

Both methods test design variations against each other by dividing live site (or application) traffic across variations.
Both measure which design alternative (i.e., variation) leads to the highest rate of conversions for specified goals.
In an A/B test, the variations being tested could be completely different from each other, and not the result of manipulating a small set of variables. For instance, you could have two pages with completely different layouts, different copy, different navigation, different visual design, and so on. The result of the A/B test will indicate that one variation performs better than the other, but you will not know whether it’s because your copy is better, your visual design is better, or your layout is better (or the combination).

In contrast, if you use MVT, you will usually be able to assign credit to one particular variant or combination of variants. Thus, you may, for example, find out that a product video makes a much bigger difference in your conversions than changing the button label, which can give you further strategy and design insights (for instance, it may tell you that it’s worthwhile to invest in producing good product videos).
MVT Measures Interactions Between Elements
Let’s come back to our original ecommerce example. You may wonder whether two sequential A/B tests would produce the same result as an MVT.  Specifically, let’s pretend that you’d first run an A/B test to compare the video vs. the image — assume that the video wins. Next, on the winning variation (i.e., video), you’d do another A/B test between the two possible button labels, and the Buy Now label proved better. Wouldn’t this result be equivalent with the MVT test?
The answer is: not necessarily. It may be the case that the optimal combination is image × Buy Now, but you won’t have tested that version.
The main advantage of running a multivariate test rather than an A/B test is the ability to determine how various elements on a page interact with one another. Only by testing each combination of various variants can you not only figure out that visual A performs better than visual B, and that button C performs better than button D, but you can also discover the best combination of these overall.
Limitations of MVT
The variations generated from every combination of the variants multiply like rabbits. Even our fairly simple ecommerce example has 4 design variations to compare, corresponding to all the possible combinations between the 2 variables. Adding 1 more variant for the call-to-action variable (e.g., Purchase) would create 2 more variations — generated by combining this variant with the other 2 variants of the visual-representation variable. (In general, the number of variations will be obtained by multiplying number of variants for each variable; so if you had 2 variables, one with 2 and the other with 3 variants, you’ll get 2×3=6 variations.)

Two variables that have each 2 variants leads to 4 design variations in a multivariate experiment, to represent all the possible combinations of these variants.

The large number of variations that need to be tested in a multivariate test leads to this method’s greatest limitation: much more traffic is typically required to run a multivariate test compared to an A/B test, in order to reach statistical significance. This is because each variation added to the comparison causes the live traffic to be divided into smaller pieces, and thus it also can take a long time to gather enough data points for each design alternative. (However, keep in mind that the time needed to run the test is dependent not only on the overall traffic, but also on the expected change in conversion rate for the experiment goal, because bigger improvements are easier to measure than tiny differences.) In general, splitting the live traffic among more variations leads to longer-running experiments.
Another limitation of MVT is that all combinations of the variants must make sense together. For example, when testing variants of an image and of a headline on a page, don’t write headlines that refer to details of an image variant (such as “Great Spa Vacations” vs. “Great Beach Vacations” with corresponding photos), because each headline will accompany each image in a variation for the test. That type of experiment would be better set up as an A/B test, so the combinations could be more controlled.
Use MVT to Refine a Design, Not Completely Change It
Multivariate testing is a great way to make incremental improvements to a design, rather than dramatic redesigns. Because it requires that you identify certain elements of interest on a page to test multiple variants of that variable, you cannot easily compare radical changes across variations.

For major redesigns, run an A/B test between the original and the proposed new version to find which is better. Then, refine various elements of the winning design using multivariate tests. Always keep iterating!

If your goal is to move towards a substantial redesign (such as major layout redesign), an A/B test to compare this new design against the current one is more appropriate than an MVT. Once the higher-performing design is discovered, use multivariate tests to further refine specific elements in the winning layout."
67,2018-02-18,"Have you ever involved stakeholders in design critiques, ideation sessions, or meetings devoted to analyzing research findings only to find them uninterested? And were the ideas and research findings ever ignored or forgotten? UX workshops have numerous benefits: educate people and make them empathic towards users, make stakeholders feel involved and responsible for ideas and research findings, create awareness of usability issues and design challenges, build common ground across all parties involved, and bring together many types of backgrounds and expertise.
However, in such UX workshops, it can be challenging to engage the team, and create order among diverse ideas and facts.
One method that helps teams collaboratively analyze research findings as well as ideas from ideation sessions is affinity diagramming. Often used in UX, affinity diagramming is adapted from the KJ diagramming method (named after author Kawakita Jiro).
Definition: Affinity diagramming refers to organizing related facts into distinct clusters.
Affinity diagramming is also known as affinity mapping, collaborative sorting, snowballing, or even card sorting. (However, in UX, ‘card sorting’ stands for a very specific research method for determining the IA of a site or application. In it, users sort index cards with category names and commands written on them.)

Affinity Diagramming in UX
While affinity diagramming as a method can be used by individuals as well as groups, in UX, it is used primarily by teams for quickly organizing:

observations or ideas from a research study
ideas that surface in design-ideation meetings
ideas about UX strategy and vision


Affinity diagramming in UX usually involves two steps:
A. Generating the sticky notes. In this step, team members write down ideas or facts on separate sticky notes.

During a usability session, the facilitator and the observers write observations, findings, or ideas — each on one sticky note.
During an ideation workshop, attendees or the workshop facilitator write each idea on a sticky note.

B. Organizing the notes in groups. After the test or the ideation session, the team has a workshop devoted to analyzing the notes by:

sorting them into categories
prioritizing each note, and determining the next steps in design or further research

Workshop Leader
It can be tiring to interpret and sort many notes, and attendees can lose interest if the meeting drags on. So, it’s important to have a leader to keep the meeting moving quickly and productively. This person’s job is to:

Communicate the agenda and goals.
Describe what people should be doing at each phase of the workshop.
Track and communicate time for each step.
Get all the notes on the wall.
Facilitate moving past any issues that may arise, such as helping the person who seems lost, or pairing people when one is idle and another is very busy.

Steps for Affinity Diagramming in UX
Before the Affinity-Diagramming Workshop

Choose a room that has walls on which sticky notes can be attached (e.g., glass or whiteboard). Alternatively, tape flip-chart paper on the walls and stick the notes on top, or use easels if walls are not tape-friendly.
Just before the meeting begins, enlist some help and put all notes on the wall, in no order.


All the sticky notes are posted on a wall in an affinity-diagramming workshop. (This picture shows notes from testing sessions; each note color corresponds to a different test participant. The color coding makes it easy to tell from which test each finding originated.)

3. Consider creating a few category names to help get the sorting started. For webpages, some common categories are: Search, Global navigation, Homepage, Legibility, Footer, Page layout. Stick these category names high on a blank part of a wall, so many notes can fit below.
4. Add a “?” category name for notes that are not understandable.
5. Put some pens or markers and blank pads of sticky notes on a table.
At the Start of the Meeting: Announce the Steps and Timing Guidelines
The leader should announce what the different steps of the process are and how long the workshop will take:
6. The general steps we’ll follow are:

Sort into top-level categories.
Sort each of those categories into subcategories.
Summarize those categories.
Determine priorities (e.g., by taking votes).
Plan subsequent design meetings as needed.

No talking as you sort, please. It gets distracting and makes the process take longer. This meeting should take not more than 3 (usually 2) hours. We’ll all need to work hard and quickly to make that happen.
Sort Notes into Top-Level Categories
The leader should describe the process and announce the time for this step:
7. Pluck a note off the wall and read it.
8. Look for a category under which it would make sense.
9. Stick it there.
10. If there is already a note that says the same thing, stack the notes on top of one another, so the most descriptive note is the only visible one.
11. If the note doesn’t fit under any category, think of a new one. Use markers and blank sticky notes to write down any new categories.
12. Yell out the names of new categories as they are stuck on a blank section of the wall, so everyone knows when a new category has been created.
13. If you can’t understand a note, move it to the “?” category. We’ll try to make sense of that in the end.
14. Let’s work to get these sorted within 30 minutes (or however much time you feel it will take).
15. I’ll periodically yell out how much time we have left.
16. Questions? Let’s go!

Your browser does not support the video tag.

In most browsers, hover over the video to display the controls if they're not already visible. Affinity-diagramming workshop for analyzing findings from user research on Marriott International's intranet


Sort Notes from Each Top-Level Category into Subcategories
Here are instructions for this step:
17. Everyone choose a category.
18. Categories with a lot of notes may require two people to sort them. Some people will be fast and will be able to sort multiple categories.
19. Look for related themes and sort those notes into subcategories.
20.  Write the names for the subcategories on sticky notes.
21. When you finish sorting a category, look for another category that needs sorting, or for a person who could use help. Don’t be idle.
22. Questions? Let’s go!
Present Each Category
23. Anyone who sorted a category can start. Stand by it and read each finding, then summarize the category.
24. It’s okay for the rest of the attendees to make related points or ask questions now. (But these comments add time to the meeting, so you may choose to skip this instruction.)
25. One-by-one, each person stand by her category and present it. Go clockwise around the room until all categories are done. If multiple people sorted a category, they can present it together, or choose one delegate.
26. Questions? Let’s go!
Determine Priorities
Slightly different methods are used depending on whether the purpose of the workshop is to analyze usability-test findings or organize new ideas.
A.     Usability-Test Findings
There are two common ways to determine which usability-testing issues have the highest importance: either by going through all findings and having people vote on their severity (high, medium, low), or by having people assign a finite set of points to those items they think are important and need to be fixed. The latter method often can be more effective and more immediately actionable than the former (which can result in many problems rated as “high importance”).
Severity
27. The meeting leader reads each finding again and asks people to vote about its importance by show of hands: high, medium, and low. Whichever rating gets the most votes is the rating assigned to the problem.
28. Write the rating (H, M, or L) on each note.
OR
Points
29. Each attendee gets a certain number (usually 3) of importance points which she can assign to the items she thinks most need to be addressed.
30. Then, people walk around and stick a sticker (or draw a dot with a colored marker) on the items they wish to cast their votes for.
31. The voting rules differ by team and local preferences, but usually people are allowed to assign multiple importance points (up to their allocated maximum) to a single issue if they think it’s crucial enough. For example, an attendee with 3 votes could assign two votes to one issue and one vote to another.
B.     Design Ideation
Design ideas from ideation workshops can also be voted on, or assigned value in some other way. Two common methods are the one-hundred-dollar test and the NUF test.
One-Hundred-Dollar Test
32. When there are several design ideas for the same problem, give the group $100 (any currency will do) to split among the ideas generated, such that the total for the group of ideas equals $100. This method forces people to think in terms of each design’s worth, and usually one design emerges more valuable than the other(s).
NUF
33. NUF stands for: New, Useful, and Feasible. Each design idea is rated on a 1- to-7 scale for each of these three attributes: 1) whether the team has used that design before (usually newer items are considered better); 2) whether the idea is useful and it solves the problem; and 3) how feasible it is for the team to implement the idea. The ratings are totaled, and the overall score is used to rank the ideas.  
Capture
The resulting clusters and the determined priorities are the end product of the affinity diagramming. To record it:
34. Write the main points on flipchart paper so all can see them.
35. Take photos of the walls. By now the stickies are usually falling down, which signals the meeting’s end.
End the Meeting
36. Assign follow-up tasks (or announce when these will be assigned).
37. Announce that a written report and (or) video recordings will be available.
38. Announce that design-workshop meetings will be scheduled, if appropriate, to design based on the outcome from affinity diagramming.
Timing for Each Step
The timing for each affinity-diagramming step can vary depending on the:

Number of notes
Number of meeting attendees
Attendees’ experience with user research and affinity diagramming

Even with an abundance of notes, we suggest limiting the meeting to about three hours, and having a scheduled 10-minute break. It’s also helpful to bring out some treats at key times — for example, while people are sorting categories into subcategories.
We like to keep most meetings between 90 minutes and 2 hours. Below are some general timing guidelines for each step:
 

Variations on the Affinity-Diagramming Process

In ideation workshops, teams often sort ideas after they generate them, within the same workshop.
For affinity diagramming of research findings, some teams prefer to sort the sticky notes after each participant’s test session, and again after all (four to eight) sessions. Here are some pros and cons of this variation:

Benefits
Benefits of sorting after each session are that:

It helps the team gain a shared understanding after each user.
It reminds the team members of what they saw, right when their memories are fresh, which can help with further discussions and with observing future test sessions.
It may ultimately be faster if your team members tend to take a lot of notes.
It enables you to summarize results quickly when you plan to make design changes between user tests — for example, for rapid prototype testing with very early design iterations.

Drawbacks

It takes a lot of time, and the same benefits may be achieved with just a quick discussion after each test.
It can deter stakeholders from coming to the final, posttest affinity-diagramming meeting.

Who Should Sort Notes
Anyone can come to the affinity-diagramming meeting. In particular, anyone who:

Attended all or some of the usability-test sessions or design workshop
Has an interest in the design that was tested, or in any changes that will be made to that design

Conclusion
I used to think that, as a user researcher, my job was to make the information I collected about users as understandable and digestible as possible for my team. And as a designer, I was supposed to come up with the best design solution. While I still feel these things are true, I have learned that insulating stakeholders from all the false starts, design ideas, details and messy notes taken in tests does them a disservice. To educate other team members, involve them in design, build consensus, and increase sensitivity to usability issues, get them involved in taking notes, sorting them, and discussing them rather that protecting them from these processes.
Reference
Karl Ulrich, 2003. KJ Diagrams. The Wharton School, University of Pennsylvania, Philadelphia, PA
Raymond Scupin, 1997. The KJ Method: A Technique for Analyzing Data Derived from Japanese Ethnology. The Society for Applied Anthropology"
68,2018-02-11,"During usability testing, UX researchers often ask participants to provide a subjective assessment (usually in the form of a rating) of their experience using a product or a site. Instruments such as the SUS, NPS, or the task-difficulty question are among the most popular choices. However, for many new UX practitioners, the precise purpose of these tools (and the correct way to administer and analyze them) is often mysterious. What do they truly assess, and what’s the proper methodology for using them? When should they be administered — in-between tasks or at the end of the session, and why does that matter? Why would you use one of the standard questionnaires rather than create your own?
All these questionnaires represent self-reported quantitative data; they are rarely meaningful by themselves, with no performance data (such as success rates or task times) to complement them. The format for all these questionnaires is usually a rating scale: the participant is given a question, and asked to select an answer, typically on 5- or 7-point scales (we don’t recommend using a scale with more than 7 options). The actual method of collecting the data is straightforward: the questions can be administered on paper by the test facilitator, or using a digital survey tool (which is the typical method used in remote unmoderated testing). However, knowing when and why to use which questionnaire is much more elusive for many budding test facilitators.
Post-Task vs Post-Test Questionnaires
There are two categories of questionnaires used during usability testing:

Post-task questionnaires are completed immediately after finishing a task and capture participants’ impressions of that task. When each task is followed by one such questionnaires, there will usually be many subjective answers collected from each user, since there are usually many individual tasks in a usability-study session.
Post-test questionnaires are administered at the end of a session (or after the participant has finished all the tasks pertaining to a site). They reflect how your users perceive the usability of your website or app as a whole (i.e. what their lasting, overall impressions are). User impressions of the experience as a whole are subject to the peak-end effect (that is, the most intense and last parts of the experience, either positive or negative, impact participants’ recollections and evaluations the most).

Post-task and post-test questionnaires aren’t incompatible; in fact, in most quantitative studies, it’s useful to collect both. (But be careful about tiring out your participants!) While these metrics do correlate fairly strongly, it’s not a perfect relationship; one type of quantitative data gives us a rather limited picture of the overall usability of the system, and the more metrics, the clearer the picture we can develop.
Both these types of instruments are indicators for the current state of the subjective user experience; you can use them to compare your current design against future iterations (or known industry benchmarks). These metrics do not tell you why users struggle with your design, nor do they provide direct insights as to how you can improve it. They simply are a way of keeping track of how your users feel about the experience of using your product.
It is critical to note that these questionnaires are quantitative instruments, and therefore they require larger sample sizes (typically at least 20–30 users) to be confident that their results generalize. Collecting quantitative data with small sample sizes (such as the 5 users we typically recommend during formative, qualitative usability testing) will almost certainly not generate statistically significant findings. However, if you combine a subjective rating scale with the follow-up question, Why did you give [site X] a score of [Y]? you can derive useful qualitative insights into what people feel about the design, even if you only test a handful of users. (Just don’t make a big deal about the average score.)
In most cases, we recommend using standard questionnaire over homegrown ones, since the former are supported by a lot of research to demonstrate their validity (that they actually measure what they intend to measure), reliability (that users will consistently answer the questions in the same way), and sensitivity (that they can detect meaningful differences).
The System Usability Scale (SUS): Post-Test Assessment of Usability
The most well-known questionnaire used in UX research is the System Usability Scale (SUS). The SUS has been around since the command-line interface days of the 1980s, and has been repeatedly demonstrated experimentally to be valid and reliable. It was invented by John Brooke at Digital Equipment Corporation. The SUS is a post-test instrument, given to a participant after an entire usability testing session is over (or, when testing multiple sites, like in competitive evaluations, after the participant has worked on all the tasks related to a site).

The System Usability Scale is a post-test questionnaire containing 10 different questions that address the usability and learnability of a system. Do not alter the order or the wording of the SUS questions if you want to compare your score with the scores collected from other designs.

The SUS is a series of 10 Likert-scale questions and produces a score from 0–100. However, the 0–100 score is not equivalent to a percentage score, such as on an exam — Jeff Sauro has done extensive benchmarking of SUS scores on many different systems, and has found an average SUS score of 68 across 500 studies. For your site’s usability to be in the top 10% of all sites, you would need a score of 80 or higher, whereas a score of 73 would place you only in the top 30%.
One of the biggest advantages to using the SUS is that it’s such an old scale that there is a large amount of industry-wide data available to help benchmark your score and understand it in context of your peers and competitors — something that less widely used survey instruments can’t provide. Be aware that the SUS correlates strongly with a much simpler metric, the single-question Net Promoter Score. They do provide different data, but for many organizations, the NPS may be more useful overall, as it’s a simpler metric to collect (one question versus SUS’s 10), and is a well-established general bellwether for the company (even if it’s not as sensitive to UX-focused concerns).
Single Ease Question (SEQ): Post-Task Satisfaction
In contrast to the SUS, post-task questionnaires are administered at the end of every task in a test session. They are useful for two big reasons:

They allow you to compare which parts of your interface (or workflows) are perceived as most problematic, since you collect this data after every task.
Since the task itself just concluded, it’s fresh in the participant’s mind, and therefore she is more able to provide a clear indication of her attitude toward the experience, without subsequent tasks coloring her memory.

Post-task questionnaires need to be short (1–3 questions) to interfere as little as possible with the flow of using the site in a testing session.
There are several widely used questionnaires in use; in most cases a single question instrument is the right fit for quantitative usability testing, because it takes little time and effort for participants to answer it after a task and is minimally disruptive. Since time with users is precious, it’s best to use an efficient survey instrument. More rating questions only give you marginally more insights than what you derive from a single question, so it’s better to invest your time budget in other activities, such as additional test tasks, than to ask more subjective-rating questions.
The “Single Ease Question” (SEQ) is a useful and simple version of this idea that has been experimentally validated and demonstrated as reliable, valid, and sensitive. The SEQ asks the user to rate the difficulty of the activity they just completed, from Very Easy to Very Difficult on a 7-point rating scale.

The Single Ease Question, (SEQ) a single-question post-task questionnaire measures users’ perception of usability based on the last attempted task. Since the task is still fresh in the participants’ minds, their answer provides a useful assessment of the experience for that particular task.

The finer granularity of post-task questionnaires may suggest that they could generate more actionable findings for design teams than the coarser finding of the user’s overall impressions through a post-test instrument. However, there is less data available for comparing your SEQ results with those from other companies (and the tasks being compared would need to be comparable anyway), so you’re mainly restricted to finding out what tasks are relatively easier or harder within your own system.
NASA-TLX: Post-Task Workload
The NASA-TLX (Task Load Index) is another type of post-task questionnaire that is useful for studying complex products and tasks in healthcare, aerospace, military, and other high-consequence environments. It tends to be used less frequently in UX work, but it is the standard questionnaire used by many studies in Human Factors and Ergonomics. The NASA-TLX emerged in the 1980s, as a result of NASA’s efforts to develop an instrument for measuring the perceived workload required by the complex, highly technical tasks of aerospace crew members.
The NASA-TLX contains 6 questions that users must answer on an unlabeled 21-point scale, ranging from Very Low to Very High. Each question addresses one dimension of the perceived workload: mental demand, physical demand, time pressure, perceived success with the task, overall effort level, and frustration level. After this initial assessment, users weigh each one of the six categories they just completed, to indicate which category mattered most to what they were doing. It’s a complex instrument to score, but thankfully NASA has released the TLX as a free iOS app.

The NASA-TLX instrument asks participants to rate each task they have performed on these 6 scales, each in 21-point increments. It provides rich data about what sorts of demands the task had on the user in multiple different areas, but requires time and expertise to collect during a study. Image from the official NASA-TLX paper and pencil worksheet.

While the NASA-TLX is often used as a key metric in human factors studies about complex, mission-critical systems, it can also be used in other types UX research, with a few caveats:

It’s a relatively complex questionnaire that needs to be answered after every key task, and so will add a lot of time (and potential participant fatigue) to the overall test process.
It can disrupt the study flow and make the experience quite a bit less natural for participants than if they progress smoothly through a test scenario.
It will often require that the facilitator explain the instrument multiple times (particularly with things like the difference between effort and mental demand, for example).
It is mostly helpful when studying situations where human errors are highly undesirable (healthcare, transportation, complex financial domains, and so forth).

Because of the complexity of this instrument, it’s not typically a good match for UX studies of consumer products or simple workflows. For highly complex processes, performed by trained workers, where users cannot choose which application they use and errors have high consequences, the NASA-TLX is the questionnaire of choice. Like the SUS, the NASA-TLX has published many studies and industry benchmarks to help you understand the scores in context, and to be able to meaningfully compare them to those of competitors.
Limitations of These Metrics
All the various satisfaction metrics discussed in this article suffer from the following limitations:

They are self-reported data, which can be unreliable.
They measure subjective user perception, not objective performance. While there is some correlation between satisfaction and objective performance metrics (like task completion rates, time on task, or errors), satisfaction metrics usually tell a clearer story when combined with performance metrics.
These metrics tell you what the user’s satisfaction level was, but do not pinpoint any weaknesses or strengths of the experience (or what you can change to improve it). Moreover, each participant may have a wildly different sense of what a 5 out of 7, for example, means.
Like all quantitative metrics, low sample sizes (like the 5 users we typically recommend for each round of qualitative usability testing) are unlikely to provide statistically significant or meaningful results. Numeric data from 5 users should not inform design decisions, and reporting numbers collected with such a small sample is highly misleading.

Summary
Self-reported data that addresses users’ satisfaction and perception of usability is often collected in quantitative studies together with other types of performance measures. Three popular instruments are: the post-test System Usability Scale (SUS), which provides helpful information about a user’s takeaways and overall experience; the post-task Single Ease Question (SEQ), which offers information about the usability of different task flows; and the post-task NASA-TLX, which is appropriate for measuring workload in complex, mission-critical tasks. Since all of these are quantitative measures, they require a reasonably large sample size to provide valid measurements.
For most practical UX research, we recommend simple satisfaction questionnaires, with as few questions as possible. The question to ask depends on your research goals:

In most formative, qualitative studies:

Ask How satisfied were you with this website? plus the follow-up question of Why did you give a score of [X]? This will give you insights into whatever aspects of the user experience matters most to your users’ satisfaction, which is the main thing to learn from subjective user feedback.
If you’re specifically interested in the usability of the individual components of the UI, use the Single Ease Question after each task and ask users to explain their score. (However, usually it’s more accurate to judge the usability of design elements through direct observation than from subjective scores.)


For summative quantitative studies meant to benchmark the usability of your site (either by comparing it with its other design iterations, or with competitors):
	
In most cases, use the SUS after the test and the SEQ after each task, as satisfaction metrics to complement other performance metrics such as success rates and time on task.
If you have the special case of complex, mission-critical workflows, replace the SEQ with NASA-TLX.


If you want to assess the business impact of your user experience, ask the NPS question, How likely are you to recommend this website to a friend?

Learn more in our full day seminar on Measuring User Experience."
69,2018-01-21,"A carefully crafted set of tasks is necessary for success in any type of usability testing. There are some characteristics your tasks should always have, regardless of the specific methodology of your research. However, what precisely counts as a good task depends on whether you’re running a quantitative (quant) or a qualitative (qual) usability study.
Background: Quantitative vs. Qualitative Studies
There are many differences between quantitative and qualitative usability testing. Each one is appropriate for different research goals.
Quant studies closely resemble scientific experiments, with rigorous well-controlled conditions intended to enable the capture of metrics (such as success or time on task).
In contrast, qual studies attempt to discover problems in the UI. Qual research aims to understand the thinking and the difficulties experienced by individual users. It often relies on presenting users with open-ended activities that have the potential to expose issues in the UI.
As a rough summary, the goal of quant user testing is numbers, while the goal of qual testing is insights.
Since the goals and characteristics of these two methodologies are quite different, you’ll need to write tasks differently depending on the type of study. In many cases, a task that would be perfect for a qualitative study wouldn’t work at all for a quantitative study, and vice versa.
Let’s first look at the basic principles of writing tasks for any user testing — quant or qual.
Writing Tasks for Any Kind of Study

Look at what your users need to do with your product for inspiration for your tasks. Remember that tasks must be as realistic as possible. You don’t want to force participants to do something they wouldn’t do in real life, or you’ll measure something that’s irrelevant to your project. User interviews, search-log data, customer-service calls, and analytics data are good places to find out what people want to use your product for.
Avoid accidentally providing any clues in your task. Don’t describe the exact steps users need to take — let them figure it out on their own. Avoid the exact language used as labels in your UI. You don’t want to prime users to look for a specific word.

Bad: Sign up for the Alertbox.
Better: Sign up for this site’s newsletter.

Try to keep the task emotionally neutral. Avoid making assumptions about your user’s personal life or relationships when you’re writing tasks. When possible, it’s better to write a task referencing a “friend,” rather than “your father,” “your spouse,” or some other family member.

Bad: Find a gift for your mom’s birthday.
Better: Find a gift for your friend’s birthday.

Always pilot test your tasks. This is a critical step that far too many researchers skip. You may think your tasks will work well, but you don’t know until you try them out with one or two representative users. Always plan for a pilot in your study budget and schedule. It’ll save you from wasting your resources by accidentally using a bad task and from getting bad data.

Writing Tasks for Qualitative Studies

Open-ended tasks are OK. For qualitative studies, it’s OK to leave the task open to interpretation. While sometimes you’ll want  to give users specific criteria, you can also leave tasks open to see what they care about, and what factors into their decision-making.

Specific qual task: Open a Cash Rewards Credit Card.
Open-ended qual task: Find a credit card that fits your needs.

Provide enough detail to set the scene, but don’t go overboard. It’s important to make sure you’re establishing the participant’s motivation for doing this task, but you don’t have to write a novel. If you’ve made sure this participant is representative of your actual users, and you’re sure this task is realistic for them, then you shouldn’t need to provide too much context.
If you aren’t getting the insights you need, change the task. In a qualitative study, it’s OK to change your task wording mid study, or even throw out or add a new task altogether. Qualitative research is all about getting useful observations, and it’s fine if you don’t have every participant do the exact same tasks. (In fact, better than wasting precious participant time on repeating a useless task.)

Writing Tasks for Quantitative Studies

Make sure there’s only one way to do the task. In contrast to qualitative tasks, quantitative tasks must have only one possible interpretation and solution. They can’t be ambiguous.
	If the instructions are too broad or open-ended, each participant may do essentially different tasks and take different paths through the interface. The metrics you collect won’t describe the same thing, and you’ll have a lot of variability in your data. And how will you know what counts as a “success” if there are many different possible solutions?

Bad: Find a credit card that fits your needs.
Better: Open a Cash Rewards Credit Card.

Provide as many details as necessary to keep the task narrow and focused. In some cases, this means providing a level of detail that would feel excessive in a quantitative study.
	Often, you might think you have enough detail, but then in a pilot test you’ll realize you forgot something. For example, it isn’t enough to tell users which hotel to book at which time period — you also need to tell them what kind of room to book. This level of direction would not be required in a qualitative study, but it’s necessary in a quantitative test to make sure everyone will perform exactly the same task.

Bad: Book a room for 2 people at the Hyatt Regency in Chicago from January 17 to January 19.
Better: Book a Queen Room for 2 people at the Hyatt Regency in Chicago from January 17 to January 19.

Provide fake credentials (dummy data) for login, checkout, or any other task that requires entering personal information. The fact that everybody will enter the same information minimizes variability. All participants will type the same strings. Additionally, some people may be more hesitant than others to share their data. It may take up extra time to come up with an acceptable solution. (For many qual studies, you may want to have participants use their real data instead.)
Each task should stand alone. Ideally, you want to be able to randomize the order of tasks in quantitative studies. If you have two tasks that build on each other, those tasks must always be presented in the same order. Additionally, someone who fails the first task will automatically fail the second.

Bad:
Task 1: Find the personnel file on Joe Smith.
Task 2: On Joe Smith’s personnel file page, find his direct manager.
Better:
Task 1: Find the personnel file on Joe Smith.
Task 2: On the page provided at this link, find who the direct manager of Jim Grant is.

Each task should have one single success criterion. Avoid combining two tasks into one. In quantitative studies, you need a single, clear end point to help you determine time on task and whether the user was successful. If you have multiple success criteria in a single task, what happens if a user finds one piece of information but not the other? Fix stacked tasks by splitting them up.

Bad: Find the museum’s address and holiday hours.
Better:
Task 1: Find the museum’s address.
Task 2: Find the museum’s holiday hours.

Once you’ve begun the study, don’t change the tasks. It’s important that each participant is doing the exact same task in quantitative studies. These studies should hold all conditions constant except your independent variable (for example, your app vs. your competitor’s app). That means you don’t want to get halfway through your study and realize you want to change the wording of a task. Doing so may contaminate your final result. Pilot testing is critical for any research, but particularly important for quantitative testing, when you don’t have the flexibility to change things mid study.
Focus on top tasks. Quant studies are expensive, and it does not pay off to test tasks that aren’t high priority for your users and your organization. Whereas qual tests may have more flexibility to try tasks that are realistic edge cases, quant studies must focus on the most important, core tasks.

Recap: Qualitative vs. Quantitative Tasks




 


Qualitative


Quantitative




Task characteristics


Open-ended, exploratory


Concrete, focused




Details in task wording


Provide as necessary to establish motivation


Provide as necessary to ensure one single way to do the task successfully




Randomization of task order


Nice to have but not required


Required




OK to change tasks midstudy


✓


✗




Realistic tasks that are based on user research


✓


✓




Task wording that does not give any clues


✓


✓




Emotionally neutral tasks


✓


✓




Dummy data


✓


✓




Pilot testing of tasks


✓


✓




 
Regardless of the type of study you’re planning, writing good tasks takes practice.
 
For hands-on practice writing good qualitative tasks and tips for facilitating a qualitative usability study, check out our full-day seminar Usability Testing."
70,2017-12-17,"Quick quiz. Which phrasing of a follow-up question will result in more accurate results in your next user research?

“I saw you were having difficulty with the navigation. What happened?”
“Why did you have difficulty with the navigation?”
“What was easy or difficult about getting to the content you wanted?”

The third question will get you the most reliable response, but why? Because each of the first two questions is leading – meaning that it includes or implies the desired answer to the question in the phrasing of the question itself.
Let’s see what is problematic about the first two questions.
Question 1: “I saw you were having difficulty with the navigation. What happened?”
Problem: The interviewer rephrases what was observed, which may not be an accurate representation of the user’s experience. To the interviewer or observer, it may have looked like the respondent was struggling with navigation, but she may have been deciding what information was most important, confused by the task, or exploring various areas of the site. The question also names a user interface element — the navigation — which is a term that users may or may not fully understand, relate to, or normally use.
Question 2: “Why did you have difficulty with the navigation?”
Problem: Again, this question implies the answer and assumes that navigation was the problem. It also puts the blame on the user, rather than on the site. It focuses the question on the user’s actions as opposed to the elements in the site that may have contributed to the user’s actions.
Question 3: “What was easy or difficult about getting to the information you wanted?”
Improvement: This question steers the user to the topic of interest — moving around the site and finding content — without suggesting terms or feelings to the user. The user can say it was simple to move around or difficult, without disagreeing with the interviewer.  Here the interviewer offers a general frame for the topic of the question, rather than suggesting a response.
Much of user research is observational — we watch what users do. But we also listen to what users tell us, and in many instances, we request clarification about what they tell us. We ask follow up questions after tasks. We may prompt users to share more information in the moment. We start a session by asking for or confirming some basic information about users.
Honest, unbiased participant feedback is critical for user research. When we ask questions, we want to learn more about the user’s actions. Why was this piece of content clear? Why did an interface element cause difficulties? Leading questions are a problem because they interject the answer we want to hear in the question itself. They make it difficult or awkward for the participant to express another opinion. This is particularly true in a usability-study interaction, where often the interviewer is the “authority” in the room and many participants will not want to disagree.
Leading questions result in biased or false answers, as respondents are prone to simply mimic the words of the interviewer. How we word these questions may affect the user response and also may give them extra clues about the interface. We may end up with inaccurate feedback that may or may not truly reflect the user’s experience, mental model, or thought process. Or, even worse, we may alter that user’s behavior for the rest of the session. For example, an unexperienced facilitator asked “What do you think this button does?” in a session and made the user realize that the text she was pointing to was in fact an active link.
Leading questions ultimately rob us of the opportunity to hear an insight we weren’t expecting from the user. The more leading our questions are, the less likely the user will comment in a way that surprises or intrigues us, or makes us think about a problem or solution in a different way. They may be good for “validating” designs, but are definitely bad for testing designs.
Keep in mind that sometimes the best question is not a question at all, but a redirection to help users continue their thoughts. When we do want to ask questions, how can we avoid leading the user?
What Makes a Question Leading
You may be familiar with the idea of leading questions from courtroom dramas, as lawyers call out: “Objection! Leading the witness!” There are many ways in which we can prime a user to simply repeat or confirm our bias, agenda, or assumption. Some practitioners may do it intentionally, trying to get confirmation for their own theory about what does or does not work in a design. However, most of us do it unintentionally. To gather user insights, we should ask open-ended questions — questions designed to elicit explanations from users, rather than single-word yes, no, or multiple-choice answers. How we ask these questions is essential to the value and validity of the feedback we receive. Here are some common traps to avoid:

Do not rephrase in our own words.

	
Participant: “I notice this picture here…”
Researcher: “You mentioned that the picture was helpful. What about it did you like?”
Improvement: “You mentioned the picture…?”


Do not suggest an answer.

“How well would this save time for you during your workday?”
Improvement: “How might this affect your efficiency, if at all?”


Do not name an interface element.

“The related links on the side of the page here — where would those lead?”
Improvement: “This area on the side of the page… [point to area]. What is that?”


Do not assume you know what the user is feeling.

“When you were struggling with this task, what was happening?”
Improvement: “What was easy or difficult about completing that task?” 



Practice Makes Perfect…or at Least Better
Some questions that we ask participants are prepared ahead of time — for example, we may start or end all research sessions with a list of standard questions. In that case, we can easily review those and rewrite them as many times as necessary to make them neutral.
It is much more difficult to make up questions on the fly without impacting what the user says. Everyone makes mistakes in phrasing now and then, and it is difficult to ask a question that is unbiased in every way. However, being aware of the problem is a good way to start fixing it. Watch out for instances where you ask leading questions, or observe others doing so, and see how it impacts the user’s response."
71,2017-10-01,"Introduction
All usability-testing studies involve a participant performing some assigned tasks on one or more designs. There are, however, two types of data that can be collected in a user-testing study:

Qualitative (qual) data, consisting of observational findings that identify design features easy or hard to use
Quantitative (quant) data, in form of one or more metrics (such as task completion rates or task times) that reflect whether the tasks were easy to perform

Qual Research
Qualitative data offer a direct assessment of the usability of a system:  researchers will observe participants struggle with specific UI elements and infer which aspects of the design are problematic and which work well. They can always ask participants followup questions and change the course of the study to get insights into the specific issue that the participant experiences. Then, based on their own UX knowledge and possibly on observing other participants encounter (or not) the same difficulty, researchers will determine whether the respective UI element is indeed poorly designed.
Quant Research
Quantitative data offer an indirect assessment of the usability of a design. They can be based on users’ performance on a given task (e.g., task-completion times, success rates, number of errors) or can reflect participants’ perception of usability (e.g., satisfaction ratings). Quantitative metrics are simply numbers, and as such, they can be hard to interpret in the absence of a reference point. For example, if 60% of the participants in a study were able to complete a task, is that good or bad? It’s hard to say in the absolute. That is why many quant studies usually aim not so much to describe the usability of a site, but rather to compare it with a known standard or with the usability of a competitor or a previous design.
While quant data can tell us that our design may not be usable relative to a reference point, they do not point out what problems users encountered. Even worse, they don’t tell us what changes to make in the design to get a better result next time. Knowing that only 40% of the participants are able to complete a task doesn’t say why users had trouble with that task or how to make it easier. Often researchers will need to use qual methods to supplement quant data in order to understand the specific usability issues in an interface.
Statistical Significance
One advantage of quant over qual is statistical significance. When quant data are presented in a sound way, they come with a certain protection against randomness: usually, mathematical instruments such as confidence intervals and statistical significance will tell us how likely it is that the data reflect the truth or whether they may be just an effect of random noise —  perhaps an artifact of the specific participants that we happened to recruit or of the conditions in which the study was run. Although seasoned qual researchers will deploy an arsenal of good practices to protect themselves from chance and to make sure that their results are not biased, we have no formal assurance that the findings from a qual study are indeed objective and representative for the whole target population.
Differences Between Qual and Quant
Qualitative and quantitative data need slightly different study setups and very different analysis methods. They are rarely collected at the same time — hence the distinction between qualitative and quantitative user studies. Both qualitative and quantitative testing are essential in the iterative design cycle. Although qual studies are more common in our industry, quant studies are the only ones that allow us to put a number on a redesign and clearly say how much our new version improved over the old one —  they are the essential instrument in calculating return on investment.
The table below summarizes the differences between the two types of research. In the rest of the article we discuss these differences in detail.



 
Qual Research
Quant Research





Questions answered


Why?


How many and how much?




Goals


Both formative and summative:

inform design decisions
identify usability issues and find solutions for them



Mostly summative:

evaluate the usability of an existing site
track usability over time
compare site with competitors
compute ROI





When it is used


Anytime: during redesign, or when you have a final working product


When you have a working product (either at the beginning or end of a design cycle)




Outcome


Findings based on the researcher’s impressions, interpretations, and prior knowledge


Statistically meaningful results that are likely to be replicated in a different study




Methodology



Few participants
Flexible study conditions that can be adjusted according to the team’s needs
Think-aloud protocol




Many participants
Well-defined, strictly controlled study conditions


Usually no think-aloud





The Iterative Design Cycle: Goals for Qual vs. Quant
The basic user-centered design cycle starts with an evaluation of an existing design, followed by a redesign intended to address the current system’s usability challenges. Once the new version is complete, it can be evaluated and compared against the initial version.

The iterative design cycle: Steps 1 and 3 involve summative research (done with either quantitative or qualitative methods), while step 2 involves formative research (done with qualitative methods). 

The first and third stage of the iterative design cycle are summative — they are intended to provide an overall assessment of a design. In these steps both qual and quant research methods (or combinations such as PURE) can be used to evaluate the design. However, when the goal is to link the entire redesign effort to actual financial savings or explicitly figure out how much the redesign has improved, quant studies must be used. Organizations with mature UX often have such a quant usability-tracking process in place. (Sometimes this process of quantitatively evaluating each version of a design and comparing it with previous versions is called benchmarking.)
During the redesign stage, user research has a formative role: it is meant to inform the design and steer it on the right path. In this phase, designers and researchers need to get user data relatively quickly and cheaply in order to be able to choose among different design alternatives and create a usable UI. At this stage, qual studies are usually the most appropriate. We know that, with 5 users, a qualitative study is likely to uncover 85% of the usability problems in a design (provided that the design is not already close to being perfect), so, in the redesign step, it makes sense to run one quick study with a few users, determine the big issues, fix them, and then test the new version again with another small set of users.
When to Use Qual vs. Quant
Qual studies are well suited for identifying the main problems in a design: for example, we can easily run a qualitative study to see what (if anything) prevents users from submitting a form successfully, and, based on that study, we may determine that we need to lengthen the form fields, present password requirements, or use labels outside the fields rather than inside them.
In contrast, most quant studies are done on a complete version of the site, with the purpose of evaluating the usability of the site, rather than directly informing the redesign process. This is not because one could not employ quantitative methods during the redesign iterations, but rather because quant usability studies would be too costly if used often and early in the design process. Quant studies usually involve a large number of users, and most organizations cannot afford to spend a lot of money on such studies to investigate whether the page copy is clear or whether a button is findable. However, the numbers obtained from quantitative testing can be invaluable when it comes to convincing upper management that your site is in need of a complete redesign.
Outcome: Qual vs. Quant
Qual data usually will consist of a set of findings, which identify (and prioritize according to severity) the strengths and weaknesses of a design. These findings are estimates — they are based on the knowledge and level of experience of the researcher facilitating the task and interpreting the meanings of the users’ actions. Different practitioners will often identify different issues in the same user-testing session (a phenomenon known as the evaluator’s effect). Plus, even if we’ve been very careful in recruiting participants who match our target demographics, when we include only a few people there is always a chance that they are not truly representative of the whole user population, and so our findings may be skewed.
Quant studies usually involve a relatively large number of users (often more than 30) and use statistical techniques to protect themselves against such random events. When reported correctly, quantitative studies will include information about the statistical significance of the results. For example, a margin of error will help you understand how much you can trust the results from the study.  Or, if the difference in task-completion time between your site and the competitor’s site was statistically significant, you will know that, even if you were to recruit a different set of users and rerun your study, your results will point in the same direction, even if the exact averages may be slightly different.
Thus, when quantitative studies are carried out and analyzed correctly, you can have confidence that their results are sound. Namely, that they are not due to a lucky or unlucky throw of the dice.
These types of analyses are based on statistics and usually involve other types of skills than you will find in a qual usability researcher. That is why many companies have separate job requirements for quant and qual UX researchers.
Methodology: Qual vs. Quant
On the surface quantitative and qualitative user testing can look quite similar (i.e., they both involve users performing tasks on a design). Both types of studies need to follow the basic rules for good experiment design, by making sure that they have:

External validity: participants are representative of the target audience and the study conditions reflect how the task is done in the wild. For instance, testing a mobile site on a desktop simulator lacks external validity because people would normally use that site on a touchscreen phone.
Internal validity: the experiment setup does not favor any one condition. For example, if design A is tested in the morning and design B is tested in the afternoon, it is possible that fatigue play a role in how participants use design B.

But, because quant studies strive to obtain results that are meaningful statistically, there are some important differences between the two types of studies:

As discussed above, quant studies involve more users than qual studies.
Because differences in the session setup and in participant backgrounds can increase the measurement noise and lead to larger margins of error, quant studies aim to minimize variability as much as possible. Thus:
	
The conditions in quant studies need to be stringently controlled from session to session. That is, you need to make sure that your participants are all run in pretty much the same environment as possible: you cannot have two sessions done in person, and three sessions done remotely.
Quant studies often start with a practice task intended to make all participants familiar with the study setup and with the site being evaluated. In this way, possible individual differences between, say, expert and novice users are ironed out, as novices get a chance to learn the interface.
The think-aloud protocol is the de facto method in qual studies, but is sometimes not recommended in quant studies. Researchers are split as to whether the think-aloud protocol can be soundly used in quant studies. To some degree, because some people are more talkative than others, it is likely to increase the measurement noise. As a result, many quant studies do not ask participants to think out loud.
Personal information such as names, addresses, or birthdates will increase the variability of the study, because different people have different data. Whereas for a qual study, you want people to enter their own, real information, in a quant study everybody should have the same experience and thus should type in the exact same strings. That is why participants should be provided with a set of made-up data that they can all use. (This constraint will sometimes create backend difficulties for a live system.)


Conversely, for qual studies, it’s okay to vary the study conditions between sessions. For example, if you discover that a certain task doesn’t give you the insights you need, by all means rewrite it before running your next user. Changing the task would make it invalid to average measures across users who had performed the different tasks, but in a qual study you aim for insights, not numbers, so you can take liberties that will ruin numbers (which are not your research goal anyway).
For quant studies, tasks need to have a single well-defined answer. Thus, while a task such as “Find the phone number and the address for John Smith” may be appropriate for a qual study, it is not good for quant studies because it is hard to code success for it: if the participant finds the phone number but not the address, should that be considered a failure?
	Moreover, all participants should understand the same thing when they read the task. A task such as “Research the requirements for obtaining a drone-flying permit in California” is too vague for a quant user study, as different people may understand different things by the word “research,” but can be okay in qual studies if you’re trying to figure out what types of information people may be interested in.

Whereas it is good practice to use task randomization in both qualitative and quantitative experiments, often qual studies won’t be completely randomized. In quant tests, randomization ensures that the order of the tasks does not bias the results in any way.

Conclusion
Qualitative and quantitative user testing are complementary methods that serve different goals. Qual testing involves a small number of users (5–8) and directly identifies the main usability problems in an interface. It is often used formatively, to inform the design process and channel it in the right direction. Quant usability testing (or benchmarking) is based on a large number of participants (often more than 30); when analyzed and interpreted correctly, results from quant tests come with higher protection against random noise. Quant studies offer an indirect, summative evaluation of the usability of a site through metrics such as task-completion rate, task time, or satisfaction ratings and are typically used to track the usability of a system over design iterations.
For an overview of popular quantitative research methods, guidance on which to use each one, and how to calculate return on investment, check out our course Measuring UX and ROI."
72,2017-09-17,"For some usability studies, there may be an obvious and limited set of interface elements that should be tested. In many cases, however, the system is complex, new to the researcher, or the list of candidate features for testing is long. This article shows a 7-step method that helps everyone focus on the most important elements to test first, while also prioritizing everything of concern for later research.
The 7 Steps

Determine the most important user tasks.
Discover which system aspects are of most concern.
Group items from 1 & 2, then sort issues by importance to users and organization.
For each top issue, condense the information into a problem statement.
For each problem statement, list research goals.
For each research goal, list participant activities and behaviors.
For each group of goals, write user scenarios.

Our checklist for planning usability studies describes the larger process.
It’s important to involve stakeholders in these planning steps. After conducting user studies, researchers can sometimes encounter resistance to the findings: someone says the researcher recruited the wrong participants or tested the wrong elements. Going through part of the test-planning process as a team ensures that everyone understands and supports the research goals, priorities, and methods.
If you have only limited access to your stakeholders, you can do steps 1 and 2 in advance, steps 3–5 can be done in a one-day workshop with the stakeholders, and then the UX researcher or UX team can do steps 6 and 7 alone.
For each product, you might do this end-to-end process only once, because the process generates important mutual understanding, many useful research issues, and potential metrics. New features and issues that come up from time to time can be easily added into the artifacts resulting from this one-time process.

This process flow depicts the steps discussed below.

1. Determine the Most Important User Tasks
What do people need to do? Make a list of the tasks that users must be able to accomplish with the system.
Normally, every organization has a list of the top user tasks, because this list is instrumental not only in evaluating the usability of your website, device, or application, but also in tracking improvement over time. Ideally, you should use a systematic way of compiling the set of top tasks, by having users rank all possible tasks that the system can support.
But even if you haven’t yet formally engaged in this exercise, you can still determine the top tasks by looking at traffic data from website analytics, search-log queries, survey data, a competitor analysis, or other types of user data. And sometimes the important tasks are well-known or obvious. For example, on an ecommerce website, one of the most important tasks is buying a product. This task is composed of a number of subtasks:

Top task: Buy a product
	
Look for a desired product.
Compare similar products and decide which one to buy.
Put product in the shopping cart.
Complete the checkout process.
Be sure the purchase is complete.
Understand when the product should arrive.



Other important tasks on an ecommerce website likely include: return a product, ask customer service for information, and look up a previous purchase.
2. Discover Which System Aspects Are of Most Concern
What are stakeholders most worried about? Often organizations have concerns that motivate them to seek usability testing. Examples include making sure users understand new features, reducing training and support costs, or discovering why people do (or don’t do) something. Look for issues that make users unhappy, problems that cost the organization money, and tasks that take too long to accomplish.
If stakeholders are not nailing these lists to your door already, you can discover top concerns and burning questions by interviewing members of product teams, customer-support staff, sales people, social media representatives, documentation specialists, and trainers. Other evidence may exist, such as search logs, FAQs, journey maps, and materials made by support and training staff that point to issues that might be useful to explore in testing. Doing a design review of the system is another good way to find issues that would be useful to test.
3. Group Items from 1 & 2, then Sort Issues by Importance to Users and Organization
Group related user tasks, stakeholder concerns, and questions. Sticky notes, a whiteboard, spreadsheet, or card-sorting system can help with this process. Your list of grouped issues will probably be too much for one research study, but it will be useful over time, so keep everything. It’s important to include user tasks, because stakeholder concerns don’t always encompass what users want and need to do; yet top tasks must be usable and often need testing.
If possible, prioritize issues as a team, with representative stakeholders.
Name each group of related issues, then put their names in a spreadsheet to sort and prioritize. You can use various methods to decide priorities; here’s a simple example that takes into account both user and business goals.




Issues


Importance Ranking




Name


Users


Organization


Total Score




Customer-service quality complaints


5


5


10




Search-results quality issues


5


3


8




Comparing stereos is difficult


2


5


7




Sharing feature is not being used enough


0


5


5




Few people listen to the podcast


0


4


4




A spreadsheet and a point system can help with ranking: Sort the named issues from most important to least important, considering both the concerns of users and the costs and benefits to the organization.
Rank issues by importance to both organization and users. When there are many stakeholders or many issues, consider voting with tokens (sticky dots or coins, for example) or using a survey platform’s ranking system.
The objects of this step are to:

Capture everything of importance
Prioritize the list of issues quickly
Create consensus

Don’t obsess over the method of scoring. List the top issues in a spreadsheet, with one column for importance to users and another column for importance to the organization. If the number of issues is large, rank only the most-important 10 or 15 issues in the table. Add up the user and organization numbers for each issue, and then sort the rows by total score.
Together, choose the issues to focus on for the next usability study. Keep in mind that although the total scores are important for prioritization and consensus, sometimes a test should include issues that score 0 for users but high for the organization — for example, expensive problems or new features.
Tips: If you require more accuracy — for example when the list of tasks is very large, the frequency of testing is very low, or changing the product is very expensive — you could survey users and stakeholders separately in order to discover and rank top tasks and concerns. (See our 2 min. video about ranking techniques for UX projects.)
4. For Each Top Issue, Condense the Information into a Problem Statement
Problem statements help everyone focus on what’s important and help you tie research goals to issues everyone wants to solve. Maintaining a list of known problems helps justify and get buy-in for research activities. Problem statements also suggest metrics you can measure to track improvement over time. Sometimes these statements will also include questions. Related issues can be combined into one problem statement, so you may end up with fewer problem statements than top issues. If that is the case, you will need to prioritize the problem statements — using either the issue priorities determined in step 3 or some other method.
Encourage stakeholders to participate in this step as well. It’s much easier to get people to accept and act on the results of a user study when they have been part of the process.

Explain that the purpose of the meeting is to help decide what is most important to test first, and how.
Share the detailed list of issues from step 3 with everyone.
Write the issue names on a big whiteboard (or share a document for writing text together, if your team is distributed), leaving blank space under each for the problem statements.
Combine related issues into problem statements; for example:

Problem: People call customer service because they can’t find instructions on how to set up their networked lighting system or because they don’t understand the instructions. Calls are expensive for the company, and customers are angry when they call.
	 
Prioritize problem statements for testing, either by combining issue scores (as in step 3) or by voting.

5. For Each Problem Statement, List Research Goals
Problem statements suggest goals for the study. For example, given the problem statement above, here are some corresponding study goals:

Discover where in the process people need instructions.
Learn why people can’t find the instructions.
Determine the best locations to put the instructions.
Find instructions that need improvement.
Find opportunities to improve the product in order to minimize instructions.

Some research goals can’t be addressed with usability testing. In those cases, note which methods would be better to use (for example, a survey or analytics) and set those goals aside.
6. For Each Research Goal, List Participant Activities and Behaviors
List the user activities and behaviors that you need to observe in order to address each of the research goals.
Research goals and problem statements can be quite abstract and high level. In order to tackle them in usability testing, you will need to observe users attempting to complete activities that are related to these goals and problems. This step helps you identify (1) types of user activities that are related to your research goals; (2) user behaviors that signal success or failure for a particular activity; (3) possible steps and problem-solving strategies that people could use in doing the activity.
For example, for the goal “Discover where in the process people need instructions,” some likely activities for the process would be:

Unbox the lighting system
Assemble the lighting unit
Connect the unit to the network

Some possible participant behaviors that signal success or failure might include:

Asking questions (note their exact wording)
Getting stuck for more than a minute
Wanting to contact someone for help (note who and how)

Some problem-solving strategies people may use to complete the activity include:

Looking for instructions in or on the box
Looking for instructions on the website (note search queries and navigation paths)

After you have done this breakdown for each goal, group goals that have all or many of the same activities and behavioral observations so you can try to write one scenario that covers several goals.
In this example, it’s likely that the listed activities and behaviors also cover the goals: “learn why people can’t find the instructions,” “determine the best locations to put the instructions,” and “find instructions that need improvement.” The final goal for this problem statement, “find opportunities to improve the product in order to minimize instructions” will probably be met by ideas that occur to you and observers during the study.
Planning at this level of detail has four main advantages:

Focuses scenarios (step 7) on what you hope to observe
Helps observers focus on what’s important to note during the sessions
Allows you to decide on the granularity of the success and failure criteria
	For example, instead of having one, monolithic user activity (set up the lighting system), you can consider whether to assign points to (or just track success and failure for) some or all of the subtasks that the participants complete successfully.
Helps you decide in advance how far you want to let the participants go, how extensive your test environment needs to be, and what you might need to prompt participants for

For example, in this situation, you need to decide whether you want to:

Listen to participants make calls and find out how well that process works for them
Watch them create support tickets to discover how difficult filling out the form may be
Capture the questions they want to send to support (how?)
Let them look through the support website (for how long?) or just find the support landing page
Prompt or encourage the participant to do or not do any of these things (what’s the best way to phrase what the facilitator should say?)

7. For Each Group of Goals, Write User Scenarios
Scenarios are the instructions that your test participants read during the study. For our example list of research goals (step 5) and the corresponding activities and behaviors (step 6), here’s a good user scenario:
Please show me how you would set up your new Acme Home Illumination Kit, and explain what you are thinking and doing in detail while you put it together.
With any luck, participants who attempt this scenario will find and read instructions as needed and they will think out loud by way of explaining, so you can understand their difficulties with both the system and instructions.
Because the example situation is quite straightforward, this scenario seems obvious and unworthy of the steps to arrive at it. (However, all scenarios should encapsulate your thinking and planning process into a simple statement that participants can understand quickly.)
Nonetheless, this method can untangle big and complicated systems, tame and educate a room full of stakeholders having competing priorities, generate a prioritized list of research problems to work on over time, help organize the test plan, the facilitator script, and observer study guide, generate usability metrics, and help you think through the logistics and contingencies for your usability sessions.
Tips:

Some goals might lead to several scenarios. Some scenarios might cover several goals. As you work through this process, think about how you can combine goals into scenarios without making them too long or too complicated.
Similarly, across problem statements, some duplication will naturally occur when making lists of goals and activities. After you make the lists, consider whether it makes sense to remove some steps or to combine or sequence scenarios in order to reduce repetitive activities or to reduce biasing effects (when doing one scenario teaches people a lot about another scenario). Sometimes it makes sense to watch each person do many searches, but you might need to watch other activities only once or twice for each participant.

Guidelines for Writing Scenarios
We wrote entire articles to help you write effective scenarios and avoid common errors in the specific task instructions. Briefly:

Focus scenarios on outcomes your users would be motivated by.
Write scenarios in a way that should cause people to do the things you need to observe.
Don’t instruct people to operate the user interface.
Don’t mention labels found in the interface. (An exception might be “search,” when you’re testing search, but consider whether you want to find out if people notice or locate the search tool on their own before you mention it.)
Use general words your participants should understand, not technical or branded terms.

Next Steps
Once you’ve generated scenarios for your research goals, the next steps usually involve:

Engage stakeholders in observing the study sessions to help you focus on the most important issues and to build consensus and buy-in for your user research activities.
Decide what to measure and how to score it.
Pilot test your session to debug the scenarios and your process, and to determine how many scenarios to attempt per session.
Include the information generated from these 7 steps in your research plan for all of the scenarios in your study. Document the rest of the information for future research studies.
Invite stakeholders to your study sessions and encourage them to help you take notes.

Conclusion
This 7-step procedure makes it easy to decide what to test, how to test it, and how to write comprehensive user scenarios. The lists generated at each step help you to prioritize research goals, to get buy-in for your research sessions, to organize your study sessions and materials, and to track important issues over time."
73,2017-08-06,"Introduction
Personas have long been a useful tool in a user-centered design process; however, in recent years, jobs-to-be done, a new technique for focusing on customer needs, has been gaining steady prominence.
Definition: Jobs-to-be-done (JTBD) is a framework based on the idea that whenever users “hire” (i.e., use) a product, they do it for a specific “job” (i.e., to achieve a particular outcome). The set of “jobs” for the product amounts to a comprehensive list of user needs.
With the popularity of the JTBD paradigm, there are calls in some corners to abandon personas, suggesting that JTBD has emerged as a more useful technique. This point of view is based on a fundamental misunderstanding of the purpose of personas as primarily demographic representations of users, missing the key behavioral considerations that are essential to good personas and that provide much needed guidance for interaction design and product strategy.
Jobs-to-Be-Done: A Useful Tool to Focus on Outcomes Rather than Features
The Jobs-to-Be-Done framework is a representations of user needs born out of qualitative user research, such as field studies, interviews, and discount usability testing. It involves identifying for which goals customers “hire” your product (and, ideally, also finding out if there are competitor products that these users are ready to “fire”). Armed with this understanding, a product team can think about the nature of the users’ core problems and needs from a fresh perspective, and devise product features that solve that main need as best as possible.
While the JTBD framework is new, it is similar in many ways to established methods such as task analysis and use cases, which focus on the context, goals, and steps involved in the interaction with a product. The core differentiator between JTBD and these traditional system-analysis techniques is that JTBD is much less prescriptive about what exactly the users’ task is, and how they will accomplish it. Task analysis and use cases aim to understand the best way in which the product can handle the typical activities that users need to do (and often end up being biased by existing solutions); the JTBD approach moves the focus on desired outcomes and questions whether those typical activities are the way of reaching the outcomes that users really seek.
For example, if a traditional task analysis unearthed that delivery drivers frequently needed to print out directions that showed how to navigate between each stop on their daily route, it’s likely that the design team would focus on making it as easy as possible for the drivers to format and print the directions; however, a JTBD-focused approach would focus on the delivery driver’s “job” (that is, getting navigation guidance while driving), and would look for solutions to that problem (such as a GPS system providing voice guidance).
The JTBD framework suggests that innovation and good design come from assessing the customers’ real needs, and creating a solution that is unencumbered by the existing products that fulfil those needs. However, radical innovation can be a costly and risky product-improvement strategy.
Oftentimes, we hear JTBD advocates referring to the famous Theodore Levitt quote, “People don't want to buy a quarter-inch drill, they want a quarter-inch hole.” Rather than focusing on a list of features for a product, the JTBD framework forces designers to think about outcomes: would users be able to (happily and easily) complete the job they “hired” the product for? Does this solution provide a better outcome than existing ones?
While the JTBD approach does not prescribe a specific format or deliverable, most often a job-to-be-done is defined in sentence format, noting what users have to do, and any key contextual information, such as why or where they do it. Finally, a JTBD description typically captures the functional success criteria (the objective, clear requirements for this job to be successful), as well as the emotional success criteria (which may be further broken down into the users’ individual emotional criteria, and any social considerations, such as how they imagine they’ll be perceived by others).

Jobs-to-be-done are typically summarized in a single sentence describing what the user needs to accomplish, and any important context that might impact this job (in this example, work travel for a conference, rather than vacation travel). Jobs-to-be-done also typically include some information on the objective, functional success criteria as well as the subjective emotional success criteria that cover what counts as a good experience. The emotional criteria are often broken down into two levels: personal criteria and social considerations.

Good Personas Go Beyond Demographics
Many of the arguments that suggest that personas have become irrelevant with the introduction of JTBD are based on the flawed assumption that personas are primarily demographic depictions of customers. Demographics are troublesome for product or design decisions, as they don’t provide behavioral or attitudinal data, and are mostly suited for making marketing and advertising decisions.
In fact, personas are meant to be rich representations of users, and go further than mere demographic or personal details. Most well-crafted personas include a multitude of information such as:

Demographic details, such as age, marital status, or income
Personal details, such as a short biography, photograph, and name
Attitudinal and/or cognitive details, such as information about the persona’s mental model, pain points, and feelings about the tasks that need to be accomplished
Goals and motivations for using the product
Behavioral details about how the persona tends to act when using the product

The demographic and personal details exist for two main reasons:

 to build empathy among the product team toward the user
 as mnemonic devices to aid in making them memorable to the team

Unfortunately, many personas (really, marketing segments being masqueraded as personas) don’t go any deeper than the demographic or personal level, which is why personas can often be derided as less valuable for making design decisions than jobs-to-be-done.
Well-executed personas are based largely on rich behavioral characteristics, attitudinal data, and insights about mental models, and they require qualitative research with real users to uncover the why behind users’ behaviors. These rich personas typically will include information related to specific goals that users must achieve when they use the product; these goals are directly comparable to the information found in the jobs-to-be-done definition.

Well-crafted personas include details about user goals that are similar to those in jobs-to-be-done descriptions, but are enriched with attitudinal, contextual, behavioral, and personal data that can provide a well-rounded set of considerations to guide UX designers and product teams in decision making. (We made up this persona as an example for the report on Effective Agile UX Product Development, but it’s representative for personas made in projects that employ a user-centered approach to design.)

Jobs-to-Be-Done Don’t Promote Empathy
One of the main reasons that Personas originally focused on realistic user depictions was to move away from the box-checking mentality of lists of features and requirements, and to focus on what the experience should be like for users. While JTBD do include some key considerations about the emotional and social context of a user goal, they generalize them among the entire user base, and therefore miss that key sense of context about users, and lose the opportunity to create empathy among the design team.
Personas Help with Prioritizing Among Different Users
Imagine the following scenario: you are on a design team building a new version of a popular productivity desktop application. In recent years, competitors have entered the market with innovative products, and there is a desire among your company’s leadership to redesign the application to compete with the new products in your market. While it is useful to interview your existing and potential new customers to find out which jobs-to-be-done are important to them, it’s also worth noting the differences that will be key among these groups: if you design from a clean slate, trying to solve the jobs-to-be-done in a way which suits the brand new users, you will likely severely alter the workflows of your dedicated, existing customers (and thus negatively impact their productivity, as they have to relearn the product). If you completely redesign a legacy feature (or, as is often suggested by the JTBD framework, create an entirely different, innovative solution to the problem), you might harm your existing user base.
UX practitioners have to balance design considerations among many different kinds of users, who often have competing interests. While all the purchasers of a drill have the same job-to-be-done of putting holes in things, a professional contractor will care about the durability of the tool, while someone hanging a few pictures in their home might care more about price. These two considerations are in conflict with one another, so if we attempt to address the job without differentiating (and prioritizing) between these users, we’ll probably come up with an unsatisfactory solution, no matter how innovative.
While the same job-to-be-done may have different requirements for different user groups, the reverse is also true: one user group (or persona) may “hire” the product for different jobs in different contexts. For example, I use the same flight-booking website for both work travel and vacation travel. These different types of travel are ultimately distinct JTBD with very different considerations, but, regardless of which situation I’m using the site for, I share the same mental model of how the system works, attitudinal characteristics, and behavioral traits — whether I’m flying to an NN/g UX Conference, or to Peru to hike the Inca Trail. I’m likely to have behaviors and attitudes that are similar to some other users, and quite different than other clusters of users. This is why we create multiple personas: to reflect on the key differentiating factors between our users, so we can balance needs and prioritize among personas.
Personas and Jobs-to-Be-Done Are Compatible
Despite many contrary voices suggesting that JTBD can entirely replace personas, the two are in fact, quite compatible. Depending on your organization’s needs and on whether your team already uses personas or JTBD, they can be used in a complementary fashion, or core jobs-to-be-done information can be integrated into personas.
In organizations that have already embraced JTBD, there is no need to duplicate this work in personas: each persona artifact can reference the already existing jobs-to-be-done that apply to that particular persona, along with any unique, differentiating information about that persona’s functional or emotional success criteria for that JTBD.
If your organization uses personas already, but they don’t include the rich goals and behavioral details that are key to having effective personas, start by augmenting the persona artifacts with JTBD-like information: instead of simply listing the persona’s goals, consider formatting this information as jobs-to-be-done, and ask: what is the user trying to accomplish? What are the key success considerations (both functional and emotional) for those jobs?
If there is a lot of resistance in your organization to creating personas (such as problems getting buy-in from leadership, or skeptical colleagues), but there are resources and appetite for user-centered data to influence product design, JTBD can be a useful alternative. As JTBD are a new and popular technique that has a lot of support in the business literature, there may be enthusiasm for this technique. The JTBD that emerge can always be used in combination with personas (as discussed above) if you are able to get buy-in for a personas project later on.
Summary
The usability of any given design can only be assessed relative to two variables: who are the users and what do they need to do? That’s why it’s critical for the validity of a usability study to recruit representative test users and give them representative tasks to perform. A particular design may be great for one category of users and terrible for another category of users, so if you test with the wrong users, the test results won’t tell you anything about real use. For that exact reason, we need to specify both the target audience and their goals during the design process, so that we don’t design for the wrong users or create the wrong features. Users and tasks: we need both for a UX process to be successful.
Personas are more than simple demographic or marketing segments, and they include rich information on users’ goals, needs, pain points, and expectations, while embedding this information in a narrative format that promotes empathy among the design team. Although jobs-to-be-done can provide a useful way to articulate specific user needs, this information is already represented in well-executed personas.
Learn more about Personas in our full-day seminar."
74,2017-07-09,"This article is a follow up to Tree Testing Part 1: Fast, Iterative Evaluation of Menu Labels and Categories.
 
Tree testing evaluates the categories and labels in an information architecture. We recently explained the process for designing a tree test; once you’ve planned your study, the next step is to collect data and interpret the results. Unlike think-aloud usability testing, most tree tests are run as unmoderated studies and generate only quantitative results. This method allows you to quickly collect data from a large number of users, but requires a different approach to extracting insights. You can’t just sit through a day of testing and jot down notes, but instead you need to take a systematic analysis to identify data trends and evaluate their significance.
Collecting Data
Study participants. Just like with usability testing, a good tree-test study must recruit representative users as study participants, particularly for products with specialized target audiences. Don’t recruit college students to test a website about life insurance.
Since tree testing allows you to easily collect data from a large group of users, aim for at least 50 users, to allow trends in user behavior to emerge and minimize the impact of any unmotivated participants who provide poor-quality data. If you plan to test two trees and compare their performance, you’ll need twice as many participants, because the comparison requires a between-subjects study design (i.e., different people test each version).
Tasks per participant. Ensure that each participant performs only 10 tasks (or fewer). Even though tree-testing tasks can be completed quickly, it’s still not a good idea to have people do 30 tasks in a row. Once someone has clicked through the same menu 15 times, they are in quite a different state of mind than an average user who has just landed at a website and may have never seen the menu before at all.  If you need to test more than 10 tasks, recruit more users, and use the tree-testing tool’s randomization feature to assign only 10 tasks to each participant.
Pilot test. Finally, invite a small number of users to complete the study and review their responses before sending it to your entire group. The pilot test can expose any unintended problems with your task wording early enough to correct them.
Tree-Testing Metrics 
Once the results are in, a variety of metrics capture how users understood (or misunderstood) your categories. Treejack and UserZoom, the two most common tree-testing tools, each use a slightly different style for presenting these metrics, but both provide these quantitative measures for each task in your study:
Success rate: The percentage of users who found the right category for that task
Directness: The percentage of users who went to the right category immediately, without backtracking or trying any other categories
Time spent: The average amount of time elapsed from the beginning to the end of the task
Path measures:

Selection frequencies for each category
First click: the category most people selected first
Destination: the category most people designated as their final answer

Depending on the type of tree and tasks in a study, some of these metrics may be more useful than others at predicting how well the information architecture will perform in real life.
Success Rate
In order to calculate the success rate, you must assign at least one ‘correct’ answer for each task. The success rate for that task indicates the percentage of users who found the correct location in the tree and identified it as the right place to complete that task. Any trials in which users selected a different final location are reported as failures. For example, if, when asked to find information about the New Mexico state library, 67 out of 100 participants selected the correct location, the success rate for that task is 67%.

Tree-testing tools calculate a success rate for each task for which you define a ‘correct’ answer. This screenshot from UserZoom indicates that 67% of users found the correct location for the task Where can you find directions and hours for the New Mexico State Library.

On the surface, the success rate seems simple: higher is better. But to take action based on this metric, you first need an appropriate frame of reference to determine what a ‘good’ success rate is for both the overall tree and for a specific task.
Remember that, by its very nature, tree testing eliminates many helpful design elements, such as the search function, secondary navigation options (like related links), and any context cues from the visual design or content. Users see only the stripped-down navigation menu itself.

A tree test shows participants only the task instructions and a stripped-down menu of category labels, as you can see in this task from a UserZoom tree test. Users do not have access to a search function, content, layout, dropdown menus, or any other context to help interpret the menu options.

Because tree tests are so basic, success rates are often much lower than in regular quantitative usability studies. A 67% success rate on a tree test could easily become a 90% success rate on the final design. (However, this increase would happen only if the rest of the design was well executed; a bad search implementation or poorly designed menus can also reduce success rates below levels observed in a tree test.)
Instead of expecting to achieve a 100% success rate, use a more realistic frame of reference to evaluate what success rate is acceptable for each task, taking into account:

The importance of that task to the overall user experience
How each success rate compares to other similar tasks (e.g., tasks which target content at the same level in the hierarchy)

For example, consider two tasks and their respective success rates in the table below. The success rate for the food-stamps task is much lower than for the other task, but this result is partially because users must drill down three more levels to find the right answer.



Task
Correct Answer(s)
Success Rate


Where can you
			find directions
			and hours
			for the New Mexico
			State Library?

Citizen
   >Education
      >Libraries
         >Library, New Mexico State

67%


Find the rules
			that determine
			who qualifies
			for food stamps
			in New Mexico.

Citizen
   >Health and Wellness
      >General Health and Wellness
         >Human Services Department
            >Looking for Assistance
               >Food Assistance
                  >Supplemental Nutrition Assistance Program

43%



Rather than comparing these two success rates, it would be more realistic to compare either:

The success rate for the food-stamps task to that of another task which also targets content that is 6 levels down; or
The success rate of the food stamps task performed on two different trees with different labels — one which uses the term Food Assistance and one with the term Food Stamps.

Directness and Time Spent
In addition to measuring how many users got to the right place, it’s important to also consider how much they struggled on the way. Two common tree-testing metrics signal this: time spent, which indicates how long it took users to find the right answer, and directness, which captures how many users went immediately to the right answer, without backtracking or changing categories. Direct navigation is also sometimes called the ‘happy path’ because it suggests smooth interaction, with minimal confusion or detours.
Tasks with high success rates can still be a poor user experience if users must retrace their steps and try multiple places before finally finding the right answer. For example, consider this task about finding the cost of tuition. Even though 74of users eventually found the right answer, only 50% of them took a direct path. . Half of the successful users had to retrace their steps at least once before locating it — although the information was actually available in 3 different locations in the tree.

This task result from a Treejack study indicates that even though 74% of all users were successful at finding the tuition amount, half of those people took an indirect path and had to retrace their steps at least once.

Both time spent and directness give an indication of how easy a task was for users. Directness is especially important for tasks frequently done by novices or occasional users, because they won’t have the benefit of learning and remembering locations from past experience.
Pathways: First Clicks to Final Destinations
Success rate and directness tell you whether a category is findable; detailed pathway analysis helps you figure out how improve categories that don’t work well.
The first click for a task is the category users select first when they begin that task. In tree testing, the first click is always a top-level category, because none of the subcategories are visible until a parent category is selected.
The first click is critical because it often predicts whether a user will eventually be successful in finding the right item. Imagine you are looking for the food court in a shopping mall. If the food court is on the top level and you start by taking the escalator down, your chances of finding it any time soon are slim. But if you start by going to the right level, chances are you’ll be able to wander around a bit and find it, if only by the smell of food.
The first click operates in the same way. Once users get in the general vicinity of the correct category, context cues and local navigation make it more likely that they find it. But incorrect first clicks are often disastrous; the table below show the first-click data for a task which had only a 20% success rate. The correct top-level category, Directory, received only 14% of the first clicks. Instead users started in the Program or School sections, and most ended up wandering around those areas and never making it back to the Directory.

Only 14% of users clicked Directory as their first choice when looking for a list of faculty who teach environmental law; this led to an overall task success rate of only 20% in this task result from a Treejack study.

Examine the first click data carefully when:

A task has low success rate and/or directness. The first clicks indicate where users initially expected to find that information, and suggest locations where the item should be moved (or at least crosslisted).
The final design will use mega menus that expose both the 2nd and 3rd level categories at a glance. The ability to see and compare multiple sublevels simultaneously can drastically improve success rates above what you would observe in a tree test – but this only works if the first click is successful, and users make it to the right mega menu.

If you have many tasks where first clicks are distributed across multiple categories, you may have too many overlapping categories. Do a card sort, or review the tree-test results again and look for other possible organization schemes.
Review the final destinations selected by users when the first clicks are correct, but the success rates are low. This pattern suggests that lower-level categories overlap too much.

First clicks and final destination for the task of looking up information about an arts festival; all users correctly clicked into the Recreation category, but 35% selected Arts & Culture as their final destination, while 22% chose Explore the Land of Enchantment. Only 30% correctly selected What’s Happening in New Mexico. This result indicates that these sibling subcategories overlap too much and either of them feel as appropriate destinations for the users.

Turning Data into Action
Although tree testing yields quantitative data, the conclusions are by no means black and white. Task success rates are just the first step, and must be interpreted within the context of how much users struggled to get to the right answer (directness), and where they expected the right answer to be (first clicks).
Once this analysis is complete you can identify appropriate solutions. For example:

When first clicks are evenly distributed in multiple areas, list topics in multiple categories. If this issue occurs for many tasks, consider changing the overall organization scheme.
When success rate is low but first clicks are correct, change the labels of subcategories to be more distinct.

Learn more about selecting organization schemes and labels in our full-day course on Information Architecture."
75,2017-05-07,"Overlapping information categories and confusing labels are two of the most pervasive problems in website design. Fortunately, there are fast and effective techniques you can use to create categories and labels that will make sense to your audience.
The most well-known technique is probably card sorting, in which users are given a list of representative content items to group and label as they see fit. Card sorting is invaluable for understanding how your audience thinks, but it does not necessarily produce the exact categorization scheme you should follow. For example, participants in a card sort often create a generic category to hold a few items which don’t seem to fit anywhere else; this is understandable, but if you were to actually include an “other stuff” category in your menu, the same users would avoid it like the plague. (Website visitors are notoriously reluctant to click on vague labels because they quite rightly suspect they’ll have to do a lot of work to sift through the content.)
For best results, a card sort should be followed up by a tree test to evaluate the proposed menu structure.
Definition: A tree test evaluates a hierarchical category structure, or tree, by having users find the locations in the tree where specific tasks can be completed.
Tree testing is incredibly useful as a follow-up to card sorting because it:

Evaluates a hierarchy according to how it performs in a real-world scenario, using tasks similar to a usability test; and
Can be conducted well in advance of designing page layouts or navigation menus, allowing inexpensive exploration and refinement of the menu categories and labels.

To conduct a tree test, you don’t need to sketch any wireframes or write any content. You only need to prepare two things: the tree, or hierarchical menu, and the tasks, or instructions which explain to study participants what they should attempt to find.
Defining the Tree
Your tree should be a complete list of all your main content categories, and all their subcategories. Even if you are interested in testing only a specific section of the tree, excluding the other sections is risky because it assumes that users will know which section to go to. For example, if your website had both a Products and a Services category, and you chose to test only the Products tree, you would miss out on finding whether your audience understands the difference between these two categories.
Depending on what part of the hierarchy you are most interested in, your tree may need to be 3, 4, or even 5 levels deep. Include the full depth down to the lowest level of subcategories you want to test. Each subcategory should provide the full list of all the options in that area in order to elicit realistic behavior from users. Users often evaluate link labels by comparing them with nearby alternatives. For example, users interested in history might be tempted to try a category labeled Culture — but not if there was also an option for History Resources.
Competitive Tree Testing: Labels vs. Locations
If you are considering different labels for the same tree category, you may want to test two different trees in order to compare how the terms perform.  Such a test is especially easy to do with Userzoom’s tree-testing tool, which allows you to randomly assign participants to different versions of the tree, in a manner similar to an A/B test on a live website. If you do test multiple trees, avoid showing the same user two alternative trees in the same session — users’ behavior when interacting with the second tree would be skewed by their experiences with the first one.
There’s no need to prepare and test a separate tree if you just want to compare different locations for a label — such as whether tomatoes should be placed under Fruits or Vegetables. Instead of testing two different trees for each location, you can test a single tree and compare how many users clicked Fruits vs. how many clicked Vegetables. (You’ll also be able to tell which category they tried first, if they clicked on both.)
Preparing to Test: Tools and Formatting
You could conduct a tree test using a paper prototype (or any clickable prototyping tool), but a service designed specifically for tree testing will vastly expedite the process of analyzing your results and is well worth it. Userzoom and Treejack are both good options for conducting tree testing.
Prepare your tree in a spreadsheet, where you can easily visualize and edit it, then simply copy and paste the entire hierarchy into your tree testing tool. The spreadsheet should be formatted with your homepage in the top cell of Column A, then lower levels listed out in columns from left to right. Make sure to list only one category on each row, so that your levels will be correctly parsed when you import the hierarchy.

This spreadsheet illustrates the tree, or menu hierarchy, for the New Mexico State government website. Each category appears on a separate row, and subcategories are placed in columns to the right of the parent category which contains them.

Once you have pasted your hierarchy into the testing tool, the categories are parsed and used to automatically create a clickable menu hierarchy in which each category can be expanded to show the corresponding subcategories.

A tree testing tool such as Treejack, pictured above, will automatically parse your spreadsheet hierarchy into a clickable menu with categories and subcategories.

Tree-Testing Tasks
The tasks you ask users to complete are just as important as the tree itself. First you need to decide which categories and labels to target. Ideally you should include tasks which target:

Key website goals and user tasks, such as finding your most important product (Success rates in your primary navigation tasks can serve as a baseline against which you can compare secondary tasks, and a reference point for future testing.)
Potential problem areas, such as new categories proposed by stakeholders or participants in a card sort

Label or location comparisons — any alternate labels or locations for the same category. For each task you write, you should also define the correct answer(s), corresponding to where the information is actually located within the tree. This information allows the testing tool to automatically calculate success rates for each task.

This screen from the Userzoom tree-testing system is used to indicate which category is the correct answer for a particular task.

Task Phrasing
Each task should test a category label by asking the user to find something contained within that category. As with usability-testing tasks, tree testing task instructions should avoid using terms that give away the answers. Preventing priming can sometimes be accomplished by describing a scenario and motivation, but also keep in mind that users may not read the instructions carefully, and could easily miss important details if they are buried in a lengthy story.
As an example, here are a few different possible phrasings for evaluating the Starting a Business category on the New Mexico State Government tree (depicted above):

Find information about starting a business.
You are moving to Santa Fe next year, and once you arrive you would like to supplement your income by opening a side business providing lawn-care services. Find out what regulations you will need to follow.
You are considering opening a lawn-care service. See if there are any resources on this site that can help you begin the process.

The first example gives away the answer by using the exact label term, Starting a Business; while the second is long and packed with extraneous words that a user might easily mistake for the main point of the task if they were quickly scanning. The third option avoids both the label terminology and misleading details.
Limitations of Tree Testing
Tree testing is often executed as a remote, unmoderated study. After recruiting representative users, you simply send them a link to the study, and the testing tool walks them through the process of completing the tasks using their own computer. The testing tool is much better than a human would be at keeping track of exactly which categories users click on.
However, this format does not capture the full context of user behavior (such as comments made while performing a task) and you can’t ask personalized follow-up questions.
To minimize the effects of the format, conduct at least a few moderated pilot sessions before collecting the bulk of your data. In these moderated sessions you can ensure the task wording is understandable and also get a chance to pick up on nuances that might otherwise be hard to spot in the quantitative data. For example, in a recent tree test we noticed in the pilot testing that many users avoided a certain category for the first half of their session, because the label was so broad that they feared the contents would be overwhelming. This trend wasn’t noticeable in the quantitative results due to the task order randomization, but it was quite obvious as you sat through each session and saw task after task where users ignored an obvious choice. That insight alone made the pilot testing a day well spent.
You can also partially compensate for the inability to ask follow-up questions by including a short survey after the tree test. Rather than asking users to recall any labels they found confusing, provide them with a list of labels and ask them to check which were difficult to understand. This question can be followed up with an open-ended question inviting users to share any further comments and feedback, to elicit unexpected assumptions or misunderstandings that may not be apparent from the click history.
Conclusion
Tree testing focuses exclusively on evaluating category labels. This is both its great strength and a significant weakness. Since the menu that users interact with is completely devoid of visual styling and content, the experience is significantly different than interacting with the full design. For example, a design with mega menus provides a quite different browsing experience than the one tested in a tree test, since it simultaneously displays the contents of several subcategories.
However, even these inherent limitations can often be overcome or minimized with careful data analysis — for example, by focusing on whether the user selects the correct top-level category, rather than on success rates for sites with mega menus.
Overall, these limitations are a small price to pay for the benefit of quickly being able to iterate and evaluate major structural changes to an information hierarchy early in the design process. You can create a completely new tree to test just by editing your spreadsheet — with absolutely no design or coding required."
76,2017-04-16,"The Business Context
Face it: Businesses need metrics in order to operate. When it comes to a company’s user experience, the desire to measure is just as strong, despite the risks of doing so. As a result, the use of analytics, A/B testing, surveys, and usability metrics have all grown significantly over the years. This practice is likely to persist, if not grow further, which makes it worthwhile to scrutinize the metrics we use and consider what’s missing to meet the goal of truly measuring user experience.
Broadly speaking, traditional metrics can be broken down into behavioral (what people do) or attitudinal (what people say) measures. Behavioral metrics are gathered from usage, as users perform actions on software or websites, and are commonly used in analytics and A/B testing. They include counts (users, page views, visits, downloads), rates (bounces, conversion, installation, task success), and times (time on page, time on task, engagement). Common attitudinal measures come from surveys (Net Promoter Score, System Usability Scale, customer satisfaction) or user ratings. While these are all useful, there are significant limitations:

Numbers alone don’t usually provide the insights needed to understand why an effect was observed or how to fix a problem.
The metrics used in analytics and A/B testing are typically indirect indicators of the quality of the user experience: they reflect software performance, not human experience.
Classic measures of user experience, such as those derived from usability benchmarking studies, are expensive and time-consuming, so they aren’t used frequently enough to provide regular assessment and tracking.

PURE (Pragmatic Usability Rating by Experts) is a relatively new usability-evaluation method that attempts to sidestep these problems in a way that is reasonably quick, cheap, reliable, and valid. The metrics resulted from PURE can be used frequently and comparatively, making it practical to publish metrics for each version of a product or across a set of competitors, with just a few days of effort. When used with other measures, PURE scores fill in an important gap left by the limitations of traditional metrics.
The PURE Method: Metrics of Another Kind
Attitudinal and behavioral metrics are not the only way to produce useful numbers to represent user-experience quality. Another type of metric can serve a similar purpose, but is much more practical to generate: one based on a type of expert review, resulting in a detailed rating of an experience. This is the basis for the PURE method.
Definition: PURE is a usability-evaluation method in which usability experts assign one or more quantitative ratings to a design based on a set of criteria and then combine all these ratings into a final score and easy-to-understand visual representation.
To understand PURE, consider an analogy in the movie-going experience. Movies are judged by popularity and by how they are perceived by both critics and audiences. In particular, to quantify critic appeal, the movie-review site Rotten Tomatoes features a Tomatometer, indicating the percentage of approved movie critics who have given the movie a positive review.  The site also includes an Audience Score, showing the percentage of users who liked the movie.

Rotten Tomatoes assigns each movie two metrics: the Tomatometer, reflecting critics’ perception, and the Audience Score,representing audience ratings.

PURE is like a Tomatometer for usability. It provides a score obtained by aggregating ratings (on a predefined rating scale) from a panel of experts familiar with UX principles and heuristics. Similarly, when rating a movie, critics consider elements common in all movies ­­— such as plot, acting, entertainment value, aesthetics, technical aspects, and social relevance.
One important difference, however, is that, unlike movie critics who rate movies based on their own preferences and world views, in the PURE method expert raters attempt to provide a score representing how good the experience would be for a specific, well-defined target user type. This approach increases the consistency and reliability of the ratings provided by a PURE panel reviewing the same experience and also allows the PURE scores to be legitimately used for comparison purposes.
PURE Scores: Measures of Friction
PURE is focused on just one component of user experience: ease of use. Other aspects of user experience, such as aesthetic appeal, effectiveness (meeting user needs), or resulting emotions are not addressed. But having a measure of ease of use is critical, because, if the target users aren’t able to easily use a given product or service, they cannot unlock its potential benefits. Here’s an example of PURE scores for two tasks supported by a product or service:

PURE scores for two tasks

Each task has a series of colored bars, which represent a step in that particular task. Each of those steps is rated and colored, based on how easy or difficult that step is judged to be for the target user. The rating on each step is based on a simple 1–3 scale, defined by the following scoring rubric:







The step can be accomplished easily by the target user, due to low cognitive load or because it’s a known pattern, such as the acceptance of a terms-of-service agreement.







The step requires a notable degree of cognitive load (or physical effort) by the target user, but can generally be accomplished with some effort.







The step is difficult for the target user, due to significant cognitive load or confusion; some target users would likely fail or abandon the task at this point.




The PURE score for a given task is simply the sum of the scores of all steps’ ratings in that task. The color of the task is determined by the worst rating score in the task. For example, a single step rated a red 3 causes the whole task to take on the red color.
The numbers and colors shown in PURE scores represent friction, the opposite of ease of use. The higher the number and the “hotter” the colors, the more friction there is — similar to usability-severity ratings. Comparing the PURE scorecard for the same task across different product versions or among competitors allows you to easily see the variation in friction for different designs of the task.  Although lower numbers usually mean less friction, the quality of the steps should also be considered, as indicated by their colors. One of the big benefits of PURE is that it considers overall user effort, rather than just clicks or steps. This can help counter overly simplistic arguments that fewer clicks will result in higher levels of success, and instead refocus attention on reducing user effort, rather than just clicks. (Note that you should generally avoid comparing the PURE scores of different tasks, since their nature and goals are often quite different. )
Because PURE measures the friction in a set of tasks, it is important to define the tasks to be reviewed. Pragmatically, not every task can be measured, so in PURE, we only score the “fundamental tasks” — those critical for the target user and the business. Here is a sample PURE score for a product with 7 fundamental tasks:

The PURE score for a product is the sum of the scores for each fundamental task that can be accomplished with that product.

The PURE score for the product (38 in this case) is the sum of the PURE scores for all fundamental tasks. Just like for tasks, the overall color for the product is determined by the worst color of the fundamental tasks in the product. This means that a single red step (rated 3) in any fundamental task causes that entire task and product to be colored red. The rationale for this convention is that no consumer product should have a step in which the target user is likely to fail a fundamental task. The color red has a tendency to make that statement clearly and focus attention to potential points of failure in the product.
If this sounds difficult to understand or explain, consider a simple analogy: PURE is like golf — lower numbers are better, and green is good.
The PURE Method’s Impact on Business Practices
When business stakeholders have an easy to understand, numeric representation of an important aspect of their product or service, like ease of use, they tend to be highly motivated to improve on it, and may set goals to do so. This response is the same for any metric — whether PURE scores or other traditional metrics, like the total number of users or minutes used per week. Stakeholders will want to improve these numbers as well. But, unlike these other metrics, PURE scores are operational — they show what caused poor metrics and where the user experience needs improvement, providing a clear roadmap for refining the design.  Showing PURE at regular business meetings, where product or business metrics are discussed, helps ensure that projects aimed at improving user experience are prioritized and executed.
Once the PURE method is learned, it is relatively easy to conduct a PURE evaluation and to compare PURE scores on competing products, because competitors typically have the same set of fundamental tasks and the same target audience. Business stakeholders are even more motivated to address issues identified by PURE when they see how their product stacks up against the competition and what they need to improve to win. The competitive nature of business culture becomes a significant ally in the effort to build great user experiences.
Another benefit of PURE is that you can use it on user experiences that haven’t been completely built yet. While it is more accurate when conducted on fully functioning products, PURE can be applied to medium-fidelity prototypes or to clickable wireframes — to either compare possible solutions to the same design problem or see how a proposed flow fares in terms of ease of use before committing to coding it.
How to Conduct a PURE Evaluation
Using the PURE method to score a given product or service requires certain steps to be taken, many of which are helpful for any crossfunctional product, design, and development team. There are 8 required and 2 optional steps to follow:

Clearly identify the target user type(s).
Select the fundamental tasks of this product for target users.
Indicate the happy path (or the desired path) for each fundamental task.
Determine step boundaries for each task and label them in a PURE scoresheet.
Collect PURE scores from three expert raters who walk through the happy paths of the fundamental tasks together and silently rate each step.
Calculate the interrater reliability for the raters’ independent scores to ensure reasonable agreement among experts.
Have the the expert panel discuss ratings and rationale for individual scores, and then agree on a single score for each step.
Sum the PURE scores for each fundamental task and for the entire product; color each step, task, and product appropriately.
(Optional) For each step, provide a screenshot (or photo) and a qualitative summary of the experts’ rationale for the scoring of that step.
(Optional) If comparing multiple products or product versions, prepare a comparative PURE scorecard, showing the same PURE task scores side by side.

Here is a little more detail on each of these steps.
Step 1: Target User Types
In order for the PURE experts to consistently estimate ease of use, they must have a specific user type in mind. Assumed user qualities such as technological savvy or familiarity with the current product will heavily impact the experts’ evaluations. For example, users who have forgotten their passwords may be asked to enter a one-time passcode texted to their smartphones. Those familiar with this pattern will find it relatively easy, but users never exposed to it may encounter difficulties. The target user type can be supplied by the product manager or the lead designer, or can be decided upon by the expert team. The decision and any assumptions about the target user’s context must be documented, especially if future comparative PURE scores are expected. Target user types based on personas work well, because they are usually easier to understand and already familiar to the team. However, a well-defined user description can work in practice as well, as long as behaviors and rationale for those behaviors can be understood by the expert panel. A clear target user type will help the PURE raters be consistent in their ratings, and it has the additional benefit of getting crossfunctional teams to agree on who is most important for their product or service and why.
It is possible to identify and score against multiple target user types. This doesn’t necessarily double or triple the work involved in PURE, but it will significantly increase it.  In practice, target user types should be limited to no more than 2–3, or this method loses its “pragmatic” nature.
Step 2: Fundamental Tasks
Fundamental tasks are defined as tasks that either:

are critical for the business to succeed (e.g., payment/checkout), or
allow target users to meet their core needs (assuming the product or service offers a value proposition that meets some of those needs).

For most consumer products and mobile apps, there are typically less than 10 fundamental tasks, and they form the basis on which a PURE score is generated. However, websites or complex applications may have more than 10 fundamental tasks. My recommendation is to keep the number of tasks close to 10, and no more than 20, at least when you first start using this method. It’s always possible to add more fundamental tasks, if your first attempt at PURE is found valuable (though you shouldn’t try to compare product PURE scores without explaining that new tasks were added to later analyses).
Step 3: Happy Paths
A given task can often be fulfilled in a variety of ways, with a different number of steps for each method. PURE requires the team to identify the “happy path,” which is the most desired way in which the target user would accomplish this task. This path is our best shot at making the task easy for users, so it makes sense to focus PURE scoring on this particular flow more than on any other.
It would be reasonable to evaluate multiple paths for the same task, but, just like having more than one target user type, doing so increases the time and effort required to conduct a PURE evaluation. Also, other methods, like heuristic evaluation or standard usability studies, would be sufficient to find and fix problems in other paths. I would only use PURE on multiple paths if it seemed critical to measure and compare them.
Lastly, some teams have chosen to use PURE to evaluate the “popular path” by looking at clickstream analytics to determine which flow is most likely for a given goal. This is a reasonable decision, and it may take the place of a happy path for some teams.
Step 4: Step Boundaries
Once the happy paths are determined, it is critical to go through them and identify where each step begins and ends. Depending on the type of interaction provided by the product or service, this process can be harder than you might think. The place to start is the “default step” definition:

A step begins when a system presents the user with a set of options (e.g., a user interface is rendered).
A step ends when the user takes an action and expects significant system response to that action.
A step may contain microinteractions, such as manipulating form fields; these are considered part of the step.

This definition works for a majority of tasks on screen-based interfaces, but may require some refinement for certain situations. That’s OK, as long as you document why you deviated from the default definition of the step, so you can repeat this decision in later PURE analyses. Also, there can sometimes be debate about what “expected significant system response” is. For example, a web page may hide and show content as users interact with certain elements, which can be unexpected. Or, when a long page contains several sections, it is tempting to call each section a step. However, keep in mind that changing the page sectioning may affect future PURE scores, so be clear on the implications of choosing those step boundaries. The lead researcher should make the decision and document it for future PURE-scoring consistency.
Step 5: Review by Three Expert Raters 
The PURE method uses three usability experts (ideally UX researchers) to provide the initial PURE scores. It is important that the designers or other product professionals responsible for the rated flow NOT be on the panel of experts, because, in practice, it is very hard to be objective about one’s own designs.
The panel assembles (either in the same room, or remotely) and all members watch as the lead researcher goes through each task on a common screen and declares when each step begins and ends. Each panel member silently rates and reviews each step, making notes about the rationale for the rating. The notes can include observed usability problems, as in heuristic evaluations.
One big difference from heuristic evaluation is that, in PURE, the panel sees the same experience together, which ensures that they rate the same thing — otherwise their scores would be wildly different. This point highlights an important difference in goals between heuristic evaluation and PURE. In heuristic evaluation, the goal is to find as many usability issues as possible and get a complete view of the usability of the product or service. In contrast, PURE aims to provide a reliable measure of how easy it is for the most important user type to accomplish only the fundamental tasks, through the best design offered at this point. The analysis starts here, because these are most important areas to get right. Once improved, the PURE method can be used elsewhere, although it may not be necessary to provide a numerical score for all paths, tasks, and user types, once buy-in to address ease of use is achieved in general.
Experts should be able to enter their PURE scores without being exposed to other experts’ scores. This is easily accomplished with the use of an online spreadsheet, with tabs for each rater. Task names are propagated to each tab, and all scores can be automatically rendered onto a master tab for review in later steps.
Step 6: Interrater Reliability Calculations
To see how much the experts agreed with each other in their individual PURE scores, which were provided silently, you should calculate the “interrater reliability” (IRR). IRR is a measure of how much the raters agree, given their understanding of the target user type and the 1–3 rubric. While this calculation may seem overly academic, it does ensure that there is a reasonable level of agreement among experts, and is important for methodological soundness. Reviewing this number will help the experts understand whether they made the same assumptions as they rated products in PURE.
There is more than one way to calculate IRR. I recommend using Krippendorff’s alpha. To compute it, you can use a free online calculator such as ReCal (select “ordinal” data type, since the 1–3 rubric is ordinal).
IRR ranges from -1 to 1, but is typically between 0.5 and 1.0. If the experts are not able to achieve an IRR of at least 0.667, they should discuss why they varied so much and simply consider this PURE evaluation to be a training session. It typically takes 2–3 rounds of trying PURE before a panel of experts has sufficiently understood the rubric and the user type to be consistent, so plan for a few rounds of trial and error for learning purposes.
Step 7: The Decided PURE Score
After the experts have recorded their individual ratings, they should walk through the steps of each task and discuss them together.  This discussion is invaluable for two main reasons: (1) expert raters will learn from their colleagues, and, over time, will become better and more consistent raters; and (2) the expert panel will be able to decide on a single score for each step. This “decided score” will be the reported PURE score for this step, and it will benefit from the collective wisdom of the entire panel of expert raters.
The decided score is easiest to determine when all raters gave the same individual score. If 2 out of 3 agreed on a score, it usually becomes the decided score, but not always. Sometimes the ensuring discussion may cause the team to choose the less popular score as the decided score. This situation most often happens when a specific assumption or key insight about the experience was missed by 2 raters, but is explained by the other rater.
Very infrequently, all three raters will have three different scores. This is almost always due to a different understanding of the method or set of assumptions, which the discussion will no doubt clarify. As with other aspects of PURE, the assumptions decided upon should be documented for future review and PURE scoring.
The PURE method does not use the average rating from the three expert reviewers for some good reasons — some pertain to the ordinal nature of the 1–3 rating scale, but, most importantly, an average would take away from the power of the decided PURE score, which represents the collective wisdom of three expert reviewers, rather than their average, undiscussed assessment.
Step 8: Summing It Up into Green, Yellow, and Red
The next step is simple and gratifying: summing up the decided PURE step scores into the task PURE scores, and then summing the task PURE scores into the product PURE score. Because these numbers are not normalized, they can be as large as the tasks and their complexity are. There is almost always room for improvement, once these numbers are summed and shown.
Just as important is the visual representation of the PURE scores. Using bar heights and colors to represent friction has shown to be very effective at conveying trouble spots in a given product. Red is particularly troublesome when you consider that PURE is focused only on the most important user types, and their potential experience on the most important tasks with the company’s best shot at accomplishing these tasks.
If many steps are rated a yellow 2, it means the target user had to spend some degree of effort to get through them. This may be OK or even unavoidable, depending on the nature of the task, but it will be important for the team to really consider what it can do to improve the ease of use. Even steps that are rated a green 1 can have areas of improvement. It is wise to ensure that the notes from the expert team are collated and used in the next part of the PURE method.
Step 9 (optional): It’s Not All Quantitative
As raters go through each step of each task from the perspective of the target user, they should capture their rationale and notes for their scores and call out areas that could be improved in the user experience. These observations describe exactly what could be addressed to improve the PURE score, and ostensibly, the usability of the product. The expert panel should collect a screenshot or photo of each step, and document these observations into the appendix of the PURE report to help design and development teams know why and how to make improvements. Ideally, PURE scores are generated by a panel of usability experts who have seen the product perform in qualitative usability studies. It is not a requirement to have witnessed the specific product in other studies, but, at a minimum, PURE reviewers should have a deep understanding of user experience and usability, and be well versed in design principles and general heuristics.
An expert reviewer who has been exposed to user studies (such as standard usability testing) on the same product will be able to apply insights from such studies to the PURE evaluation of the product.  Used together, qualitative usability studies and a separate PURE analysis of the same or similar tasks can complement each other and provide in-depth information about the main usability hurdles in a design. This combination of methods can be cost-effective and time-saving, especially compared to traditional quantitative approaches, such as usability benchmarking.
Step 10 (optional): Comparing PURE Scores
One of the most gratifying aspects of the PURE method is comparing scores of the same task among product versions or competitive products, especially when improvement on one’s own product is demonstrated. Below is an example of one of the first PURE scorecards to be conducted, on an actual product that went to market, after showing drastic improvements in ease of use over 5 months. The task names were genericized for confidentiality reasons, but you can see that big improvements were realized through redesign iterations by simplifying some of the task flows (cutting steps) and also improving ease of use for individual steps.

Scores from three PURE evaluations on the same product indicate significant UX improvements in the new versions of the product.

Is PURE Valid and Reliable?
While the metrics defined and described here are not as precise as empirical measures based on user data, they are directionally accurate, and have been shown to have reasonable validity and reliability scores. When comparing PURE results with metrics obtained from running a usability-benchmarking study on the same product, we found statistically significant correlations with SEQ and SUS (popular ease-of-use survey measures) of 0.5 (p <0.05) and 0.4 (p < 0.01), respectively. These numbers show that PURE has at least reasonable validity, when compared with standard quantitative metrics, at statistically significant levels (p< 0.05). Interrater reliability calculations for PURE have ranged from 0.5 to 0.9, and are generally very high (above 0.8), after expert raters are trained on the method. The PURE method was first documented in a case study that I published at CHI 2016 together with my coauthors James Wendt, Jeff Sauro, Frederick Boyle, and Sara Cole.
In a recent PURE evaluation at Capital One, three experts achieved an interrater reliability score of 1.0 (100% agreement) across 9 fundamental tasks. As of this writing, PURE is known to have been used with over 15 different products at 3 companies. I expect to see this number grow, as the practice becomes better understood and is improved by new adopters.
Conclusion
While learning to conduct the PURE method takes some effort and not everyone is qualified to do it, experience has shown it to be an extremely valuable tool to use along with the landscape of user-research methods. PURE scores capitalize on the ever present appetite for quantitative metrics and provide concrete numbers that orient the organization toward fixing ease-of-use barriers. In the end, everyone benefits: users, employees, and business stakeholders. Sometimes all it takes is showing the right metrics in an easy to understand format, with just enough frequency to effect significant positive change."
77,2017-04-09,"Qualitative usability studies are dependent on a few key pieces: a design to test, a participant to test it, and (often) a moderator to run the session. The other essential element: the tasks.
A task needs to accurately and adequately reflect the researcher’s goal, as well as provide clear instructions about what participants need to do. Good tasks are essential to having a usability study that results in accurate and actionable findings.
Writing tasks for a usability test is not easy. As any experienced usability researcher can tell you, how the task is written directly impacts the success of the study. If you give study participants bad instructions, you can bias them and completely change the outcome of the study. At best, you won’t learn that much, and the study won’t reflect real-world use very well. At worst, your “findings” will be directly misleading and cause you to make the product worse, rather than better.
After you’ve written your tasks, take another pass through them, looking for common mistakes that can impact the value or depth of your findings, or the well-being of your participant.
1. Telling Users Where to Go
Do words from the interface appear in your task? If so, you’re priming your participants and testing their reading comprehension and ability to find matching words, rather than your labels and navigation. Rewrite the task to remove any words that appear in your interface, so you give yourself a fair chance to see if users can find their way around the site.
Task goal: Use the location finder tool (labeled Find a Branch)
Leading user task: Find a branch near you and see when it is open tomorrow.
Improvement: When is the bank location that’s most convenient to you open tomorrow?
2. Telling Users What to Do
As part of a task, users may need to go through several steps, such as registering for the site, installing software, or downloading a document. Take the opportunity to gather more about the process by not warning study participants about what they will need to do. When your task includes prompts to register, install, or download, you may miss out on the valuable feedback that users might offer when encountering that step in the process. For example, participants may be surprised or annoyed by an additional or unexpected step.
Task goal: Find the price for consulting services
Overly-structured task: Locate information about consulting services, provide details about yourself and your company, and set up a time to talk to a consultant about pricing.
Improvement: Find out how much a consulting project costs.
3. Creating Out-of-Date Tasks
Often, we write tasks only a few days before the usability study is scheduled. Even so, consider the timeliness of your tasks. If your task includes a future event, make sure that event is going to still be in the future during testing. If the task is to find a flight leaving February 20, don’t run that test on February 22. A task about the latest news on a site should be updated the day before or the day of testing to include current content. Be cautious if tasks include information that is typically relevant or updated only in specific months or seasons. Users may think the site has out-of-date information or that the tasks aren’t realistic.
Task goal: Find sports teams’ scores (Assume testing is taking place in February. In the United States, baseball is played from April to October. Hockey is played October through April.)
Outdated task: Find out how the Cubs did in their last baseball game.
Improvement: Find out how the Blackhawks did in their last hockey game.
4. Making Tasks Too Simple
If you want to know if people can effectively use charts, graphs, or information on the site, don’t just test if they can navigate to it. Create tasks that make study participants work a bit for the information. Your goal isn’t to make tasks unnecessarily complex, but to give users a realistic task that requires processing, rather than just locating information.
Task goal: Find and use player statistics (Points Per Game is the first item listed in player statistics, sorted from highest to lowest)
Too-easy task: Who scored the most points, averaged across games, in the league?
Improvement: Who scored more points, averaged across games, during the season: Russell Westbrook or LeBron James?
5. Creating an Elaborate Scenario
Some tasks may benefit from a small scenario to give the activity some context. A short description may help study participants understand the reason for such a task or clarify the exact information you would like them to find. You may suggest a genre of music that the participant should investigate, a reason for looking for particular information, or provide a name and address for a purchase. For instance, you may include a detail such as when a gift recipient’s birthday is, to see if users can find a shipping option that will ensure a gift is delivered on time.
Scenarios can be helpful, but be cautious when using them. They are not always necessary. They may add complexity to a task that could be straightforward. They can increase the number of details users must read through and remember. Sometimes such scenarios are used to justify an unusual or abnormal activity. If it takes a long story to explain why a user would want to do an activity, it’s likely not a realistic task to test.
Task goal: Find and use nutritional guidance information
Unnecessary backstory: You are helping babysit your friend’s 3-year-old boy for a week and want to know more about healthy diets for kids. Find out how much grain should be in his diet.
Improvement: Find out how much grain should be in a 3-year-old’s diet.
6. Writing an Ad, not a Task
Don’t let marketing language or internal lingo sneak into tasks. Make sure your tasks don’t include marketing phrases like “exciting new feature,” business phrases like “thinking outside the box,” or mysterious corporate acronyms. Use user-centric language, not maker-centric language. For specialized audiences, it may make sense to use technical terms or audience-specific language, but that is the exception, rather than the rule.
Task goal: Use the new social sharing feature
Promotional wording: Check out the exciting new feature that lets you quickly and easily share articles with colleagues.
Improvement: Send an article to a colleague.
7. Risking an Emotional Reaction
While writing a task that revolves around someone’s mother may seem harmless, you never know the specific circumstances of your study participants. Mentioning a specific relationship in a task may add unnecessary emotion to the user test. What if the participant has a difficult relationship with the person you’re referencing, or that person has passed away? Don’t risk upsetting a user and derailing a task or even an entire session. Part of the responsibility of running a usability test is to ensure the well-being of your participants. Stick to harmless and vague relationships instead – friend, colleague, a friend’s child.
Task Goal: See how participants shop for gifts.
Potentially upsetting task: Mother’s Day is coming up. Find a bouquet to send your mother.
Improvement: Send your friend flowers to celebrate her new job.
8. Trying to Be Funny
Don’t joke, use famous names in tasks, or otherwise try to lighten the mood. Doing so can backfire and make some participants feel awkward or, even worse, as though you are making fun of them. Even using gender-neutral names, such as telling the user to register as Kelly or Jesse, can be a distraction from the task.
Task Goal: Identify problems in the gift subscription checkout flow.
Distracting joke in task: Send a subscription to your friend for her birthday. Her name is Ima Customer and she lives at 826 Main Street in Tempe, Arizona, 85280.
Improvement: Send a subscription to your friend for her birthday. Her name is Jen Smith and she lives at 826 Main Street in Tempe, Arizona, 85280.
9. Offending the Participant
Avoid potentially offensive details in tasks. Societal issues, politics, health, religion, age, and money all have the possibility of offending a participant.
Task goal: Find and use information about exercise and calories.
Potentially offensive task: You need to lose a few pounds. See what types of exercise will help you lose the extra weight.
Improvement: See what types of exercise burn the most calories.
10. Asking Rather than Telling
While you want to be polite to your participants, don’t overdo it. Don’t ask participants “how would you” complete a task — unless you want them to talk you through what they theoretically would do on a site, rather than doing it. The point of usability testing is to see what users do, not to hear what they would do.
Task goal: Find the symptoms of the flu
Instructing the user to talk instead of perform: How would you find the symptoms of the flu?
Improvement: Find out what the symptoms of the flu are.
Tip: Start with the End Goal
Sometimes, with all the things you need to avoid in a task, it can feel like you’re writing a riddle that the participant needs to solve. It can be hard to avoid navigational labels, steps, stories, or marketing language in your tasks. This is why task writing is more of an art than a science.
If you find yourself struggling to write a task, consider the user’s end goal rather than the task’s end goal. Rather than focusing on the section or the feature you want to test, consider why people would use that section or feature. What would they ultimately try to accomplish?
To test your checkout process, give the user a task to buy something. To review your newsletter subscription process, ask the user to sign up to receive information via email. To see if a user can understand content, write a task with a question about the information contained in the content. Starting with the users’ end goal helps streamline task writing, and reviewing these common mistakes can finetune the tasks.
Learn more about task-writing in our Usability Testing course."
78,2017-02-26,"User research offers valuable opportunities for VIPs, product owners, managers, customer support representatives, marketers, designers, developers, and other team members to learn how users interact with products and services firsthand. Without users, you can’t properly be said to do “user” experience work: as we’ve said before UX – U = X (with the second “X” meaning, “don’t do it.”). With users, almost everything in a UX project improves, but only if the remaining stakeholders (besides the researchers themselves) know what was done.
Having an audience for your research is a wonderful opportunity for UX practitioners to gain allies and get buy-in for problems that need to be solved. Stakeholder participation in both research and design is best, because you can show the value of UX activities and get the whole team focused on the people you are trying to serve. Collaboration doesn’t reduce the need for UX expertise and guidance. Instead, involving stakeholders helps everyone appreciate the user-experience effort.
Collaborating on the User Experience Benefits Everyone
UX-research participation can remove obstacles to doing future research. It can build organizational support for UX activities by demonstrating their value. It can also show how user testing in the early stages of design can prevent integration and maintenance problems more efficiently. And when everyone understands research activities and results through direct participation, less time is needed to explain methods and results.
Participation in design and research projects can motivate stakeholders to fix the user interface or to modify the requirements to align better with customer and user needs. It will also encourage empathy for users, create group ownership for usability outcomes, and solve problems faster. You don’t need to argue when you can test. (In fact this one advantage can sometimes provide full cost-justification for a round of user testing: the cost of recruiting 5 users and having a UX researcher spend a day or two testing the main design options can be less than the cost of a big team spending endless meetings arguing over what to do.)
Teams create better design patterns and make better decisions when team members are immersed in user behavior and typical usability difficulties. User-interface designs improve when everyone understands how to prevent problems in the future.
Motivating Stakeholders to Participate in Research and Design
User research reduces the likelihood of building something that doesn’t meet user needs, but only when everyone knows what those are. Here are some ways to increase motivation to participate in user research:

Evangelize UX research. Show how various UX-research methods can be used to reduce risk and align products with user expectations. Dispel the notion that you’re the graphic-design syrup on the top of the product stack or a time-consuming form of quality assurance. Point out that you’ll save work (and rework) for developers by helping as early as possible with research, design, and validation. Make sure everyone knows how deep user experience design needs to be and what you can accomplish. Explain UX concepts, terms, and capabilities by having UX practitioners discuss their skills, typical methods, and outcomes.
Make time and space for collaboration. Schedule regular UX office hours and design clinics. Keep the chat channel open with your team so you can listen and share information. Invite everyone (regardless of their role) to use you as a resource for user-research insights and design solutions. Create a physical space for team members to interact with UX R&D staff and information.
Educate everyone about the benefits of participation in UX research. Teach upper management about the benefits of stakeholder participation. Find a champion who can encourage other people with her own participation stories.
Align your research goals with your stakeholders’ goals. Run strategy meetings with stakeholders to understand their goals and establish the best ways to measure success. Then set a design vision and UX strategy before development begins, so everyone understands how to ensure good usability for users. Later on, at the end of the project, give credit for successes to stakeholders. They are more likely to help projects that make them look good.
Demonstrate utility with small pilot projects. Start with small, easy, yet vital opportunities, such as soliciting advice and concerns while sharing user insights and data.
Make participation easy. Schedule key UX activities (such as customer-site visits, usability testing, and participatory design sessions) when stakeholders are available, and plan for that time in the development cycle. Personally invite each person and emphasize their importance to obtaining good outcomes.
Create accessible user-testing deliverables that reflect your audience’s interests and technical understanding. Share UX research, process, and prototypes widely, in person if you can. Brown-bag lunches are a good way to build interest. Tell compelling stories, communicate risks, and show visual information everyone can understand quickly. Share data and insights as soon as you can with your immediate team, using a lightweight system, such as bullet lists and screenshots in a shared document. Annotate higher-fidelity prototypes and designs rather than writing a separate spec. Show interactions rather than talking about them. Summarize key insights and implications for larger audiences. Track UX issues and metrics in a visible location.
Make UX deliverables useful for other processes within the organization. Create design patterns for reuse and make living style guides for issues that recur. Make usability test cases for QA that adhere to the main design principles. Use storymapping and project-management tools to integrate user tasks and UX tasks into developer tools, stories, and tasks. Generate needed user stories for the product backlog.
Increase the visibility for UX-research and design artifacts, such as personas that change as insights deepen, user-journey maps, hi-fi prototypes, and user-task flows. Post these artifacts on walls in public locations. For remote team members, build a project room in a shared environment to display digital UX artifacts and data.

Getting Started with UX Collaboration
Involving stakeholders in gathering data is a good way to begin. It’s best to make the whole research effort into a team activity, however. Ideally everyone should be involved in planning the study, writing scenarios, creating participant profiles and screening questions, and in reviewing potential recruits for participation.
Start with a short study, such as two days of user sessions. Invite stakeholders to attend any of the sessions they can make time for. Encourage them to attend at least two sessions, so they will be less likely to take a single session at face value. Explain your research plan and method, to set expectations correctly. To prepare stakeholders for participating in user-research sessions, give them observer guidelines and explain how to take the most effective notes. After the study, circulate the top findings widely and publicly thank your collaborators for their valuable input.
Besides user testing, other UX group activities such as design-thinking exercises can create team ownership of the design vision and concept, through generating designs and critiquing them together. Schedule regular design reviews and group walkthroughs. Point out quality nuances that make a big difference, such as microinteractions, streamlined forms, and helpful messages. Show competitor products that do a better job at task flow, fit, and finish.
Conclusion
User experience outcomes improve when UX is a team activity. User research should be an essential part of design and testing processes in order to refine designs before development and to help everyone understand what needs more work. Collaborating in user research provides many points of view and motivates everyone to make products and services more usable.
Want to learn more about creating a great user experience through collaboration? Join us at the Nielsen Norman Group’s full-day Lean UX and Agile course or Engaging Stakeholders to Build Buy-In for more in-depth recommendations."
79,2017-02-12,"User-experience research methods are great at producing data and insights, while ongoing activities help get the right things done. Alongside R&D, ongoing UX activities can make everyone’s efforts more effective and valuable. At every stage in the design process, different UX methods can keep product-development efforts on the right track, in agreement with true user needs and not imaginary ones.
One of the questions we get the most is, “When should I do user research on my project?” There are three different answers:

Do user research at whatever stage you’re in right now. The earlier the research, the more impact the findings will have on your product, and by definition, the earliest you can do something on your current project (absent a time machine) is today.
Do user research at all the stages. As we show below, there’s something useful to learn in every single stage of any reasonable project plan, and each research step will increase the value of your product by more than the cost of the research.
Do most user research early in the project (when it’ll have the most impact), but conserve some budget for a smaller amount of supplementary research later in the project. This advice applies in the common case that you can’t get budget for all the research steps that would be useful.

The chart below describes UX methods and activities available in various project stages.

The diagram lists potential UX research methods and activities that can be done as projects move through stages of design. Think of this as a menu of recommended options. Your process will vary and may include only a few things on this list during each cycle. The most-frequently used methods are shown in bold. (Graphic by Sarah Gibbons.)

Each project is different, so the stages are not always neatly compartmentalized. The end of one cycle is the beginning of the next.
The important thing is not to execute a giant list of activities in rigid order, but to start somewhere and learn more and more as you go along.



Top UX Research Methods




Discover
• Field study
			• Diary study
			• User interview
			• Stakeholder interview
			• Requirements & constraints gathering


Explore
• Competitive analysis
			• Design review
			• Persona building
			• Task analysis
			• Journey mapping
			• Prototype feedback & testing (clickable or paper prototypes)
			• Write user stories
			• Card sorting


Test
• Qualitative usability testing (in-person or remote)
			• Benchmark testing
			• Accessibility evaluation


Listen
• Survey
			• Analytics review
			• Search-log analysis
			• Usability-bug review
			• Frequently-asked-questions (FAQ) review



When deciding where to start or what to focus on first, use some of these top UX methods. Some methods may be more appropriate than others, depending on time constraints, system maturity, type of product or service, and the current top concerns. It’s a good idea to use different or alternating methods each product cycle because they are aimed at different goals and types of insight. The chart below shows how often UX practitioners reported engaging in these methods in our survey on UX careers.

The most-frequent methods used by UX professionals, from our free UX Careers survey report. Percentages refer to the proportion of respondents who said they use each method at least every year or two.

If you can do only one activity and aim to improve an existing system, do qualitative (think-aloud) usability testing, which is the most effective method to improve usability. If you are unable to test with users, analyze as much user data as you can. Data (obtained, for instance, from call logs, searches, or analytics) is not a great substitute for people, however, because data usually tells you what, but you often need to know why. So use the questions your data brings up to continue to push for usability testing.
Discover
The discovery stage is when you try to illuminate what you don’t know and better understand what people need. It’s especially important to do discovery activities before making a new product or feature, so you can find out whether it makes sense to do the project at all.
An important goal at this stage is to validate and discard assumptions, and then bring the data and insights to the team. Ideally this research should be done before effort is wasted on building the wrong things or on building things for the wrong people, but it can also be used to get back on track when you’re working with an existing product or service.
Good things to do during discovery:

Conduct field studies and interview users: Go where the users are, watch, ask, and listen. Observe people in context interacting with the system or solving the problems you’re trying to provide solutions for.
Run diary studies to understand your users’ information needs and behaviors.
Interview stakeholders to gather and understand business requirements and constraints.
Interview sales, support, and training staff. What are the most frequent problems and questions they hear from users? What are the worst problems people have? What makes people angry?
Listen to sales and support calls. What do people ask about? What do they have problems understanding? How do the sales and support staff explain and help? What is the vocabulary mismatch between users and staff?
Do competitive testing. Find the strengths and weaknesses in your competitors’ products. Discover what users like best.

Explore
Exploration methods are for understanding the problem space and design scope and addressing user needs appropriately.

Compare features against competitors.
Do design reviews.
Use research to build user personas and write user stories.
Analyze user tasks to find ways to save people time and effort.
Show stakeholders the user journey and where the risky areas are for losing customers along the way. Decide together what an ideal user journey would look like.
Explore design possibilities by imagining many different approaches, brainstorming, and testing the best ideas in order to identify best-of-breed design components to retain.
Obtain feedback on early-stage task flows by walking through designs with stakeholders and subject-matter experts. Ask for written reactions and questions (silent brainstorming), to avoid groupthink and to enable people who might not speak up in a group to tell you what concerns them.
Iterate designs by testing paper prototypes with target users, and then test interactive prototypes by watching people use them. Don’t gather opinions. Instead, note how well designs work to help people complete tasks and avoid errors. Let people show you where the problem areas are, then redesign and test again.
Use card sorting to find out how people group your information, to help inform your navigation and information organization scheme.

Test
Testing and validation methods are for checking designs during development and beyond, to make sure systems work well for the people who use them.

Do qualitative usability testing. Test early and often with a diverse range of people, alone and in groups. Conduct an accessibility evaluation to ensure universal access.
Ask people to self-report their interactions and any interesting incidents while using the system over time, for example with diary studies.
Audit training classes and note the topics, questions people ask, and answers given. Test instructions and help systems.
Talk with user groups.
Staff social-media accounts and talk with users online. Monitor social media for kudos and complaints.
Analyze user-forum posts. User forums are sources for important questions to address and answers that solve problems. Bring that learning back to the design and development team.
Do benchmark testing: If you’re planning a major redesign or measuring improvement, test to determine time on task, task completion, and error rates of your current system, so you can gauge progress over time.

Listen
Listen throughout the research and design cycle to help understand existing problems and to look for new issues. Analyze gathered data and monitor incoming information for patterns and trends.

Survey customers and prospective users.
Monitor analytics and metrics to discover trends and anomalies and to gauge your progress.
Analyze search queries: What do people look for and what do they call it? Search logs are often overlooked, but they contain important information.
Make it easy to send in comments, bug reports, and questions. Analyze incoming feedback channels periodically for top usability issues and trouble areas. Look for clues about what people can’t find, their misunderstandings, and any unintended effects.
Collect frequently asked questions and try to solve the problems they represent.
Run booths at conferences that your customers and users attend so that they can volunteer information and talk with you directly.
Give talks and demos: capture questions and concerns.

Activities
Discover
Ongoing and strategic activities can help you get ahead of problems and make systemic improvements.

Find allies. It takes a coordinated effort to achieve design improvement. You’ll need collaborators and champions.
Talk with experts. Learn from others’ successes and mistakes. Get advice from people with more experience.
Follow ethical guidelines. The UXPA Code of Professional Conduct is a good starting point.
Involve stakeholders. Don’t just ask for opinions; get people onboard and contributing, even in small ways. Share your findings, invite them to observe and take notes during research sessions.
Hunt for data sources. Be a UX detective. Who has the information you need, and how can you gather it?
Determine UX metrics. Find ways to measure how well the system is working for its users.

Explore

Follow Tog's principles of interaction design.
Use evidence-based design guidelines, especially when you can’t conduct your own research. Usability heuristics are high-level principles to follow.
Design for universal access. Accessibility can’t be tacked onto the end or tested in during QA. Access is becoming a legal imperative, and expert help is available. Accessibility improvements make systems easier for everyone.
Give users control. Provide the controls people need. Choice but not infinite choice.
Prevent errors. Whenever an error occurs, consider how it might be eliminated through design change. What may appear to be user errors are often system-design faults. Prevent errors by understanding how they occur and design to lessen their impact.
Improve error messages. For remaining errors, don’t just report system state. Say what happened from a user standpoint and explain what to do in terms that are easy for users to understand.
Provide helpful defaults. Be prescriptive with the default settings, because many people expect you to make the hard choices for them. Allow users to change the ones they might need or want to change.
Check for inconsistencies. Work-alike is important for learnability. People tend to interpret differences as meaningful, so make use of that in your design intentionally rather than introducing arbitrary differences. Adhere to the principle of least astonishment. Meet expectations instead.
Map features to needs. User research can be tied to features to show where requirements come from. Such a mapping can help preserve design rationale for the next round or the next team.
When designing software, ensure that installation and updating is easy. Make installation quick and unobtrusive. Allow people to control updating if they want to.
When designing devices, plan for repair and recycling. Sustainability and reuse are more important than ever. Design for conservation.
Avoid waste. Reduce and eliminate nonessential packaging and disposable parts. Avoid wasting people’s time, also. Streamline.
Consider system usability in different cultural contexts. You are not your user. Plan how to ensure that your systems work for people in other countries. Translation is only part of the challenge.
Look for perverse incentives. Perverse incentives lead to negative unintended consequences. How can people game the system or exploit it? How might you be able to address that? Consider how a malicious user might use the system in unintended ways or to harm others.
Consider social implications. How will the system be used in groups of people, by groups of people, or against groups of people? Which problems could emerge from that group activity?

Test

Protect personal information. Personal information is like money. You can spend it unwisely only once. Many want to rob the bank. Plan how to keep personal information secure over time. Avoid collecting information that isn’t required, and destroy older data routinely.
Keep data safe. Limit access to both research data and the data entrusted to the company by customers. Advocate for encryption of data at rest and secure transport. A data breach is a terrible user experience.
Deliver both good and bad news. It’s human nature to be reluctant to tell people what they don’t want to hear, but it’s essential that UX raise the tough issues. The future of the product, or even the company, may depend on decisionmakers knowing what you know or suspect.
Track usability over time. Use indicators such as number and types of support issues, error rates and task completion in usability testing, and customer satisfaction ratings, to show the effectiveness of design improvements.
Include diverse users. People can be very different culturally and physically. They also have a range of abilities and language skills. Personas are not enough to prevent serious problems, so be sure your testing includes as wide a variety of people as you can.
Track usability bugs. If usability bugs don’t have a place in the bug database, start your own database to track important issues.

Listen

Pay attention to user sentiment. Social media is a great place for monitoring user problems, successes, frustrations, and word-of-mouth advertising. When competitors emerge, social media posts may be the first indication.
Reduce the need for training. Training is often a workaround for difficult user interfaces, and it’s expensive. Use training and help topics to look for areas ripe for design changes.
Communicate future directions. Customers and users depend on what they are able to do and what they know how to do with the products and services they use. Change can be good, even when disruptive, but surprise changes are often poorly received because they can break things that people are already doing. Whenever possible, ask, tell, test with, and listen to the customers and users you have. Consult with them rather than just announcing changes. Discuss major changes early, so what you hear can help you do a better job, and what they hear can help them prepare for the changes needed.
Recruit people for future research and testing. Actively encourage people to join your pool of volunteer testers. Offer incentives for participation and make signing up easy to do via your website, your newsletter, and other points of contact.

Conclusion
Use this cheat-sheet to choose appropriate UX methods and activities for your projects and to get the most out of those efforts. It’s not necessary to do everything on every project, but it’s often helpful to use a mix of methods and tend to some ongoing needs during each iteration.
Related article: When to Use Which User-Experience Research Methods"
80,2017-01-22,"Design deliverables should communicate content quickly, succinctly, and directly. But UX professionals often produce dense, complex, and inscrutable documents that don't convey our intent, or are so laden with jargon and insider UX terminology that they go over the heads of our colleagues and stakeholders.
(It’s ironic that UX professionals focus so much on understanding their products’ end users, but don't extend the same sort of user centeredness to coworkers who are the target audience for their deliverables.)
We can, however, use a simple method called the sketch test to learn about the effectiveness and understandability of deliverables, documents, reports, and visualizations. The idea is simple and resembles the telephone game: you give your deliverable to a colleague and ask her to create a short sketch or summary of your document.  Observe what she sketches and writes. Then identify confusing or unclear elements, and iterate on the deliverable.
How to Perform a Sketch Test
The procedure is simple:
1. Print a copy of the deliverable (such as a wireframe, usability testing report, wireflow, persona, journey map). For deliverables that are inherently interactive (such as a high-fidelity prototype), consider representing them with a series of screenshots.
2. Recruit a sketch-test participant from your target audience. For example, if you're preparing a wireframe for developers, recruit a front-end developer. If you'll be sharing a usability-testing report with a product owner, find a product owner or project manager. Note that while it is only rarely appropriate to recruit usability testing participants among your coworkers, for the sketch test it is perfectly fine, since they are the actual end users of your deliverables.
Your colleagues' time is valuable, so be sure to compensate them with a small, but attractive incentive. Since a sketch test takes about 15 minutes or less, a nice cup of coffee at a local coffeehouse, lunch, or another small thank-you gift can often suffice.
3. Give the sketch-test participant the printed copy of the deliverable. Place a blank pad of paper and a pen or pencil right next to the person.
4. Invite the participant to write directly on the deliverable, but also make scratch paper available. Explain that the printed version of the deliverable is just a scratch copy, and that you welcome any comments.
5. Ask the participant to explain the concepts in the deliverable. But first:

Explain that the current version of the deliverable is a work in progress, and that you are still developing the format of the document. While content suggestions can be welcome, at this point do not ask for them explicitly — keep the focus on making the deliverable clear and direct.
Note that you want to make this document easier to understand.
Give the participant time to read the deliverable. Ask your participant to use the think-aloud protocol during the initial reading of the document.
Ask the participant to pretend that she is presenting this document, and have her explain the deliverable to you.

Simply making the participant aware that ideas can be written down either on scratch paper or directly on the printed copy of the deliverable will encourage your participant to be more forthcoming.
6. Watch and listen as your participant explains the concepts in your deliverable. While it is tempting to focus primarily on what the participant says (especially direct suggestions for changes), the observation process can provide equally or even more important insights. Watch for areas of the  document that the participant refers to when explaining ideas in order to figure out if your document needs to be reorganized and if there are any missing points. (If the participant explains something “wrongly,” you should follow standard user-research protocol and avoid correcting the misconception, or you will bias the data from the rest of the session.)
7. Ask your participant open-ended follow-up questions if he offers vague or potentially insightful explanations of your deliverable. Be careful not to ask leading questions, or ones that propose solutions to problems that emerge during the sketch test — instead, probe to find the nature of any misunderstandings, and the assumptions that your participant brought to the session. The echo, boomerang, or Columbo methods can be valuable techniques to prompt your participant to explain an intriguing statement further without unnecessarily priming or leading their responses.
8. At the end of the session, explain the intended “correct” meaning of the document to your participant, to avoid having him leave with an erroneous understanding of your work. If the participant made a “mistake” in interpreting your document, make an effort to avoid having him feel stupid by pointing out that you clearly hadn’t explained that item well enough and that catching issues early, before they impact the full team, is why you’re testing the deliverable. Finally, thank the participant for helping you and the team.
Feedback to Look for During a Sketch Test
The sketch test can help surface two types of problems with deliverables: (1) content that is not easily perceptible in the document (e.g. using blue text to annotate a screenshot made it difficult to notice against a similar background), and (2) content that is not comprehensible (e.g. the purpose of a dendrogram being misunderstood as showing user preferences for various content). Whether or not a deliverable is comprehensible depends largely on the audience member's prior experience, expectations, and mental models, and these contextual details can be exposed by the sketch test.
During a sketch test, notice if your participant exhibits any of the following behaviors, which may indicate that something in the deliverable is difficult to understand or may need more emphasis:

Circling or underlining elements
Drawing something to represent content that your deliverable presents in text, or employing a different visual metaphor than you currently use for data visualization or infographics
Making notes or edits on your sketch, or suggesting terminology or wording for concepts other than those shown in the deliverable
Reading the same passages or studying the same images several times
Struggling to explain something to you
Explaining something incorrectly to you


A dendrogram is a type of data visualization produced by card sorting (a popular information-architecture research method). It shows all the main terms presented to users along the left-hand side, and the tree-like structure on the right half of the document shows the percentage of participants that grouped each particular set of items together. In this example, 69% of users agreed that the highlighted terms (in green) belonged together in a group. During a sketch test, an internal team member (who was not involved in the card sorting) was asked to explain this dendrogram, and drew directly on the diagram, making notes such as ""why are there vertical lines directly next to some groups of terms, but other groups have a horizontal line that extends for a bit before connecting to a group?""

Improving the Deliverable Based on the Sketch Test
If your participant struggles to explain your deliverable easily, you will likely need to revise it before sharing it with your colleagues.
Look at your participant’s sketch pad; these notes can be invaluable to you as you revise your deliverable. Pay attention to statements that:

Use a different visual metaphor or presentation for the same data. While the particular suggestions your participant offers for alternative visuals may not be what you ultimately choose to use in your deliverable, there can be useful insights to be learned from how your participant chooses to express the idea.
Attempt to keep track of steps or event sequences (especially with documents such as proposed user workflows, or usability-testing reports that note steps taken to accomplish a task).  
State intermediate inferences, explanations, or reiterations of facts presented earlier in the document. By not making these explicit when needed, you may have put an extra load on the reader’s short-term memory.
Expose the participant's unfamiliarity with core concepts in your deliverable, demonstrate lower domain expertise than expected, or different expectations for what the content of the deliverable should be.

Feedback About the Content of the Deliverable
During a sketch test, you may receive feedback not only about the format of your deliverable, but also about its content. For example, if you're testing a wireflow, your sketch-test participant may suggest changes to the layout of the various screen designs represented in the wireflow. This is perfectly acceptable — while the main intention with the sketch test is to find flaws in the format of your deliverable, getting an additional review for your ideas is an opportunity to further refine them before formally sharing a finalized deliverable with project stakeholders.
Conclusion
Conduct audience-based testing in order to increase understanding of UX deliverables such as documentation, infographics, wireflows, and data visualizations. Do a quick sketch test: provide a printed copy of your deliverable, recruit a representative colleague, and ask them to explain to you what your deliverable communicates. Provide scratch paper for the participant to take notes on while explaining ideas to you. Your-participant’s notes and the verbal feedback can provide insights about how your deliverable can be improved to better match your audience's mental models and expectations.
Learn more about communicating UX ideas clearly in our full-day UX Deliverables seminar.
Reference
Michael J. Albers, ""Infographics and Communicating Complex Information,"" Design, User Experience, and Usability: Users and Interactions (July 21, 2015)"
81,2017-01-01,"Usability testing in the field is an effective and quick way to learn about users and their context of use. Like other types of field research, it typically takes place where users live, play, or work. The following lessons learned from our own studies can help you avoid common problems.

Do a pilot study session in order to debug your materials and understand how much you might get done during the allotted time.
In the likely case that you have too many questions or tasks and not enough time, either prioritize and make some of the last ones optional or plan how to best rotate questions and tasks among participants so you get good coverage of your research issues.
Consider making an editable script for each session, so that you can take notes into the script. A tablet with an ink app can be handy, or bring a superlight laptop. Use a checklist for important things to take with you.
Your questions should evolve or change over the sessions as you learn. Prepare any stakeholders and observers for this process and explain why in this situation (unlike for other types of research such as surveys or field studies) consistency is not essential. (These are decidedly not measurement studies.)
The onsite project manager may be able to arrange incentives and a host gift for you. This help can be extremely useful when your research takes place in a different culture or country than your own.
Monitor recruiting closely to make sure you aren’t getting all superusers, trainers, administrators, or support-escalation people when you actually need normal users. Don’t let well-meaning helpers stack the deck with “good” (but unrepresentative) or expert participants in a misguided attempt to ensure that people perform well in the study.
Recruit diverse participants in terms of ethnicity, role, gender, and experience with the task domain and the system. Try to include at least one person whose native language is not the language of the interface.
Everyone will want to watch. Limit the number of observers per session. Don’t overwhelm your participants by crowding them out of their cubes. If needed, schedule the observers too. The more the better, but make sure you are running the show so they won’t prevent you from getting the data you need.
Weigh locations carefully. Think about whether you need to conduct research sessions in each person’s normal workspace or to move it to another venue, such as a conference room — for example to get enough privacy and quiet. Sometimes a noisy environment is essential in order to recreate all the distractions that people will normally have to deal with when using the system.
If you are able to pay the incentive at the time of the research session, the consent form could also serve as your signed receipt. That way you can pay cash, so that no one will worry whether you’ll really send them a check in the mail or not.
Observers and stakeholders will want to talk to the participants. Provide guidelines for observers, including when and how other people can ask questions. You may need to reword some questions before they get asked in order to remove biases. Be ready to alert people who cause problems through a prearranged signaling system. (Passing notes works well for communicating with observers during sessions.) Don’t let your research be derailed by other people if at all possible. Reserve 5 minutes at the end of the session for observer questions. Sometimes a quiet discussion after a session is necessary in order to regain cooperation. Stakeholders will very likely have other (and better) opportunities to find out more, but you might have only one shot at doing the research. If power struggles appear during the planning phase, build in extra participant-question time for the stakeholders at the end of each session and extra debriefing time.
When conducting research at a business location, reserve a conference room or another private area, if possible, for the researchers and observers to occupy when not in sessions. A whiteboard and a projector might come in handy too. Make snacks and drinks available. Ensure that research participants can’t overhear the team talking.
Encourage and welcome observers, especially if they are usability research skeptics. Stakeholders who observe research studies can become your biggest advocates for change later, and it saves a lot of explaining time to have them onsite. Having a few extra people along can also be quite handy when problems crop up and you need people working in parallel with you, running errands, or intervening when political or emotional issues intrude.

During the Study

Make sketches. Consider sketching notes and ideas on copies of the user interface screens (and even taking environmental photos).
Keep separate copies of the original images and documents, so you can have as many of them as you wish to annotate.
Date documents for version control.
Number participants and their documents so you won’t be attaching names to data.
Take good notes, even if you are allowed to make recordings. Recordings take just as long to listen to (or longer) than the original session, and recordings sometimes fail. Capturing observations and insights in real time can be crucial.
Don’t rely on people to remember to send you promised material after the session. Get permission for someone to email them one reminder if needed.
Pay attention to everything in the environment.
Don’t rely on your memory for anything. Note your questions, ideas, insights, to-do items, and concerns as they arise.
Debrief observers and any onsite research team after each session.
Make debriefing notes so they can become the source for preliminary top findings.

After the Study

If you have recordings that need to be shared with stakeholders, add data-confidentiality instructions and warnings at the beginning of each video or audio file. Release recordings only to a responsible person who fully understands the need to keep research participant data safe and anonymous and the need to destroy raw data and personally identifiable information as soon as it’s no longer needed.
Set expectations for when you’ll deliver results.
Compile and share preliminary top findings as soon as possible, while everything is still fresh in your mind.
Thank everyone who helped make the research effort successful. If allowed, bring a business gift for the host(s), such as a UX book or great office supply item (or make plans to go out for a meal together).

Conclusion
When preparing for field research projects, collaborate with stakeholders to make a research plan, ensure you take the equipment and supplies you need, review these tips, and then relax and have a great usability study.
We can teach your team a full-day course on how to do ethnographic field research. (Also available as a 2-day course, for further depth.)"
82,2016-12-18,"What Is a Testable Prototype?
A user interface prototype is a hypothesis — a candidate design solution that you consider for a specific design problem. The most straightforward way to test this hypothesis is to watch users work with it.
There are many types of prototypes, ranging anywhere between any of these pairs of extremes:

Single page vs. multipage with enough menus, screens, and click targets that the user can completely finish a task
Realistic and detailed vs. hand-sketched on a piece of paper
Interactive (clickable) vs. static (requiring a person to manipulate different pages and act as a “computer”)

The choice of prototype will vary greatly depending on goals of the testing, completeness of the design, tools used to create the prototype, and resources available to help before and during the usability tests. But, whatever prototype you may use, testing it will help you learn about users' interactions and reactions, so you can improve the design.
Why Test a Prototype?
Ripping up code is very expensive. Ripping up a prototype is not, especially if it’s just a piece of paper.
Let’s first consider common arguments for  not  testing a prototype. These are:

Waiting for a design to be implemented before you test it means that the design actually works, and test participants can use it in a natural way.
There is no adjustment to be made to the Agile or Waterfall processes to accommodate UX and iterative design.
Some Lean proponents say that, with no prototype testing, there is no prototype to throw away when it (inevitably) tests badly, so there is no waste.

These arguments may seem good at first glance. But in reality,  testing final products is uninformed and risky. Enlightened teams create prototypes, have users test them, and iterate the design until it’s good enough. (Note: We also test final products to benchmark the usability at the end of a project or at the beginning of a new one, to assess  ROI, to run  competitive studies, and to do a final check and make small tweaks.)
Interactive vs. Static Prototypes
Work needs to be done to bring a prototype to life for usability testing. To make it respond to user actions, we can spend time implementing the interaction before the test or we can “fake” the interaction during the test. Both methods have benefits and drawbacks.
Interactive (Clickable) Prototypes
With interactive prototypes, the designer must set a response for each possible user action before the test happens. Even with the right tool, building an interactive prototype can be time consuming: you have to get all the click targets right, and make the system respond only when the user interacts with a clickable target.
Static Prototypes
With static prototypes, the responses to users’ actions are given in real-time during the test by a person who is familiar with the design. There are several methods that can be used with static prototypes:

Wizard of Oz.  This method is named after the famous Frank Baum book (and more famous movie) with the same name. (If you’re not familiar with the book or the movie: in it, the great Wizard of Oz is impersonated by an ordinary human hiding behind a curtain.) The “wizard” (the designer or someone else familiar with the design) controls the user’s screen remotely from another room. None of the user’s clicks or taps really do anything. Rather, when the user clicks, the “wizard” decides what should come next, then serves up that page to the user’s screen. The “wizard” may even create the page on-the-fly and serve it up. Users don’t know what produces the response. In fact, they are usually told very little beyond that the system is “unfinished and slow.”

	Wizard of Oz testing is particularly useful for testing AI-based systems before you have implemented the artificial intelligence. The human who controls the computer can simulate the AI responses based on natural intelligence.

Paper-Prototype “Computer.”  The design is  created on paper. A person who knows the design well plays the part of the “computer” and lays the papers out on a table, near the user’s test table but not in her line of sight. As the user taps with the finger on a paper “screen” in front of her, the “computer” picks up the page (or modular part) representing the response and places it in front of the user. (In this article, we use the notation of “computer” to refer to the human who’s implementing the user interface during the test session.)
	TIPS: 

The “computer” should indicate to the users when “it” has finished working and they can proceed with the interaction. This can be done either by using a designated gesture consistently (e.g., hands folded in front of you) or by using a special “Working” or hourglass-icon printout that is shown to the users while the “computer” is looking for the appropriate response and that is removed as soon as the “computer” has finished working.
The facilitator should avoid overexplaining the design elements or the process.


Steal-the-Mouse “Computer.”  This method is a version of the Wizard of Oz technique in which the “wizard” is in the same room with the user. (The “wizard’s” role could be played by the facilitator.) The prototype is shown to the user on a computer screen. As the user clicks, the facilitator asks the user to look away from the monitor for a moment and the “wizard” navigates to the page that should appear next. The user is then prompted to look at the monitor and continue.

Criteria to help you decide which type of prototype is right for your project:


The fidelity of the prototype refers to how closely it matches the look-and-feel of the final system. Fidelity can vary in the areas of:

Interactivity
Visuals
Content and commands

A prototype may have high or low fidelity in all or some of the above 3 areas. The table below explains what high and low fidelity mean in each of these areas.



 
HIGH-FIDELITY PROTOTYPE
LOW-FIDELITY
			PROTOTYPE


Interactivity


Clickable links and menus

Yes: Many or all are clickable.


No: Targets do not work.



Automatic response to user’s actions

Yes: Links in the prototype are made to work via a prototyping tool (e.g., InVision, PowerPoint).


No: Screens are presented to the user in real time by a person playing “the computer.”



Visuals


Realistic visual hierarchy, priority of screen elements, and screen size

Yes: Graphics, spacing, and layout look like a live system would look (even if the prototype is presented on paper).


No: Only some or none of the visual attributes of the final live system are captured (e.g., a black-and-white sketch or wireframe, schematic representation of images and graphics, single sheet of paper for several screenfuls of information). Spacing and element prioritization may or may not be preserved.



Content and Navigation Hierarchy


Content

Yes: The prototype includes all the content that would appear in the final design (e.g., full articles, product-description text and images).


No: The prototype includes only a summary of the content or a stand-in for product images.




Benefits of High-Fidelity Prototypes

Prototypes with high-fidelity interactivity have realistic (faster) system response during the test.  Sometimes it can take extra time for the person playing the computer, whether online or on paper, to find the right screen and respond to a user’s click. Too long of a lag between user’s action and the “computer’s” response can break the users’ flow and make them forget what they did last or expected to see next.

	A delay also gives users extra time to study the current page. So, with a slow prototype, usability-test participants may notice more design details or absorb more content than they normally would with a live system.
TIP: If the page supposed to appear next is hard to find in a paper prototype or slow to load in a clickable prototype, take away the current screen the user is looking at, so she is instead looking at a blank page or area. When the next page is ready, first display the previous page for a few moments again so the user can get her bearings, then replace that screen with the next one. The test facilitator can help this process by saying just a few words to help the user recover the context — for example, “Just a recap, you clicked  About Us.” 

With high-fidelity interactivity and/or visuals, you can test workflow, specific UI components (e.g.  mega menus,  accordions), graphical elements such as affordance, page hierarchy, type legibility, image quality, as well as engagement.
High-fidelity prototypes often look like “live” software to users. This means that test participants will be more likely to behave realistically, as if they were interacting with a real system, whereas with a sketchy prototype they may have unclear expectations about what is supposed to work and what isn’t. (Though it’s amazing how  strong the suspension of disbelief is for many users in test situations where not everything is real.)
High-fidelity interactivity frees the designer to focus on observing the test instead of thinking about what should come next. Nobody needs to worry during the test about making the prototype work.
Interactive-prototype testing is less likely to be affected by human error. With a static prototype, there is a lot of pressure on the “computer” and a fair chance of making a mistake. Rushing, stress, nerves, paying close attention to user clicks, and navigating through a stack of papers can all make the “computer” panic or just slip during the test.

Benefits of Low-Fidelity Prototypes

Less time to prepare a static prototype, more time to work on design, before the test. Creating a clickable prototype takes time. Without having to make the prototype work, you can spend more time on designing more pages, menus, or content. (You still need to organize pages before the test so the “computer” can easily find the right one to present. But doing this is usually a lot faster than preparing a clickable prototype.)
You can make design changes more easily during the test. A designer can  sketch a quick response, and erase or change part of design between test sessions (or during a session) without worrying about linking the new page in the interactive prototype.
Low-fidelity prototypes put less pressure on users. If a design seems incomplete, users usually have no idea whether it took a minute or months to create it. They may better understand that you are indeed testing the design and not them, feel less obliged to be successful, and be more likely to express negative reactions.
Designers feel less wedded to low-fidelity prototypes. Designers are more likely to want to change a sketchy design than one with full interaction and aesthetics. Once we invest more time and sweat in a design, it’s harder to give it up if it does not work well.
Stakeholders recognize that the work isn’t finished yet. When people see a rough prototype, they don’t expect it to ship tomorrow. Everybody on the team will expect changes before the design is finalized. (In contrast, when a design looks very polished, it’s easy for an executive to fall into the trap of saying, “this looks good, let’s make it go live now.”)

Interaction with the User During Any Prototype Test
In prototype tests, facilitators often talk a bit more with participants than they do in tests of live systems, mainly for the following good reasons:

They need to explain the nature of the prototype medium (not how the design itself works) to the user, before the study starts.
They occasionally may need to explain the state of the system (e.g., “This page doesn’t work yet”) and ask “What were you expecting to happen?”
They may have to find out whether users who sit idle are waiting for a response (from a slow system) or think that the task is completed.

Even with the above necessary interactions between the test facilitator and the user, the test facilitator’s ultimate goal should be to quietly observe the person interacting with the design, not to have a conversation with the participant.
TIPS:

If users click an item for which there is no prepared response yet:
	
Say: “  That isn’t working.”
Ask:  “What were you expecting to happen when you clicked that?”
Present the second-best page if there is one, and say something as an explanation. For example, “You clicked the compact cars link. We don’t have those screens today. So please pretend you clicked midsize cars. Okay?” After the user confirms, present the midsize-cars page. Then say as little as possible, and stay neutral.


If the wrong page appears after the user clicks, the “computer” should take that page away as soon as possible and revert to the previous page. The facilitator should tell the user immediately that the page was wrong, then repeat what the user did on the current page, “You tapped About Us.” Then the “computer” presents the right page.

“Computer” Errors Have a Negative Impact
Note that “computer” errors can seriously impact the test. As screens appear, users form a mental model of how the system and the research method work. If the wrong page appears, don’t assume that you can make users forget what they just saw. (Brain wipes only work in Star Trek.) Even if you take the screen away or try to explain the error, users may infer that the wrong screen is related to the task or get some more knowledge from your explanation, and may be later influenced by that information. Seeing the wrong page also breaks the users’ flow, and can confuse them. Finally, later in the test, if a screen appears that is unexpected, they might think the prototype is just malfunctioning again. This impacts the users’ expectations, trust in the research method, and ability to form a consistent mental model.
Since computer errors can negatively impact the study, take the time to pilot test and fix issues with the prototype before any sessions occur.
Conclusion
Make no mistake: You cannot afford to not test prototypes. Your design will be tested, whether you plan for it or not. Once your system goes live and people begin to use it, they are testing it. And rather than collecting feedback in a low-risk research setting, where you can learn, and then react and change the design, you’ll have actual unhappy customers on your hands. With them will come a litany of problems such as lost sales, abandoned orders, lack of understanding of content and products, alienation due to poor tone of voice, returned products, increased support calls, increased training costs, social shares of bad experiences, low Net Promoter Scores, and brand abandonment. The business will have to figure out how to fix all these. The development team will react by scrambling to fix the design, taking out working code, visuals, and content, and trading it for rushed, only marginally better replacements. All will come at a great cost. Redesigning, taking code out, coding again with the new design, quality testing that code, and, if applicable, changing marketing and documentation materials, is far more expensive than discarding a prototype.
Test prototypes, whether clickable or static, whether high- or low-fidelity. Aim to learn how to change and improve your design. That way, your customers will never see your design failures.
 
Read the companion article: IA-Based View of Prototype Fidelity.
For more information about preparing testable prototypes, get our Paper Prototyping training video, or attend our “Wireframing and Prototyping” course."
83,2016-10-23,"Introduction
It’s difficult to identify gaps in one’s own understanding. Reading and discussing issues with other UX professionals and subject-matter experts can help; but, especially when designing new things with new technologies and capabilities, it’s best to begin by taking an open mind to where the action is.
UX researchers are responsible for learning about users, their goals, challenges, and activities, and for bringing that understanding to the organization. Lab-based studies and analytics can help only to the extent that you ask the right questions and look in the right places for the right data. Studying users and tasks in context can inform design decisions and can put the focus on outcomes, not features. When you notice gaps in your knowledge or understanding, it may be time to get out of the office and investigate, watch, and learn.
What Is a Field Study?
Definition: Field studies are research activities that take place in the user’s context rather than in your office or lab.
The range of possible field-study methods and activities is very wide. Field studies also vary a lot in terms of how the researcher interacts (or doesn’t) with participants. Some field studies are purely observational (the researcher is a “fly on the wall”), some are interviews in which the questions evolve as understanding increases, and some involve prototype feature exploration or demonstration of pain points in existing systems.
Examples of field studies include:

Flexible user tests in the field, which combine usability testing with adaptive interviews. Interviewing people about their tasks and challenges gives you very rich information. In an adaptive interview, you refine the questions you ask as you learn.
Customer visits can help you better understand usability issues that arise in particular industry or business contexts or those that appear at a certain scale.
Direct observation is useful for conducting design research into user processes, for instance to help create natural task flows for subsequent paper prototypes. Direct observation is also great for learning user vocabulary, understanding businesses’ interaction with customers, and discovering common workarounds — for example by listening in on support calls, watching people moving through amusement parks, or observing sales staff and customers in stores.
Ethnographic research situates you in the users’ context as a member of the group. Group research allows you to gain insight into mental models and social situations that can help products and services fit into people’s lives. This type of research is particularly helpful when your target audience lives in a culture different from yours.
Contextual inquiry is a method that structures and combines many of these field-study activities.

Field research is usually done with one of the following goals in mind:

Gather task information. You can find out how people do things today and why they do them in particular ways, before proposing something new. Early design research can help prevent big mistakes when creating products and services.
Understand people’s needs and discover opportunities for addressing them.
Obtain data for journey maps, personas, use cases, and user stories. Field studies help you to understand your users in depth, so you can better describe them for your team.
Test systems under realistic conditions. You can discover social defects and understand environmental factors before releasing products. Contextual research helps discover things you wouldn’t know to ask about, such as problems that crop up when new tools or processes are introduced into existing work practices.

When to Leave the Lab for the Field
Any of the following are good reasons for running a field study:

You need big-picture insights. Field studies can be done at any time, but it often makes sense to do them before design (or redesign) begins, because such research can lead to fundamental shifts in understanding your users and can change what you would design for them.
You don’t know enough about your actual or prospective users. 
You need to understand how people normally do their work and how they set up their environment to support their tasks. Watching people do particular activities can illuminate what people really do versus what they say they would do. Field studies that focus on specific tasks help researchers learn how to improve the experience of doing them.
You don’t know enough about your users’ context:

Cultural context: For example, your users may live in a different region or country.
Context of use: Your customers may be using the interface or engage in a behavior of interest in a particular location or circumstance that is hard to replicate in the lab (for example, while walking, shopping, attending an event, riding the bus, or when it’s raining).

	Observing people in their natural environment allows you to learn the unexpected and to understand how well systems work when people are distracted, in noisy places, and in normal situations for them, such as with family, social, or work groups.
You need to understand how groups of people behave, for example to find out how they collaborate, interrupt, and communicate, or to watch people use systems, workflows, and tools together. With many people in the mix, you can observe a wide range of behavior, knowledge, experience, and concerns.
Your participants can’t travel to your location. For example, you may need to go where the users are when you’re conducting research with people with physical or transportation challenges, extremely limited availability (doctors or others who can’t leave work), or children at school.
Lab research might bias your results, for example because the tasks can’t all be done in a lab, the lab context is too unrealistic, intimidating, or otherwise excludes people whom you want to observe. Familiar surroundings and normal equipment are often preferred because they come closest to natural user conditions.
You need to work with systems you can’t access in the lab, such as B2B applications, specialized equipment (anything from bulldozers to battleships), or secure systems.

When You Might Want to Use Other Methods
If money were no object, we would probably all do much more field research. Unfortunately, field methods have not become cheaper at the same rate as other usability methods, and they can be challenging to budget or schedule. Field studies are still worthwhile, for example when you’re researching how and whether to make a new product, but it’s best to gather as much data as possible with cheaper methods. Beyond reasons of resource constraint, you might decide to stay out of the field in certain other cases.
Research in the Lab or Office
It’s sometimes best to conduct in-person research in labs, conference rooms, or other spaces that are not where people normally do the activity you want to study. For example: When what you are testing or researching is particularly confidential, sensitive, or private; when you have many observers for the research sessions; when you need to record but you can’t do that where participants work; when you’re testing systems or prototypes; or when the research focus is mainly on the usability of the system, rather than on people’s context, nature, and situation.
Remote, Attended Usability Research
UX researchers can get some of the advantages of both field and lab studies by conducting research live, using various audio–visual tools, with participants and facilitators each in their chosen locations. The remote, interactive approach can often be cheaper and faster than field or lab studies. Everyone avoids expensive and time-consuming travel to unfamiliar places. Being in your own space also offers comfort, familiar tools, and convenience.
A tradeoff with remote research is that you can’t see what the user’s camera doesn’t show you. That missing context is often important when you are trying to understand people and their environment.
Remote, attended studies make sense: When your participants are all over the map, and traveling to meet in person is too difficult or expensive; when it’s important to get some specific answers quickly and cheaply, and you already understand the people, tasks, and contexts in depth; when you need to conduct sessions a few at a time, for example when testing early designs with only a couple of users for each iteration.
How to Plan a Field Study
Make a research plan.
Location. Decide where best to observe people in action. Go where your potential users are most likely to be found: workplaces, schools, shopping centers, airports, and so on.
Assistance. When applicable, work with an ally onsite. When visiting a business, for example, you might need help recruiting, scheduling, reminding, rewarding, and briefing participants. An onsite helper can escort you, introduce you, and help you with equipment or space issues. You may need to get permission in advance to conduct research in public or commercial spaces.
Participants. Study people who are representative of your target audience groups. Depending on the research method you use, you might need a professional recruiter or a team member to help you screen and schedule people.
Observers. Decide whether to allow stakeholders to watch. Although it’s often strategically important and desirable to involve stakeholders in observing user research, it’s not always possible with field studies.
Sometimes observers won’t fit in the space, or they would make the research situation too intimidating or otherwise create a weird situation for the users. When that happens, you won’t get to observe the most natural behavior and you might not get the candid information that you need.
On the other hand, with B2B site visits to customer companies, it’s common for stakeholders from both companies to want to be present for the research sessions to some extent. Sometimes outside researchers can’t be left alone with participants, so observers must be present. Observers often need a place to sit, talk, and work through issues raised in your debriefings. Observers may also need guidance in how to observe and how to help collect data, so they won’t behave badly.
Conclusion
When you encounter problems or behavior that you don’t understand around existing products or services, field studies can help you take a step back and find a new perspective, in order to correct your own mental models.
Doing research where people are can be crucial to understanding whether new products and services will help, hinder, or fall flat for the people you aim to assist. Set aside assumptions and allow insights to reframe what you’re creating and how that will affect the experiences of the people you’re designing for.
(We can come to your team and teach a full-day course on how to conduct ethnographic field studies in your UX projects.)"
84,2016-09-25,"Qualitative surveys ask open-ended questions to find out more, sometimes in preparation for doing quantitative surveys. Test surveys to eliminate problems.
Sooner or later, most UX professionals will need to conduct a survey. Survey science from the quantitative side can be intimidating because it’s a specialized realm full of statistics, random selection, and scary stories of people going wrong with confidence. Don’t be afraid of doing qualitative surveys, though. Sure, it’s important to learn from survey experts, but you don’t have to be a survey specialist to get actionable data. You do have to find and fix the bugs in your questions first, however.
Quantitative vs. Qualitative Surveys
Quantitative surveys count results: how many people do this vs. do that (or rather, how many say that they do this or that). Use quant surveys when you need to ask questions that can be answered by checkbox or radio button, and when you want to be sure your data is broadly applicable to a large number of people. Quantitative surveys follow standard methods for randomly selecting a large number of participants (from a target group) and use statistical analysis to ensure that the results are statistically significant and representative for the whole population.
Qualitative surveys ask open-ended questions. Use them when you need to generate useful information via a conversation rather than a vote, such as when you’re not sure what the right set of answers might include. Qualitative surveys ask for comments, feedback, suggestions, and other kinds of responses that aren’t as easily classified and tallied as numbers can be. You can survey fewer people than in a quantitative survey and get rich data.
It’s possible to mix the two kinds of surveys, and it’s especially useful to do small, primarily qualitative surveys first to help you generate good answers to count later in a bigger survey. This one-two-punch strategy is much preferable to going straight to a closed-ended question with response categories you and your colleagues thought up in your conference room. (Yes, you could add an “other” option, but don’t count on valid statistics for options left to a catch-all bucket.)
Tips for Qualitative Surveys
Unordered lists can be more time-consuming to look through than lists that have an obvious ordering principle, but unordered lists seem to yield better answers, especially if you can sort the list differently for different respondents.

Test your survey. Here’s the procedure that we recommend:

	
Draft questions and get feedback from colleagues.
Draft survey and get colleagues to attempt to answer the questions. Ask for comments after each question to help you revise questions toward more clarity and usefulness.
Revise survey and test iteratively on paper. We typically do 4 rounds of testing, with 1 respondent per round. At this stage, don’t rely on colleagues, but recruit participants from the target audience. Revise between each round. Run these tests as think-aloud studies; do not send out the survey and rely on written comments — they will never be the same as a realtime stream of commentary.
Randomize some sections and questions of the survey to help ensure that (1) people quitting partway through don’t affect the overall balance of data being collected, and (2) the question or section ordering doesn’t bias people’s responses.
Test the survey-system format with a small set of testers from the target audience, again collecting comments on each page.
Examine the output from the test survey to ensure the data gathered is in an analyzable, useful format.
Revise the survey one more time.


Don’t make your own tool for surveys if you can avoid it. Many solid survey platforms exist, and they can save you lots of time and money.
Decide up front what the survey learning goals are. What do you want to report about? What kind of graphs and tables will you want to deliver?
Write neutral questions that don’t imply particular answers or give away your expectations.
Open vs. closed answers: Asking open-ended questions is the best approach, but it’s easy to get into the weeds in data analysis when every answer is a paragraph or two of prose. Plus, users quickly tire of answering many open-ended questions, which usually require a lot of typing and explanation. That being said, it’s best to ask open-ended questions during survey testing. The variability of the answers to these questions during the testing phase can help you decide whether the question should be open-ended in the final survey or could be replaced with a closed-ended question that would be easier to answer and analyze.
Carefully consider how you will analyze and act on the data. The type of questions you ask will have everything to do with the kind of analysis you can make: multiple answers, single answers, open or closed sets, optional and required questions, ratings, rankings, and free-form answer fields are some of the choices open to you when deciding what kinds of answers to accept. (If you won’t act on the data, don’t ask that question. See guideline #12.)
Multiple vs. single answers: Often multiple-answer questions are better than single-answer ones because people usually want to be accurate, and often several answers apply to them. Survey testing on paper can help you find multiple-answer questions, because people will mark several answers even when you ask them to mark only one (and they will complain about it). If you are counting answers, consider not only how many responses each answer got, but also how many choices people made.
Front-load the most important questions, because people will quit partway through. Ensure that partial responses will be recorded anyway.
Provide responses such as, “Not applicable” and “Don’t use” to prevent people skipping questions or giving fake answers. People get angry when asked questions they can’t answer honestly, and it skews your data if they try to do it anyway.
People have trouble understanding required and optional signals on survey question/forms. It’s common practice to use a red asterisk “*” to mark required fields, but that didn’t work well enough, even in a survey of UX professionals — many of whom likely design such forms. People complained that required fields were not marked. Pages that stated at the top that all were required or optional also didn’t help, because many people ignore instruction text. Use “(Optional)” and/or “(Required)” after each question, to be sure people understand.
When marking is not clear enough, many people feel obligated to answer optional questions. Practically speaking that means you don’t have to require every question, but you should be careful not to include so many questions that people quit the survey in the middle.
Keep it short. Every extra question reduces your response rate, decreases validity, and makes all your results suspect. Better to administer 2 short surveys to 2 different subsamples of your audience than to lump everything you want to know into a long survey that won’t be completed by the average customer. 20 questions are too many unless you have a highly motivated set of participants. People are much more likely to participate in 1-question surveys. Be sensitive to what your pilot testers tell you, and realistically estimate the time to complete the survey. The more open-ended questions and complex ranking you ask people to do, the more you’ll lose respondents.
People often overlook examples and instructions that are on the right, after questions. Move instructions and examples to the left margin instead (or the opposite side, for languages that read right to left), to put them in the scannability zone and place them closer to the person’s focus of attention, which is on the answer area.
Use one-line directions if you can. Less is more. Just as in our original writing for the web studies, people read more text when there is a lot less of it. People complain about not getting enough information, but when it’s there they don’t read it because it’s too long.
People tend not to read paragraphs or introductions. If you must use a paragraph, bold important ideas to help ensure that most people, who scan instead of reading, glean that information.
Think carefully about using subjective terms, such as “essential,” “useful,” or “frequent.” Terms that cause people to make a judgment call may get at how they feel, but such questions can be confusing to evaluate logically. Ratings scales are more flexible. If you do need to know how participants perceive a certain aspect, indicate that’s what you want them to base their answer on (for example, instead of asking “Is X essential for Y?” say “Do you feel that X is essential for Y?”).
Define terms as needed in order to start from a shared meaning. People might quibble about the definition, but it’s better than getting misleading answers because of a misinterpretation.
Don’t ask about things that your analytics can tell you. Ask why and how questions.
Include a survey professional in your test group. Your survey method may be criticized after the fact, so get expert advice before you conduct your survey.
Answer ordering and first words matter, especially in long lists. Logical groupings, randomized lists, and short lists work better than long, alphabetical lists. Ordering issues can skew your data, so test alternative list orderings when you test your survey. When selecting from a list, many people choose the first thing that sounds like it might be right and go to the next question.
	
Items at the top and bottom of lists may attract more attention than items in the middle of long lists.
Because people scan instead of read, the first words of items in lists can cause them to overlook the right choice, especially in alphabetical lists.


Test where best to place page breaks. Sometimes it’s important for people to be able to see all the topic’s questions before they answer one. Otherwise they volunteer answers for the questions they have not yet seen and write, “see previous answer” later, which adds extra interpretation steps in data analysis. To find questions with these kinds of problems, you can test the survey with each question on its own page first, and then collocate the questions that need to be shown together on one page in the next test version. In some cases, simply forcing one question to come before another one can fix these problems.
If possible, don’t annoy people by asking questions that don’t apply to them. When respondents choose a particular answer, show them one or two more questions about that topic that would be applicable in that case. Choose a survey platform that allows conditional questions, so you can avoid presenting nonapplicable questions and keep your list of questions as short as possible for each respondent. If most of your questions are conditional, you might be able to put a key conditional question early in the list, then branch to different versions of the survey for the rest of the questions.


Take your data with a grain of salt. Unlike for quantitative surveys, qualitative survey metrics are rarely representative for the whole target audience; instead, they represent the opinions of the respondents. You can still present descriptive statistics (such as how many people selected a specific response to a multiple-choice question) to summarize the results of the survey, but, unless you use sound statistics tools, you cannot say whether these results are the result of noise or sample selection, as opposed to truly reflecting the attitudes of your whole user population.
Count whatever you can count. Researchers often refer to coding and normalizing data during analysis. Coding data is the process of making text answers into something you can count, so you can extract the bigger trends and report them in a way that makes sense to your report audience. You can capture rich textual data for understanding and quoting, and code some types of responses as 0, 1, or 2 (no, partially, yes; or none, some, all) for example, or you may be able to define many different phrases as meaning the same thing (for example when people use synonyms or express the same ideas). This coding can be done after the data is collected, in a spreadsheet.
Show, don’t tell. Use lots of graphs, charts, and tables, with an executive summary of key takeaways.
Consider graphs before you decide on a spreadsheet layout. Unfortunately some spreadsheets won’t make reasonable graphs until you switch columns to rows or rows to columns. It’s easiest to plan for this necessity before you analyze your data. It’s also possible to take the chart data, put it on its own spreadsheet page, and then reorder it to make the charts. Just be careful not to make data transfer errors.
Beware of disappearing chart data. Some spreadsheets hide data in charts silently when font-size changes or chart-size changes are made.
Don’t embed data if you can screenshot it. Screenshots (PNG format is recommended) are lovely and robust over time, unlike embedded data, which tends to cause document corruption, become unlinked, or could be changed by mistake.

Conclusion
Qualitative surveys are tools for gathering rich feedback. They can also help you discover which questions you need to ask and the best way to ask them, for a later quantitative survey. Improve surveys through iterative testing with open-ended feedback. Test surveys on paper first to save time-consuming rework in the testing platform. Then test online to see the effects of page order and question randomization and to gauge how useful the automated results data may be."
85,2016-07-31,"What Is a Customer Journey Map?
In its most basic form, journey mapping starts by compiling a series of user goals and actions into a timeline skeleton. Next, the skeleton is fleshed out with user thoughts and emotions in order to create a narrative. Finally, that narrative is condensed into a visualization used to communicate insights that will inform design processes.
Journey mapping combines two powerful instruments: storytelling and visualization. 
Storytelling and visualization are essential facets of journey mapping because they are effective mechanisms for conveying information in a way that is memorable, concise and that creates a shared vision. Fragmented understanding is chronic in organizations where KPIs are assigned and measured per individual department or group because many organizations do not ever piece together the entire experience from the user’s standpoint. This shared vision is a critical aim of journey mapping, because without it, agreement on how to improve customer experience would never take place.
Journey mapping creates a holistic view of customer experience, and it’s this process of bringing together and visualizing disparate data points that can engage otherwise disinterested stakeholders from across groups and spur collaborative conversation and change.
Deconstruction of a Customer Journey Map

While journey maps vary based on the specific context for which they are used, they tend to follow a general model that includes zones for the “lens,” the mapped experience, and the insights learned throughout the process. See below for diagram annotations. 

Zone A:  The lens provides constraints for the map by assigning  (1)  a persona (“who”) and  (2)  the scenario to be examined (“what”).
Zone B:  The heart of the map is the visualized experience, usually aligned across  (3)  chunkable phases of the journey. The  (4)  actions,  (5)  thoughts, and  (6)  emotional experience of the user has throughout the journey can be supplemented with quotes or videos from research.
Zone C:  The output should vary based on the business goal the map supports, but it could describe the insights and pain points discovered, and the  (7)  opportunities to focus on going forward, as well as  (8)  internal ownership.
Why Do You Need a Journey Map and When Should You Have One?
Journey maps should always be created to support a known business goal. Maps that do not align to a business goal will not result in applicable insight. The goal could be an external issue, such as learning about a specific persona’s purchasing behaviors, or an internal issue, such as addressing lack of ownership over certain parts of the customer experience. Some potential business goals that journey mapping could be applied toward are listed below.
Shift a company’s perspective from inside-out to outside-in.  If an organization lets internal processes and systems drive decisions that affect customer experience, a journey map could help turn the culture of that organization by refocusing on the thoughts, actions and emotions of customers. Journey mapping sheds light on real human experiences that often organizations know very little about.
Break down silos to create one shared, organization-wide vision.  Because journey maps create a vision of the entire customer journey, they become a tool for creating cross-department conversation and collaboration. Journey mapping could be the first step in building an organization-wide plan of action to invest in customer experience, as it helps answer the question, “Where do we start?” by highlighting areas of friction.
Assign ownership of key touchpoints to internal departments.  Often, areas of inconsistencies and glitches in customer journeys exist simply because no internal team has been tasked with ownership of that element. Journey maps can create clarity around alignment of departments or groups with different stages or key touchpoints in the journey that need addressing.
Target specific customers.  Journey maps can help teams focus in on specific personas or customers, whether that means understanding differences or similarities across the journeys of multiple personas, prioritizing a high-value persona or exploring ways to target a new type of customer.
Understand quantitative data.  If you are aware through analytics or other quantitative data that something specific is happening—maybe online sales are plateauing or an online tool is being underutilized—journey mapping can help you find out why.
Key Elements of Customer Journey Maps
While journey maps can (and should) take a wide variety of forms, certain elements are generally included:
Point of view.  First and foremost, choose the “actor” of the story. Who is this journey map about? For example, a university might choose either students or faculty members, both of which would result in very different journeys. “Actors” usually aligns with personas, if they exist. As a guideline, when creating a basic journey map, use one point of view per map in order to provide a strong, clear narrative.
Scenario.  Next, determine the specific experience to map. This could be an existing journey, where mapping will uncover positive and negative moments within that current experience, or a “to-be” experience, where the mapper is designing a journey for a product or service that doesn’t exist yet. Make sure to clarify the user’s goal during this experience. Journey maps are best for scenarios that describe a sequence of events, such as purchasing behavior or taking a trip.
Actions, mindsets, and emotions.  At the heart of a journey map’s narrative is what the user is doing, thinking, and feeling during the journey. These data points should be based on qualitative research, such as field studies, contextual inquiry, and  diary studies. The granularity of representation can vary based on the purpose of the map. Is the purpose to evaluate or design an entire, broad purchasing cycle or a contained system?
Touchpoints and channels.  The map should align touchpoints (times when the actor in the map actually interacts with the company) and channels (methods of communication or service delivery, such as the website or physical store) with user goals and actions. These elements deserve a special emphasis because they are often where brand inconsistencies and disconnected experiences are uncovered.
Insights and ownership.  The entire point of the journey-mapping process is to uncover gaps in the user experience (which are particularly common in omnichannel journeys), and then take action to optimize the experience. Insights and ownership are critical elements that are often overlooked. Any insights that emerge from journey mapping should be explicitly listed. If politically possible, also assign ownership for different parts of the journey map, so that it’s clear who’s in charge of what aspect of the customer journey. Without ownership, no one has responsibility or empowerment to change anything.
Even with all the above critical elements included, two journey maps could look completely different, yet both be perfectly suitable for the context in which they were designed.Tradeoffs in scope, focus, and breadth vs. depth are required when deciding on what elements to include. To make informed decisions on those tradeoffs, consider the following:

What level of detail is needed in order to tell the complete story?
What elements (such as device, channel, encountered content) are also necessary in order to provide the most truthful narrative?
Is the purpose of this journey map to diagnose issues with a current experience or to design a new experience?
What’s the balance between external actions (on the customer side) and internal actions (on the organization side)?
Who will be using this journey map?

Rules for Creating Successful Journey Maps
Successful journey maps require more than just the inclusion of the “right” elements. Journey mapping should be a collaborative process informed by well-defined goals, and built from research. It requires hard work to keep the process on the right track and to build the buy-in needed to evangelize the insights it provides. Below are some tips for making sure that the process starts and stays in the right direction:
Establish the “why"" and the “what.” First, identify the business goal that the journey map will support. Make sure there are clear answers to these basic key questions before you begin the process:

What business goal does this journey map support?
Who will use it?
Who is it about and what experience does it address?
How will it be shared?

Base it on truth.  Journey maps should result in truthful narratives, not fairy tales. Start with gathering any existing research, but additional journey-based research is also needed to fill in the gaps that the existing research won’t cover. This is a qualitative-research process. While quantitative data can help support or validate (or aid in convincing stakeholders who may view qualitative data as “fuzzy”),  quantitative data alone cannot build a story.
Collaborate with others.  The activity of journey mapping (not the output itself) is often the most valuable part of the process, so involve others. Pull back the curtain and invite stakeholders from various groups to be a part of compiling the data and building the map.
Don’t jump to visualization.  The temptation to create an aesthetic graphic or jump to design can lead to beautiful yet flawed journey maps. Make sure the synthesis of your data is complete and well-understood before moving to creating the visual.
Engage others with the end product.  Don’t expect to get “buy-in” and foster interest in your journey map by simply sending a lovely graphic as an email attachment. Make it a living interactive document that people can be a part of. Bring up your story in meetings and conversations to promote a narrative that others believe in and begin to reference. One idea is to create a journey-mapping showroom where anyone not on the direct team can come experience the process and resulting artifacts.
To learn more, check out our course,  Journey Mapping to Understand Customer Needs, coming up at the UX Conference later this year."
86,2016-06-05,"A diary study is a research method used to collect qualitative data about user behaviors, activities, and experiences over time. In a diary study, data is self-reported by participants longitudinally — that is, over an extended period of time that can range from a few days to even a month or longer. During the defined reporting period, study participants are asked to keep a diary and log specific information about activities being studied. To help participants remember to fill in their diary, sometimes they are periodically prompted (for example, through a notification received daily or at select times during the day).
The context and time period in which data is collected for a diary study make them unlike other common user-research methods, such as surveys (which are designed to collect self-reported information about a user’s habits and experiences outside of the context of the scenarios being studied), or usability tests (which yield observational information about a specific moment or planned set of confined interactions in a lab setting). They are the “poor man’s field study”: they are unlikely to provide observations that are as rich or detailed as a true field study, but they can serve as a decent approximation.
When to Conduct a Diary Study
If you’re looking for a contextual understanding of user behaviors and experiences over time, it can be very difficult to appropriately create scenarios in a lab setting to gather these kind of insights. Diary studies are useful for understanding long-term behaviors such as:

Habits — What time of day do users engage with a product? If and how they choose to share content with others?
Usage scenarios — In what capacity do users engage with a product? What are their primary tasks? What are their workflows for completing longer-term tasks? (These scenarios can be used for user testing later in the process.)


Attitudes and motivations — What motivates people to perform specific tasks? How are users feeling and thinking?
Changes in behaviors and perceptions — How learnable is a system? How loyal are customers over time? How do they perceive a brand after engaging with the corresponding organization?


Customer journeys — What is the typical customer journey and cross-channel user experience as customers interact with your organization using different devices and channels such as, email, phone, websites, mobile applications, kiosks, social media, and online chat? What is the cumulative effect of multiple service touchpoints?

The focus of a diary study can range from very broad to extremely targeted, depending on the topic being studied. Diary studies are often structured to focus on one of the following topic scopes:

Product or Website — Understanding all interactions with a site (e.g., an intranet) over the course of a month.
Behavior — Gathering general information about user behavior (e.g., smartphone usage, college-student web-visitation patterns)
General activity — Understanding how people complete general activities (e.g., sharing information via social tools or shopping online)
A specific activity — Understanding how people complete specific activities (e.g., buying a new car or planning a vacation)

Methodology
A diary study is typically composed of five phases:

Planning and Preparation. Define the focus of the study and the long-term behaviors that you need to understand. Define a timeline, select tools for participants to report data, recruit participants, and prepare instructions or support materials.
Prestudy brief. Take time up front to get participants ready to log. Schedule a face-to-face meeting or phone call with each participant to discuss the details of the study. Walk through the schedule or calendar for the reporting period and discuss expectations. Discuss the tools they will be using and be sure each participant has familiarized themselves with the technology; answer any questions they may have before beginning.
Logging period. To support effective activity logging, provide a simple framework. Be as specific as possible about what information you need participants to log, without stifling natural variability and differences that you cannot plan for. (Discovering the unexpected is after all one of the primary reasons to do user research.) Create clear and detailed instructions for logging. Give users example log entries to help them understand the level of detail you need from them. (But make sure you don’t bias participants toward those types of entries that you happened to provide as examples.)
	There are two common techniques researchers employ to collect diary data from participants.
	
In-Situ Logging — This method is the most straightforward method to collect data. Participants are asked to log information about relevant activities in the situation they occur (or in situ). When participants engage in a relevant activity, they must report all important details about that activity right away. Since this technique requires participants to take the time to report this information at the time of the event, this technique is best reserved for situations when you don’t foresee a large volume of diary entries occurring or if the context is such that participant’s daily activities will not be adversely effected by logging in situ. In-situ logging is best supported by channels and devices that can handle structured long-form text entry such as, email, web-form questionnaires, traditional paper diaries, or digital customer-insight tools such as FocusVision or 7daysinmylife. Audio or video diaries are also great tools for participants, but the output may need to be transcribed for analysis.
Snippet Technique — Another popular, less intrusive method for logging activities is the snippet technique. With this technique, participants only record short snippets of information about activities as they occur. Then, at the end of each day, or when participants have time, they elaborate on each snippet by providing additional details about the activity. This 2-step technique ensures that relevant information is captured in situ, before being forgotten but without requiring participants to provide extensive detail at the time of capture, which can be intrusive and unnatural in certain situations. Common channels for study participants to report snippets to researchers include email, text message, Twitter, or Facebook. These channels are widely familiar for short-form communication. Participants are encouraged to use their mobile phones to report events as they happen, since these devices are accessible. Expanding upon reported snippets can be accomplished with the channels and tools mentioned above for more in-depth reporting. Consider asking participants to expand on their snippets by filling out a questionnaire about each of them. This will enable you to get specific and consistent insights about each snippet.


Post-study interview. After the study, evaluate all the information provided by each participant. Plan a follow-up interview to discuss logs in detail. Ask probing questions to uncover specific details needed to complete the story and clarify as needed. Ask for feedback from the participant about their experience participating in the study, so you can adjust your processes for the next time.
Data Analysis. Because diary studies are longitudinal, they generate a large amount of qualitative data. Revisit your research questions, then take a deep breath and dig into all of the rich insights you’ve collected to find the answers. Evaluate the behaviors you’ve targeted throughout the study. How do they evolve and change over time? What influences these behaviors? If the focus of your study is around a particular product or service relationship, look at the entire customer journey. Construct a customer journey map to help you understand the end-to-end user experience from the perspective of your customers.

Motivating participants
Getting the insights you need will take some involvement with participants throughout the study. Plan to check in with participants or give periodic reminders as needed (each day or every few days). For participants that are engaged and creating appropriate snippets, recognize their efforts and ask them to keep up the good work. For participants that are less engaged, give encouragement or offer to answer any questions they may have to get them on track. Let participants know up front that you will be reaching out throughout the study and agree on a means of contacting them, so you can give encouragement or ask for clarification without being overly intrusive.
Diary studies require time and dedication from participants. To ensure you get the level of involvement you need from participants, provide an incentive that will keep them engaged. This compensation is typically much more than what you would offer for a 60-minute usability test. Align the incentive with the amount of work required over the period of the study. Consider breaking apart the total incentive and offering smaller installments as participants reach specific milestones (e.g., 3 days of logging), to keep them motivated throughout the duration of the study.
In a recent diary study with college-educated participants from various different regions across the United States, we paid each participant $275. Users had to complete a pre-assignment to install software on several personal devices before the logging period, log snippets for 2 weeks, fill out a web-form questionnaire for each snippet, and participate in two phone calls (a pre-study brief and post-study interview). The incentive was dispersed in 3 phases as users reached specific milestones throughout the study, to keep participants engaged throughout. This study had a completion rate of 90%.
 

Timeline of activities that take place throughout a typical diary study.

More Tips for Diary Studies

Plan for an appropriate reporting period. Make sure your study is long enough to gather the information you need, but be cautious about designing a very lengthy study. If your study is too long, participants may become less engaged as the study progresses, which could result in less accurate data.


Recruit dedicated users. Since diary studies require more involvement over a longer period of time, be extra prudent in the recruiting process. Let users know what is involved and expected of them up front. Ask screening questions that will help you gauge the level of commitment you will get from them during the study, and be sure to confirm they will be available for the entire study period.
Be on top of the data as it comes in. If you are getting data digitally or immediately as it comes in, evaluate it right away. This allows you to ask follow-up questions and prompt for additional detail as necessary, while the activity is still fresh in the minds of the participants


Conduct a pilot study. Diary studies can take quite a bit of time to plan and conduct, so it’s helpful to conduct a short pilot study first. The pilot study does not need to be as long as the real study and it is not meant to garner data for analysis. Its purpose is to test your study design and related materials. Practice the process of briefing and debriefing pilot participants. Try out your logging materials to be sure they’re understandable. Tweak your instructions and approach to ensure you get the data you need. Ask pilot participants for feedback about materials and the diary study experience, and adjust accordingly.


Rehearse your study and test your materials by adding a short pilot study to your overall timeline.

Conclusion
While diary studies can require more time and effort to conduct than other user-research methods, they yield invaluable information about customers’ real-time real-life behaviors and experiences. If you’re looking for organic behavioral insights and you can’t create a valid scenario in the lab or you can’t get the data you need from a single survey, don’t force-fit the research into these methodologies. Diary studies allow you to get a contextual understanding of users’ behavior and experiences over time."
87,2016-05-22,"Introduction
When conducting usability studies or field studies, it’s a great idea to ask lots of open-ended questions. Typically, researchers ask questions before, during, and after research sessions. It’s easy to focus on what you want to know rather than on how you ask, but the way you ask questions matters a lot in terms of what and how much you can discover. You can learn unexpected and important things with this easy technique.
Definition
Open-ended questions are questions that allow someone to give a free-form answer.
Closed-ended questions can be answered with “Yes” or “No,” or they have a limited set of possible answers (such as: A, B, C, or All of the Above).
Closed-ended questions are often good for surveys, because you get higher response rates when users don’t have to type so much. Also, answers to closed-ended questions can easily be analyzed statistically, which is what you usually want to do with survey data.
However, in one-on-one usability testing, you want to get richer data than what’s provided from simple yes/no answers. If you test with 5 users, it’s not interesting to report that, say, 60% of users answered “yes” to a certain question. No statistical significance, whatsoever. If you can get users to talk in depth about a question, however, you can absolutely derive valid information from 5 users. Not statistical insights, but qualitative insights.
How to Ask Open-Ended Questions




Don’t (Closed)


Do (Open)




Are you satisfied?


How satisfied or dissatisfied are you with this process?




Did it act as you expected?


What would (did) you expect to happen when you ... ?




Did you find it?


Before a task:

Please tell me when you’ve found the item.
Explain how you would find that.

After a task:

Where did you find the answer?
Where was the item?
What did you find?





Do you think you would use this?


How would this fit into your work?
How might this change the way you do that today?




Does that work for you?


What do you think about that?




Have you done this before?


What kinds of questions or difficulties have you had when doing this in the past?
What happened when you did this before?
Please describe your level of experience with …




Is this easy to use?


What’s most confusing or annoying about … ?
What worked well for you?




Did you know … ?


How do you know ... ?




Do you normally … ?


How do you normally ... ?




Did you see that?


What just happened?
What was that?




Do you like this?


What would you most want to change about … ?
Which things did you like the best about … ?




Did you expect this kind of information to be in there?


Before a task:

What do you expect to see when you … ?

After a task:

Which (other) kinds of information would likely be in there?
What were you expecting?





Why Asking Open-Ended Questions Is Important
The most important benefit of open-ended questions is that they allow you to find more than you anticipate: people may share motivations that you didn’t expect and mention behaviors and concerns that you knew nothing about. When you ask people to explain things to you, they often reveal surprising mental models, problem-solving strategies, hopes, fears, and much more.
Closed-ended questions stop the conversation and eliminate surprises: What you expect is what you get. (Choose your favorite ice cream: vanilla, strawberry, or chocolate.) When you ask closed-ended questions, you may accidentally limit someone’s answers to only the things you believe to be true. Worse, closed-ended questions can bias people into giving a certain response. Answers that you suggest can reveal what you are looking for, so people may be directly or indirectly influenced by the questions. Don’t ask, “Does this make sense?” Ask, “How does this work?” and listen closely to discover how well the design communicates its function. Note users’ word choices, because it might help to use their terms in the interface.
Tips
Start open questions with “how” or with words that begin with “w,” such as “what,” “when,” “where,” “which,” and “who.”
Don’t start questions with “was” (an exception to the “w” tip) or other forms of the verbs “to be” and “to do.”
In general, avoid “why” questions, because human nature leads people to make up a rational reason even when they don’t have one. We normally ask “why” only about ratings, to tease out more open-ended feedback. Say “Please tell me more about that,” instead.
Aim to collect stories instead of one- or two-word answers.
Even when you must ask closed-ended questions, you can ask an open-ended question at the end, such as, “What else would you like to say about that?”
Adding Other __________ to a set of multiple-choice answers is also a good way to get open-ended feedback.
When to Ask Open-Ended Questions

In a screening questionnaire, when recruiting participants for a usability study (for example, “How often do you shop online?”)
While conducting design research, such as on
	
Which problems to solve
What kind of solution to provide
Who to design for


For exploratory studies, such as
	
Qualitative usability testing
RITE (paper prototype) design research
Interviews and other field studies
Diary studies
Persona research
Use-case research
Task analysis


During the initial development of a closed-ended survey instrument: To derive the list of response categories for a closed-ended question, you can start by asking a corresponding open-ended question of a smaller number of people.

When To Ask Closed-Ended Questions

In quantitative usability studies, where you are measuring time on task and error rates, and you need to compare results among users
In surveys where you expect many (1000+) respondents
When collecting data that must be measured carefully over time, for example with repeated (identical) research efforts
When the set of possible answers is strictly limited for some reason
After you have done enough qualitative research that you have excellent multiple-choice questions that cover most of the cases

Bottom Line
Whenever possible, it’s best to ask open-ended questions so you can find out more than you can anticipate. Test your questions by trying to answer them with yes or no, and rewrite those to find out more about how and what. In some cases, you won’t be able to accommodate free-form or write-in answers, though, and then it is necessary to limit the possibilities."
88,2016-02-14,"In UX design, a persona stands for a group of users with similar behavioral patterns, goals, motivations, and expectations; personas keep product-design decisions focused on user needs. They are snapshots in time of your users. As industries and business goals evolve or new technologies emerge, users change, too, and personas risk becoming obsolete if they no longer reflect your current users’ behaviors, needs, or goals. Eventually, the personas may need to be updated to reflect the new reality and make the designers serve the actual (new) user needs.
Is It Time to Update Your Personas?
Don’t revise personas just for the sake of it — focus on identifying the need to do so, and react accordingly. Start with asking the right questions and proactively investigate whether your personas are outdated. Look for two types of changes that can signal a transformation of your user base:

Changes in your business and technology
Changes in user demographics and interaction patterns

1. Business and Technology Changes
Assess whether your products or your competitors’ products have significantly evolved. If that is the case, it’s likely that users’ goals and tasks have also changed and that your personas need an update.

Has the business changed? What was the state of your business when the personas were created or last updated?  If your business offerings and objectives have remained fairly steady since then, it’s less likely that your personas are out of date.  On the other hand, if your business has pivoted or evolved significantly, your customer base has likely evolved as well. Evaluate the accuracy of your personas and update them to reflect your current users and your current products.
Has the competitive landscape changed? It may be that your competitors’ offerings have changed since your personas were created. Competition has an impact on your customers’ expectations for your product. If competitors’ offerings have changed significantly, these changes can shape the needs and behaviors of your users. Take that as a cue to gauge whether your personas are still accurate reflections of your customers.

2. Changes in User Base
Large or obvious changes in your business or customers make it clear that it’s time to revisit your personas. Sometimes, however, the changes may be subtle or may accrue gradually over long periods of time. In these situations, it’s less apparent whether your personas have become outdated.
To figure out whether your users have changed, look at broad demographics of your users and at fine-grain user data collected through analytics, usability testing, or support calls.

Demographic data. Has the composition of your customer base changed? The makeup of your user base as a whole may have changed, sometimes for reasons completely unrelated to your business. You may have acquired new customer segments that you didn’t have before, or customer segments you had in the past may have disappeared over time. For example, the healthcare reform brought an entirely different type of customer into the health-insurance marketplace. Many of these new users had different needs, goals, and behaviors than the insurance customers before the reform. Similarly, in the past 10 years, the makeup of the student body at many universities has changed to include students who attend classes online only. These students’ goals and needs are different than those of regular students. Keep an eye on the changing composition of your customer base over time. Your personas may need to be updated to reflect the dynamics of your users.
Analytics. Segment your analytics data by your personas. Capture persona-specific metrics and analyze how each customer segment uses your website. If the behaviors of any one segment differs from your expectations for how the corresponding persona would behave, your persona may need an update. If a large amount of your website traffic is not easily captured by your persona-specific analytics, you may need to investigate whether another persona should be created.
User-testing data. Recruit users who closely reflect the defining attributes of your persona segments and watch them as they perform activities on your site in a usability study. Figure out if the participants’ behaviors, expectations, and needs match those expected from your existing persona description.
Support-center data. Routinely meet with support staff to maintain an understanding of what types of support calls they receive most often. Have there been any changes in the calls they receive? Support calls can be used not only for updating the product to address common issues, but also for monitoring changes in customers’ needs by tracking new questions, complaints, or points of confusion.

If you do identify a mismatch between actual customer behavior and your personas, explore further. Do additional research to better understand the conflict. It’s possible that this mismatch is caused by a design problem and the product simply needs to be fine-tuned. However, this mismatch could also be an indication that your personas no longer accurately reflect your customers’ needs, and they need to be revised.
How Frequently Should Personas Be Updated?
There is no defined schedule stating when and how often personas should be updated. The appropriate timing depends on many factors, including the variability of your business and user base or the nature of the projects and products supported by the personas.
To understand how often organizations update personas, we surveyed 156 user-experience professionals. Nearly half of respondents (46%) update personas every 1–4 years. The other responses were split fairly evenly: 28% of respondents update personas quarterly or more frequently, and the remaining 26% make revisions after 5 years or not at all. These data tells us how often revisions occurred, but not the extent of those revisions. Presumably, those teams reporting quarterly or more frequent revisions make small tweaks to personas rather than entire overhauls.

Most teams update personas every 1–4 years.

Our survey respondents also rated how successful the personas were in creating a better user experience in the final product on a 1–7 scale (1= lowest; 7=highest). Teams who did not revise their personas for 5 years were less satisfied with their personas’ impact than the other groups.
The average personas’ impact rating was 5.5 for the group with the most frequent updates (quarterly or more often), 4.5 for the group who updated their personas every 1–4 years, and 3.9 for the group who updated their personas least frequently (every 5 years or not at all). The differences between the very frequent group and the other groups were statistically significant, but the difference between the frequent (every 1–4 years) and rare groups (5+ years) was not.

Average personas’ impact rating segmented by revision frequency. The bars represent 95% confidence intervals.


Histogram of personas’ impact rating segmented by revision frequency

Adopting a quarterly revision schedule paid off for many projects.These trends suggest that teams who keep their personas up-to-date perceive their projects as more successful in improving the user experience of their final products than teams who fail to keep persona data updated. The effort of updating personas itself indicates that the team values personas enough. Those teams that let personas sit untouched for 5 years likely did not put personas to good use.
However, if you revise personas several times a year, consider the impact of these frequent changes on your team: are team members able to keep up with the changes or do they feel overwhelmed and confused? Any change should be communicated effectively. Explain briefly what changed and why. Make sure that all team members using the personas know about changes and get rid of any old copies of personas.
If you have not updated your personas in 5 or more years, chances are they are performing for your products about as well as a dull butter knife cutting steak. It’s time to take a good hard look at the validity of your existing personas and determine whether it’s time to sharpen the tool.
Conclusion
Monitor shifts in your users’ behaviors and needs and reevaluate your existing personas periodically to determine whether they still reflect your user base. Depending on the frequency and degree of the changes needed, you may have to tweak your existing personas with minor updates, refresh them with large changes, or retire them altogether and create new personas that better reflect your current business and customers.
Learn More
We discuss how to effectively update personas in our full-day course Personas: Turn User Data into User-Centered Design"
89,2016-02-07,"Designers often rely on icons to save space and to take advantage of the speedy recognition of visuals. With increasing popularity of small-display devices — smartphones, wearables, and so on — the use of icons has likewise increased. But, how usable are these icons? The only way to know whether a particular icon will work is to test it with users.
What Do You Test When You Test an Icon?
Different testing methods address different aspects of icon usability. But what makes an icon usable? Here are 4 quality criteria for icons:

Findability: Can people find the icon on the page?
Recognition: Do people understand what the icon represents?
Information scent: Can users correctly guess what will happen once they interact with the icon?
Attractiveness: Is the icon aesthetically pleasing?

All of these issues will be critical for the success of the final design, but must be considered separately to determine how to improve an icon.
Methods for Icon Testing
There are several techniques for evaluating icon designs, and which one you use will depend on your goals and on your stage of design. The methods can be separated into 2 main categories: out-of-context and in-context testing, depending on whether the icon is shown to the user in isolation or in the context of the actual interface. More importantly, however, is choosing a method based on what you need to learn in order to move forward with your design confidently.
Keep in mind that, even with methods where the icon is presented out of context, your test participants should always be part of the intended target audience and thus familiar with the overall industry and with relevant concepts.
Findability Methods
To gauge findability, icons must be shown in their native habitat — in the context of the full interface. In-context testing can help you determine if multiple icons appear too similar and users will have a difficult time distinguishing among them, or if the icon is hidden under a false floor or in an ad-rich area and is thus overlooked.
Time-to-locate tests are the best measurement of whether or not users can easily find an icon (or some other interface element) among the crowd of the full design. In these tests, participants must click or tap the UI element to achieve a given task. Measure how long it takes people to successfully select the correct icon, as well as the rate of first-click selections (that is, how often their first click is on the right icon: wrong selections indicate that the icons are not suitably differentiable, while slow-but-correct selections are a discoverability issue).
Recognition Methods
Testing for recognition is best done out-of-context: the icon is shown in isolation, in the absence of a text label or of other interface elements. Users presented with an icon must guess what that icon symbolizes. In some ways, this is the icon version of a Rorschach inkblot test. The purpose of this test is to make sure that icons are recognizable, and that people can easily deduce the object that it depicts.
Look for common phrases and terms to gain insight into people’s initial interpretations. If users’ guesses don’t relate to what you intended to represent, ditch that icon idea and start over.
If you know that your icon will be accompanied by text, you may think that it would be reasonable to show users the label and ask them to select the icon that best represents that label among several possible options. However, we don’t particularly advocate this method, because in real life some users may actually ignore the label in the final UI and look only at the image. This testing method therefore only makes sense in cases where users would somehow already know to look for a particular functionality within an interface, and are simply trying to locate a matching graphical representation (which is not a common circumstance).
Information-Scent Methods
What matters in the end is not only whether users can recognize what real object the icon resembles, but also if they can infer what functionality that icon may stand for. In fact, as long as people understand what function a symbol represents, it doesn’t matter if they don’t know what the object is — we needn’t worry about finding an old floppy disc to present to youths as long as they continue to understand that the strange square symbol means Save.
The same out-of-context testing method used to assess recognition can also be applied to judge information scent. However, rather than simply asking people what the icon may represent, instead ask what they would expect to happen if they selected that icon. Unlike for recognition tests, you should provide some minimal contextual information about the type of system where that icon will appear. For instance, study participants may be told that a suitcase icon is part of an e-commerce website and asked to guess what the icon may denote in the context of that type of website. (Note however that no specific information about what that website may look like, nor any hints to possible functionality are actually provided to the users.) This general reference framework allows researchers to understand whether the mental model behind the iconography corresponds to users’ expectations.
Later down the road, A/B experimentation can be used to help designers assess which out of several candidate icons may have the stronger information scent. With this form of A/B testing, a percentage of users are exposed to one version of the icon on the live site, while others see another. Measure any difference in interaction rates between versions of the icon, as well as whether users click on the icon and go back to the original page very quickly. This behavior is called probing, and usually is a signal of poor information scent; it indicates that users were disappointed in the content behind the icon and hence returned to the prior page. Be sure to maintain the same position and label for the icons when testing for the optimal graphic, to make sure that no other variable produced the change in user behavior.
Attractiveness Methods
Besides testing for recognition, icons should also be tested for attractiveness, both individually and as part of an icon family. One of the common reasons to use icons in the first place is to add visual appeal to a design, but not all icons are equally good-looking.
The simplest attractiveness test is to ask people to score each icon on a 1–7 scale. If you have alternative designs of the same icon, you can also ask people to pick the most attractive from each set of alternatives and explain why they like or dislike particular images. Finally, you can show people an entire icon family and ask them to pick out the one they like the best and the least. This last test can help you avoid the common problem where most of your icons are fine, but there are one or two less attractive ones that require a do-over to better match the aesthetic of the full design.
Standard Usability Testing and Icon Testing
Standard usability testing can also reveal issues with an icon. However, keep in mind that there can be many reasons why an icon may be ignored in a standard usability test, some unrelated to the usability of the icon per se. For example, users may get distracted by some other elements in the interaction or in the site design and may not get to ever complete the task. And even if the icon design is at fault, it’s hard to pinpoint exactly which of the features of the icon are problematic: is it the case that people cannot recognize the icon, cannot understand what its meaning is, or maybe they simply cannot find it?
For example, no participant interacted with a clock icon representing Recently Viewed Pages during a study of a controversial site redesign, but it’s hard to know whether the reason was because they did not notice the icon in the header or because the clock did not have a clear meaning in that context.
Because of the multitude of factors involved, you should not rely on standard usability testing as the only way to determine the usability of your icons.
Phases of Product Development
Like with all UX research methods, when choosing a testing method for your icons consider the current stage within the project lifecycle.

Strategize: In this early concept phase, focus on methods to ideate and explore numerous design options. Out-of-context techniques for recognition and information scent are the most applicable during this time, to determine the feasibility of using icons at all and to hone in on the appropriate mental models for the icons.
Execute: During this design–and–implementation stage, focus on research methods that will continually guide you toward the best icon design for your system. Once icon designs are recognizable, focus on out-of-context testing for attractiveness until clear winners emerge. Once more of the UI is designed, transition to in-context icon-testing methods. Time-to-locate testing is helpful to quantify the findability of an icon and its placement within several potential variations of the interface. Usability testing (beginning with paper prototypes and transitioning to higher fidelity versions) can give you some additional insights about the expected meaning for icons and their discoverability.
Assess: Once the system or feature is launched, methods that measure success and allow for incremental improvements are the most applicable. Benchmark testing with usability studies and time-to-locate tests can be conducted periodically to track performance. To continually improve an icon, A/B experimentation is the best method to measure performance and determine the optimal design.

Other Considerations
As with all methods of research, be sure to avoid introducing biases in tests. Pay particular attention to the terms used in the phrasing of tasks, as they can easily prime the imagery associated with the icons. Especially for out-of-context testing methods, consider conducting the study multiple times with various ways to phrase the survey question (using synonyms, omitting branded terms, and so on) to ensure that task wording did not influence the response selection.
Not all these testing methods need to be used in order to reach a usable icon design, but they are each helpful for different purposes and at different stages of the design process. Additionally, each method should be used iteratively, to incrementally move toward a meaningful icon–label relationship, and an optimal placement within the interface.
For more information on icons and their usage in mobile designs, consider our full-day training course on Visual Design for Mobile & Tablet."
90,2015-10-25,"Personas are representations of a cluster of users with similar behaviors, goals, and motivations. As such, personas are fictional, yet still realistic because they embody the characteristics and behaviors of actual people. In product development, personas are used to communicate information about users and create a common ground among different team members by unifying their views and understanding of target users. Many design and development teams create personas early on in a project to ground their product decisions in user data and achieve user-centered design.
Personas are a familiar, but often underutilized tool in the field of user experience. Many organizations perceive persona efforts as time consuming or too complex to undertake in a design project. To understand more about the “typical” persona project, we surveyed 216 user-experience professionals who shared their approaches to creating personas and the time it took their teams to conduct or review research, analyze data, and craft the personas.
Empirical vs. Nonempirical Personas
Organizations take various approaches to creating personas, ranging from empirical, data-intensive efforts that combine qualitative and quantitative data sources to nonempirical, assumption-based efforts that rely on existing knowledge about the user population. (Unless the personas are pure guesswork, which we don’t recommend: you should design for the customers you actually have and not for dreamed-up customers you might hope to have.)
Data-driven efforts are informed by specific empirical user research (both qualitative and quantitative) conducted for the purpose of creating the personas. These more extensive research undertakings may use insights from a variety of sources, such as usability testing, surveys, analytics, field studies, interviews, and market research.
On the other hand, other companies effectively create personas with little to no empirical-research data by taking advantage of assumptions and existing organizational knowledge from previous interactions with target users. Teams working with lower budgets, tighter schedules, or limited resources may choose this less data-driven approach, so they can still benefit from a user-centered focus but spend less time and money on the persona effort. I like to think of this type of personas as crowdsourced, as they are often created collectively by extracting insights from many individuals who have a deep understanding of the target user group. Personas should not, however, be completely fictional; they should be built on real insights from several people, and eventually validated for relevancy.
Time Spent Creating Personas
Our survey data offers an understanding of the level of investment that is involved with persona-related efforts and can help organizations plan ahead and evaluate the budget and resources needed for their projects. This data has implications for teams considering whether to create personas for their project and to what degree.
Regardless of the type of approach taken to persona creation, the process involves 3 different stages:

Conducting research or gathering data
Analyzing data
Crafting the persona

Given that the empirical and nonempirical approaches to persona creation involve completely different types of work, to look at the amount of time it takes to create personas, we segment the survey data by the type of effort undertaken: primarily empirical or research-driven, or primarily nonempirical or assumption-driven. We also split the data by company size: teams with vast resources may have more time or budget available for persona efforts in comparison to teams at smaller organizations with potentially fewer resources. Additionally, we break out the total time by the 3 stages of persona creation: conducting research or gathering data, analyzing data, and crafting the persona.

Figure 1: Staff hours taken to create personas by company size and amount of empirical research. Organizations with 500 employees or less are considered small companies; organizations with more than 500 employees are considered large companies. Personas based on more than 50% empirical-research data are considered empirical. Those based on 50% or less empirical data are considered nonempirical.

Efforts Range in Size
As expected, small companies spent less time creating personas. The time expended ranged from 22.5 to 72.5 staff hours, depending on the amount of empirical-research data informing the effort. For large companies, timing ranged from 55 to 102.5 staff hours.
Based on these results, smaller companies could realistically expect to create a persona in 3 to 9 working days if only one employee participates in the process, or just about 1.5 to 5 working days with 2 employees. For larger companies, a team of four could effectively research and create personas in 2 to 4 days. These numbers are medians, so there are of course instances where the effort could be much higher or lower.
Proportion of Empirical Research Is Not Dependent on Company Size
Both large and small companies reported that around 54% (54.5% for small companies, and 54% for large companies) of their personas were based on empirical research. Although smaller companies spent fewer hours creating their personas, they based them on empirical research in approximately the same percentage as their larger counterparts.
Happily, we can say that although personas take less time to create at smaller companies, the methodology for approaching persona research is as data-driven as that of the larger companies. Smaller companies are not saving time by limiting the amount of research-based insight that goes into creating them; they are simply taking less time for the effort as a whole.
Why are small companies more efficient? That’s a topic for future research, but possible explanations include having fewer stakeholders with competing goals, simpler approval processes, and spending fewer staff hours on large-team meetings to discuss the project. Smaller companies may also have smaller, more tightly defined target audiences that are easier to study.
Empirical Research Increases Time Investment
Personas based on empirical data take longer to create than those based on nonempirical data. For both large and small companies, the amount of empirical research used to create the personas increases the timeframe of the effort as a whole. This increase is 86% for large companies, and 222% for small companies. Thus data-based persona creation almost doubles the time budget for large companies and it more than triples it for small ones. (Why is there a difference between small and large companies? Simply because the baseline in our comparison — the time taken for nonempirical personas— is so much lower for small companies than for large companies, as Figure 1 shows. )
Gathering Data for Personas Takes Most Time

Figure 2: Percentage of total time by persona-creation phase, split by empirical vs. nonempirical persona creation: Regardless of the type of approach, the biggest time-share was taken by Phase 1 (conducting research). (Percentages in the chart were calculated by calculating percentage by phase per company in each segment and averaging these values.)

Regardless of the empirical or nonempirical approach, teams spent most time in the first phase of the persona-creation process — conducting research and gathering data. The other two phases (analyzing data and crafting personas) were smaller, with data analysis taking somewhat longer than crafting the actual personas. This ordering holds true for both small and large companies ­— although smaller companies take less time overall, both large and small companies spend a similar percentage of their time in the respective phases of persona creation.
Teams creating nonempirical personas spent a smaller percentage of the overall effort in Phase 1 (conducting research), compared with teams that took an empirical approach (41% vs. 45% respectively; however this difference was not statistically significant). However, even for the personas based on a majority of nonempirical data, the research phase still existed and in fact took the largest share of the overall effort. This phase likely included some amount of empirical research, as well as gathering assumptions and existing insights from stakeholders and other individuals with a deep understanding of target users. Therefore, the research still exist, but, as Figure 1 shows, in absolute terms, it is less time intensive than for projects that are primarily empirical.
A final conclusion from Figure 2 is that the actual creation of the personas is the step that takes the least time. Of course, the personas are the goal for the entire process, but creating this deliverable is only the tip of a more substantial iceberg.
Planning For Personas
Personas can be a powerful tool for focusing teams and creating user-centered interfaces. Not knowing how much time and effort are required to create this supplementary design tool could deter some project teams from investing in them at all. Ignorance is fear, but knowledge is power: our research can help teams decide whether to invest in personas, to what degree, and using what approach.
Now you know that personas won’t break your budget and can be created in half a week in a pinch. (Followed by the later step of updating the personas from time to time.)
We discuss creating personas and selecting the right size of persona effort for your company in our full-day course Personas: Turn User Data into User-Centered Design."
91,2015-04-12,"When people have problems finding content, both time and clicks are wasted hunting through extraneous pages on a site. The hub-and-spoke pattern of navigating from a routing page—a search-engine results page (SERP), section landing page, product-category page, or similar—to a page deeper in the site’s hierarchy, then immediately back to the routing page is referred to as pogo sticking.
Recently, I found myself pogo sticking through the Redbox website. Why, you ask, was I doing this? I had wanted to walk to my nearest Redbox location, to get a little exercise and avoid an extra trip in the car, and grab a movie to watch that evening. On the website, I searched for my specific Redbox kiosk location and was shown a list of movies to browse. Upon seeing a movie that interested me, I clicked to get more information only to find that it was not available to reserve at my chosen location. Disappointing. I returned to the list of movies to try another option and again it was not available. It became clear that this browse page contained numerous movies that were not available at my chosen location. This did not match what I had specifically asked for! The excess work and frustration nearly dissuaded me from renting a movie altogether—had it not been for others in my company wanting to watch a new movie, I would have just streamed something on Netflix.
When a company’s website makes customers think lovingly of the biggest competitor, something is wrong.

Pogo-stick behavior on the Redbox website: After specifying a kiosk location, the list of movies still included options not available to rent at the chosen kiosk. This did not match the expectation set by selecting a location, and forced me to pogo stick through each movie in the list until an available movie was found. Note: the browse page does display a tooltip showing availability when a user hovers over a movie, but the delay is too long and the hidden help can be easily missed.

The Problem with Pogo Sticking
Pogo-stick behavior is often discussed in regards to people bouncing off a site and returning to an external search engine. While this pattern may be generated by a poorly designed landing page, high bounce rates could also be due to visitors who are not truly interested in the product or service offered, and thus are not valid prospective customers.
Within a website, pogo-stick behavior is an indication that people are struggling to find relevant content. Navigating repeatedly through the same pages quickly becomes tiresome, and is highly discouraging. Users in this situation may give up entirely, or settle with the most satisficing answer encountered during the arduous journey through the site. Because of the excessive effort exerted, these users are not likely to extend their visit for a second purpose, nor do they ever return. This demonstrates the interaction elasticity of websites, where a higher interaction cost results in lower usage over time.
Users pogo stick from a page because the content found upon arrival was disappointing—somehow the destination did not match the expectation set by the page leading to it. This mismatch between a link and its corresponding page chips away at the credibility of the website, and at the brand as a whole. People resent sites that trick them into spending a click while getting nothing in return. In user’s eyes, these are not only wasted clicks, but also wasted time.
Identifying Pogo-Stick Behavior Using Web Analytics
Most often, pogo sticking is discovered by directly observing users during usability tests. However, web-analytics tools can also help identify this behavior and monitor improvements once changes have been implemented. The process of examining the sequence of pages that a user has visited during a single session on a website is known as path analysis.
The tool needed to find this data may be named differently in various web-analytics platforms, but it should be available in each of them. Analyzing specific navigational paths within a website can be a time-consuming task, so it is best to first identify those pages that are candidates for pogo sticking. Suspects may include your homepage, landing pages for main sections of content, your site’s internal search-results page, and product-browse pages. Prioritize which pages you tackle first, based on their visibility to your audience (overall unique traffic) and their potential for improvement. (Some personal judgment will be required for the latter, but you’re likely to gain the most from fixing those pages within your conversion funnel that have a high exit rate.)
The most simplistic way to identify a page where pogo-stick behavior may be occurring is to look at routing pages with a high ratio of page views to unique page views. This measure indicates that many users repeatedly navigate to those pages. However, this number alone does not necessarily mean that users pogo stick or otherwise fail to find information.
The exact same click behavior can be indicative of a positive user experience instead of pogo sticking if users repeatedly follow links from a routing page because they find all the destinations interesting. Consider, for example, a newspaper homepage with many news stories. If a user clicks, say, five of the headlines and reads all those articles, then this is good, not bad. On the other hand, if a user clicked those exact same five headlines and was disappointed with every single article (because of misleading teasers), then it would be bad. Simply counting clicks won’t differentiate between these two scenarios.
To get a better picture of how users interact with a routing page and with the pages it links to, the data must be narrowed to the session level and exclude paths where the user visited several pages before returning to the routing page or when the user spent substantial time engaging with the destination pages. In other words, you want to include only patterns of the type X-Y-X, where X is a routing page and Y is a page linked from X.  By creating a segment identifying that path sequence, you can measure the percentage of users exhibiting this pattern of navigation, and monitor it over time.

Example of setting up a path-sequence segment using Google Analytics. Note the use of is immediately followed by… rather than the is followed by… option, in order to properly separate visits navigating directly between the specified pages without any detours.

Investigate Using Qualitative Methods
Once pogo sticking has been identified, follow up with qualitative research—usability testing—to determine what may be spurring this behavior. Are people looking for a page or some content that they expected to find but doesn’t actually exist (or at least, isn’t available through that page)? Did a link on the page promise cute pictures of kittens but instead presented users with a wordy article scrutinizing society’s infatuation with prepubescent animals? Or, did that browse page not clearly display key differentiating information for people to easily find the product that fulfills their need—like what happened to me on the Redbox website? Quantitative data from analytics cannot explain why a behavior occurs, nor how to improve the experience, but only if and where that behavior may be happening.
Here are some of the common problems that can cause pogo-sticking behavior, along with suggested solutions and research activities that can be used to uncover the best way to implement those solutions.



Problem
Solution
Research





Users want something that is not currently on your site.


Create that content.


User research—field studies, task analysis, and search-log analysis (people tend to search for something if they want it but can’t find it)—can identify users’ top information needs.




Misleading links cause people to be disappointed after clicking.


Write plain-spoken links, using user-oriented language, instead of ‘teasing’ links or links using your own internal terminology.


Search-log analysis can reveal the users’ preferred terminology, as can listening to users thinking aloud during usability studies.




Users can’t determine which of the items listed will meet their needs.


Improve the information scent of each item’s description on the routing page.


Discover the strongest differentiating parameters by watching users solve problems in usability studies.




After fixing a pogo-sticking problem, you will probably observe an immediate decline in your page views. But this is good! Users aren’t wasting as much time and effort navigating to pages they don’t need, and so the overall page count drops. This is an example where a more nuanced approach to interpreting website analytics data is required than simply wanting ever bigger numbers. Additionally, if you track analytics data for returning visitors you will likely find an increase in user loyalty, driven by improved usability now that people can easily find what they’re looking for. Higher loyalty will again drive increased page views over the long term. But short-term numbers will be down, and that’s positive because it means that you stopped annoying users with wasted clicks.
Learn more tips on using analytics data to identify usability issues during our full-day course on Analytics and User Experience.
"
92,2015-04-05,"It’s usability-study day. Your first participant walks in. You brief him, explain the session, and dive into the study. Only to realize that the participant does not understand what you’re asking him to do. It’s not a problem with the site design — it’s a problem with the study itself. You’ve just begun your study and you’re already digging yourself out of a hole, trying to quickly determine how to salvage the session. Now what?
When running qualitative usability studies, we often recruit only a handful of users. Small numbers of users can lend great insights into design and usability strengths and weaknesses. So if users struggle not with your site design, but your study design, it can be difficult to get the necessary results. Session data may need to be thrown away, depending on the severity of the problems.
Pilot testing can go a long way to alleviating such problems. In a pilot test, the usability practitioner (and team, if possible) runs through a session or two in advance of the main, scheduled study. Typically only a small number of sessions are needed to prepare for the full study and make sure everything is in order. The point of the test is to test the study itself, running 1–2 sessions to help ensure that the full study goes as smoothly as possible
Pilot testing is particularly important if you are:

New to running a usability test. Better to have your first try with a session that you can throw away if needed.
Testing in an unfamiliar subject area. If this is your first test of a site aimed, for example, at rocket scientists or nuclear physicists, and you aren’t an expert in the area, pilot tests can help prepare you. (Also do work ahead of the study to familiarize yourself with the topics and terminology, for instance by meeting with subject-matter experts as well as designers and developers.)
Running a remote, unmoderated study. Any time that study instructions need to stand on their own, they need to be tested to try to limit possibilities for misinterpretation. If you are running a diary study, unmoderated online study, or study conducted via email, all communications, from recruiting information to confirmation emails to study instructions to follow-up questions, should be pilot tested. These instructions need to stand on their own, as no one will be there to answer questions or make clarifications if a participant runs into a problem.
Running a quantitative study. Larger scale studies are typically done so that statistically significant results can be calculated. In such studies, each session needs to be run the same way. As such, a solid script must be created and tested.
Testing a high-visibility project. Even if you’ve worked in usability for years, there may be the occasional high-visibility project that requires some extra care. Maybe the results are going straight to the CEO. Maybe the test is on the company’s premier product. Of course, every usability project is important, but some may be a bit more important than others.
Doing a one-shot research project. If you’re doing many rounds of iterative testing, then the damage is limited if you make a few mistakes in the first test. The second study will be better. But if a design project only gets a single dose of user research, you want to get it right the first time, because there won’t be a second time.

Even veteran usability practitioners can benefit from running pilot tests. The longer you work in the field, the better you can get at writing strong tasks and instructions in the first place, but it never hurts to run a test plan past a participant in advance. Does every usability study require a pilot test? No, but it’s extra insurance that the study will run smoothly, leaving the team able to focus on the results, rather than the study itself.
Benefit #1: Dress Rehearsal
Pilot testing is a dress rehearsal for the study. It’s a great dry run to make sure the facilitator and team are prepared for the study. Are the materials printed? Consent forms copied? Payment prepared? Site ready and functional? Checklists can help, but running through the study in a low-pressure setting first is a good way to double check that the team is prepared.
Benefit #2: Test the Tasks
If a user is derailed by a poorly written task, it takes precious time away from studying the interface. The facilitator first needs to realize that the participant didn’t understand the task as it was intended, and then has to spend time redirecting or reinstructing the user. Any on-the-fly changes run the risk of accidentally giving users clues about how to complete an activity, or making participants feel they did something wrong. Tasks that can seem perfectly clear to you and your team can be confusing or misleading to participants. Find and fix issues with tasks before your study to help ensure a smoother test and stronger results.
Benefit #3: Timing
It can be hard to know how much time to schedule for user testing. Will participants complete 3 or 7 tasks during your testing session? Running through the tasks yourself and allowing more time than it took you is a very rough way to estimate task time. Running through the full study with 1 or 2 pilot users can help you better estimate how long users might take, allowing you to prioritize tasks effectively for the real study.
Benefit #4: Data You Can Use (Maybe)
If all goes well, or pretty well, in a pilot session, you’ve gotten a jump on the rest of the study. Run the session as though it’s the real thing, and if there are no major hiccups, the data can be used. Even if one or two tasks go awry, you may still be able to salvage information from the sessions. Don’t be too quick to dismiss any learnings just because it was a pilot test. At the same time, don’t be overeager to use the session as part of your final learnings if the session does not go well or something goes wrong at the beginning that might affect the reliability of the rest of the session (such as leading the user or overexplaining an offering or interface.)
Tips for Pilot Studies
To get the most out of a pilot session, schedule it at least 1 day in advance of the scheduled study, and longer if you are testing instructions for an unmoderated study where you might need to pilot instructions several times before they are solid enough to use. This allows you and your team time to make any necessary changes. (Assume there will be changes, because there always are.)
Recruit participants who match your target profile for pilot studies. This means that any feedback — about the study or about the site being tested— will be more relevant. In a pinch, recruiting someone who doesn’t quite fit the profile is typically better than not running a pilot test at all, but the results from those sessions then would not be applicable in the final study.
Does running a pilot study take extra time? Of course. Materials need to be ready earlier, extra participants need to be recruited, and time needs to be scheduled to run the sessions. The payoff, however, is that the final study will run more smoothly, making it easier to get the results that matter to your team and, ultimately, your product."
93,2015-02-16,"The field of user experience centers on the idea that we must design products around people, rather than teaching people how to use products: user-centered design (UCD), not technology-centered design. In order to do so, we must understand people—their behaviors, attitudes, needs, and goals. Whether the final product is a website, software application, mobile app, or interactive kiosk, a user-centered design can only be achieved if we know who is going to use it and if that knowledge informs our design. An entire arsenal of user-research methods can be employed to achieve a user-centered design. Personas are yet another tool that can be used to encourage decisions based on a real person’s needs, and not on those of a generic and undefined “user.”
Definition of a Persona
A persona is a fictional, yet realistic, description of a typical or target user of the product. A persona is an archetype instead of an actual living human, but personas should be described as if they were real people.
The description should be thorough, including details about the persona’s needs, concerns, and goals, as well as background information such as age, gender, behaviors, and occupation. This focus on a singular individual—or a small set of individuals, if using multiple personas—fosters empathy for the specific users we are designing for, and helps us break away from the attempt to design for everyone. A persona doesn’t need to document every aspect of the imaginary individual’s life, but rather should focus on those characteristics that impact what is being designed. It is likely that a business will have several personas to cover the various aspects of their organization, with one or two of them identified as the main targets for each product or service, feature set, or content area of a website.
Personas work because designers and developers have the same tendency as all other people to be captivated more by concrete instances than by abstractions and generalizations. We need all product-team members to empathize with users and be willing to go the extra step to develop something that will work for the actual users. But if users are described in statistical terms and as broad profiles, that information will simply not lodge itself as deeply in team members’ brains as a distinct persona will.

Example of a persona, targeted at helping design the About and Careers content for a company website.

Personas Are Not User Groups
Defining user groups or market segments is not the same as creating personas. When discussing broad categories of users, ranges must be used in order to summarize attributes of the entire group. These statistics are too impersonal, and are difficult to keep in mind when designing. In contrast, a persona is a singular user derived from these data ranges to highlight specific details and important features of the group. It thus creates a narrative that is much more digestible and memorable, which in turn increases the likelihood of continued use throughout the design phase and beyond.
When to Create Personas
“As early as possible” is the best guideline for when to create personas. Of course, personas must be based on user research in order to be at all accurate and representative of actual users of a product. Personas are made-up people, but they should be made up based on information about real people. (Imaginary-friend personas that you dream up without any basis in the real world may describe the users you hope to get but will not reflect the way people actually are. Design for somebody who doesn’t exist and you’ll have no customers.)
Ideally, the persona-creation process should be a part of the research phase for a product or feature, before the actual design process starts. Field studies, surveys, longitudinal studies, interviews, and other methods of user research should be conducted first to define characteristics of typical users. (Keep in mind that any self-reported data, such as that resulting from focus groups and surveys, is possibly misleading and should be verified through other methods.) Once user research has been completed, personas and scenarios can then be derived from that data.
Guidelines for Creating Personas
Persona creation is best done as a team, not because it is difficult, but because it will garner more support for the use of personas from team members able to contribute to the process. One of the main arguments against using personas as a design tool is that they are not realistic. By including team members in the process and exposing them to the raw data from user research, it will be clear that the traits and behaviors of each fictional user are based on actual aggregated data accumulated from real users.
To initiate the persona-creation process, start with identifying characteristics of users observed from user-research activities. Group these attributes into clusters to begin forming clear characters. If several seem too similar, merge them together or eliminate any groups that appear less important to the business. Once distinct roles emerge, add details to make the character more realistic, believable, and memorable.
Common pieces of information to include are:

Name, age, gender, and a photo
Tag line describing what they do in “real life”; avoid getting too witty, as doing so may taint the persona as being too fun and not a useful tool
Experience level in the area of your product or service
Context for how they would interact with your product: Through choice or required by their job? How often would they use it? Do they typically use a desktop computer to access it, or their phone or other device?
Goals and concerns when they perform relevant tasks: speed, accuracy, thoroughness, or any other needs that may factor into their usage
Quotes to sum up the persona’s attitude

Your goal should be to create a believable and alive character. Avoid adding extraneous details that do not have any implications for design. While a name and photo may seem irrelevant, their function is to aid memorability, which is the #1 job of a persona: to make sure that all team members remember the users they’re building the product for. On the other hand, a lot of unessential details can overwhelm the relevant ones and make them harder to remember.
Thus, each piece of information should have a purpose for being included: if it would not affect the final design or help make any decision easier, omit it. For example, identifying a persona’s favorite variety of wine would likely not help when designing an intranet, but could be very useful when designing a forum or marketplace for sommeliers and serious wine enthusiasts. Behavioral qualities such as being detail oriented may imply that the individual would need to see specific data points or aspects of a product before being convinced to buy, while valuing speed could suggest a need for an efficient and streamlined design.
Persona-Focused Design
The main benefit of using personas is that they create a common, more precise vocabulary for describing a certain type of user and thus focus design efforts on a common goal. In meetings, the persona’s name acts as shorthand for the full set of attributes, desires, and behaviors that need to be considered when making design decisions. If instead you tried to reference what “the users” would want, each individual in your meeting would interpret who those users are and their needs differently. Further, this “user” definition would shift based on each person’s mental model of what is being discussed. Framing a statement around a specific persona breaks the listeners out of self-referential thinking and removes the speaker’s reliance on opinions, shifting the discussion away from personal judgments toward that particular persona’s needs. Once the team can easily picture the same set of users, it can create better designs for them.
When making design-strategy decisions, those functionalities that would most benefit a persona should inform which features to implement and prioritize. Usually, one or two personas should be identified as the main targets for a product, or for a feature set within a larger product. Avoid wasting time designing and building extraneous features that aren’t useful to the target persona(s). This practice of focusing on only pleasing a particular user—or even two users—helps designers avoid having to create an interface that handles every edge case imaginable. You can’t design something to please everyone! But, it is possible to design an experience that is enjoyable and usable to a defined set of target users.
Ongoing Benefits of Personas
Strategizing and making design decisions are the primary uses of personas, but there are several other ways in which they can be of use past an initial design phase. In fact, even if personas were not created at the onset of the design phase, it’s still worthwhile to create them for these later activities.
When working with an outside agency or consultants, clearly defined personas provide an easy way to describe the target audience of a product or service: just list the personas’ top attributes, or hand over the full documents for the agency to reference. Once a design is created, personas can be used as a guide for expert reviews. For each top task or area of a website or application, consider how each persona would deal with the process to determine potential issues within an interface.
Recruiting participants for usability studies can also be made easier with personas. Consider personas as templates for prospective recruits; common traits across several personas or a persona’s distinct characteristics might be useful screening criteria for some if not all of your study participants.
If the website or application is already live and running, personas can be used for segmenting analytics data to evaluate the behaviors and use of real users. These segments are not only useful in cutting down the sheer amount of data to analyze at a time, but they also support the ongoing validation and refinement of the personas as living documents rather than something too precious to ever alter.
For more information on creating and using personas within a project lifecycle, consider our full-day training course on Personas: Translating User Data Into User-Centered Design Solutions."
94,2014-12-07,"So, Your Site is Ready for a Redesign
Where should you start? There are many UX activities that can help your project start off on the right foot. One of them is competitive usability testing. That is, usability studies of the existing design along with several competitor websites. Resist the urge to jump on your computer and start designing right away.
Even if a redesign (or refresh) is required, don’t be so quick to throw away the existing design. You can learn from it. Use it as a starting point for your new project by gathering user feedback and feeding it into your new design. Before you begin, know how aggressive a change you need. For Agile teams, competitive testing is a worthwhile activity to perform during sprint zero.
Your old site is the best prototype of your new site: it’s already fully implemented and it solves exactly the design problem you’re targeting: a website for your business.  For sure, the old site doesn’t solve the design problem perfectly, but it’s not enough to say that it’s bad. You need to know in which specific ways it’s bad. And you also need to find out what aspects of the old design are loved by users. The short history of the Internet is rife with examples of major web properties that redesigned only to be met by a storm of user requests for great features that had been inadvertently removed.
Competitive usability studies provide a way for you to assess different design patterns and user flows, and determine which concepts might work and not work for your audience. They allow you to test ideas without having to build the designs yourself and to discover new interactions for your site and avoid mistakes made by others. Usability studies lay the groundwork for the redesign process and keep you and your team focused on the right issues. You feel more confident about your decisions because you’ve witnessed people interacting with websites in similar situations.
We’ve all wondered about our competition — How well are they presenting information? How are they better than us? It is good to browse websites for inspiration. However, don’t stop there. Test them. If you find concepts and ideas you like, don’t rush to copy them. The design might look good but might not be usable. Minimize risk by testing designs and know for certain whether or not they work before it’s too late. Resist the temptation to emulate without proper knowledge. This is why we often find bad design replicated across many different websites.
Learn From Competitive Usability Tests
Teams waste much time debating over design solutions and making decisions based on personal preference or bias. Rather than continue the debate, competitive testing allows teams to get feedback from target users early, so they can make informed decisions. It’s much easier to argue over opinions than user data.
Competitors’ sites are the second-best prototypes of your new site: they solve almost the same design problem as you have, and they are fully implemented instances of designs that these other companies have invested major resources in polishing.
Competitive tests can help you with the following:

Evaluate future features. Before you offer or build a new feature, learn whether customers consider it valuable or how it could be designed better. Seeing how customers react to features on the competitor’s site can help you determine whether they’re worth the effort.
Examine similar features. Your site may offer features similar to those on competitor sites, but one process might work better than the other. By examining varying designs of similar features, you can quickly identify the elements that work and make your design better while avoiding mistakes made on other websites.
Discover better ways to doing things. Your site might test well, but testing other sites might reveal features and interactions that you haven’t thought of. Having people react to different designs gives them an opportunity to compare and contrast. Participants are often better at articulating their thoughts and retrieving memories when they have several examples to which they can refer.

Conducting Competitive Usability Tests
Competitive studies don’t need to take long; usually 2 days are sufficient. Spend a few days studying your website along with 1-2 competitive websites and the study will yield fruitful findings that answer complex UX questions and provide inspiration for new ideas. 2 days is quick—a small price to pay in the context of a project timeline.
We run most competitive usability tests based on the thinking-aloud methodology. In this type of study, each participant performs the same tasks on every website (e.g., Site A, Site B, and Site C) while thinking out loud. We observe how people interact with each website and note verbal feedback. At the end of each session, we may ask users which version they prefer and why. The key is to understand the rationale behind people’s behavior.
To combat order effects, alternate the sequence of the websites that participants evaluate. For each participant, keep the number of websites they evaluate at 3 or less. When evaluating too many sites, tasks become monotonous and difficult to track for participants.
Select sites with interesting features. The sites you select do not need to be direct competitors. It’s best to aim for diversity by comparing sites that have features and designs distinctly different than yours. Don’t waste your time testing sites that you know are bad. The study will generate more valuable ideas when you include sites that might outperform your site. It’s OK to lose now you will win later.
Conclusion
Before you redesign your site, make sure that you understand the strengths and weaknesses of your current design. Garner design ideas and alternatives by studying your competitors. The focus of competitive tests is not to crown a winner, but to gain deeper insight into why design elements work or fail so we can make informed decisions moving forward."
95,2014-11-30,"Many teams create personas during the initial ideation and design phase of a project, but then fail to leverage them beyond settling design debates. One method of incorporating personas into the ongoing maintenance of a website is to create persona-inspired segments in your analytics tool. These segments can not only check whether the user described in a persona is characteristic of your website’s real visitors, but can also help you uncover patterns of use and trends in behavior that would otherwise be masked when lumping together the data for all visitors to the site.
What Is a Persona?
A persona is a fictional character representative of a unique group of users who share common goals. An organization should possess several personas to reflect the variety of visitors that its website attracts (most commonly, 3–7 personas will cover the majority of an audience without creating too specific or spurious distinctions in your user types). These user archetypes should ideally be based on qualitative, ethnographic user research in order to include accurate behaviors, environments, attitudes, and needs of real users. Personal details such as a name, photo, and specific contextual narratives should be combined with a description of gender, age, marital status, job title, device ownership, and other demographic information to create an easy-to-imagine, relatable character.
Benefits of Using Personas
Without personas, organizations risk creating a one-size-fits-all design expected to suit all users. For example, during our research for our E-Commerce report series, we identified 5 distinct types of e-commerce shoppers, all of which may visit the same website and expect different levels of detail and types of information about a product. Even a system as contained as a company intranet will have multiple types of users accessing it, all with different goals and tasks that they need to accomplish. Without identifying the various characteristics of the user groups visiting your site you cannot hope to design an experience that includes the key elements that each type of user needs. Instead, you will end up creating a website that doesn’t perform well for anyone.
A major strength of personas is that they focus design efforts around user types and their specific needs or behaviors, and facilitate discussions among team members and stakeholders. Framing the design conversation around a persona creates context and makes it easier to empathize with the users affected by the design decisions. (Another way to garner empathy: have the stakeholders observe a usability test.) Once there is a common understanding of customers across your team, these personas help establish a culture of verifying instead of assuming: rather than debating whether or not “people” need a proposed feature, ask, “How would this help Barbara?”
Persona-Inspired Segments
Once personas are created, it’s easy to let them fall into disuse when the site goes live. Don’t do that! A lot of time and research effort (hopefully!) went into forming these realistic representations of your audience, and return on investment increases if you can continue to leverage the personas to drive improvements. By creating a segment in your analytics tool inspired by the established personas, you are able to analyze how real users actually use the website or application. This information can validate any assumptions made during the persona-creation process. It also allows the refinement of the personas if new information is uncovered—personas need not be a document created once and never touched again. Additionally, revisiting personas through analytics is much more maintainable than continuously contacting customers to conduct interviews, diary studies, and other forms of intensive user research.
The key step to creating a persona-based segment in your analytics tool is to determine what characteristics of the persona to include in the segment filters. When reading through the persona details and any accompanying user stories, you must separate out the characteristics that are representative of that specific group of users from those added merely to lend verisimilitude. For example, a user story from a persona for a software–as–a–service (SaaS) may describe a return visitor, David, as follows:
“David is a return visitor. He receives our weekly email newsletter with tips on using new features on Monday morning at 10AM, immediately after his status meeting at the office. He clicks through from his Android phone. He has time to read a single blog post before his next meeting.”
When creating a segment to represent David, the exact time of day is likely too detailed to include as a filter, but there may be a difference in behavior between users who visit the site during the week and those who visit it during the weekend. Or maybe between visits happening during normal office hours and those during the evenings. The fact that he is a newsletter subscriber and already a customer of the service should also be included to distinguish this group of users from those who are still in the research phase and perhaps just learning about the software and its features. In this instance, the persona’s gender likely does not make a difference (but it would for an e-commerce clothing site, for example), so it should not be included when creating the representative segment.

An example persona for a fictional SaaS company. Different organizations may have personas that cover different topics—in this example, the persona reflects one user type that our SaaS company targets: the employees of agencies looking for software solutions for their clients.

To be meaningful and worth a separate analysis, the segment derived from a persona, once created, should display user behavior that is clearly different than the rest of the site visitors and should represent a sizeable chunk of your user population. For many, this reasonable chunk falls within 7–10% of overall visitors, but ultimately it is up to you to determine what makes sense for you and your organization. The amount of details from a persona used to create the corresponding segment can be adjusted to allow the segment to represent a larger or smaller group of users. Focus on the most distinguishing characteristics of that group of users first and foremost, and then add in more details if necessary to target a smaller subset of users.
Coming back to our SaaS example, in order to create a segment based on David, we would need to know what other details from the persona description may be meaningful. Is the fact that he uses his mobile phone to access the site relevant for the group he stands for? How about the fact that he uses an Android phone as opposed to an iOS or Windows phone? To know for sure why a particular detail was included, we might need to revisit the original user research conducted when the persona was developed. Additionally, we’d have to determine whether that behavior actually distinguishes the group of interest from another: do a lot of David-like users access the site on their phone and behave differently than other mobile users?  Or perhaps all types of users sometimes access the site on smartphones and do a small number of similar things? If the latter, it would be more useful to analyze mobile usage using that group’s overall segment rather than creating a separate mobile segment for each.
Examples of ways to segment users that may be derived from a persona include:

Demographics: Age range and/or gender
Geographic locations: specific countries, regions, urban vs. suburban, and so on
Device and/or browser
New vs. returning visitor, logged-in user vs. not logged in or doesn’t have an account
Source: arriving from an email, search engine, a specific social network, or set of referring sites
Whether the visitor has searched for branded vs. unbranded keywords or terms
Whether the visitor has reached a specific set of pages: for example, visited a product detail page, visited the customer service section, or landed in the content section aimed at trade professionals or wholesale buyers

This list is by no means exhaustive, and the set of characteristics included in a segment will vary depending on the site, its audience, as well as what is technically feasible to define in the chosen analytics tool.
Use Segments to Avoid Drowning in Data
Interpreting analytics data to answer specific questions becomes much easier when you use segments to narrow down the volume of data available and hone in on the relevant statistics. Once a persona-inspired segment has been created, most reports within your analytics tool of choice can be filtered to show only the data pertaining to that group. Trends in behavior and site usage for individual user types will emerge more clearly than they could when viewing data for all site traffic.
For example, when looking at a metric such as bounce rate, it is easy to get distracted by the overall rate of all visitors who bounce after reaching a certain page or group of pages. However, this number by itself isn’t actionable information, since several different types of users land on the page from different sources, with different expectations for the page content. Each bounce rate must be analyzed separately in order to uncover anything meaningful. Let’s consider two groups of users that are represented by two personas: David, the loyal visitor and newsletter subscriber, and Mary, a marketing manager at an organization with no in-depth technical knowledge. Are David-like users (repeat, frequent visitors coming from the email newsletter) bouncing from blog posts about tips for using existing features? That’s okay, we expect loyal content consumers to bounce because they have already previously consumed the majority of content on the website­. How about users in Mary’s segment, who arrive on the page from relevant queries on search engines?  If they are also immediately bouncing, maybe there is a mismatch between what users expect from the search-engine results page (SERP) and what content is actually delivered on the page, which is something that should be investigated further. On the other hand, if Mary-type users are in fact not bouncing, nothing may need to be done to the page or to the links leading to it. Such insights can only come from segmenting the data to unmask the helpful information.




 


Number of Visitors Who Bounced


Total Number of Visitors


Bounce Rate






Loyal visitors from newsletter (the Davids)


10,000


12,000


83%




Visitors from SERP with relevant keywords (the Marys)


3,000


8,000


37.5%




Total


13,000


20,000


65%




Simplified example of analyzing bounce rate of a page: An overall bounce rate of 65% masks the difference between the rates for loyal visitors and visitors searching specifically for information on the page topic. Only when you segment the data can you see such distinct differences and better understand how the page performs given the specific goals of each audience.
Not only do segments enable us to analyze specific metrics in detail, but they can also uncover behavioral patterns and answer questions such as: “Do new visitors landing on article pages from Google also visit any other types of pages?” and “Do newsletter subscribers and nonsubscribers behave differently? Specifically, are subscribers more likely to download whitepapers, contact us for consulting services, or upgrade their membership?” A significant difference in the conversion rate for a segment not only indicates that the segment is likely a valid representation of a distinct user type, but also reveals what group of users you should continue to pursue in your content strategy and aim to grow. For example, if you found that newsletter subscribers upgrade their membership level to access more features than the nonsubscribers, it would be worthwhile to research ways of increasing subscription rates.
While quantitative data from analytics tools can never tell you why users performed in a certain way—only what they did—on your website, uncovering these behavioral patterns can focus and inform user-research activities such as qualitative usability testing. Armed with this triangulated data, you will be well on your way to optimizing your site and its content to better satisfy the needs of your real users.
Learn more about how to extract meaningful UX insights from analytics data in our full-day training course on Analytics and User Experience.
 
Persona photo credit: ""Paul reading HTML5 For Web Designers"" by Jeremy Keith, used under CC BY / Modified from original"
96,2019-09-09,"One of the biggest causes of user failure is when users simply can’t locate stuff on the website. The first law of e-commerce design states, “if the user can’t find the product, the user can’t buy the product.” So these design flaws are not just usability problems, they’re often a site’s biggest profitability problems as well.
Findability and Discoverability Issues
When site visitors routinely conduct searches for content that should be easily located via browsing or when there is insufficient within-site traffic to mission-critical pages, the site may suffer from low findability and discoverability.
Findability: Users can easily find content or functionality that they assume is present in a website.
Discoverability: Users encounter new content or functionality that they were not aware of previously.
High findability and discoverability are the results of a well-defined information architecture and well-designed navigation system. The challenge with findability and discoverability issues is determining the root cause: is it the information architecture or is it the navigation design? Here are 2 examples to illustrate the difference between IA and Navigation/UI issues:
Problem Example 1: Site visitors are not visiting two important sections of the site.
Potential issues that can cause the problem:
IA-issue: Users do not understand or are not attracted to the names of the sections.
UI-issue: Users do not notice the links to the sections.
Problem Example 2: Site visitors never use a Related Links navigation component on content pages.
Potential issues that can cause the problem:
IA-issue: The content links included under Related Links are not relevant to what users need (a classification issue).
UI-issue: Users do not notice the Related Links navigation component (because perhaps it’s too low down the page or is mistaken for an advertisement).
The cost of guessing the cause of issues can be very high. It would be a shame to spend money to redesign an entire user interface only to discover that the underlying IA is the issue or vice-versa. With limited resources and time, knowing the root cause is priceless. All of the methods that we recommend can be executed quickly, remotely, and without moderation (unless desired). There are no excuses for not testing.
Employing Multiple Methods to Determine Cause
The key to identifying the true cause of a problem is to combine multiple testing methods. By running separate studies to measure (a) the information architecture (IA) and (b) the user interface (UI), we increase the likelihood of correctly identifying the cause of website failures.
The 4 methods described below answer different questions (IA- or UI-focused) and provide results that are either qualitative or quantitative (or both).

(*) Usability testing is usually qualitative, but can be quantitative with some extra effort or by using online tools for unmoderated quantitative testing.

1. Tree Testing
A tree test is an IA-focused technique conducted to determine if mission-critical information can be found in the site’s IA. It does not display the user interface to test participants; they navigate using only link names.
Questions it answers:

Are the names of categories understandable?
Do the category names accurately convey content?
Is content categorized in a user-centered manner?
Are content titles distinguishable from one another?
Is information difficult to find because the structure is too broad or too deep?

Setup and testing:
To set up a tree test, you create an information-architecture “tree” which delineates the groupings and hierarchy of pages (you can create this in a spreadsheet and paste it into the system that you use for testing). You then create specific tasks that involve finding specific destinations (called “end nodes”) of the information architecture (e.g., “Find a health insurance plan that covers a family of four and costs less than $500 per month”). Study participants conduct the tasks using the tree.
Results:
The results are quantitative and include, but are not limited to:
Direct success rate: How many participants found the right answer without having to go back up and down the tree?
Indirect success rate: How many participants got the right answer, but had to navigate back up and down the tree to find it?
First-click data: Which tier-1 categories did users click first? First clicks are a good indicator of the strength of category names.
Tool: Treejack, UserZoom Tree Testing, Userlytics Tree Testing

Treejack tree-test interface for participants: A task is summarized at the top of the screen and participants must navigate the label names illustrated in the tree to find the desired information.

 

Treejack tree-test summary results for one task indicate direct success, indirect success, and time taken.

2. Closed Card Sorting
Closed card sorts are an IA-focused method conducted to evaluate the strength of category names.
Questions it answers:

Are the names of categories understandable?
Do the category names accurately convey content?
Is content categorized in a user-centered manner?
Are content titles distinguishable from one another?

Setup and testing:
To conduct this type of test, you provide participants with “cards” including names or descriptions of content/functionality. Then they must assign these cards to your categories. (Such closed card sorts are the opposite of traditional open card sorting where users get the same stack of cards but have to create the categories themselves.)
Results:
The results are both quantitative and qualitative, and include:
Similarity: Number of times the same content was grouped together
Standardization Grid: Number of times a card was sorted into your intended category
Logic of assignment: With card sorting, it’s recommended to moderate a portion of your tests in-person or remotely via teleconference. This allows you to interview users to understand why they grouped certain content together, why they assigned items to particular categories, and how they interpret the category names.
Tools: OptimalSort, Usabilitest Card Sorting, UserZoom Card Sorting, Userlytics

OptimalSort’s interface for closed card sort: The “cards” are on the left, the categories are in the body of the page. Participants drag cards onto the categories to conduct the sort.

 

OptimalSort: Standardization Grid for closed card sort illustrates how many users assigned the cards to each category. If most users picked a different category than what you had intended for a given card, then it’s time to reconsider your IA structure.

3. Click Testing
Click tests are UI-focused; they are conducted to determine where users click on the interface to find specific information or functionality. One drawback of click tests is that they are not interactive: participants are shown static images of a site and must show where they’d click to perform a task. However, once they click, the task is considered finished and they can move to the next task. To test interactive elements, you should conduct usability testing.
Questions it answers:

Which navigation components are utilized?
Which navigation components go unnoticed?
Which navigation components are avoided?

Setup and testing:
To conduct this type of test, you upload a screenshot, wireframe, or sketch of a page into a click-testing tool. You then create tasks. Participants must click on the image to indicate where they would go to conduct the task.
Results:
The result is a heatmap illustrating where users clicked. The heatmap helps you determine if the navigation design is noticeable or if elements are conflicting and create too much noise.
Tools: Usabilla First Click Tests, Chalkmark, UserZoom Screenshot Click Testing

Chalkmark’s click-test interface for participants: A task is summarized at the top of the screen and participants must click on the image to indicate where they would go to conduct the task.

 

Chalkmark’s click-test heatmap illustrates where users click to conduct each task in the study.

4. Usability Testing
Usability testing is conducted to determine how and why users navigate a website (or a website prototype) to conduct tasks.
Questions it answers:

How do users find information?
Which navigation components are utilized?
Which navigation components go unnoticed?
Which navigation components are avoided?

Setup and testing:
To conduct this type of test, you can use prototypes (paper or interactive) or a live site. You then create tasks and ask participants to carry out the tasks. You observe users conducting the tasks and note when they interact with navigation components, how they interact, and if they avoid or overlook navigation. You can conduct usability testing in-person or remotely. Remote usability testing can be moderated live (via teleconference) or unmoderated using a variety of online services. (Many options, from established services like UserZoom and WhatUsersDo to startups like YouEye.)
Standard user testing requires no further equipment than a live user and a computer. (Or a piece of paper in the case of paper prototyping.) However, if budget allows, you can run the study with an eyetracker which may help if you’re particularly concerned about whether users ever even see the desired navigation components.
Results:
The results include task success rate and difficulty ratings, identification of interface elements that cause friction, and better understanding of the users’ mental models of the site.
Tools: In-person testing, remote moderated tests with services like GoTo Meeting or Webex, remote unmoderated testing tools
Identifying the Cause Is Key to Successful Remediation
When running multiple types of studies you may find positive results from one test and negative results from another. Inconsistent results from different types of studies precisely illustrate the value of running multiple tests to focus on both IA and UI. For example, you may run a closed card sort and find that users have no issues assigning subcategory content into your global categories. However, a click test (of the same environment) may result in painfully low success rates, with users clicking in all the wrong places when asked where they would go to conduct mission-critical tasks. Thus, put together, the 2 tests indicate that your category names are fine, but your layout is problematic and that your best investment is in designing new layout configurations.
Low findability and discoverability is a panic-inducing problem which can lead to knee-jerk uninformed and unsuccessful fixes. It’s not uncommon for teams to have limited time to conduct studies, which is why these four methods are so useful: they can be set up quickly, run simultaneously, and conducted remotely—without any moderation if so desired. Therefore, combining two or more of these tests is a reasonable monetary and time investment, as it can help you hone in on the cause of your problems and will mitigate the risk of investing in an expensive nonsolution.
Please note:
We have listed a variety of tools for the research methods discussed in this article. While we recommend the methods, we don’t specifically recommend any individual tools. As a vendor-neutral organization, Nielsen Norman Group doesn’t endorse such products, and the one to use on any given project would depend on your specific needs and budget. We are happy to provide consulting on such questions, but the answer would be different in each case."
97,2014-04-20,"A/B Split Testing
A/B split testing allows websites to compare variations of the same web page to determine which will generate the best outcomes. The metrics used in A/B split tests are micro and macro conversions. A/B testing has become far more commonplace with the introduction of tools that require little or no involvement from developers and no other technology resources. This type of experimentation has a strong foothold amongst marketers, and, because of its relatively low cost it is becoming increasingly utilized by user-experience designers. Sites like Google, Amazon.com and many large e-commerce sites are known to “always be testing” — with multiple A/B tests running at any given time.
Garbage In, Garbage Out
A/B testing is an empowering tool when utilized appropriately. However, there are potential issues with A/B testing that run along 3 themes:

The variation doesn’t do justice to the concept. Poor design can lead to poor conversions—that’s clear. However, each design is an implementation of a concept and it is foolish to judge the merit of a concept via a single design implementation. Often it takes several design attempts to adequately serve a concept. For example, you may theorize that adding a description to an option will increase adoption of that option. However, if the description is presented in a manner which makes it look like an advertisement, it might be ignored by users. The concept is not bad, but the implementation is. It’s vital that the concept and the implementation are clearly differentiated.
The variation doesn’t address actual causes of issues. Making incorrect assumptions about what caused an issue translates to a design variation that doesn’t actually address the problem. Tweaks to this variation will never solve the problem because the design is a response to an invalid cause. For example, you may guess that the reason for poor loan application submission is that the process involves too many screens, so you condense it into one screen, but you still don’t see a lift. Instead, the real issue is that users cannot find loan interest rates and the only reason they end up on the application page is because they assume they will find it there.
Variations are based on guesses. With A/B testing you only find the best option from among the available variations. And if the variations are only based on internal experience and opinion, who’s to say that the testing includes the most optimal design?

These experimentation flaws can be mitigated by informing A/B testing with user research. When even minimal user research is conducted, we gain invaluable clues as to the potential reasons for conversion issues.
Uncovering True Causes to Define Better Variations

“A theory can be proved by experiment; but no path leads from experiment to the birth of a theory.” 
- Albert Einstein

To ensure well executed A/B tests, the following must be defined for each experiment:

You could spend all your time generating cause theories and variation hypotheses and then A/B testing all of them: that is the brute-force approach. Haphazard A/B testing is the equivalent of throwing ideas at the wall to see which ones stick. Unfortunately, you cannot afford to do it: this approach increases the risk of user abandonment and poor experience. As you’re waiting to hit the A/B test lottery, users will interact with a suboptimal design. They may eventually decide that your site is a lost cause and never come back. You need to narrow down the number of hypotheses and deploy A/B tests cautiously and effectively; we recommend user research as the method that can help you with that.
Four User-Experience Investigations to Improve Optimization Testing
1. Defining user intent and objections
It’s vital to understand why people visit the environment, whether they are (or believe they are) successful, and why they decide to leave. If you incorrectly assume why people come to the site, your cause theory and variation hypotheses will not reflect the reality of the environment’s utility as perceived by its users. It’s dangerous to assume user objections without investigation. For example, suppose you assume that visitors don’t take desirable actions because your prices are too high, so you lower your prices and your profit margin takes a hit. If it turns out that the real reason people were not converting was not because of price, but because they did not understand the needs that your service addressed, you may not have a job come Monday.  
How to define intent and objections: Brief on-site, on-exit and/or follow-up surveys (via tools like Qualaroo) that ask two simple questions:

Why did you visit?
Were you successful? If not, why?

2. Exposing interface flaws
If you overlook significant usability problems such as confusing interaction flows or misunderstood cues, you may not see conversion lifts from A/B testing tweaks because your design variations do not address the kernel of the issue. For example: If you have a form where several of the fields ask for information that users do not feel comfortable providing, running an A/B test experiment to see if changing the submit button color increases the conversion rate is a wasted effort. Understanding true drivers of low conversion is critical to running smart, successful experiments.
How to expose interface flaws: Usability testing (remote moderated or unmoderated, or in-person) can be quickly conducted and may uncover 85% of the massive flaws in the site with only 5 users.
3. Measuring findability
Navigation-label and menu-design experiments allude to findability issues. However, poor findability can and should be confirmed before running A/B tests that directly impact information architecture and navigation. (You can learn more about information architecture in our 2-day class.)
How to measure findability: Tree tests measure the findability of elements within an existing or proposed information architecture without any influence from the interface design. They can tell you whether labels, link grouping, hierarchies or nomenclature are intuitive. If you are conflicted on naming of sections, pages, links and labels in your site, this test can identify the most problematic names and help define new labels that will improve findability. Tree tests can be performed with a tool such as Optimal Workshop’s Treejack, which allows you to generate tasks to test an information architecture.
4. Cleaning up design variations before going live
The simplest application of user testing is to clean up a design by removing stumbling blocks for users. A few hours of testing usually reveals anything really bad in your design. While more advanced forms of user research have their advantages, don’t overlook the humble clean-up study. For A/B testing, you want to make sure that all the design variations get a fair chance that is not marred by usability problems that keep customers from getting the full benefits of each variation. Clean them first.
Combine Methods to Maximize Conversions
A/B testing is a wonderful tool that can, unfortunately, be poorly utilized. If A/B tests are used in lieu of research, the variations are essentially guesses. You can improve outcomes of A/B tests by incorporating UX research to improve cause identification, develop more realistic hypotheses, and identify more opportunities for experimentation.
Learn more about A/B testing in our full-day course Analytics and User Experience"
98,2014-02-02,"Google Analytics provides about 95 standard reports, and, with just a bit of simple configuration to set date ranges and select user segments, you can learn all sorts of interesting things such as how users interact with your site, where they come from, and what channels deliver best on your current goals.
Unfortunately Google Analytics has rather poor web application usability; it can be difficult to find your way around the many available reports. Even worse, it’s often unclear which reports provide the best insight based on your specific data needs. This article helps you short-circuit the confusion with step-by-step instructions for those analyses that are most likely to help you improve your user experience.
1. How Fast Is Mobile Access Growing?
This information is helpful when you are planning, pricing, and comparing approaches to making your site mobile friendly. How much should you spend? What level of priority and resource dedication should mobile initiatives receive?
To contrast how mobile traffic quantity changed between two similar periods of time (e.g., January 2013 versus January 2014), you can use the date-comparison feature in Google Analytics and some simple offline calculations.
Report: Audience Overview
1) Report: Go to Audience > Mobile > Overview.
2) Select the date range and add a comparison date range.
3) Optional: Select a specific goal from the Conversions drop-down menu.
4) The % Change line in this report represents percent change in absolute mobile visits for each date range – not what you are seeking, so you should ignore this.
5) To determine the rate of growth in the percentage of mobile visits, you must perform a calculation. For each period: take mobile visits and divide by the total visits, and then calculate the rate of change.

Recent time period (indicated by blue squares in the image below): 22,091 mobile visits divided by 267,933 total visits = 8.2% of all visits are via a mobile device.
Same time period one year ago (indicated by red squares in the image below): 15,791 mobile visits divided by 270,769 total visits = 5.8% of all visits are via a mobile device.

Conclusion: On the sample site, mobile use currently accounts for less than 10% of unique visits, but it has increased by about 40% between these two time periods. If this rate remains consistent, almost 45% of users would be accessing via mobile in 5 years.

To calculate how mobile traffic has changed between two similar periods of time (e.g., January 2013 versus January 2014), you can use the Mobile Overview report. Choose two time periods to compare and perform the calculation detailed above.
2. How Much Do Social Networks Impact Our Ability to Meet Goals?
This information is helpful when you research and plan content strategy, to determine where your content gets traction and what content, is shared most often. With this information, you can conduct some manual categorization (or use Google’s new Content Grouping functionality) to make higher-level determinations of what topics and types of content build awareness and deliver traffic.
Report: Network Referrals
1) Go to Acquisition > Social > Network Referrals.
2) The report illustrates referral traffic via social channels. Click on any of the named social networks to see specifically which content people are sharing via that channel.

The Network Referrals report details visits, page views, visit duration, and pages/visit by social network.

This list of specific links/content can be very helpful in determining which content types and topics attract site traffic. Note: For reasons of client confidentiality we have blanked out the specific URLs in this screen shot. In this example, there’s one clear #1 social topic and two runner-ups, but that’s obviously going to be different from one site to the next. It’s almost always true that a small number of key topics are much better than others at stirring the social juices. In this example, #1 is 35 times better than #10.
3. What Sources Drive the Most Conversions?
This report can provide granular data on how specific channels contribute to acquisition, how the users who come via those channels behave on your site, and how those channels contribute to meeting your goals (as you have defined in Google Analytics). This insight helps content and marketing teams make decisions about the amount of effort, resources, and budget to dedicate to specific channels.
Report: Goals Overview
1) Go to Conversions > Goals > Overview.
2) Select Source/Medium.
3) Select view full report.

The Goals Overview report summarizes goal URLs, number of completions, and percent of completions.
4) On the full report screen, select what you want to view: (a) Source (e.g., Google, Twitter, etc.) and (b) select the goal(s) by which you want to filter.

This report can provide granular data on how specific channels contribute to acquisition, how the users who come via those channels behave on your site, and how those channels contribute to meeting your goals. Note that “t.co” is Twitter.
We can’t emphasize enough the need to look at goal completions and not simply count the number of users arriving from each channel. Some sources of traffic are notorious for mainly sending low-value visitors who are unlikely to do business with you. Let’s take a personal example: for our own website, nngroup.com, users coming from LinkedIn are about 12 times more valuable as users coming from Facebook. So, even though Facebook is bigger and refers more visitors, LinkedIn is the more valuable traffic source for us. Obviously, we target business professionals, so it makes sense why traffic from LinkedIn would be better for us. For other types of sites, the value ratio may be reversed.
4. How Many Visits Does It Take for Visitors to Convert?
When evaluating and planning website experience, many UX teams create customer-journey maps for their target personas. These maps detail the interactions that prospects are likely to take (online and offline) along their path to becoming a customer. The Path Length report can help in defining a realistic range in number of site visits before users convert or take desirable actions (as defined by the goals that you set up in your analytics system). This statistical data can be used in concert with qualitative user research generated from methods such as interviews and contextual inquiry.
Report: Path Length
1) Go to Conversions > Multi-channel funnels > Path length.
2) Select the specific goals by which to filter.

The Path Length report indicates how many visits to the site occur before transactions take place. It shows this as percentage of total transactions and percentage of the value of those transactions. For example, you can quantify the value of conversions that occur after 1 visit to the site versus 6 visits. This tracking goes back 90 days from each conversion.
5. What Desirable Actions Do People Take on the Site?
You can use this report to determine what actions users take on the site (besides moving from page to page). This can help in predicting potential hurdles with major shifts in site purpose and content. In order for this report to be useful, the site must be tagged for Events. Events, in analytics lingo, represent actions that do not result in loading of a new page (e.g., playing a video, pausing a video, downloading a document, and filling out form fields). In the past, event tagging required working with developers to add tracking code to specific elements throughout the site. This is becoming easier now that Google has introduced the Google Tag Manager which provides a user-friendly interface for adding, removing, and editing event tags.
Report: Events Overview
1) Go to Behavior > Events > Top Events.
2) Optional: Select user segments (mobile users, users who convert, users referred by specific sources/mediums, etc.).
3) Select Event Category, Event Action, or Event Label. These are the designations assigned to tags and act as a kind of information structure where a series of Labels can be grouped under an Action, and a group of Actions can be grouped under a Category. This system makes it easier to measure types of events as a whole. For the most granular view, you would select Event Label.

This report is based upon specific tracking tags that you or your developers include in your site to measure how many times visitors take specific actions. Event Tracking is designed to measure “events” like watching a video, downloading a report, filling out a form, and many other actions that don’t involve moving from one page to another.
Developing a Sound UX Strategy Requires Access to Behavior Data and Information
Analytics provides a wealth of information to help you answer both high-level and detailed questions regarding how your site serves users and meets business goals. These 5 reports are just the tip of the iceberg — there are so many more questions that you need to answer and Google Analytics has so much more data that can be utilized in that effort.
Learn more about incorporating analytics into UX practice in our new course, Analytics and User Experience.
Please note:
This article uses screenshots from Google Analytics to illustrate a variety of website-analytics reports that should be reviewed to drive design improvements. We are vendor neutral and do not endorse or recommend Google Analytics. Depending on the characteristics of your company, you may get better results from another product, and it should be possible to get similar — if different-looking — reports from other analytics services. We simply show screenshots from Google Analytics because many of our clients and course participants happen to be using this product."
99,2014-01-26,"Introduction
""Echo. Boomerang. Columbo."" No this is not a bastardized version of the NATO phonetic alphabet, rather it’s a handy way of remembering 3 safe and productive approaches for interrupting or answering users during usability tests and other research studies.
Facilitation Techniques for Handling Test Users’ Questions During Usability Studies
How does an unprepared usability-test facilitator respond when the user asks her a question or offers an indecipherable comment? Usually, not well. Admittedly, facilitating a usability study is not a natural way to interact with other human beings. So it is totally understandable why most of us have trouble facilitating, making classic mistakes such as:

Commence nervous chatter or panic and fumble for words, worrying about the negative impact they may be making as they speak.
Treat the test session as a conversation rather than an observation.Talking too much, at inappropriate times, or leading the user can affect what he does and says, which can invalidate part or all of the research findings. Interviewing methods are different from observational methods. While interviews can reap fruitful information about people’s desires and impressions, direct observation of live user behavior is an invaluable research method when trying to learn what people actually see, act on, and how they interact.
Go the complete opposite direction and stay completely silent, succumbing to fears about interrogating the user. Sitting completely mute is probably better than saying too much, but it's not an advanced facilitation practice as it doesn't enable gathering the most possible information during a study. Probing at the right times, without making mistakes (such as leading the user or asking closed questions which beckon only yes or no answers) can reap rich responses and lead to insights about exactly why a design is or is not useful and usable.

There is no need to panic or blather on when a user asks a question. Instead try the echo, boomerang, or Columbo techniques described here.
Echo
With the echo technique, the facilitator repeats the last phrase or word the user said, while using a slight interrogatory tone. Using the exact word(s) that the participant used ensures that the facilitator does not bias the participant by making a suggestion or describing anything in the interface. Instead, they just parrot and probes in a benign way. Here are two examples of good echo technique:
User: This table is weird, well, hmmm, not sure what, uh…
Facilitator: Not sure what?
Or
Facilitator: Table is weird?
Say these few words while using a tone that makes it clear that the phrase is a question. This will naturally put the user in the mindset of answering the question by elaborating on what he meant by those same words.
Echo Audio Example (Note that I play both the user and the facilitator.)

 

Boomerang
With the boomerang technique, the facilitator formulates a generic, nonthreatening question that they can use to push the user’s question or comment back to him. Examples of such nonthreatening questions are “What do you think?” or “What would you normally do?” So if a user asks a question such as, ""Do I have to register to buy this?"" the facilitator should not say, ""Erm. I guess so,"" or whisper, ""No, that’s okay."" Rather, the facilitator should try the boomerang technique. For example:
User: Do I have to register to buy this?
Facilitator: What do you think?
Or
Facilitator: What would you do if you were at home now?
Or
Facilitator: What would you do if you were really doing this on your own?
The facilitator does not answer the user’s question; rather, they bounce the user's question back to them. The idea is to remind the user that they are to try and work out the issue as they might if they were not in a research environment.
Boomerang Audio Example (Note that I play both the user and the facilitator.)

 

Columbo
With the Columbo technique, be smart but don't act that way.
Peter Falk immortalized the character of Lieutenant Columbo in the late 1960’s television series in which he caught a myriad of criminals, mainly by enticing them to underestimate his investigative skills. Columbo seemed forgetful and inarticulate, but he was actually perceptive and astute.
While facilitators in a usability study aren’t trying to catch anyone as Columbo was, they are trying to craft tasks and questions in a way that coaxes people into saying what they think and into doing what comes naturally. Facilitators can take a page from Columbo’s handbook and act less like experts, and more like investigators. One way to do this is to ask just part of a question, and trail off, rather than asking a thorough question. This may sound strange, but it achieves the following:

Saying fewer words means the facilitator is less likely to teach or sway the user.
Forming less complete and perfect questions and pausing can cause the user respond more quickly, so they don't have to wait for the facilitator to finish they thought. And some users try to help the facilitator by answering before a question is fully formed.

So if a user asks a question such as, ""If I close here will I lose my work?” the facilitator may be tempted to, but should not say, ""I am not sure” or “Try it” or “I don’t think so.” Rather, the facilitator should try the Columbo technique. For example:
User: If I close here will I lose my work?
Facilitator: Uhm, you are wondering if [pause] you might [pause.]
User: I am just not really sure if I should pick ""close"" or ""cancel"" or ""ok."" I guess I don't know the difference between these buttons.
Columbo Audio Example (Note that I play both the user and the facilitator.)

 

A facilitator may also apply the Columbo technique effectively when they want to initiate asking a post-task question. For example, they may be tempted to but should not ask the user, ""Did you see the filters on the left?"" Instead they might try this:
Facilitator: I was wondering if you could look at the page [pause] and this section [pause and motion to an area in the interface.]
Deciding Whether to Speak to the User
Don’t mistake any of the tips mentioned in this article as license to interrupt the user any time he makes a sound. Instead, follow these guidelines before talking to the user:

Decide whether what the user said was a real question that you actually need to answer, or it was a rhetorical question, or just thinking aloud.
Determine whether the noise or comment that the user made was indecipherable, or whether it was actually enough to draw a fair conclusion from.
Consider whether you will truly benefit from probing the user further, or whether you have enough information from just observing what he is doing.

Key events which signal an appropriate moment to interrupt a person during a test session occur when the user has:

offered some comments
asked the facilitator a question or is otherwise seeking guidance from the facilitator
naturally interrupted their own flow in some way.

Conclusion
When in doubt about whether to talk to the usability test participant, count to 10 silently. Then decide whether you should say something. If yes, try using echo, boomerang, or Columbo techniques for speaking with the user. Remembering these 3 practices can help you learn more from your users, and improve your own test-facilitation skills.
More Facilitation Advice
Many more guidelines for improved test facilitation techniques in the full-day training course about Usability Testing."
100,2013-12-15,"Defining Competitive Evaluations
Definition: Competitive usability evaluations are a method to determine how your site performs in relation to your competitors’ sites. The comparison can be holistic, ranking sites by some overall site-usability metrics, or it can be more focused, comparing features, content, or design elements across sites.
Evaluations can take the form of expert reviews, where an experienced usability practitioner reviews the designs based on her expertise and knowledge of usability, or competitive usability testing, where users complete a set of tasks using 2 or more competing sites.
Rather than simply looking at a competitor’s site to see what they’re doing and what you personally think is interesting or different, doing an evaluation allows the design team to understand what works and what doesn’t from a user’s perspective or an expert’s perspective.
Goals
Competitive evaluations let you assess if your design is better or worse than your competitors as well as discover the relative strengths and weaknesses of competing designs. They allow you to take an in-depth look at how others solve the same design problems. The goal of any competitive evaluation is to see what competitors are doing, how they’re doing it, what’s working and what’s not.
Competitive evaluations are often a good initial research activity for a project. They can help determine the direction of a design or the need for the development of a feature. The goals of the competitive evaluation should be clear before any work is started.  What design challenge are you trying to solve? What features of your competitors seem interesting or appealing? What feature on your site do you want to compare to others? 
Organizations with a substantial investment in strategically-managed user experience can also conduct longitudinal competitive evaluations to track relative status over time. This is expensive and not for companies at lower levels of organizational maturity. (Nielsen Norman Group typically charges clients $45,000 for a competitive usability test, so to do a lot of them requires a beefy budget.)
Benefits
A usability evaluation, whether a test or review, can help the team make decisions based on knowledge about what elements work well for users, rather than based on personal opinions. Competitive evaluations do this, and also have a few other benefits:

Risk reduction: Often you'll find that features that are pushed hard at industry tradeshows don't do much for your customers. Get this insight early, before investing in the feature, by testing some competitive sites that are early adopters.
Adding value: Other times you'll identify features or design approaches that dramatically benefit users. In these cases, you can proceed to add this value to your site.

Your team can gain these insights based on the usability work of your competitors. Testing others’ designs can show you the strengths and weaknesses of a design that may have already been through one or several rounds of usability testing or reviews. Your team benefits from the work that the competitors’ teams have already done. Competitive evaluations allow you to test several approaches to the same design – and can be done before doing any design work.
In addition, competitive evaluations reflect what your customers do already.  As they look for information on the web, they compare the content, functionality, and overall experience that your site offers to what others offer. People typically browse in parallel, moving between several sites before deciding which company to patronize. It is beneficial for your team to know what your users see and do as they look around at the competition. 
Defining the Competition
A typical review or test focuses on 2 to 4 competitors’ sites. Any more than that can be too expensive and too overwhelming to analyze. If you have a large number of competitors, do an initial review of several of them to determine which sites:

Offer similar content and functionality to your site, or to what your site strives to provide
Provide the best overall user experience
Use innovative designs that set them apart
Are your strongest or most important competitors
Are the competitors that your customers are most likely to compare you against 

Don't simply pit your site against those hated big competitors that are your worst enemies in the marketplace. Competitive evaluation is supposed to derive insights that can drive your user experience to the clichéd next level. Often the best insights come from comparing yourself to smaller or more innovative companies. You can also often learn much from companies that are only tangential competitors because they may do similar things in different ways based on traditions from different industries or market segments.
Competitive evaluations aren’t about picking competitors who have poorly designed sites and saying your site is the best. While you can learn a lot about what doesn’t work from looking at poorly designed sites, it is crucial to also learn what works by looking at sites with better usability than your own.
Competitive Reviews
In a competitive review, a usability expert takes an in-depth look at a series of related sites. The expert is looking for relative strengths and weaknesses, trends, patterns, and differences. Looking at similar content across several sites can help identify holes in your content or functionality; holes on others’ sites may inspire an addition to your site.
Competitive reviews can be as broad or narrow as desired. They can dive into a particular feature or area, or encompass an entire site.  You may look only at ordering processes, or you may review the overall site experience. In any broad evaluations, it is best if some key areas are identified to help focus the review and its results.
Competitive Testing 
In a competitive test, an expert runs a usability test on your design and on the designs of your competitors.   Each study participant typically completes a series of tasks on 2 or 3 different sites. Testing more than 3 sites with any given participant can be overwhelming to the study participants. Alternate which sites are paired together and which is tested first for each user.
Usability tests are focused on specific user tasks and can give an in-depth look at key areas and functions of the site. You want to see what users do, rather than ask them to look at the site and express their opinions. Focus on representative and key tasks that can be completed across sites, as well as on tasks where sites offer different approaches to information, functionality, or design.
At the end of each session, ask participants to compare the sites they used. People often naturally do this as they start to use the second site, with the experience of the first site still fresh in their minds. Comparing the two designs can help users verbalize what was clear or confusing in each design and can help you gain further insight into strengths and weaknesses.  
Competitive Tests on Design Variations
Competitive evaluations can also be completed with variations on a single design developed in a parallel design process. Not sure which of several designs is best? Evaluate them and find out. 
Rather than sitting around a conference table arguing about the best approach, do some usability testing or an expert review to determine which design works best for users. Competitive evaluations, like any usability evaluations, can be done in the early stages of design. Designs don’t have to be complete – or even interactive – to be tested. Depending on the design, paper-prototype testing can get the results you need before you spend any money on development.
Analyzing Results
By collecting metrics, a “winner” can emerge from competitive testing or competitive evaluations. This can be based on success scores, time on task, users’ subjective ratings, or a scorecard developed to analyze findings. However, the main goal of a competitive evaluation is not to declare a winner. The goal is to improve your design. While metrics can emphasize the point that your site performed, say, 30%  better – or worse – than your competitors’ sites, the important information lies in the details of what worked and what didn’t across designs.
Focus on what’s relevant to your design questions or challenges. What are the biggest strengths of competing designs? What trends arise across sites – and are they good or bad? Did any sites offer unique solutions to a common problem? How did they perform? Is there any opportunity to stand out from the competition? Were there any gaps in the information, content, or features that the sites provided? Don’t forget to look for opportunities beyond what other sites offer. You want to beat the competition, not copy them.
When writing – or reading – a competitive report, keep in mind that what matters is the information that can help you improve your site. Knowing what worked well and why it worked well can help direct design decisions. Knowing what failed and why can help teams avoid others’ mistakes or fix their own.  A good competitive evaluation not only assesses the competition’s designs, but also translates that assessment into recommendations for the site, whether written in the report or brainstormed with the team. The results need to help you design your own solution.
Summary
Competitive evaluations can give your team the benefit of reviewing several sites’ designs, to understand what the competition is doing and what they’re doing well. Learning from others’ designs can help your team create a better site."
101,2013-11-17,"Analytics has traditionally been used to inform marketing strategy and tactics, but we now see more usability and user-experience professionals relying on this quantitative-data source to aid in research and design.
The biggest issue with analytics is that it can very quickly become a distracting black hole of “interesting” data without any actionable insight.
Probably the worst thing you can do is teach someone how to use an analytics system and hope she will get you some interesting findings. Even a “free” analytics service will cost you a fortune if it redirects resources from more productive uses. Many beginners get stumped over one of three hurdles:

Scope of metrics: So many things that can be measured, but which are meaningful?
Difference between metrics: Which metrics best answer specific questions?
Interface complexity: How do you get the analytics system to tell you what you want to find out?

Because of this last point, many people end up jumping in the deep end and focusing on the tool instead of the work that it is intended to support. With that in mind, we recommend that UX professionals back up a step and think about how analytics data can supplement current methods and processes.
After interviewing a variety of UX teams regarding their use of analytics and other web data, we discovered some interesting high-value UX uses for analytics. We’ll cover 3 in this article (and more in our course on Analytics and User Experience):

Issue indication: Notifying the team of potential problems reaching goals
Investigation: Identifying potential causes of issues
Triangulation: Adding data to supplement qualitative research

Use #1: Issue Indication
Some UX teams collaborate with optimization specialists while designing the site or launching new features to develop and implement a measurement plan. UX teams receive reports daily or weekly in order to monitor the site’s ability to meet stated goals. They can use the web metrics to diagnose specific issues or rely on them as clues to guide further investigations.
A measurement plan consists of:

Goals/macro conversions: These are the big-picture actions that users need to complete on the site in order for it to be successful. Examples include the number of purchase completions and the number of lead submissions.
Desirable actions/micro conversions: These are smaller actions that, when combined, support the meeting of the goal—such as progressing along a lead-generation funnel. Examples include visiting a specific page, clicking a particular link, or entering data in a form.
Web metrics: These are web-analytics data that indicate whether these desirable actions occur; they help UX teams identify potential issues.





Measurement Plan Example




Goal


Desirable Action by Site Visitors


Web Metric


Description




50 consulting leads per month


Visit consulting service section


Unique page views


“Unique page views” indicates how many site visitors visited this specific page. “Unique” means that the page is not counted multiple times if the same user visits it multiple times.




Read about consulting services


Average time on page


“Time on page” indicates the average amount of time spent on the page. More or less time is not necessarily a good thing. So, it’s important to always look at this data in context and compare it to similar time periods. 




Download whitepapers, enter data into form fields, submit the lead form


Events


""Events"" refers to any user action on the site that you are tracking. In Google Analytics (and many other systems) you can define what event instances should be recorded. Examples of events include downloading a PDF, clicking a particular button, entering text in a field, and watching a video.




Use #2: Investigation
In this mode, UX teams—either on their own or with the assistance of optimization specialists—develop hypothesis for macro-conversion issues and use analytics to prove/disprove the hypotheses. There are several problem categories that guide the investigation: traffic, technical, content, navigation, visual design. Most examples we provide below are from Google Analytics' free offering.
1) Traffic Issues

Example investigation: Determine if there is one traffic source (Google, Bing, Yahoo, direct, email campaigns, etc.) that is responsible for a decrease in page visitors.
Useful analytics report: Pages (filtered by the page URI and using Source as the secondary dimension)
In Google Analytics, you can generate page-specific reports that display where the traffic to the page originated (search, email, direct, etc.).



Example Pages report (in Google Analytics) filtered by URI and using Source as Secondary dimension

2) Technical Issues

Example investigation: Determine if a page element is not loading properly.
Useful analytics report: Event Pages
The Event Pages report lists all the pages where events are tracked. You can select the specific page being investigated to get metrics on that specific-page’s events.



Example Event Pages report in Google Analytics


3) Content and 4) Visual-Design Issues

Example investigations:


Determine if new wording may not effectively communicate the benefits of or the process for taking a specific action.
Determine if imagery, typography, colors, and/or layout are distracting from calls to action (CTAs).


Useful analytics report: In-page Analytics
In-Page Analytics indicates what links users select.
Note: You need to set up Enhanced Link Attribution in the Google Analytics admin to see separate percentages for links on a page that all have the same destination. We also recommended supplementing In-Page Analytics data with clicktracking services like Clicktale and/or CrazyEgg.



Example In-Page Analytics indicating what links users are selecting




An example of a CrazyEgg Heatmap display; clicks can also be expressed in numbers via the Overlay view.


5) Navigation Issues



Example investigation: Determine if specific links/buttons are not being clicked.
Useful Analytics Report: Pages (filtered by the page URI and selecting the Navigation Summary tab)
Navigation Summary is a tab you can select from any Pages report (illustrated below). It details from which website pages people came before visiting the page of interest and where they went after visiting that page.



Example Pages report with Navigation Summary selected


Use #3: Triangulation
In this mode, the UX team uses analytics to verify findings derived from qualitative research (e.g., usability testing) and gather additional clues to help in defining a solution. If the original usability test was run with about 5 users — as we often recommend — then there is always the risk that estimates like success rates will be wrong. But such a quick test has the advantage of rapidly pinpointing a potential trouble spot, which can then be instrumented for targeted collection of a few thousand analytics data points that support much more accurate estimates.
Examples of usability-test findings to verify with analytics data:
Finding: Study participants don’t know where to find information about a topic because the word used on the site is different than what they use.

Additional questions to answer: Are people searching for those terms that participants mentioned in the study?
Useful analytics report: Search Terms
The Search Terms report lists the terms users enter into the website’s own search box (not web-wide search). You can download the search-terms lists corresponding to any time period and conduct more extensive analysis. Terminology that users typed into your own search box is a prime candidate for use to turn your content into user-centered language.



Example Search Terms report in Google Analytics


Finding: A feature is not used or a page is not accessed because study participants didn’t notice the link.

Additional question to answer: Where are users going instead?
Useful analytics reports: Pages (filtered by the page URI and selecting the Navigation Summary tab) and In-Page Analytics (see examples above in the Investigations section)

Finding: A form is not being completed because people don’t feel comfortable providing required information.

Additional question to answer: On which fields are people abandoning the form?
Useful analytics report: Event Pages (see example above in the Investigations section)

Conclusion
Quantitative data is increasingly becoming a key ingredient in usability and user-experience work. With this change comes the need for UX professionals to become familiar with the language and tools and determine which metrics and features are useful in UX practice.
Adding analytics to UX work enables us to:

Take early action to prevent unnecessary conversion decreases
Quickly prove/disprove causation/correlation theories
Better persuade the more data-oriented stakeholders in our organizations

Learning analytics systems can be daunting because they are complex and have been purpose-built for marketing activities, not UX. The learning process can be improved by starting with existing UX processes and determining where metrics can add value in those processes.
Beyond what is covered here, there are many more analytics metrics that can be incorporated into UX research and design processes. We will cover these in our course, Analytics and User Experience."
102,2013-10-12,"Remote usability tests are like traditional usability tests with one key difference: the participant and facilitator are in two different physical locations. Rather than the usability expert going to a participant’s location or vice versa, the participant interacts with the design in his own home, office or other location, and the expert watches remotely.
Generally, we recommend in-person usability testing whenever possible. It is easier for usability facilitators to read users’ body language and to recognize an appropriate time for a probing or follow-up question when they are in the room with the user. It can also benefit product teams to see users interact with their designs in real life, rather than watching them on a remote feed. (For much more on how to conduct user studies, see the full-day course on Usability Testing.)
However, when in-person testing simply isn’t possible due to budget or time constraints, remote testing is preferable to the alternative: skipping the test altogether.
Remote usability sessions don’t require either the participant or the facilitator to travel. As such, remote testing is a great solution for teams with limited budget, or for testing products whose users are geographically dispersed. Scheduling a series of online studies can be preferable and far less costly than traveling around the country or the world.
Online testing is also a good solution in a tight timeframe — travel doesn’t have to be coordinated, and facilities for testing don’t have to be secured. Further, participants can be from any geographic area rather than concentrated in one location, which can make recruiting faster and easier.
Remote research allows participants to use their own computers for the study, letting you and your team see how they set up their desktop, navigate between programs, and use tabs, for instance. This insight into how people work with their own machines is valuable, but it also makes it more difficult to troubleshoot any problems participants have with the remote tools needed to conduct the study.
In moderated remote testing, users and facilitators are in the same “virtual” space at the same time — the facilitator is watching the usability test remotely as it happens, and communicating directly with the participant via the telephone, email, chat, or a combination of methods. In an unmoderated remote session, the participant completes the study on his or her own schedule, recording the session for later review by the usability expert.
Moderated Studies
Moderated sessions allow for back and forth between the participant and facilitator, because both are online simultaneously. Facilitators can ask questions for clarification or dive into issues through additional questions after tasks are completed.
It can be difficult to know when to ask a question in a remote study, however. Silence on the other end of the line may mean that the user is confused, immersed in content, looking around the page, or distracted. It can be difficult to find the balance between letting users know you are listening and interrupting them. Although the same is true in face-to-face studies, the problem can be magnified in remote studies.
Unmoderated Studies
Unmoderated usability sessions are completed alone by the participant. Although there is no real-time interaction with the participant, some tools for remote testing allow predefined follow-up questions to be built into the study, to be shown after each task or at the end of the session. Questions can also be emailed to be completed after the user has finished her session. In both cases, questions are the same across users. There is no opportunity to ask detailed questions specific to the user’s actions.
Users don’t have real-time support if they have a question, need clarification, or can’t get the technology to work, although you can provide them with an email address or phone number to contact someone for assistance. This disconnect also means you don’t know what the session was like until it’s finished. If a user did run into a problem, skipped tasks, or failed to complete what was asked, you don’t know until it’s over. Some sessions may end up being unusable or less valuable, depending on the issue.
Unmoderated tests can also be quieter than moderated tests. We typically use the think-aloud protocol in usability testing, asking users to talk us through what they’re doing as they’re doing it. In a moderated study, the facilitator can gently nudge a quiet participant to share more about what he’s doing. In an unmoderated study, you can ask a user to think aloud, but there is no one there to remind her if she doesn’t do it.
Because of the lack of detailed follow-up, it is preferable to use unmoderated remote usability tests when the main focus of the study is a few specific elements, rather than an overall review. Remote studies are great for gathering data on an element or widget or for seeing the impact of a relatively minor change.
Unmoderated studies can also be good for tight timeframes: users can complete sessions on their own schedules and even simultaneously, rather than trying to fit into scheduled time slots.
Tips for Remote Usability Testing
Practice the technology. Even if you’ve used your company’s tools a million times before, test them with someone you know outside the company, mocking up a real test situation. Make sure the instructions for signing in are clear. Practice sending URLs or tasks to your user and make sure you know how the technology works on your end — and theirs.
Rewrite everything. Write tasks far enough in advance to pilot-test them. In a moderated session, the facilitator can get a user back on track if a task is misunderstood. In an unmoderated session, there is no safety net. The written instructions need to stand on their own. Every instruction, task, and question needs to be fine-tuned to eliminate the potential for misunderstanding. As we know from every study ever done on instructional design, anything that can be misunderstood will be misunderstood.
Be available. Even for unmoderated studies, be available by email (if not by phone) as much as possible to help with any potential user questions. In moderated sessions, sign in to the testing tool early, in order to know when your participant arrives and to troubleshoot if needed.
Recruit more users. No-show rates for any remote study can be higher than for in-person studies. You also don’t know the quality of an unmoderated session until you’ve watched it. It’s better to add a few more users than you think you need in order to accommodate such problems.
Learn more about remote and in-person user testing in our full-day Usability Testing training courses, which include hands-on practice writing tasks, facilitating sessions, and more.
"
103,2013-09-14,"If product teams don’t know what to do with usability results, they’ll simply ignore them. It is up to usability practitioners to write usability findings in a clear, precise and descriptive way that helps the team identify the issue and work toward a solution. 
One of the keys to helping teams improve designs through usability is making results actionable. Running a test, analyzing results, and delivering a report is useless if the team doesn’t know what to do with those results. Usability practitioners — particularly those new to the field — sometimes complain that teams don’t act on their studies’ results.  While this may be due to a myriad of issues, it is often caused or exacerbated by problems with the findings themselves. 
Usability findings have to be usable themselves: we need meta-usability.  Below are 5 tips for practitioners, old and new, to improve the usability of their results. These tips are also handy for managers, clients, product teams and customers of usability reports to help better assess the value of the reports they’re receiving.
Be specific
Vague findings don’t give product teams much to work with. A lack of detail or explanation can leave teams wondering what the problem was or at a loss about how to fix it.
A finding of “Registration was hard” doesn’t identify the problem or hint at solutions. Why was registration hard? Could users find it? Were fields for existing users to log in confused with fields for new users to register?  Were there too many questions? Were field labels unclear? Did the form ask for information the user didn’t have, didn’t know, or didn’t want to share?  Were any steps unexpected? Were buttons poorly placed? 
Findings need to be specific: “The button to register for the site had poor color contrast and faded into the page background.” Whenever possible, identify the specific area of the design, flow, or interaction that caused the user to have a problem. 
Don’t blame the user
It’s easy to fall into the trap of writing results in relation to the user. “The user was able to do this.” “Three users could not find that.” This type of finding focuses on the user, rather than on the design. This is a problem because it seems to blame the user.  It’s very easy for a team member to read a finding and think, “OK, that user couldn’t find the link, but others can” and dismiss the issue altogether. 
This also makes it hard for teams to know how a user activity translates into a change in the design. Findings should explain the elements of the design that confused users or led them down the wrong path. Was the navigational structure of the site unclear? Was a label or link poorly named? Did the way the site was organized match the users’ expectations? 
When starting a usability session, we always tell the test user, “we’re not testing you, we’re testing the system.” This is not propaganda — it’s really the most fruitful way of thinking about the test results.
Look for the bigger picture
It is easy to become so focused on the details in a usability test that a huge issue goes without notice.  While the smaller issues are important to identify, it can’t be done at the expense of seeing larger issues with the design. For example, changing button designs and link names may improve steps in a process, but it can’t fix a process that doesn’t match user expectations or meet their needs. 
Focusing too tightly on the details in a report can cause teams to add many band-aids to a design that’s really suffering from a broken leg. If users had trouble at every step, perhaps it’s the overall flow or structure that’s to blame, rather than small details along the way.
The reality is many projects don’t have the time, resources, or budget to fix large issues with designs — at least not in the near future. So keep the smaller details in the report, but include those findings about larger, over-arching issues so they don’t get overlooked. 
Help identify solutions
On most teams, the usability professional’s job is to identify issues with the design; fixing the design is the job of design and development. However, usability experts often have unique expertise in thinking about design solutions. They have first-hand knowledge of what worked — and what didn’t — in usability testing and may have years of experience in understanding what does and doesn’t work in a design. They can offer insights about potential design solutions. 
However, usability reports suffer when usability practitioners overstep their roles. Findings should not take the form of elaborate wireframes reworking the whole design. Handing over a set of new wireframes rather than a list of findings can cause resentment on the team. A quick mock-up here and there to illustrate a point is acceptable, but a full redesign document can quickly distract the team from the value of the findings.
Work closely with the design and development team, rather than simply delivering a report and walking away from the project.  When results are presented as a discussion the usability expert, who witnessed the problems (and successes) users had with the design first-hand, can add expert insights into what design solutions may or may not address the problems seen in the design. Schedule and participate in meetings where the team decides how to address issues that came up in testing. Better yet, invite team members to usability sessions and debrief with the team between sessions. 
Adding redesign recommendations to usability reports is another option. Such recommendations can help the team understand the issue and start to think of potential solutions. Suggest moving a button or changing a label, combining navigational categories or writing more explicit link names. But present recommendations as recommendations. Label them as such and explain to the team in writing or in person that the recommendations are intended to jumpstart thinking about design solutions and to illustrate usability issues, and are not presented as the only or best solution. A creative designer may come up with something even better.
Organize and rank findings
Every issue that’s discovered through usability testing is not equally important.  Further, a usability report may have 5 or 100 findings, depending on the scale of the study, the design tested, and the usability practitioner.  Teams need a way to parse through the findings and discover 1) what’s relevant to the screens, designs or elements they have responsibility for and 2) which problems were the biggest issues from a usability perspective.
Findings should be grouped with similar findings, meaning that there may be a section about navigational issues and other sections about particular pages or task flows. Beyond this, findings should also be ranked in terms of severity. Was the problem a slight hiccup that caused one user to stumble? Was it a problem seen in only 2 sessions, but which derailed the users’ entire experience? Was it something so big that the product can’t possibly be successful unless the issue is addressed? Ranking findings as low, medium or high severity helps the team understand what critical issues the usability study exposed. Don’t forget to include positive findings as well, letting the team know what’s already working. 
Accrue future value from descriptive reports
The advice in this article provides immediate value by increasing the chance that usability insights are acted upon in the current design phase.  This again increases the ROI from the organization’s usability investment by making the product better, resulting in higher conversion rates, higher customer satisfaction, fewer errors, and other measurable benefits. (Findings that are not acted upon don’t generate profits.)
Better descriptions of the study findings also provide future value by enhancing the organization’s cumulative knowledge about its customers.  If you maintain an archive of usability findings people can draw upon these insights as they plan and execute future design projects. No reason to make the same design mistake a second time."
104,2013-08-31,"Many times, when a client wants to do a usability testing evaluation of a currently in-service environment/application, they are looking for a traditional summative test: A smallish set of rigid tasks geared toward measuring key performance indicators (success rate, task completion time, and errors) with the goals of 1) discovering issues and 2) providing evidence to support investment in fixing those issues.
However, as more organizations embrace agile and lean methods, decision-making has become decentralized and traditional metric-based evidence is not as crucial for some. Organizations still want to hire usability experts to plan and conduct usability testing (until it becomes an internal competence), but they may not be as interested in generating “evidence” to secure budget for changes – instead, they want actionable findings and recommendations covering the broadest scope of their environment/application. To meet this need, we’ve found there is tremendous value in being more flexible by:

Iterating tasks between and within sessions
Improvising and customizing tasks to better suit individual participants
Inviting real-time client participation

(Note that by ""clients"" we simply mean the people who have to take action on the usability finding. Your clients can be a traditional external consulting client if you're a consultant or work in an agency, or they can be internal stakeholders if you work inside the organization.)
For flexible testing you still create a test guide, but the tasks within are not treated as unchanging components. Instead, the tasks are treated as a starting point.
There are two benefits to this:

Decreased pressure on the moderator/planner to create the perfect tasks right out of the gate: Tasks that we agree will be tested and changed if necessary are a lot easier to create.
Decreased pressure on the client: Often times, members of the client team have never observed a real usability test, so to ask them to come up with or approve a definitive list of tasks and expecting it to be thorough is silly; they simply don’t know what they don’t know. Once they start observing testing, the task ideas start flowing. 

Sample Test Guide Contents:

Study logistics (times, locations, number of participants, session duration, incentives, moderator)
Target participant criteria
Study goals
Study questions
Tasks

Top 10 tips for Flexible testing
To get the most out of flexible testing, we have provided some advice focused on the most critical aspects of planning, facilitating and management.
1. Define testing goals and intentions first
You need to determine two things:

Why the client is sponsoring the study.
What they intend on doing with the results.

It is imperative that the client answers both these questions honestly and that they are engaged in a conversation about how these goals and intentions will inform the research method selection and output. Of all your discussions, this is the one that will bear the most significant impact on your ability to meet expectations.




Matching Client Goals/Intentions with Testing Methods




 


Summative Testing


Flexible Testing




Generate statistical evidence


x


 




Convince executives to make changes/allocate budget


x


x




Inform design strategy


 


x




Guide design specifications


 


x




Inform design changes


 


x




Identify/prioritize issues with existing design and content


x


x




Identify good characteristics of existing site


x


x




Acquire a “safe feeling” of signing off on an exact, unchanging test plan before the research starts


x


 




2. Generate very specific questions the study must answer
The easiest way to come up with tasks to test is to start with the questions the study should be answering. I used to focus on asking clients to tell me their “mission critical tasks”, but that can limit your ability to come up with tasks that actually touch and test attributes that clients believe are problematic.
After goals and intentions are defined, ask the client to give you a list of questions they want answered in this study. You can use some of these sample questions to get the ideas flowing:




Potential Questions to be answered in Usability Testing 
(Web site/Application)




Navigation




Do people recognize the global navigation as navigation?




Do people use local navigation in [location]?




Do people use the navigation system to understand where they are?




Product/Content Findability




Does site search yield good relevant results?




Do overview pages effectively route users down a path?




Do people use filters? Under what circumstances?




Product information




What information do people fixate on?




What information do they have trouble finding?




What information do they want to see?




What is information is confusing?




Do users use related links?




Do users seek reviews?




Checkout




Can people easily find a way to checkout?




Are they nervous about any required/requested information?




Are there any issues filling out forms?




3. Design tasks to answer questions
The best way to ensure clients get what they need is to create testing tasks that expose the qualities of the site they are interested in evaluating. This doesn’t mean that you have to create a task for each question. It’s likely that typical user task scenarios will address several questions each and may overlap as well – and that’s OK.
4. Get as many client observers as possible
Testing is always better when observed by as many stakeholders, creators, and controllers as possible. But this is even truer when the tasks are intended to change. More brains lead to more ideas, which lead to more movement.
5. Incorporate competitive offerings into the test plan
If possible, you want to include some tasks to be performed on competitive sites so there is context. Also, this tends to be one of the most controversial topics in any organization along the lines of “So-and-so does it this way and it’s better.” The first benefit of observing competitors in any usability study is that it can prove/disprove those statements. The second benefit, specific to flexible testing, is that observing people using environments beyond your own can generate all sorts of additional task ideas.
6. Use pre-task questions to inform task improvisation
Define a series of questions to ask at the beginning of each session to better understand your participants.
Questions can focus on:

Needs: What are they seeking?
Experience: How do they use particular information/content/products?
Intentions: Why do they use particular information/content/products?
Knowledge: What do they know about this particular topic?

7. Physically separate the client team from moderator/participant
You want your team to be reacting and discussing together, so they can suggest task changes and formulate questions for the moderator between sessions. If they are in the same room as the participant, team interaction is limited to facial expressions and maybe passing notes (which, by the way, is totally noticeable to the participant and can be unnerving). It’s best to keep your client team in a separate room from the facilitation. This applies to traditional “lab” sessions as well as remote-user sessions where, ideally, the facilitator should be alone in the room.
8. Define ground rules for real-time participation so clients know how to help instead of hurt
Inform your client team of what is happening and how they can optimize the process for their benefit:

Tell them this testing is flexible – it’s intended to change.
Tell them why: Because as you watch the testing, you may get additional task ideas or become curious about other features or content that may be suitable to a particular participant (you know your site best!).
Do not allow task suggestions/changes in the first session – ask the team to watch it without real-time feedback.
Reserve a generous amount of time (at least one hour) to debrief after the first session: Collect feedback and make test plan changes.
Have the team elect one representative who will be responsible for sending you real-time task suggestions starting with the second session.
Tell them what is useful and what is not:
	
Do not send requests to ask for user opinion (“Do you like this logo?”).
Do send ideas about products you have that this user may find useful (“This user said she likes to golf, we sell a line of golf shoes – maybe we can see if she realizes that and then see if she’s able to find a pair that meets her needs?”).
Do send ideas about features/content that might suit a user based on their pre-test answers or behavior (“This user seems really fixated on peer reviews; any way to see how they react to our Q&A content?”).



9. Make sure users are minimally disturbed by the conversation between moderator and client
If you are using conference calling technology to broadcast and/or record your sessions (e.g., GoTo Meeting, Webex), do not also use that channel for the client team to talk with you. Cell phone texting (on mute) is less disruptive, so the user does not lose their train of thought or see information intended only for the research and/or client team.
10. Debrief and prep between each session
Between each session, reserve enough time to chat with the client team, get feedback on the facilitation and information derived, and make changes to the tasks.

Review each task and ask for edits.
Ask if they want to retire any tasks: This can happen when a team has seen enough  people failing on the same task. If they saw it two or three times, they’d rather use that time to expand the scope.
Quickly outline the tasks for the next participant.
Make your task sheets (if facilitating in person): If you have access to a printer, then type these up, but it’s fine to use pen and paper when pressed for time.

Getting the Most from Flexible Testing
Using a flexible testing method can help you observe a greater range of site features because tasks are allowed to change as client questions are answered and as tasks are adjusted or created to better suit individual participants. This method works well for organizations who are less concerned with generating metrics from usability testing and more interested in immediate action items.
Ideally, use experienced usability facilitators that can accommodate this constantly changing game plan and welcome the challenge. Experience helps, because there's no time to pilot-test the changes, so you need to get them right the first time. However, inexperienced staff can use a modified version of flexible usability testing where they treat the entire study as an extended exercise in iterative pilot testing and gradually make their tasks and test methodology more and more appropriate. Not as good, of course, but then the only way anybody gets to be an experienced usability expert is to start off with no experience and gradually get better. Flexible testing allows you to get better faster."
105,2013-04-14,"When can you trust recommendations from usability studies or other user research?
This question arises in two main cases:

Planning your own research. You need to prioritize your research investment to maximize its impact on your project, thereby increasing the probability of deriving a higher-profit design.
Reading about outside research. It’s important to know how much you can rely on other people’s findings—is it safe to base your design decisions on outside research? Which research should you trust when findings conflict?

Luckily, in both situations, the answer is almost the same. You should trust the same kind of research that you should invest in—that is, research that derives its conclusions from a broadly diversified base.
The main difference in the two situations is that ROI considerations dictate a smaller investment in your own studies. The sample sizes and other costs of making really, really sure are not worth it. Thus, you should implement the research recommendations even if, say, a statistical analysis indicates a 10% probability of being wrong. Better to have the right design 90% of the time than to shoot blind and design without the benefit of research.
(Make no mistake: if you require perfection in your internal research, you’ll have no research. Nobody has the budget to conduct exhaustive research on every small design decision; the only realistic choice is between no research and decent—if imperfect—research. Go with the option that gives you some data, because it’s much better than guessing.)
Strength in Numbers?
The one factor most people consider when judging research evidence strength is the sample size, usually known as N. How many people were in the study? Another common consideration is the level of statistical significance, often known as p.
However, a big N or a small p is a horrible indicator of validity when it comes to research findings.
Yes, statistical significance can be computed precisely, but that only tells you something that’s not terribly important: the probability of getting the same result by repeating the exact same experiment again.
Crucially, this says nothing about whether the experiment was done right or has any predictive power for your design problem. And these two issues are essential when deciding whether to trust research findings.
Statistically significant research findings are vulnerable to 3 key problems:
Study was done wrong. Almost all usability research has weaknesses. The most common one is biasing the respondents, such as by talking too much. Sometimes, the study is just poorly designed; many eyetracking studies, for example, simply show people a static screenshot and record how they look around the image. But the way people look at a standalone image is completely different than the way they look at a sequence of screens encountered during normal website navigation. Obviously, if the methodology was inappropriate or outright incorrect, it doesn’t matter that it’s highly likely to get the same (wrong) outcome if you run the same (wrong) study a second time.
Study doesn’t generalize. Almost all academic research uses university students as test participants. Unless you happen to be designing a site for students, this means that the findings could be irrelevant for your target audience. Even if a study recruited users with a similar profile to your customers, the types of tasks and design styles were probably different than your own. Usability is highly context-dependent; what’s good for one set of users and tasks might be terrible for other people doing something else.
Study was a fluke. A study might claim a finding to be statistically significant at the level of p<.05—that is, there’s only 5% probability that the finding was a random coincidence. That sounds pretty good, until you realize that many more than 20 usability studies are run every day around the world. Given publication bias, however, you’ll hear only about the one study with a freak outcome; the 19 (or more) other studies that arrive at the truth are too boring to be published because they confirm what we already know. (Usability findings don’t change much over time.)
Research Diversity
Rather than a big N employed narrowly in a single study, it’s better to trust in research that covers a diverse set of circumstances. If findings are derived from a broad base, they’re more likely to generalize and apply to your specific situation—and not just to the study stimuli.
Usability research should be diversified in several ways:

Users: test with consumers, business professionals, executives, geeks, doctors, children, teenagers, students, senior citizens, and many other groups.
Skills: test experienced users, high-IQ elites, people who know little about computers, low-literacy users, etc., and include users with disabilities.
Tasks: shopping, health research, checking news and investments—the list of tasks people do online goes on and on. If you want to know, say, how people search, don’t just ask them to search for one thing.
Companies or sites tested: big sites, small sites, famous brands, no-name sites.
Technology platforms: text-only UI, GUI, mobile phones, tablets, 3D.
Longitudinal research: compare results from 10–20 years ago vs. today. If they’re the same, they’ll probably hold in the future as well.
Internationally: test in multiple countries.
Methodologies: triangulate findings by combining methods across the many different ways of doing user research, including user testing, A/B testing, eyetracking, diary studies, and field research.

Finally, for internal use, you should also conduct research across the project lifecycle stages—that is, before you’ve done any design, with early prototypes, through iterative refinement, and after the product launches.
By conducting a wide variety of research, you ultimately achieve the goal of having tested a large number of users. For example, Nielsen Norman Group has tested 2,048 users in one-on-one usability sessions. More important, we tested these people with 1,524 websites and intranets in 14 different countries across North America, Europe, Asia, Australia, and the Middle East. Given this broad diversity, our findings are very likely to hold in circumstances beyond those in our testing.
When allocating your own research investment: spread the budget across multiple, smaller studies. When reading outside research: place greater trust in sources that conduct diverse research than in sources that have a large number of users do a single thing that probably doesn’t generalize to your circumstances.
See Also
Nice analysis in The Economist of the high probability of wrong quantitative results being published in academic journals. In one example, cancer researchers tried to replicate 53 published studies but could only confirm the findings from 6. In another example, pharmaceutical researchers only got the same result a quarter of the time when repeating 67 so-called ""seminal"" studies.
The article has a helpful visualization of the statistical outcome of 1,000 research studies under fairly reasonable assumptions: 125 of the studies would be published with 80 correct results and 45 wrong results. The remaining 875 studies would have a much higher accuracy rate of 97%, but because they didn't find anything interesting they would not be published. Because of this publication bias, only 64% of published results would be true, in this example, despite following research protocols that produced good accuracy among all (published, as well as non-published) studies.
Given the low accuracy of published research, I am extra proud of Nielsen Norman Group's track record over the last 15 years. The vast majority of our findings have subsequently been confirmed by other usability specialists running independent studies. We have a better chance of being proven right by other studies, not because we're more honorable than other researchers, but because we rely on qualitative insights before we publish a guideline. Other researchers' errors are rarely caused by outright falsified data. Mistakes are much more frequently due to statistics than to fraud. So two researchers can be equally honest, but the one that uses our method will be right more often."
106,2012-10-07,"Do users like systems with better usability? You might think that's a stupid question, but research shows that the full story is a bit tricky.
To give away the answer: yes,  users prefer the design with the highest usability metrics 70% of the time. But not 100%.
Measuring Preference
To operationalize the question, we must get into details. My assessment of user  preference  comes from a simple satisfaction question:  On a 1–7 scale, how satisfied were you with using this website (or application, intranet, etc.)? Averaging the scores across users gives us an average satisfaction measure.
It's extremely important to note that we always give the test users our (very short) satisfaction questionnaire  after  they've tried using the design. It's completely  invalid to simply show people some screens  and ask them how well they like them. If people haven't actually used a user interface to perform realistic tasks, they can't predict how satisfied they'd be when actually using the system. (And real use is what matters, after all, not what people say in a survey.)
Because we measure preferences by simply asking users, the metric is inherently subjective. But it's a metric nonetheless. The question here surrounds the possible relation between this subjective metric and more objective measures of system usability.
Measuring Performance
Referring back to the  definition of usability, we find several measurable quality attributes that combine to form the bigger construct we call ""usability."" One is subjective satisfaction, as just discussed. Other — more objective — criteria include time on task,  success rate, and user errors.
To calculate objective performance metrics, we basically ask users to perform representative tasks and record how long it takes them (and whether they can do the task at all).
Quantitative measures are harder to collect than simpler usability insights, so we don't include them in all our studies. Of the 1,733 sites and apps we have systematically tested in Nielsen Norman Group, we have good  quantitative and subjective metrics for 298 designs.
Comparing Objective and Subjective Metrics
The following chart shows the combined objective and subjective usability metrics for the 298 designs where we measured both. Each dot is one website, application, or intranet.


The x-axis shows how well users performed with that design on the objective performance metrics, such as speed or errors.


We recoded the raw numbers to a uniform system that lets us compare very different classes of systems. After all, whether it's good or bad to have a task take 5 minutes depends on how quickly users can perform that task with alternate designs. I thus calculated how many standard deviations each system scored relative to the mean of its peers. Also, I made sure that bigger scores in the chart always represented better usability. So, for example, for user errors, smaller numbers are better, so being one standard deviation below the mean error rate would be shown as a score of +1.
The  y-axis shows how favorably users rated each design on the subjective satisfaction survey. To make this metric comparable with the  x-axis, I also converted those raw scores into standard-deviation scores.
Thus, dots to the  right  of the vertical axis represent designs on which users performed  better  than average; dots to the left represent designs on which users performed  worse  than average.
Similarly, dots  above  the horizontal axis represent designs that users liked  better  than average, while dots  below  it represent designs that users rated  worse  than average in terms of satisfaction.
Correlating Performance and Preference
The red line is the best-fit regression between the two types of usability metrics. It's clear that there's a strong relation between the two, with a  correlation of  r  = .53.
In other words, if people have an easier time using a design, they tend to rate it better in satisfaction surveys. But the correlation is not a clean 1.0, so there's more at play.
The  paradox of subjective satisfaction  is that  objective and subjective metrics sometimes conflict. It doesn't happen often. Here, for example, 70% of the dots are in the expected quadrants:

Upper right:  designs on which users performed better than average and that they liked more than average.
Lower left:  designs on which users performed worse than average and that they liked less than average.

The  paradoxes are the 30% of dots  in the unexpected quadrants:

Upper left:  designs on which users performed worse than average, but that they liked more than average.
Lower right:  designs on which users performed better than average, but that users liked less than average.

However, there are  no strong paradoxes  — that is, cases in which users performed  much  better and strongly disliked the design, or cases in which users performed  much  worse and strongly preferred the design. (Such strong paradoxes would have appeared as dots in the chart's extreme upper left or lower right corners, respectively.)
Here, we find only  weak paradoxes: cases in which users performed a  little  better and slightly disliked the design, or cases in which users performed a  little  worse and slightly preferred the design anyway.
(If anyone counts the dots in the chart, they'll notice a small twist: The chart includes 298 dots, representing the 298 Nielsen Norman Group studies that measured both subjective and objective usability metrics. But the 30% paradox estimate comes from an analysis of 315 cases — that is, it includes a few more cases where I could determine the agreement or disagreement between performance and preference, but didn't have enough data to plot those last 17 dots.)
Consider Both Satisfaction and Performance Metrics
There are two practical takeaways from this data analysis:

Performance and satisfaction scores are  strongly correlated, so if you make a design that's easier to use, people will tend to like it more.
Performance and satisfaction are  different usability metrics, so you should consider both in the design process and measure both if you conduct quantitative usability studies."
107,2012-09-09,"It almost doesn't matter where you conduct user testing. If you're lucky, you have a fancy  usability laboratory  with multiple cameras and one-way mirrors. The key benefit of such labs is the ability to  stick observers in a separate observation room  and leave the user and facilitator alone to  concentrate on the test tasks.
The #1 method for evangelizing usability is to get as many  stakeholders as possible to personally observe real users. A good usability lab with a comfy observation room (well-stocked with snacks) encourages an invaluable side effect of user testing: designers, developers, and managers become emotionally invested in fixing usability problems they have seen themselves.
What should you do if you  don't have a lab? After all, dedicated lab space usually appears only after a company has reached stage 5 on the  1–8 scale of corporate UX maturity.
Luckily,  user testing can be done anywhere, as long as you can close the door and ensure the privacy and focus of your study participant. Some of the more common locations for user testing include:

Conference rooms
Private offices
Rented space in ""office hotels""
Hotel meeting rooms
Airport lounges (assuming you can book a boardroom-style space)
Tradeshow booths (ideally in a booth with a private back room, though you can do short 5-minute tests standing at a demo station in the back of a regular booth)
Company cafeterias
Coffee shops (not the Amsterdam kind :-)

The key elements in user testing are the facilitator's skill in drawing out  user behavior  without biasing the user, and the facilitator's analytical skills in determining  valid and useful design conclusions  from observations of this user behavior. These two crucial points don't depend on the specific location or on having lots of advanced equipment at hand.
Portable Lab Equipment
Nielsen Norman Group runs  many international studies and often tests at client locations. For the vast majority of our user research, the  equipment fits in a carry-on bag.
Here are photos of the set-up for our study in Sydney, Australia, last month:

 

The equipment:

a big laptop for the user;
a small laptop for the facilitator;
a webcam to record the user's  thinking aloud comments and facial expressions; and
a real mouse for the user (we don't want users to have to struggle with a touchpad to click links).

That's all. We ran this study at our conference hotel, and they provided the coffee cups.
The  user laptop should be hefty  for three reasons:

A good, full-sized keyboard encourages users to type as much as they want with minimal typos.
A large screen ensures that even middle-aged or older users can see the information.
There's sufficient computer power to run whatever websites or applications users encounter while still having leftover capacity to run Morae (the screen-recording software we use).

The big screen also allows an additional observer (besides the facilitator) to sit slightly behind the user and still see what's on the screen. You want the observer to be out of sight—and out of mind—as far as the user is concerned.
(Unfortunately, when testing  mobile devices, we need a bit more equipment. Although users typically bring their own phones and tablets, we need a fancier camera to capture the small screens. That camera requires a bag of its own, so we usually ship it ahead of time.)
Testing in Progress—Keep Out
Here's one more artifact from the Sydney study:

We stick a sign like this on the door when we test in locations that are unaccustomed to usability studies and their associated protocols. We really don't want the hotel staff to interrupt the study; nor do we want people wandering the hallways looking for other meetings to poke their heads into our room.
If you conduct user testing at your company, it might be nice to have the sign say something like, ""Usability Study in Progress."" This will give you a bit of internal PR, as colleagues pass the conference room and note that you're doing more testing.
But when you test in hotels or other outside locations, it's too much to assume that anybody will understand what ""usability"" is or why it's bad to disturb a test user during a study. We therefore go with ""Recording in Session,"" which everybody understands. People might even be quiet as they pass the room.
Accommodating More Observers
For international testing, you're lucky if the client will invest in flying out one observer. Still, it's sometimes nice to be able to accommodate multiple observers, who can't all squeeze in behind the user's chair.
In such cases, one option is to hook up a  slave monitor, as shown in this photo from a study we ran in Hong Kong:

Here, we simply used a mid-sized HD TV, which is available at most hotels (and thus you don't have to ship it around). Because the projected image of the user's screen is fairly large, a handful of observers can crowd around the TV.
As the photo shows, from the user's viewpoint, the observers are pretty much screened out by the bulk of the television. Again, you want observers to be out of sight and out of mind as far as the user is concerned.
If you have many observers, you can run the video cable to an adjourning conference room and use a projector to generate as large an image of the user's screen as you want. It's a bit trickier to pipe in sound, but you can do that as well—sometimes through the low-cost trick of using speakerphones and simply muting the one in the observation room.
No Excuses
No equipment? That's no excuse for not testing your design.
You can test a  paper prototype or you can use a laptop (which I'm sure you have anyway). Although we like to show video clips from our research in our training seminars, for in-house studies, you don't even need the webcam or recording software.
No usability lab? Not an excuse either.
As I've shown here, you can test almost anywhere, in any country (including your own!), with a small amount of equipment that you can either scavenge on site or bring in a carry-on bag. The one thing you do need, however, is  real users who represent the target audience.
For more info on how to set up a usability study, run it, and analyze the findings, see our full-day Usability Testing training course."
108,2012-07-15,"Consider the following scenario: you're advising the project manager of an upcoming design project (that hasn't started yet) and are told that there's  time and budget for exactly one usability activity. What should it be? And when should this one activity be scheduled in the project plan?
Take a minute to consider your answer before reading mine. What would you do in this liberating — yet frustrating — situation? You have the choice between  all the usability methods in the world, but you are limited to doing only one thing.
(For our purposes here, you can't cheat by saying that the ""one thing"" is a prolonged  iterative study with many phases even though that would be better; I really want a single, focused usability step.)
Now I'll present my two preferred solutions and then discuss the tradeoffs between them.
Option 1: Field Study before Starting the Design
Usability professionals always complain that they're called in too late to do much good. And rightfully so: Once the feature set has been decided, you're basically left with lower-value methods to clean up the UI.
In my scenario, you're asked for usability advice before the project has begun; it's thus possible to actually conduct user research up front and use the results to determine what should be designed in the first place.
So, you can go to the users' office, home, the factory floor, the hospital, or wherever people do the type of activity the project aims to support. Play the proverbial fly on the wall:  observe what people do in real life.
While on the field study, remember the key methodology lessons:

Stay quiet  so that you don't bias the users.
Don't trust what people say; watch what they  do.

As I've always said,  users are not designers, so you should take their feature requests with a grain of salt. Instead, watch for opportunities to introduce game-changing features that will improve user performance by an order of magnitude or more. People don't know to ask for these new things, but you can spot prospects for great design when you observe users wasting time on workarounds — or simply giving up on even  trying to accomplish desirable tasks.
Option 2: Early User Testing of a Low-Fi Prototype
Alternatively, you can wait until an early design has been created and make a  simple paper prototype and  test it with a handful of users. You can then revise the design to fix the myriad usability problems the test revealed.
At this early stage, it's still possible to make major changes to the user interface architecture, move things around, redesign features, and even remove features that prove useless or too confusing. You might even be able to introduce a few new system features, since you're testing before anything has been coded.
The ability to  redesign the functionality  and not just the surface UI is one of the biggest reasons to use paper prototyping.
Another reason is that it's  100 times more expensive  to make a change after a system has been fully implemented than to make the same change early in the design process when the system is just a set of drawings. Pragmatically, management is much more likely to accept your redesign recommendations if you make them while change is cheap.
If you wait until everything is done and then conduct a usability study to ""validate the design,"" it'll be too expensive to change anything substantial — and you'll be left with minor fixes, like making icons easier to recognize.
Because  user testing shows you how users think and behave, the findings will also help your team as they progress to finish the product. Ideally, it's best to get even more findings by running additional rounds of user testing on these more-refined designs, but let's stick to the rules of this exercise and limit ourselves to a single test.
Risk Management Decides Choice of Method
Which of the two options should be your one shot at improving the hypothetical project's user experience? Let's start by looking at the gains you can expect from each choice:

Field research  to decide what should be designed has the potential to uncover a  radical innovation worth maybe  1,000%  more than the competition.
User testing  followed by a single design iteration  improves measured usability by  38% on average.

The choice seems quite clear; after all, a 1,000% gain is much more profitable than a 38% gain. So, even if the field study is somewhat more expensive than the simple paper prototype test, it seems like you should pick Option 1.
Not so fast. Read the above bullets more closely: field research has only the  potential  to discover a hugely profitable radical innovation. It's a much riskier option.
I do believe that field studies are typically more valuable than a single round of user testing. But that's only true if you can  trust the design team  to deliver a high-quality user interface without major usability problems.
Most companies don't really know much about usability, and teams routinely create designs littered with UX disasters that stump users and/or drive them away from the site.
If the final user interface is terrible and makes the features virtually impossible to use, it won't matter whether the field study inspired the team to invent awesome features. If the target audience can't use something, it might as well not exist.
This is why I ultimately come down on the side of recommending one round of  user testing as the safest approach. With user testing, you're sure to get a product that users will actually use. Without it, you might still have a success — but you're more likely to ship a dud.
Now, if you  know  that your project organization has a  high UX methodology maturity, then I might reverse my recommendation in favor of a field study. In this ideal case, the team will have already conducted intensive user research on past projects and internalized the usability lessons. Also, your company will have well-documented UI standards and design patterns to draw upon.
In the real world, few companies are advanced enough to safely ship a design without user testing. More than 90% of teams would be better served with the safer (if less exciting) option of user testing instead of field studies, if they can do only one thing.
We have one last clue to estimate organizational maturity: the fact that we were limited to a single usability activity. Companies that truly believe in usability will typically  plan for a sequence of usability activities  throughout the project that includes both iterative design and several other user-centered design methods. Different types of user research supplement each other and create more value than repeatedly doing the same one thing.
Of course, there are cases where even the most customer-focused company needs a fast design without much usability input. But in most cases, having time for only one usability activity is a symptom of fairly limited usability maturity, meaning that the company cannot be trusted to deliver a usable design without having it cleaned up through user testing."
109,2012-03-25,"Let's compare 3 different approaches to achieving better design:



 
A/B Testing
Usability
Radical Innovation


Cost 
Low
Low–medium
High (unless lucky)


Benefits 
1–10%
10–100%
100–1,000%


Risk 
None
Low
High


Who can do? 
Everybody
Everybody
Geniuses


How often? 
Weekly
Monthly
Every 10 years


GDP impact 
Medium
High
Medium



Definitions:

A/B testing splits live traffic into two (or more) parts: most users see the standard design (""A""), but a small percentage sees an alternative design (""B""). After collecting statistically significant numbers, the design with the best KPI (key performance indicator, such as conversion or  bounce rates) becomes the new standard. (One can test more than 2 design variations at a time through a related method called multivariant testing: for the sake of this article I'll lump these analytics methods together and talk about AB, regardless of the number of variables being measured.) As a form of web analytics — or intranet analytics — A/B testing has the huge advantage of testing live traffic to your website (or intranet).
Usability refers to the  full range of user-centered design  (UCD) activities:  user testing, field studies,  parallel and iterative design,  low-fidelity prototyping,  competitive studies, and many  other research methods. These types of studies usually require you to recruit test participants who should be representative of your target audience.
Radical Design Innovation creates a completely new design that deviates from past designs rather than emerging from the hill-climbing methods used in more standard redesign projects. Example innovations include fundamental breakthroughs (the steam engine),  new product categories (the locomotive), and major reconceptualizations of existing categories (the iPhone).

Cost
Incrementally, A/B testing is very cheap. You do need to pay a designer to create the ""B"" design, but most of the cost lies in the software to run and analyze the test, which is a one-time expense. Thus, if you're going to do it at all, you should run lots of A/B tests.
With usability methods, the costs range from  $200 for a few quick activities to  $38,000 to have a website analyzed by an independent expert. However, even the high end of usability costs pales in relation to the full cost of any enterprise-scale project. What's the budget to run a big-company website for a year? Easily a million when including staffing costs and overhead.
In contrast, radical innovation quickly runs into tens or hundreds of millions of dollars for fancy research labs and experimentation — and even then the vast majority of inventions go nowhere. Sure, a blinding insight occasionally creates a wonderful invention without the need for elaborate research. The invention of vulcanized rubber and penicillin are canonical examples of luck causing radical innovations. Still, luck favors the prepared mind, and Sir Alexander Fleming had already spent considerable time on fruitless bacteriological experiments when he stumbled across penicillin.
Benefits: From 1% to 1,000% Improvement
There's easily an order of magnitude difference in the effect size expected from each of the 3 design approaches:

A/B testing usually identifies small improvements that might increase sales or other KPIs by  a few percent. Sometimes you're lucky and get 10% or more. Website analytics' advantage is that it's the only way to reliably determine the best design approach when there's little difference between alternatives. Perhaps 1% doesn't sound like much, but if you could realize such gains every week for a year, you'd cash in more than 50%.
In contrast, the full usability process typically  doubles the benefit of the metrics you target (i.e., you get  100%  improvement). For example, an enterprise software project might cut training costs in half or double employee productivity. If the old design was particularly poor, we sometimes see gains of 1,000% or more, but that's rare. More narrow usability efforts, such as fixing a particular design element, might result in a 10% gain for the desired usage metric.
The sky is the limit for radical design innovation. Something sufficiently good could be  1,000%  better than what went before. When defining a completely new product category, you can claim infinite gains, going from  zero  sales to  some  sales. However, more realistically, the new thing should be measured against the opportunity cost of business as usual, which surely is better than zero in any company that can afford to invest in advanced development.

Risk
There's virtually no risk in A/B testing: assuming that the statistical analysis is done correctly, there's close to 100% probability that you'll choose the design variation that makes the most money. If the difference between the two designs is small, you might have to wait a long time to collect enough traffic for statistical significance. But when the alternative design is only marginally better (if it's better at all), you lose little by sticking with the old design during a drawn-out test.
Usability methods also carry very low risk. Indeed, subjecting all designs to usability studies before shipping is prudent  risk-management. Any terrible ideas that emerge from your fevered imagination will be shot down when confronted with real customers during user testing.
Radical innovation is extremely risky. Yes, you might invent the next iPhone. But you're more likely to invent the next Newton (Apple's early and doomed attempt at a personal digital assistant). In fact, almost all innovations fail. Even if it's a good idea, an innovation might be too early for the market. Pets.com, for example, is known mainly for its sock-puppet commercials during the dot-com bubble and for its  spectacular bankruptcy, but other companies that eschewed the purported first-mover ""advantage"" are now making money selling pet food on the Internet.
Who Can Do It?
A/B testing can be done by a monkey (if the monkey has a graduate degree in statistics or can use stats software correctly). You don't need to understand design principles or user behavior. Simply try something different than your current design. If it scores better, keep it on the site. Otherwise, try something else.
Advanced usability methods often require trained specialists, but simple usability activities can be  done by any member of a design team.
Returning to the theme of luck, it's certainly possible for below-genius personnel to be lucky and stumble upon a radical innovation. But typically, to systematically target radical innovation, you need to employ the best people in the world. Ask yourself whether you can realistically recruit, say, the top 1% of experts for your project. Even if you could, that's likely true only for 1% of projects. The remaining 99% of projects must make do with less exalted staff members, who can still be plenty talented and capable of more everyday advances — like the ones actually needed for most projects to succeed.
How Often Can Improvements Happen?
Websites with enough traffic can finish most A/B studies in a day or two. The main limiting factor is your ability to dream up new design variations. Also, designers need some time to realize each idea as an integrated user experience to give it a fair chance against the current site. But basically, there's no reason you shouldn't run a new A/B test  every week.
Usability certainly can be done weekly as well. Since 1989,  I've been evangelizing much faster and cheaper usability methods than most companies employ. (By comparison,  Agile UX design is an upstart.) However, despite my best attempts at increasing the pace of usability,  monthly  turnaround is more common, so that's what I put in the table at the beginning of this article.
Realistically, for something to be truly radical, it can't happen too often. You can find a few exceptions that prove the rule, but most companies are happy to realize a radical innovation once every decade. (Most don't achieve this, even when they try; either their innovations end up being more modest or they fail completely.)
Economic Impact
Given the hugely bigger profit potential from a winning radical innovation, you might expect this approach to have the highest impact on the overall world economy. But all experience shows that most value is realized not from original breakthroughs but rather from the thousands of tweaks and implementations that build on that original work over subsequent decades.
Radical innovations soon become commodities: everybody has electricity, locomotives, and computers. What matters is what companies do with them: the railroad that invents a better way to ship grain might make more money than the company that invented the locomotive. Similarly, while the iPhone was a great advance, Android and other competitors are gradually eating market share.
On the other hand, because A/B testing usually results in tiny improvements, you might expect its overall contribution to be modest. Not so, since it's a method you can systematically apply again and again. Many small streams become a mighty river.
You can determine an approach's value by multiplying 3 factors:
V   =   G   ×   F   ×   N,
where

V   = total  value, in terms of contribution to the economy's GDP.
G   = the  gain  from each improvement. Radical innovation totally wins this one.
F   = the  frequency  with which improvements can happen. A/B testing wins here, with usability as a close runner-up. Radical innovation is left several laps behind on this racetrack.
N   = the  number  of firms that participate in creating innovations. Again, the everyday methods win here because they can be employed by all companies, all the time. Working on payroll processing software? Usability will make it much better, but we'll probably wait 50 years for radical improvements in the product line. (Cloud-based processing might be somewhat better, but surely doesn't count as ""radical"" these days.)

Radical innovation is worth a lot when it happens, but it happens rarely and thus has only a medium-sized impact on the economy. A/B testing also probably has only a medium impact on the overall economy because many of its gains consist of moving market share among competing companies. Usability offers steady product improvements across a broad range, and its productivity gains are cumulative, resulting in a high potential for raising GDP.
The rough estimate in Tom Landauer's  The Trouble with Computers  is that GDP growth would increase by one percentage point if all companies employed sound usability methods. This may not sound like much, but it would add up to  $2 trillion more  in the United States alone over the next 10 years. (€1.5 trillion in the E.U.)
What to Choose?
Because usability makes the most money on average, it's the strategy that I recommend. (No surprise, if you've read my past articles.)
But, in truth, there's  no reason to limit yourself to a single strategy  because the 3 approaches complement each other well.
If you have the budget (or the luck), do try for radical innovation. But employ usability engineering to check whether your ""innovation"" is in fact any good before investing a fortune in bringing a failed product to market. And, once you have a product, refine it through both usability and A/B tests to ensure  continuous quality improvement  and stay ahead of the competition. In the long run, the cumulative effect of many small quality improvements is worth more than rare big breakthroughs."
110,2011-11-20,"Last week, I made a slide for the new  User Experience (UX) Basic Training course with the  recommended number of test users  for different types of studies. I like teaching foundational courses because they afford me just this kind of opportunity — to distill 25 years of usability process research into a single table. Patterns crystallize when complex topics are condensed to the essence.
For example, why do we recommend  testing more users for card sorting than for  usability studies? Because the usual rule,  ""we're testing the system, not you,""  doesn't apply to card sorting. When eliciting  mental models, we're actually testing the individual users instead of a predefined artifact, and the variability is thus larger.
The thing that surprised me most about my own table: I recommend doing most  quantitative user testing with a sample size that typically entails a  19% margin of error.
19% sounds sloppy. How come a fairly low level of accuracy usually suffices in estimating usability metrics?
Two reasons:

A 19% confidence interval pretty much represents the worst-case outcome. Usually, the error is much smaller.
The  average usability difference between websites is  64%, so even in those few cases where we get a 19% measurement error, we'd usually pick the correct winner anyway.

These mathematical points suffice to defend the idea of saving budget and limiting quantitative studies to mid-sized samples.
But there are two deeper arguments that are even more important.
Focus on Big Problems
You  shouldn't care about small issues  in usability. At this stage, we still have bigger fish to fry. When redesigning a website for usability, the average  improvement in key performance indicators (KPI) is  83%. Clearly, most websites still contain horrible usability problems. Intranets and mobile sites/apps are often even worse.
Your focus should thus be on the really big design problems, where your user experience is failing to meet customer needs. Typically, there are only a few  issues with immense bottom-line impact. Better to invest heavily in those crucial improvements than mess around with changes that'll gain you only a percent or two.
Wasting your budget on overly precise measurements can easily sidetrack you from the important issues; for sure, you'd have less budget left over to work on them.
Maybe in 20 years, user interfaces will be good enough that our only remaining goal will be to fine-tune them for the last few percents' quality gain. That's definitely not the case today."
111,2010-07-25,"What users  say  and what they  do  are different — a point I've made countless times. I even wrote a column entitled ""First Rule of Usability? Don't Listen to Users"" that's as relevant today as it was in 2001. (The best usability methods are highly stable, which is why learning valid methodology has such a strong career-long ROI.)
Observant readers have complained that I violated my own prescription in my recent analysis of  response-time delays. In that article, I reported on a series of  interviews  we conducted with users when researching the concept of Brand as Experience. So, what's with suddenly asking people what they think instead of observing their actual behavior?
The answer is that interviews are in fact an appropriate user research method — if you use them only in the few cases for which they generate valid data.
What Interviews Can't Do
Before getting to the good side of interviews, let's review their many bad points. (Many of which they share with  focus groups which are vastly overused in web design projects.)
The critical failing of user interviews is that you're asking people to either  remember  past use or  speculate  on future use of a system. Both types of responses are extraordinarily weak and often misleading.

Human  memory is fallible  (as discussed further in our  Human Mind seminar). People can't remember the details of how they used a website, and they often tend to  make up stories to rationalize  whatever they do remember (or misremember) so that it sounds more logical than it really is.
Users are  pragmatic and concrete. They typically have no idea how they might use a new technology based on a description alone.  Users are not designers, and being able to envision something that doesn't exist is a rare skill. (Conversely,  designers are not users, so it doesn't matter whether they personally think something is easy.)

Envision a  timeline  of user comments: only one spot generates valid data — the present: what the user is doing  right now. Having users misremember the past or mispredict the future should both be shunned.
One of the reasons that  specs don't work  is that users (and management) can't tell whether a specification documents something that will solve their problem once built. It sure sounds good in writing, but there are endless case studies of ""user reps"" signing off on stuff that ended up as big failures.
This is why  Agile development and  paper prototyping methods are valuable. When users have something concrete to interact with, it's usually obvious when you're solving their problems in a way that's easy and pleasant to work with — and equally obvious when you're not.
Most  specific design questions can't be answered by interviewing users. Here are some of the things you won't learn in an interview:

Should the  Buy  button be red or orange?
Is it better to use a drop-down menu or a set of radio buttons for a certain set of choices?
Where should the Foo product line reside in the IA?
Is it better to have 3 levels of navigation, or should we stick to 2 levels even if it means longer menus?
How should you write the Help information to best teach people how to correctly use the system?

Sure, you could ask users each of these questions, but the answers will be completely unrelated to effective design for a real website. Dilemmas relating to specific UI elements can be resolved only by watching users interact with a design that implements a specific solution, so that you can see how well it works in real use. (Or you can implement multiple options and run a comparative test.)
Similarly, you can't ask ""Would you use (potential future) feature X?"" because users can't predict something they haven't seen. You can't even ask ""How useful is feature Y?"" for features that already exist. Indeed, in many studies, facilitators asked users to comment on specific features that didn't exist but were seeded into the interviews as ringers; the users provided copious feedback.
(The take away? If you're compelled to ask users how much they like your features, then be sure to include a few nonexistent features to collect a baseline.)
In one famous study, Microsoft asked customers to suggest new features for Office 2007 before starting work on that product. Most of the requested ""new"" commands already existed in Office 2003, so the design team correctly concluded that their main problem was the discoverability of the existing functionality.
The way to assess features is to have people use them. Definitely pay attention to users' comments while they're engaging with the features. You can even ask for supplementary comments immediately after tasks, while the features are still fresh in their minds.
What Interviews Can Tell You
For our brand study, we wanted to learn how using a website over time builds users' impressions of that site and their expectations for its brand promise. In other words, we weren't interested in individual page designs — which we'd study through user testing — but rather we wanted to know what users thought of a site after using it. And that's best assessed by asking them.
Interviews are also useful when you want to  explore users' general attitudes  or how they think about a problem. After getting this info, it's your responsibility to design features that address the problem (and to test prototype designs of those features to ensure that you got them right).
The  critical incident  method is especially useful for such exploratory interviews: Ask users to recall specific instances in which they faced a particularly difficult case or when something worked particularly well. These extreme cases are often more vivid in users' minds, and will give you the details needed to come up with useful features.
(In contrast, if you ask people how they  usually  perform a task, they'll often describe an idealized workflow without the many shortcuts and deviations that characterize real projects, whether at home or in the office. One of the key determinants for  good application workflow is to avoid idealized situations and design for the way people actually do stuff.)
Beware the Query Effect
Whenever you do ask users for their opinions, watch out for the query effect: People can  make up an opinion about anything, and they'll do so if asked. You can thus get users to comment at great length about something that doesn't matter, and which they wouldn't have given a second thought to if left to their own devices.
It's dangerous to make big design changes because ""users didn't like this"" or ""users asked for that."" If you ask leading questions or press respondents for answers, they might make up opinions that don't reflect their real preferences in the slightest.
For example, if you quiz people about your visual design, you'll inevitably get comments about the colors, even if they're not particularly important to the users. On the other hand, if you hear people mention the colors (unprompted) while they're using the site, then there's probably something to consider. (Say, a comment like ""Wow, this neon blue really hurts the eyes,"" or a more positive statement like ""The ultramarine is nice and calming."")
Combine Methods: Data Triangulation
Interviews are great supplements to other usability methods. If you could do only one thing, I would always recommend  user testing. But why limit yourself to one method? Each method can require only a few days' work, so you can combine multiple methods on all but the  very smallest budgets.
Let's return to the example that prompted this column: our latest findings regarding website response times. If you want to know the best speed for a specific pageload, forms handling, or AJAX widget manipulation, you have to watch users perform representative tasks with these designs. If something is too slow, you'll note users become impatient, forgetful, and ultimately leave the site. But if you ask them weeks later, they won't know specifically which UI element was too slow; nor will they recall the number of seconds that constituted the limit of their attention span. Conversely, if you want to know the branding impact of sluggish or snappy sites (our recent research question), then interviews are fine. For this higher-level question, you'll want to learn what made such a strong impression that it stuck in users' minds long after they used the sites.
Each method has its strengths and weaknesses. Taking the best input from each method will give you a much richer understanding than you could gain from any one method alone. Also, by supplementing each method with a range of other approaches, you can  triangulate  the findings and guard against misleading outcomes."
112,2010-01-25,"Alice Davey sent me this question:
I've been to a couple of the courses including ""Application design"", which were very good. I am currently designing an XXX application which users will quickly become ""expert"" in as they will use it frequently. 
I want to do usability testing from an early stage, but what's stumped me is how to test whether the application will serve the ""expert"" user well. By definition, all my test participants will be novice….
This issue is becoming increasingly important as the balance between novice and expert users tilts in the direction of the experts.
However, before we discuss usability for expert users, let's remember that nobody becomes an expert without having been a novice first. You can't forget about usability for new users, except in those rare cases where your user population is fixed, and you don't expect to sell any more copies of your product or hire any new staff to replace or supplement your existing people.
On the Web, the initial user experience is especially important: people — with ten years' experience using other sites — will be novices with respect to your site the first time they click through from a search engine. Unless your site meets their expectations and can be understood immediately, they'll beat a fast retreat back to the sites they already know.
Expert User Testing: The Basics
The basics of testing experts are the same as any other user testing:

Recruit  representative users 
Give them  realistic tasks 
Ask them to  think out loud  (while you shut up and avoid biasing their behavior with untimely hints)

Also, it's usually best to first test with a handful of users and then iterate the design before the next round of testing. You should conduct these small studies as early as possible in the design process, using low-fidelity design prototypes. Paper prototyping might work even better with expert users than with novices, because the experts are used to performing the test tasks. They can thus focus even more on the problem at hand, as opposed to, say, whether a dialog box is presented on an index card or as a rectangle on the screen.
One difference from testing with novices is that the ""realistic tasks"" are obviously more advanced for expert users. You can have them dig deeper and solve larger and more difficult problems.
A second difference might not be as obvious: We almost always ask users to  verbalize their thoughts  in a running monologue as they use a design. This think-aloud process tells you how people interpret the design elements, whether any are confusing, and which ones are compelling or repelling. Although the test situation is a bit artificial, a good facilitator and engaging tasks can make users suspend disbelief and thus tell you the unvarnished truth.
Expert users, however, cause difficulties for think-aloud studies:

Skilled behavior is often automated behavior (as discussed further in our seminar on the human mind and usability). When people are unaware of how they think about a certain behavior, they can't verbalize the reasoning behind their actions. For example, consider how you drive a car: If a facilitator asked you to think aloud during your drive to work, you typically wouldn't describe the steps you take to make the car go faster or slower. But, if you usually drive an automatic and were given a stick shift for the study, you'd probably verbalize your thoughts about using the clutch. In contrast, if you drive a car with manual transmission every day, odds are that you wouldn't mention the clutch during the think-aloud session. To get around this problem with expert users, you can use more elaborate usability methods to slowly (and tediously) analyze slow-motion replays of the interaction and thereby deduce what was going in those cases where users didn't tell you.
Expert users can turn into design critics and bend your ear with their opinions on the product (since they know it so well), as opposed to staying in the user role and engaging with the actual features. Gracefully accept their comments, while remembering that what people say and what they do can be very different. The reason for usability studies is to collect behavioral data, so guide participants back to the role of using the design as fast as possible.

Recruiting Experts
If you're redesigning an existing product, you're in luck: there will already be expert users in the wild. All you need is to recruit them to come visit for an hour or two. You can follow traditional methods for recruiting test participants, with a few twists. Often, a list of registered customers can short-circuit the tedious process of cold-calling study prospects. Sometimes, you can also work with managers of key accounts to contact some of their users of your product. (If so, emphasize that you want average users — not the ""star performers"" that managers often like to show off.)
For a website, you might be able to recruit existing users by posting a request on the site or by asking for volunteers in your email newsletter (which has the added benefit of reaching your most loyal users).
Other options for recruiting expert users include industry tradeshows (particularly if your company has a booth), user group meetings, and social media (especially your own site's social features).
Growing Experts Quickly
The question that prompted this column concerned a new product. In this case, expert users don't exist because the product as yet has no users.
You can't recruit people who don't exist, so you'll have to create your own experienced users. First, you can violate the rule against  testing internal staff. Usually, we don't want to test anyone involved with a design project or the company itself (except for intranet studies, of course). These people know too much, so we won't discover usability problems that stem from the misconceptions of outside users.
Knowing too much becomes a partial virtue in expert studies. Internal users are still not ideal test participants because their mental model of the system will have a much better structure than anything grown organically through mere exposure to the surface manifestation of the UI.
As an example, consider a website's structure: outside users must deduce the structure from the navigation design. Good navigation does give users a clue about the IA, but that's not their primary concern. Users are on a site to get things done, so they don't tend to pay close attention to the site structure, nor do they retain much of this knowledge from one visit to the next. In contrast, internal users' existing knowledge about the product line and the company's way of doing business forms a conceptual model that helps them grow a better mental model of the site structure.
Thus, when you test internal experts, remember that they'll know more and behave differently than external experts.
A second, and better, approach is to grow fresh experts by  fast-tracked training  of new users. You can assign a personal tutor to take test participants through the design and answer all of their questions. (Usually, we don't answer users' questions, because we don't want to bias their behavior, but if we're testing expert use and not initial use, we care less about how people overcome early difficulties. Thus, coaching is allowed.)
You can give test participants plenty of  time to practice  with your design before you start the study. Doing so can be cost-effective because you don't have to monitor them closely during practice sessions. Of course, if users need a week's practice to gain expertise, you have to pay for a week of their time, but at least you don't have to sit next to them all week. Just give them a cubicle and check in on them periodically (and give them a hotline number so they can call the tutor if they have questions).
Training and Manuals
Providing extra-supportive training is one way to quickly produce new experts. You can also use the regular training courses or instructional material if you're testing a product that offers such user support. For a new product, you can even make this part of the test plan and get empirical data on the usability of the training materials themselves.
The master guideline for all user research is to approximate the real world as closely as possible. So, if you have a manual or offer a training course, it's fine to let your test participants access these resources.
Unfortunately, in the real world, not all users will take the training course, even if you offer it upon initial application rollout. New hires who come onboard the following year are lucky if their colleagues take the time to fill them in on whatever they remember from the training sessions. (Sadly, most people remember very little from a course they attended a year ago.)
Similarly, you might offer a manual or users' guide at time of purchase, but this documentation might be long lost by the time new employees are asked to operate the machinery or application.
Thus, it's usually best to run some usability sessions with the docs and training, and some without this info.
Expect Smaller Improvements
The rule of thumb for usability's return-on-investment is that you can  double the desired business metric  (such as conversion rate) the first time you conduct user testing. Particularly on websites, you'll usually find at least one horrendous user-repelling design element that insiders never find problematic, but that's costing the company a fortune in lost business.
For expert users, the improvements from usability research are typically lower. The good news is that human beings are incredibly flexible and adaptive creatures. We live from Greenland to Equatorial Guinea and we can use Linux if we try hard enough. People who've used a software product for a decade will have invented workarounds and tricks to overcome its design flaws. And they will have internalized many of the arbitrary rules behind the UI. Many people are gluttons for punishment and grow to like bad design so much that they resist the change to something better.
After all, they already know how to use the difficult UI, so why change to an easy one that will require some amount of learning? (Users have a strong bias in favor of doing instead of ""wasting"" time learning.)
Because experienced users will have adapted to the old design, the potential for enhancing their performance is usually less than the 100% we often get for new website visitors. On average, a 1/3 improvement is more realistic.
Should You Cater to Experts?
It's more expensive to study usability for expert users, and the expected improvement is smaller. So why do it? Several reasons:

Users are novices for a short time but experts for a long time — at least for any product that they continue to use. Thus, the (smaller) benefit of better expert usability continues to accrue for more years and will eventually sum to much more than the one-time gain from novice improvements.
Some products have a large installed base with a large existing pool of users that's not expected to grow much. To compute the true gain from any usability advances, you multiply the per-user gain by the number of users, so this scenario also favors a focus on expert users.
Some products see heavy, repeated use. An app for call center reps is a classic example. Other products might see only intermittent use, but the impact of good vs. poor user performance is immense. Error handling in industrial control rooms is a good example here. In these cases, users might be highly trained and novice usability less of a concern, but it's crucial to nail expert usability.
Finally, heavy users might account for a disproportionally large share of profits, particularly on those e-commerce sites that cultivate loyal customers. On such sites, you want to make frequent, big purchases particularly easy.

Whatever the reason, it's often worth investing the extra usability resources to improve the user experience for experts."
113,2009-08-23,"A classic way to ruin usability tests is to give users problems that include the  actual command names  or navigation labels they're supposed to use.
For example, if you want to test whether people can find and use Excel's ""Remove Duplicates"" feature, you should  not  tell them: ""You have a list of companies that have previously purchased your product, but some company names appear multiple times. Remove these duplicates."" Given this task wording, users will often scan the UI for a label containing the words ""remove"" and/or ""duplicates."" Thus, you're not testing whether the label effectively communicates the command's functionality, nor are you testing the communicative benefits of combining the command name with its corresponding icon and tooltip. You're simply testing whether users can match up the terms.
(I've actually seen an even worse task description, created by third-tier usability folks: ""Use the  Remove Duplicates  command to remove the extra copies of each name."" When you tell people what features to use, you'll never get them to  approach the software naturally. They'll just do as they're told; not what they would normally do.)
A good ""Remove Duplicates"" task description is: ""You have a list of companies that have previously purchased your product, but some company names appear multiple times. Change the spreadsheet so that each company name appears only once."" Now, your test participants know their goal and it's presented in a scenario that makes sense — but they can't solve the problem by simply scanning for keywords. Instead, they must search for commands that might help them accomplish the task. This is a much better test of whether the application's user interface supports user goals.
(You should also test broader tasks that can't be solved by a single command. See our full-day course on  User Testing for more information on how to write good test tasks and the course on  Application Design for guidelines on making features discoverable and understandable.)
Keyword Matching in Card Sorting
The keyword-matching problem can also mess up other user research methods, such as card sorting.
A good example comes from our recent project to improve usability on a client's health information site. The site's goal is to offer information about various related diseases and how to deal with them. To further complicate matters, the site has information for both the general public and professionals. So, a key challenge was to determine which organizing principles would be the best top-level structuring principle for the information architecture (IA).
Card sorting is often a good way to get initial insights into users' mental model of an information space, and in our project it did indeed generate good starting point for the IA. After the card-sorting study, we conducted several rounds of user testing of  wireframes, further refining the structure and how the site presented it. All of this effort would have been wasted if we'd gotten data on users' keyword-matching skills rather than on how they approach the site's target healthcare issues.
To preserve client confidentiality, I'll show examples of the problems and solutions transposed to another domain — say, agriculture.
In the (fictional) case of our agriculture site, we cover different crops, such as strawberries, raspberries, corn, and wheat. We also cover different activities, such as planting, growing, and harvesting. Finally, we target our content to both professional farmers and people who grow a few plants in their backyard.
So, one option would be to organize the site primarily by crop, and secondarily by activity. Let's call this  IA #1  :

Strawberries
	
Planting
Growing
Harvesting


Wheat
	
Planting
Growing
Harvesting


etc.

Alternatively, we could organize the site primarily by activity, and secondarily by crop. Let's call this  IA #2  :

Planting
	
Corn
Raspberries
Strawberries
Wheat


Growing
	
Corn
Raspberries
Strawberries
Wheat


etc.

Let's say that we did a card-sorting study to help us determine the initial IA for our first wireframe. Here are two possible sets of cards:



Card Set A
Card Set B


Strawberry Planting
Planting Strawberries


Strawberry Growing
Growing Strawberries


Strawberry Harvesting
Harvesting Strawberries


Wheat Planting
Planting Wheat


Wheat Growing
Growing Wheat


Wheat Harvesting
Harvesting Wheat



Clearly, giving either card set to users would severely bias the sorting exercise. Given Set A, most users would generate IA#1, sorting all ""strawberry"" cards together, all wheat cards together, and so on. Likewise, Set B would typically result in IA#2, as most users would sort cards by activity (""planting,"" ""growing,"" etc.).
Solution: Synonyms and Non-Parallel Structures
To get a better outcome, our card-sorting study has to  make users work harder  and  really think  about how they'd approach the concepts written on the cards.
Obviously, this is opposite of our goals in usability, where we typically want to make tasks easier and reduce the users' cognitive load. But remember: card sorting isn't a user interface design; it's a knowledge elicitation exercise to discover users' mental models. So it's  okay to reduce the usability of the cards, because people don't actually  use  them in the real UI.
(Of course, you can't reduce the cards' usability so much that test participants don't understand them at all, because then their sorting will be misleading.)
One way to avoid having participants simply match up keywords is to use different words for a single concept — that is, introduce  synonyms. For example, instead of saying ""harvesting strawberries,"" we could say ""picking strawberries.""
To further mix it up, we could have a ""picking  Fragaria"" card using the scientific name for the strawberry genus. I wouldn't actually do this unless the site were targeted at professional botanists, but in the healthcare example (our actual client project), we could use both common names and medical names for the same condition on different cards and most users would still understand them.
Another way to ruin users' keyword-matching ability is to employ  non-parallel exposition structures. For example, one card could read ""Planting Corn"" whereas another could read ""Wheat Planting.""
Employing such tactics violates one of the traditional  Web writing guidelines, which says that parallel structures are faster to scan, and easier to compare and contrast and thus better for presenting lists of items. Again, ""bad design"" is okay, because we're not targeting optimal usability of the cards. We want users to stop in their tracks and think, rather than simply finish the task quickly.
Avoiding Study Bias
I usually say  user testing is easy: basically, you get some real customers and watch them use your site or app. But this article touches on one of the difficulties of running great studies: minimizing bias. To achieve this, you have to see how people behave  on their own  rather than impose your own thinking on them. In the latter case, they simply echo it back, and you don't learn how to improve your design for real-life use.
From many years of teaching usability methods, I've learned that two of usability's biggest challenges are to write good test tasks and facilitate test sessions in a neutral manner. As our (sanitized) case study here shows, it also can be hard to avoid biasing users in card-sorting research.
The beauty of usability is that the methods are so robust that they generate useful findings even if you use them wrong. This is particularly true for user testing: any time you watch customers, you'll learn something to increase your website's profitability. But, if you do usability right, you'll learn even more. And once you're aware of the potential for biasing users, you can reduce the bias and thus increase your research's value."
114,2009-07-05,"A reader recently sent me the following message, seeking my advice on a common quandary:
I read your article today (""Guesses vs. Data as Basis for Design Recommendations"") after an awful workplace bicker with subject matter experts about how their content failed to meet the Web readers' needs.

As a Web manager for a small state government agency, I am constantly frustrated by content owners' subjective opinions, which they fling at me while stubbornly refusing my suggestions:

""Yeah, see, I don't like that.""
""I wouldn't click there, and so neither will they.""
""Oh, they'll know what that means, even if you don't.""

When I recently cited one of your articles to support my stance, I was told ""there's always evidence to support any opinion."" Giving them data seemed to not impress them one iota. They seemed annoyed that I found data to support my argument and were not swayed.
Do you have any other suggestions on how to get colleagues — who are not usability experts — to trust me with their content? How do I convey that what I do is a profession and skill set? How do I gain my colleague's respect as their Web content strategist?

Sadly, this situation is common not just in government agencies, but also in most big companies. There are several different issues at play here; each requires a specific remedy.
Differences Between Users and Project Members
The first malady here is that  content owners are relying on their own opinions  and preferences. The primary cure is to point out that these subject-matter experts are completely   unrepresentative of the target audience  on almost every possible dimension:

Because they're in  charge of a particular topic, they obviously  know much more  than the target audience about all aspects of it, from background knowledge to specialized terminology.
As insiders  working in your organization, they're also aware of how you  structure the domain  and each department's responsibilities. (Similarly, product managers know not just about their own products, but also how they relate to the company's overall product line.) This is the root cause of many of the  IA failures we see in testing websites —  and intranets.
As  professionals  — usually with college degrees — they might be smarter  and better educated  than many people in the target audience. This certainly varies across websites, but it's typically true for sites that target a broad consumer audience, senior citizens, and many recipients of government services, such as welfare benefits.
Sometimes, content owners are also more  tech savvy  than the audience, with a better understanding of computers and Internet concepts. This is analogous to application design, where developers often have far greater technology skills than users.
Finally, because it's  their  project, content owners are much  more motivated  to care about the content than the users. And, the less motivated people are, the more likely they are to skim text in an attempt to  extract only the most useful information.

For all of these reasons, it doesn't matter what content owners themselves like or understand; the behavior of real users is likely to be completely different.
Luckily, these differences are fairly easy to explain to anybody who's willing to be objective. Better yet, each difference flatters subject-matter experts by emphasizing their superior knowledge.
Seeing Is the Only Way to Convert Unbelievers
Once you've successfully argued that content owners can't project their own preferences onto the target audience, you're left with a question: How should you judge usability?
Although it's good to cite external research, the sad fact is that nothing is as persuasive as testing your own users. Even if numerous outside studies have identified a certain phenomenon, many people won't be convinced until they've  seen it for themselves.
That's one of the main reasons I always recommend running your own user tests, even if you're designing a fairly simple website and virtually all of your findings will replicate the published literature.  Seeing is believing, and most skeptics will leave the lab highly motivated to change their ways (and, more importantly, to change the site so that it finally works).
This is also why you should move heaven and earth (or at least serve free pizza) to get all stakeholders to observe a few user sessions. You can also flatter them some more by explaining that it will be difficult to correctly interpret the study results without them. (That's not a lie, but the main reason to invite them is so that they'll believe the study findings.)
Simple  user studies are cheap, and it's almost always worthwhile to spend a few days observing a handful of representative users working their way through your content.
Which Data to Trust
It's true that  ""there's always evidence to support any opinion,""  but that doesn't mean you should ignore data. After all, some data is clearly better than others.
The main facts about how people read on the Web are extremely well established, and literally hundreds of studies have reproduced our  original findings over the past 12 years.
The same is true for all of our usability guidelines: most have been confirmed by other independent studies. Anyone who bothers to run a study will discover the same thing, because there are no usability secrets — it's simply a matter of looking.
Still, while most usability evidence strongly aligns, there are deviant results to be found. People who don't know any better will stumble across such findings in a Web search and proclaim that  ""the experts disagree.""  However true, this is not a license to ignore usability data and follow any random path.
Instead, you should weigh the evidence. On one scale, you have hundreds of studies from experts across industries and countries; they all agree on the big picture, and often document their findings with substantial reports. On the other scale, you have a few deviant postings (plus many guesses, but as previously discussed, you should disregard pundits who don't test their theories with real people). This simple weighing exercise usually tips the scales in favor of the consensus.
Deviant usability findings  are typically caused by one of the following:

Weak study methodology. Unless you're an expert in usability methodology, this can be hard to assess, but common problems include:

	
Unrepresentative users. Academic studies, for example, tend to test students instead of people who better represent the target audience. (See  guidelines for recruiting representative study participants.)
Unrealistic tasks  that are overly narrow compared to the  free-form roaming that characterizes real user navigation. Such bogus tasks are particularly common in eyetracking studies, which are easier to conduct when you take users directly to the pages that you want to heatmap. But people behave completely differently when they're dumped into a problem rather than coming across a page on their own. (To do eyetracking on a shopping cart, for example, you should first ask users to shop.)
Biased study facilitation, where the facilitator talks too much and guides users in a way that changes their behavior.


Statistical flukes. If you rely on a 5% significance level, then 1/20th of the findings are due to  random fluctuations and don't represent a real effect. Unfortunately, the 19 studies that correctly confirm existing wisdom are boring, and don't get much coverage. The 20th — invalid — outcome, however, is easily found in a search for hotly debated blog posts.
Second-tier usability consultants  who try to attract attention by being different.
Unusual circumstances. I designate something as a usability guideline if it holds true for about 90% of designs. So, in 10% of cases, something different happens because that design addresses a problem that's extremely different from the typical case. People always think that their projects are unique. However, 9 times out of 10, it's not  different enough  for the common usability guidelines to fail.

There's a big intrinsic reward for claiming to find something completely new that contradicts all established wisdom. Seminars sell better when they claim to reveal  ""secrets""  or  ""all-new, all different""  results. I admit that I always hope for new findings in our own research, because I know that they'd make us more money. But year after year, the usability findings remain fairly robust and steady, and I'd rather report the truth than increase revenue. Luckily, there are enough honorable usability experts in the world that the findings of most other reports are similar to our own.
When judging which data to trust, look at the economic incentives. For example, studies of Internet advertising's effectiveness conducted by advertising agencies are inherently suspect compared to those conducted by people who don't care whether you spend more or less money on ads. Similarly, the rewards from being ""new and different"" mean that those studies that confirm existing knowledge are inherently more likely to be trustworthy.
Building Respect
So far, I've presented all the logical arguments for why people should follow your advice. However, logic will take you only so far. Ultimately, your colleagues must respect your professional expertise so that you don't need to bury them in an avalanche of external research data for every decision.
Respect comes only from proven performance. Once content owners see how much better customers react to websites that are written and designed according to established usability guidelines, they'll start respecting you more. Sadly, this is a chicken-and-egg situation: you get to demonstrate the value of your advice only if it's being implemented.
This is why it takes some time to build respect. There are two ways to incrementally improve the situation:

Do use logic to get some of your arguments accepted. Logic won't win the day, but in most organizations, it's not completely ignored.
Run user studies and do whatever it takes to get others to observe some sessions. When they see first-hand that you're right this time, they'll believe you a bit more next time.

If you have the budget, a third approach can help as well: bring in an  external consultant or prod your colleagues to attend a usability seminar (say, our UX Basic Training :-) When they hear internationally recognized authorities say the same as you, there's a better chance that they'll listen to you in the future.
This is a hill-climbing process. You can't go from contempt to respect in a day, but you can gradually build respect by continuously doing your job well. This is very similar to the way an organization as a whole  builds UX process maturity: one step at a time."
115,2009-06-07,"Should you offer users help to adjust font sizes or can you simply rely on the built-in browser commands? This question was recently posted to an interaction designers' discussion group (which will remain unnamed to preserve the anonymity of the individuals dissected below).
12 people responded to this question. Most simply offered a personal opinion as to what they would prefer. Fair enough: All people are experts on their own preferences. But there were 6 postings that commented on what would be best for  other  people.
2/3 of these postings were pure guesses, whereas 1/3 was based on some form of data in the form of empirical user observations.
Guesses:

""In this day and age, [...] most people who need to increase their font sizes in their web browser already know how to do it.""  WRONG 
""People who do need to resize type will do so via the browser; it's not hard to do so.""  WRONG 
""It's not 1995; not all 50+ people are such newbies that they don't know, or wouldn't want to know, how to resize text in a browser.""  WRONG 
""The people who most need to increase font size are people 65+, which is the group least-likely to be skilled enough to have adjusted settings.""  RIGHT 

Data:

""I had to set it manually for my parents, and while the percentage of people over 65 becoming more and more savvy is increasing at an amazing rate — hidden functions like adjusting text size is something that escapes them.""  RIGHT 
""I've observed usability studies on sites that included text resize widgets [...] most, if not all, the participants [...] had no idea what it was.""  RIGHT 

Data Beats Guesses
The general guideline is to use relative font sizes that  let users resize (if they know how), but to display big and legible text as the default. This conclusion is based on numerous observations that show that many  older users don't have the skills to resize fonts.
In our discussion group example,

100%  of the designers who provided  external data  were right, whereas
25%  of the designers who relied on their  personal opinion  were right.

Most strikingly,  75% of guessers were wrong. You'd be  better off tossing a coin  than asking advice of these people.
In this simple example, basing design advice on the  smallest amount of empirical observation  of real users quadrupled the probability of being right.
A word of caution: Although data from your parents is better than no data, I don't recommend that you base design decisions on your family members because they're likely to be smarter than average users. (Because  you're  smarter, being somebody who understands usability.) We know from our studies of  children and  teenagers that average kids and teens have much greater difficulties using websites than one would think after listening to Internet executives proudly tell stories about their offspring's online skills.
Testing 2 Users Beats Guessing
While striking, our text-size example is based only on a small set of responses. Another example provides a similar conclusion with a bigger sample.
We tested two different ways of displaying bank account information with 76 users each, for a total of 152 test participants in a between-subjects benchmark test. We asked users to perform tasks such as checking their account balances and finding out what interest rate the bank was currently offering. The results were as follows:



Usability Metric
Design A
Design B


Success Rate (across four tasks)
56%
76%


Time to Complete Four Tasks (min:secs.)
5:15
5:03


Subjective Satisfaction (1–5 scale, 5 best)
2.8
3.0



On all three  usability attributes, version B scored better, though only the difference in success rates was big enough to be statistically significant. Overall, there is no doubt that  B was better.
(In contrast to this study, sometimes both designs win on different usability attributes. For example, one design might make people more successful, while the other helps them accomplish the task faster. In such cases, you might have to make tradeoffs or, when possible, create a third design that combines the best aspects of both alternatives.)
In this case, I showed designs A and B to 21 people who were taking an interaction design course and asked them which one they would recommend to the bank. Going purely on their personal  guesses  as to which design was best, the probability of getting the best design recommended was  50%. That is, no better than flipping a coin. (Asking your trusty coin is an easy way to save on consulting fees.)
I then asked another group of 38 people taking the same course to test the two designs with 2 users for each design. Now, going on empirical  observations  of 2 users' behavior for each alternative, the probability of recommending the best design was  76%  .
Another way of looking at this outcome is that testing just 2 users per design reduced the probability of being wrong from 50% to 24% — cutting it in half. Of course, a 24% probability of picking the wrong design is not good enough if you're talking about a  high-ROI design decision, so we'd obviously want to test more than 2 users per design in such cases. (I usually recommend  5 users.)
Still, even though it's an extremely scaled-back study, testing 2 users per design hugely improved the recommendation over the flipping-a-coin performance from guessing.
(In this study, the two versions looked equally good, which is important for measurement studies. If you compare a rough-looking prototype with a fully refined graphic design, you will bias the scores.)
When Guesses Go Horribly Wrong
Comparing our two case studies, the guessing camp from the text-size example had by far the worst performance. A person who based a design decision on these guesses would be wrong 3/4 of the time. In the bank example, they'd be wrong only 1/2 the time.
So, why the miserable discussion-group guesses? The answer lies in the following two statements:

""In this day and age...""
""It's not 1995...""

Sadly, too many Web designers refuse to believe in the  durability of usability findings. Thinking that  ""things that were difficult in the past must surely be easy now""  has led many websites to their doom.
When we actually study real users, we see how  slowly they learn  about technology and how little their ability to use fancy websites has improved. And, most important, we see how little users  care  about learning fancy Web techniques. People just want to get in, get their stuff done, and get out. They don't want to learn.
Guesses go wrong because many designers desperately want to believe in the potential of advanced design. They simply can't fathom  how little most people know about their pet technologies.
(Yes, in recent testing, we did find a  few small advances in users' skills, but it's slow progress; you'd better believe that simplicity will continue to win the day for decades to come.)
A Little Data Goes a Long Way
In my two examples, the probability of making the right design decision was vastly improved when given the tiniest amount of empirical data: observing your own parents, or testing 2 users per design.
Of course, a bigger study would be better, but  any  data is better than no data. How many design decisions do you make without any empirical observation of your customers' behavior?"
116,2007-01-01,"The consulting business has an old saying:  ""Fast, cheap, and good â€” pick any two.""  The notion is that if you want something done quickly and inexpensively, it'll be of poor quality; if you want it quickly and done well, it'll be expensive, and so on. Although true in many areas, this maxim doesn't hold for one important aspect of usability: methodology.
In usability, the fastest and cheapest methods are often the best.
Of course, in any discussion involving value judgments like ""good"" and ""best,"" we must define the quality criteria. My main quality criterion for usability is that it change the world. In other words, usability methods must set the product development directions and result in significant improvements to the shipping design.
Another criterion that's sometimes relevant is the quality and depth of the insights you derive into user behavior. In terms of insight, you can't be fast, cheap, and good at the same time. Truly deep insights require advanced usability methods, extensive research, and sufficient time to ponder the data. For example, you should conduct field studies where you observe users in their natural habitat. Unfortunately, this is expensive and time-consuming.
When to Go Fast and Cheap
For everyday design projects, discount usability methods are the best. In fact, generally, the faster and cheaper the study, the bigger its impact because the results will be available early enough to change your system's fundamental architecture.
This is why I'm such a strong proponent of agile paper prototypes: mock up your design ideas on a few screens before you invest the resources on detailed design and implementation.  Test each design with 5 users. And run  many  rounds of user testing. The cheaper each round, the more rounds you can fit within your budget, and the more you'll learn about user needs.
Some people complain because cheap and fast usability studies don't teach you everything about a design. But that's irrelevant. Yes, a bigger study would yield more results, but you'd get those results too late to influence the big design decisions. Also, the second or third rounds of testing will reveal anything you might have missed in the first, fast study.
Annual Usability Checkup
Fast and cheap usability is usually best, but sometimes you need to step back and get a bigger perspective. For important projects, you should do this once a year. For less important or slow-moving projects, it might be enough to have a checkup every two or three years.
The big-picture checkup has three components:

An  independent review  by an  outside expert ($38,000). Usability rests on your ability to consider the outside perspective, since your customers are all outsiders. A key downside to working for the same company year-round is that you eventually become acclimated to its thinking and assume that the way your website works is natural and intuitive. But ""intuitive"" basically means ""what you've grown accustomed to."" Having someone analyze your design from a fresh perspective shakes things up and offers a more neutral evaluation of your usability level compared to the rest of the world. That's also why it's a good idea to ask any  new hires  in your usability group to immediately write usability reviews of your design while they still have an outside perspective.

	
If you run an  intranet, your users are obviously insiders and it's less of a problem that you're embedded in the company. Still, it's valuable to have your intranet's usability independently assessed by someone who has studied a lot of other intranets.


A  competitive study  that compares your design with that of three competitors. These studies are expensive and take time, but they're the best way to get broad insight into customer behavior and strategic ideas for the next year's design projects. Your competitors spent a lot of time and money designing solutions to approximately the same problems as your own; relative to their investments, it's dirt cheap to spend $45,000 on a competitive study to find out which of their ideas work and which flop.
	
Sadly, intranet teams can't conduct competitive studies. Instead, read the  Intranet Design Annual each year to hold your own intranet up against ten award-winning designs.


A  benchmark study  to track your usability metrics and see how much better you're doing than last year.  Quantitative studies are the most expensive of all and subject to  many pitfalls unless you employ impeccable methodology. Thus, they are only for companies at the high end of  organizational maturity with respect to UX.

These three steps are expensive. Maybe you can't do all three every year, but plan to do at least one; this will give you the deep insights you need. Then, run as fast as you can the rest of the year doing quick studies that'll keep your design projects on track and keep your design team up-to-date on user needs."
117,2006-08-13,"Whenever I show double-logarithmic charts in my  usability seminars, I see the audience members' eyes glaze over. People don't like anything but the simplest data visualizations, and I've certainly learned my lessons from the feedback sheets and scaled back on the amount of statistics I present.
Still, I can't help myself: there's data underlying the usability guidelines, and I  have  to show some of it. To understand traffic patterns found by Web analytics, for example, some of those hated advanced-visualization plots are sadly necessary. Without them, you simply can't tell what's going on.
As an example, consider the following  linear graph  of my log file analysis of  how many visitors each page gets on a given website:

This linear graph shows what looks like the classic ""long tail"" distribution (which is really  Zipf's Law). And indeed, it  almost  is. The difference between theory and practice becomes clear, however, when we plot the same data on  logarithmic scales:

It's now clear that we have a  drooping tail: the site simply doesn't have enough content to supply the predicted demand at the low end.
Without this fancy log-log plot, we would have never seen the site's potential for increasing traffic by adding large amounts of low-volume content. I'm amazed at how often articles analyzing Web traffic or ""long tail""-type businesses use linear plots that fail to show what's really going on.
To compare high-volume and low-volume events in the same diagram, it's typically best to use a logarithmic plot. (If you're using Excel to plot your charts, you can get logarithmic scales by simply double-clicking each axis, choosing the  Scale  tab in the resulting  Format Axis  dialog box, and checking the box for  Logarithmic scale.)
In addition to the drooping tail, my  original analysis also found a  hump  on the traffic plot for search queries -- a different phenomenon that also only shows up on a log-log chart.
Wag the Drooping Tail
So, what would happen if our sample site could wag its traffic tail up to the straight line representing the traffic potential the theory predicts?
In my analysis, current traffic with 1,000 pages was  2.6 M pageviews  over an 8-week period. With 260,000 pages, the site could expect traffic to increase to 4.8 M pageviews over the same period. That is, the 259,000 new low-traffic pages would get 2.2 M pageviews, for an average of 9 views per page.
Now, if we extend the 8-week period to a full year, the total traffic would almost  double -- from 16.9 M to 32.2 M pageviews  -- giving each new page an average of  58 views.
What's the value of 58 pageviews?
Over the last several years, Yahoo! has made between 0.2 and 0.4 cents per non-search pageview. However, I believe that Internet advertising is over-hyped and that advertisers are deluding themselves into overpaying. In the long term, non-search advertising's value will drop to 0.1 cents or less per page.
So, at the expected long-term value of 0.1 cents per view, 58 pageviews have a value of about 6 cents. If we assume the new pages can attract traffic for five years, and then discount future cash flow by 10% per year, the present value of each new page is  24 cents.
Not much. But we're expecting to add 259,000 pages, so the total value would be  $62,000.
It sounds like a nice sum -- but could the site create 259,000 new pages for $62K? Obviously not, assuming the employees creating the pages earned salaries higher than that of the average ant.
The only feasible approach is that chosen by many sites these days: to con users into contributing content for free. However, doing so requires that sites develop a system for user contributions, which (if done correctly) requires user testing and other quality assurance before being fielded. Given that the system's features aren't particularly advanced, our sample site could probably develop it for less than $62K. But it wouldn't be free.
(Update: Chris Anderson found a  drooping tail for the popularity of movies. The drooping tail shape may be more common than previously expected. Maybe now people will go back and reanalyze their long tail statistics with the correct diagrams :-)
Analysis Outcome?
It probably wouldn't pay for our sample site to take advantage of the opportunity that log analysis revealed. The long tail's end pays for aggregators who get their products from others, but companies who must develop their own are usually better served by staying away from the full tail.
That said, pursuing the tail's end might be valuable if a site meets one of two conditions: it has a better way than low-value ads to monetize traffic, or it has so many users that the total income would be substantially more than the cost of developing the new functionality.
In any case, you should certainly run through such exploratory ROI scenarios for your own site. To do so, you need correct data analysis and this typically requires more advanced visualizations than you see in most places. It's here that logarithmic plots deserve a chance -- despite their intimidating name.
(To avoid misunderstandings: you obviously shouldn't show log charts in websites targeted at a broad consumer audience. They're for internal use only -- or for websites like mine that target an intellectual audience.)"
118,2006-07-09,"About 10 years ago, I showed that the  popularity of a website's pages followed a power law  . Briefly stated, a few pages on a website were extremely popular, a larger set was moderately popular, and  the vast majority  constituted the ""  long tail  "" of low-traffic pages.
Mathematically, the Zipf curve is a straight line on a double-logarithmic diagram, when we plot pages with their popularity rank on the  x  -axis and their number of hits on the  y  -axis.
The old analyses showed that this same distribution also described both the number of incoming references to a website from other sites and the outgoing traffic from a company's employees.
Do these findings continue to hold today? The  Web is now 2,200 times bigger  , so traffic patterns might have changed. I decided to find out.
Page Popularity
The following chart shows useit.com traffic during a recent eight-week period. Each dot represents one page, and the pages are sorted by popularity. The most popular page (the homepage) got 261,024 pageviews.
 
For the most popular 350 pages, the empirical data follows the theory remarkably well. Thereafter, however, the data trails off and the next 700 pages have less traffic than predicted. Also, in theory, there should have been about a quarter-million additional pages with low traffic, but I simply haven't gotten around to writing that much.

The small insert in the upper right shows the equivalent diagram from my analysis 10 years ago. It's uncanny how closely it resembles the new data. In particular, the old curve also trailed off toward the end and was missing a vast number of low-traffic pages.
The end of the ""long tail"" is absent  from both diagrams because the sites haven't accumulated enough old content to have the expected number of low-traffic pages. Instead, they have a  drooping tail  . It might take hundreds of years for a site written by a single person (or even a single marketing department) to accumulate a quarter-million pages.
Incoming Traffic from Other Sites
The next diagram shows the number of visitors referred to useit.com through links from other websites during the eight-week period. Each dot represents one external site.
 

In this case, the empirical data hugs the theory's red line all the way down to the x-axis. The difference here is that there's no lack of other sites that might link to useit.com, and a huge number of these sites are low-traffic blogs that only occasionally refer individual users.
The chart's one obvious exception to the theory is that the site that referred the most visitors accounted for many more visits than predicted. Google (depicted as an extra-large dot) referred 257,040 visitors; in theory, it should have referred only 52,479.
Google is five times as popular as the theory predicts, but this phenomenon could fade as other search engines catch up. Only time will tell.
Also, while Google is disproportionally important, when taken together, the other 35,631 referring sites accounted for 35% more traffic. Clearly, it's not a good idea to focus only on #1.
Search Engine Queries
To find useit.com, users employed a total of 110,399 different queries across various search engines.  Of these queries, 83% were used only once  during the eight-week period.
 
The top 10 queries accounted for 10% of the total traffic, so each one of these queries is obviously more important than those that brought only one visitor. Taken together, however, the  single-use queries accounted for three times as much traffic  as the top 10 queries. This statistic shows the folly of focusing search engine optimization solely on a few high-performing queries. Your site must be found when users enter relevant queries -- and the possibilities are typically vast.
The following diagram shows the distribution of search engine queries, sorted by the number of incoming users who arrived after searching for that string.

This chart shows roughly the expected distribution, but with a hump for queries #5300. In other words,  queries in the middle range are more important than the theory predicts  . Sample queries in this range include  response time  ,  open link in new window  ,  teenagers  ,  site maps  ,  eye tracking  , and  link color  . Useit.com is not particularly focused on any of these topics, which is why they're not at the top of the list. For each query, however, I have at least one good related article.
In general, my site covers a broad range of topics in some depth, which is probably why it has this hump of mid-range queries with extra traffic.
As we proceed down the list of query popularity, the queries become longer and longer. The following diagram shows the number of query words for each of the first nineteen groups of a thousand queries. (That is, queries #1-1,000 are first, followed by queries #1,001-2,000, and so on through queries #18,001-19,000.)

Single-word queries were fairly common among the first thousand queries (i.e., those that generated the most traffic), but drop off quite quickly. Conversely, four- and five-word queries are rare among the most popular queries, but are a big proportion of queries starting at about #7,000.
This shows the importance of considering longer queries in your search engine marketing: Multiple-word queries are the best way to capture the vast range of user interests.
In this case, long queries in the #7,000 traffic range included  radio buttons and check boxes  (five words) and  horizontal scrollbar in html  (four words).
In 10 Years, Almost No Change
In comparing the new data with data from 10 years ago, the biggest finding is that the  curves look almost the same  . Several measures of Web traffic followed a Zipf curve in 1996, and they still do.
 
The two exceptions are both search related:

A single search engine is disproportionally popular. Is this a temporary fluke or a fundamental change in the Internet's fabric? Check back in 10 years!
Users today enter more long queries. (I discuss this trend in more detail in my course on  Fundamental Guidelines for Web Usability  .)

Mainly, though, the big patterns of Web use remain remarkably robust. This is explained by the same phenomenon that dictates the  long-term durability of usability guidelines  . In both cases, conclusions are  independent of changes in technology or fashion  . Rather, they are due to the fundamental nature of human behavior.
 
Knowing that a single distribution describes these many forms of Web use can  help you analyze your own log files  : plot your statistics on a log-log scale and see if they fall on a straight line. If yes, your site follows the theory. If no, see where you deviate: In the head, the middle, or the tail? Above or below the prediction line? Any deviations help you understand ways in which your traffic is different than the norm. These insights may also help you spot opportunities for growth."
119,2006-06-25,"We can  define usability in terms of quality metrics, such as learning time, efficiency of use, memorability, user errors, and subjective satisfaction. Sadly, few projects collect such metrics because doing so is expensive:  it requires 4 times as many users  as simple user testing.
Many users are required because of the  substantial individual differences in user performance. When you measure people, you'll always get some who are really fast and some who are really slow. Given this, you need to average these measures across a fairly large number of observations to smooth over the variability.
Standard Deviation for Web Usability Data
We know from previous analysis that  user performance on websites follows a normal distribution. This is happy, because normal distributions are fairly easy to deal with statistically. By knowing just two numbers — the mean and the standard deviation — you can draw the bell curve that represents your data.
I analyzed 1,520 measures of user time-on-task performance for 70 different tasks from a broad spectrum of websites and intranets. Across these many studies, the  standard deviation was 52% of the mean  values. For example, if it took an average of 10 minutes to complete a certain task, then the standard deviation for that metric would be 5.2 minutes.
Removing Outliers
To compute the standard deviation, I first removed the outliers representing excessively slow users. Is this reasonable to do? In some ways, no: slow users are real, and you should consider them when assessing a design's quality. Thus, even though I recommend removing outliers from the statistical analyses, you shouldn't forget about them. Do a qualitative analysis of outliers' test sessions and find out what ""bad luck"" (i.e., bad design) conspired to drag down their performance.
For most statistical analyses, however, you should eliminate the outliers. Because they occur randomly, you might have more outliers in one study than in another, and these few extreme values can seriously skew your averages and other conclusions.
The only reason to compute statistics is to  compare them with other statistics.  That my hypothetical task took an average of 10 minutes means little on its own. Is 10 minutes good or bad? You can't tell from putting that one number on a slide and admiring it in splendid isolation.
If you asked users to subscribe to an email newsletter, a 10-minute average task time would be extremely bad. We know from  studies of many newsletter subscription processes that the average task time across other websites is 1 minute, and users are only really satisfied if it takes less than 2 minutes. On the other hand, 10 minutes would indicate very high usability for more complex tasks, such as applying for a mortgage.
The point is that you collect usability metrics to compare them with other usability metrics, such as comparing your site with your competitors' sites or your new design with your old.
When you eliminate outliers from both statistics, you still have a valid comparison. True, average task time in both cases will be a bit higher if you keep the outliers. But, without the outliers, you're more likely to reach correct conclusions, because you're less likely to overestimate an average that happened to have a greater number of outliers.
Estimating Margin of Error
When you average together several observations from a normal distribution, the standard deviation (SD) of your average is the SD of the individual values divided by the square root of the number of observations. For example, if you have 10 observations, then the SD of the average is 1/sqrt(10) = 0.316 times the original SD.
We know that for user testing of websites and intranets, the SD is 52% of the mean. In other words, if we tested 10 users, then the SD of the average would be 16% of the mean, because .316 x .52 = .16.
Let's say we're testing a task that takes five minutes to perform. So, the SD of the average is 16% of 300 seconds = 48 seconds. For a normal distribution, 2/3 of the cases fall within +/− 1 SD from the mean. Thus, our average would be within 48 seconds of the 5-minute mean 2/3 of the time.
The following chart shows the margin of error for testing various numbers of users, assuming that you want a  90% confidence interval  (blue curve). This means that 90% of the time, you hit within the interval, 5% of the time you hit too low, and 5% of the time you hit too high. For practical Web projects, you really don't need more accurate interval than this.
The red curve shows what happens if we relax our requirements to being right half of the time. (Meaning that we'd hit too low 1/4 of the time and too high 1/4 of the time.)

Determining the Number of Users to Test With
In the chart, the margin of error is expressed as a percent of the mean value of your usability metric. For example, if you test with 10 users, the margin of error is +/ 27% of the mean. This means that if the mean task time is 300 seconds (five minutes), then your margin of error is +/− 81 seconds. Your confidence interval thus goes from 219 seconds to 381 seconds: 90% of the time you're inside this interval; 5% of the time you're below 219, and 5% of the time you're above 381.
This is a rather wide confidence interval, which is why I usually recommend  testing with 20 users  when collecting quantitative usability metrics. With 20 users, you'll probably have 1 outlier (since  6% of users are outliers), so you'll include data from 19 users in your average. This makes your confidence interval go from 243 to 357 seconds, since the margin of error is +/− 19% for testing 19 users.
You might say that this is still a wide confidence interval, but the truth is that it's extremely expensive to tighten it up further. To get a margin of error of +/− 10%, you need data from 71 users, so you'd have to test 76 to account for the five likely outliers.
Testing 76 users is a complete waste of money  for almost all practical development projects. You can get good-enough data on 4 different designs by testing each of them with 20 users, rather than blow your budget on only slightly better metrics for a single design.
In practice, a  confidence interval of +/− 19% is ample  for most goals. Mainly, you're going to compare two designs to see which one measures best. And the  average difference between websites is 68% — much more than the margin of error.
Also, remember that the +/− 19% is pretty much a worst-case scenario; you'll do better 90% of the time. The red curve shows that  half of the time you'll be within +/− 8% of the mean  if you test with 20 users and analyze data from 19. In other words, half the time you get great accuracy and the other half you get good accuracy. That's all you need for non-academic projects.
Quantitative vs. Qualitative
Based on the above analysis, my recommendation is to test with 20 users in quantitative studies. This is  very expensive,  because test users are hard to come by and require  systematic recruiting to actually represent your target audience.
Luckily,  you don't have to measure usability to improve it.  Usually, it's enough to  test with a handful of users and revise the design in the direction indicated by a qualitative analysis of their behavior. When you see several people being stumped by the same design element, you don't really need to know  how much  the users are being delayed. If it's hurting users, change it or get rid of it.
You can usually run a qualitative study with 5 users, so quantitative studies are about 4 times as expensive. Furthermore, it's easy to  get a quantitative study wrong and end up with misleading data. When you collect numbers instead of insights, everything must be exactly right, or you might as well not do the study.
Because they're expensive and difficult to get right, I usually warn against quantitative studies.  The first several usability studies you perform should be qualitative.  Only after your organization has  progressed in maturity with respect to integrating user research into the design lifecycle and you're routinely performing qual studies should you start including a few quant studies in the mix."
120,2005-11-06,"We can consider usability at three different levels:

Individual users. At this level, we examine what happens as a person tries to operate a user interface. Is it easy or difficult to find things and make desired actions happen? We tend to focus on this level because it has the most direct impact on screen design. Also, most websites, software applications, and consumer devices are single-user designs. Finally, this level is crucial because if individuals can't figure out how to work with your design, the larger levels are irrelevant.
Groups of users. Many designs aim to coordinate multiple users; the design's usability therefore depends on more than an individual user's ability to click buttons. At this level, it also matters whether the UI helps or hinders group efforts. Examples here range from chat systems and social media to applications that support multi-user workflows, such as a company's hiring process.
The enterprise. At this level, the focus is on how the system impacts the company over time, including issues in administration, installation, and maintenance. Total cost of ownership (TCO) is often one of the most important usability metrics at the enterprise level.

Usability Issues: Enterprise Examples
Enterprise usability issues are most common in big companies, but they're an issue for smaller companies as well. I recently encountered one in my own small company, for example. To help us analyze our user research results, we use various types of software. I received a DVD with a bunch of files for one of these applications from a colleague who had conducted a major study. Since I hadn't been able to observe this study in person, I was eager to dig into the raw files and see what had happened in the lab.
When I tried to open the DVD files, I got the following error message: ""Files are missing from the recording."" I initially thought that either my colleague had forgotten to burn some files onto the DVD or that the DVD had become corrupted. Neither was true. As it turned out, he was simply using a newer version of the software than I was.
This example reveals two usability problems at two different levels:

At the  individual user level, the error message was misleading. The message should have clearly stated that the problem related to the software version, not to missing files. This violates the long-known  guidelines for error message usability.
At the  enterprise level, the deeper problem is that one version of the software can't read files produced by another version. This imposes a burden on the organization to synchronize upgrades and make sure that everybody is always using the same version. In a big company, this creates extra work for the system administrators. In a small company like mine, synchronization might be overlooked.

Complex rules  often cause enterprise usability problems. For example, airlines are notorious for the complexity of their fare structures. This causes individual-user problems on travel sites, as well as enterprise problems for the airlines themselves, because the complex fares complicate many other business processes. In addition to needing more staff to handle customer inquiries, airlines suffer lower customer satisfaction.
Government regulations  can be the worst of all. To comply with federal regulations each year, small companies in the United States spend $7,647 per employee and big companies spend $5,282 per employee (according to  the government's own estimates). This $1.1 trillion drain on the economy could easily be cut in half if legislators and agencies would consider how easy or difficult it is to handle the paperwork they impose on companies. Simplification would be especially beneficial for small companies that don't have dedicated specialists on staff and thus suffer more from usability problems in complicated government forms and rules.
Negotiation-free  deals are a rare example of good enterprise usability. When you want to advertise on a search engine, you simply go to its site and enter your ad and your bid. Several search engines offer one-click access to distribute your ads on their network of additional websites, without having to negotiate any further deals. Whenever lawyers get involved, business opportunities die because of delays and friction. Self-service deals are a boon to enterprise usability because they let individual decision-makers move ahead and spend their budgets as they see fit.
Usability Methods Vary By Scope
Because of different scopes in usage, the usability methods required to ensure good design also vary across the three levels.
User testing  assesses the UI's immediate effect: What happens while people are clicking around? It's the perfect method for evaluating usability at the individual level. We can expand user testing to the group level by testing multiple users at the same time. For example: We can network two labs together, place a user in each lab, and watch them solve a problem using the target collaborative application. Workflow support for long-term processes might be slightly trickier to test, since you can't just test all the users at the same time. However, it can be done, typically by testing one stage at a time.
Field studies  are always good for identifying unmet user needs at any usability level, but they become especially important as we broaden our scope. At the enterprise level in particular, you must observe in-context behavior to determine how to fit the design to the organization's needs.
Design standards  help individual users by enforcing UI consistency; they also promote group-level usability and collaboration by ensuring consistent element naming across screens. At the enterprise level, design standards increase help desk productivity by reducing the number of calls and making it more likely that agents will know the answers to the remaining questions.
Customer roundtables  are a little-used method that's particularly good for enterprise usability. At the individual level, we want to study  users, not  customers. That is, we want to observe the people who hold the mouse, not their managers or the big bosses who sign the checks. This is one of the key differences between usability methods and marketing methods. But, for enterprise usability, we need to study the people who run the organization and who know the pain points at levels above an individual contributor's job. Customer roundtables are a good supplement to field studies: they bring together a small group of sysadmins or managers to discuss their own experiences with larger issues of the product's use.
Individual usability is pretty much a solved problem. We have the usability methods down pat, and anybody can  learn the most important ones in a few days. We also have  thousands of guidelines that are known to enhance usability for individual users.
Group-level usability and enterprise usability are less well defined: they've been researched less and are more variable. This doesn't mean that you should ignore these levels. On the contrary, it means that you should make sure to study them in your own organization."
121,2005-08-14,"In A/B testing, you unleash two different versions of a design on the world and see which performs the best. For decades, this has been a classic method in direct mail, where companies often split their mailing lists and send out different versions of a mailing to different recipients. A/B testing is also becoming popular on the Web, where it's easy to make your site show different page versions to different visitors.
Sometimes, A and B are directly competing designs and each version is served to half the users. Other times, A is the current design and serves as the control condition that most users see. In this scenario, B, which might be more daring or experimental, is served only to a small percentage of users until it has proven itself.
Finally, in  multivariate tests, you vary multiple design elements at the same time, but the main issues are the same as with the more common A/B tests. For simplicity, I'll use the term ""A/B"" to refer to any study where you measure design alternatives by feeding them live traffic, regardless of the number of variables being tested.
Benefits
Compared with other methods, A/B testing has four huge  benefits:

As a branch of website analytics, it measures the  actual behavior  of your customers under real-world conditions. You can confidently conclude that if version B sells more than version A, then version B is the design you should show all users in the future.
It can  measure very small performance differences  with high statistical significance because you can throw boatloads of traffic at each design. The  sidebar shows how you can measure a 1% difference in sales between two designs.
It can  resolve trade-offs  between conflicting guidelines or qualitative usability findings by determining which one carries the most weight under the circumstances. For example, if an e-commerce site prominently asks users to enter a discount coupon,  user testing shows that people will complain bitterly if they don't have a coupon because they don't want to pay more than other customers. At the same time, coupons are a good marketing tool, and usability for coupon holders is obviously diminished if there's no easy way to enter the code. When e-commerce sites have tried A/B testing with and without coupon entry fields, overall sales typically increased by 20-50% when users were not prompted for a coupon on the primary purchase and checkout path. Thus, the general guideline is to avoid prominent coupon fields. Still, your site might be among the exceptions, where coupons help more than they hurt. You can easily find out by doing your own A/B testing under your own particular circumstances.
It's  cheap: once you've created the two design alternatives (or the one innovation to test against your current design), you simply put both of them on the server and employ a tiny bit of software to randomly serve each new user one version or the other. Also, you typically need to cookie users so that they'll see the same version on subsequent visits instead of suffering fluctuating pages, but that's also easy to implement. There's no need for expensive usability specialists to monitor each user's behavior or analyze complicated interaction design questions. You just wait until you've collected enough statistics, then go with the design that has the best numbers.

Limitations
With these clear benefits, why don't we use A/B testing for all projects? Because the downsides usually outweigh the upsides.
First, A/B testing can  only be used for projects that have one clear, all-important goal, that's to say a single KPI (key performance indicator). Furthermore, this goal must be  measurable by computer, by counting simple user actions. Examples of measurable actions include:

Sales for an e-commerce site.
Users subscribing to an email newsletter.
Users opening an online banking account.
Users downloading a white paper, asking for a salesperson to call, or otherwise explicitly moving ahead in the sales pipeline.

Unfortunately, it is rare that such actions are a site's only goal. Yes, for e-commerce, the amount of dollars collected through sales is probably paramount. But sites that don't close sales online can't usually say that a single desired user action is the only thing that counts. Yes, it's good if users fill in a form to be contacted by a salesperson. But it's also good if they leave the site feeling better about your product and place you on their shortlist of companies to be contacted later in the buying process, particularly for  B2B sites If, for example, your only decision criterion is to determine which design generates the most white paper downloads, you risk undermining other parts of your business.
For many sites, the ultimate goals are  not measurable  through user actions on the server. Goals like improving brand reputation or supporting the company's public relations efforts can't be measured by whether users click a specific button. Press coverage resulting from your  online PR information  might be measured by a clippings service, but it can't tell you whether the journalist visited the site before calling your CEO for a quote.
Similarly, while you can easily measure how many users sign up for your email newsletter, you can't assess the equally important issue of  how they read your newsletter content without observing subscribers as they open the messages.
A second downside of A/B testing is that it  only works for fully implemented designs  . It's cheap to test a design  once it's up and running,  but we all know that implementation can take a long time. Before you can expose it to real customers on your live website, you must fully debug an experimental design. A/B testing is thus suitable for only a very small number of ideas.
In contrast,  paper prototyping lets you try out several different ideas in a single day. Of course, prototype tests give you only qualitative data, but they typically help you reject truly bad ideas quickly and focus your efforts on polishing the good ones. Much experience shows that refining designs through multiple iterations produces superior user interfaces. If each iteration is slow or resource-intensive, you'll have too few iterations to truly refine a design.
A possible compromise is to use paper prototyping to develop your ideas. Once you have something great, you can subject it to A/B testing as a final stage to see whether it's truly better than the existing site. But A/B testing can't be the primary driver on a user interface design project.
Short-Term Focus
A/B testing's driving force is the number being measured as the test's outcome. Usually, this is an immediate user action, such as buying something. In theory, there's no reason why the metric couldn't be a long-term outcome, such as total customer value over a five-year period. In practice, however, such long-term tracking rarely occurs. Nobody has the patience to wait years before they know whether A or B is the way to go.
Basing your decisions on short-term numbers, however, can lead you astray. A common example: Should you add a promotion to your homepage or product pages? Unless you're promoting something relevant to a user's current need, every promotion you add clutters the pages and lowers the site's usability.
When I point out the usability problems with promotions, I typically get the counter-argument that promotions generate extra sales of the target item or service. Yes: any time you give something prominent placement, it'll sell more. The question is whether doing so hurts your site in other ways.
Sometimes, an A/B test can help you here, if you examine the impact on overall sales, not just sales of the promoted product. Other times, A/B tests will fail you if the negative impact doesn't occur immediately. A cluttered site is less pleasant to use, for example, and might reduce customer loyalty. Although customers might make their current purchases, they might also be less likely to return. However small, such an effect would gradually siphon off customers as they seek out other, better sites. (This is how more-cluttered search engines lost to Google over a four-year period.)
No Behavioral Insights
The biggest problem with A/B testing is that you don't know  why  you get the measured results. You're not observing the users or listening in on their thoughts. All you know is that, statistically, more people performed a certain action with design A than with design B. Sure, this supports the launch of design A, but it doesn't help you move ahead with other design decisions.
Say, for example, that you tested two sizes of Buy buttons and discovered that the big button generated 1% more sales than the small button. Does that mean that you would sell even more with an even bigger button? Or maybe an intermediate button size would increase sales by 2%. You don't know, and to find out you have no choice but to try again with another collection of buttons.
Of course, you also have no idea whether  other changes might bring even bigger improvements, such as changing the button's color or the wording on its label. Or maybe changing the button's page position or its label's font size, rather than changing the buttons size, would create the same or better results. Basically, you know nothing about why button B was not optimal, which leaves you guessing about what else might help. After each guess, you have to implement more variations and wait until you collect enough statistics to accept or reject the guess.
Worst of all, A/B testing  provides data only on the element you're testing.  It's not an open-ended method like user testing, where users often reveal stumbling blocks you never would have expected. It's common, for example, to discover problems related to  trust, where users simply don't want to do business with you because your site undermines your credibility.
Bigger issues like trust or uninformative product information often have effect sizes of 100% or more, meaning that your sales would double if such problems were identified and fixed. If you spend all your time fiddling with 1-2% improvements, you can easily overlook the  100% improvements  that come from qualitative insights into users' needs, desires, and fears.
Combining Methods
A/B testing has more problems than benefits. You should not make it the first method you choose for improving your site's conversion rates. And it should certainly never be the only method used on a project. Qualitative observation of user behavior is faster and generates deeper insights. Also, qualitative research is less subject to the many  errors and pitfalls that plague quantitative research.
A/B testing does have its own advantages, however, and provides a great supplement to qualitative studies. Once your  company's commitment to usability has grown to a level where you're regularly conducting many forms of user research, A/B testing definitely has its place in the toolbox.
(More on this angle in the full-day course on combining UX mehods and Analytics methods like A/B testing.)"
122,2005-02-13,"It's a miracle that user testing works: You bring people into a room where they're surrounded by strangers, monitored by cameras, and asked to perform difficult tasks with miserable websites. Often, there's even an intimidating one-way mirror dominating one wall. Under these conditions, how can anybody accomplish anything?
In fact, all experience shows that people can and do use websites and other user interfaces during test sessions, and that many valuable usability findings emerge from such studies. Why?
Two main reasons: the power of engagement and the suspension of disbelief.
User Engagement
When test participants are asked to perform tasks, they usually get so  engaged in using the interface  that the usability lab's distractions recede. Users know that there's a camera, and maybe even a one-way mirror hiding additional observers, but their attention is focused on the screen.
It's a basic human desire to want to perform well on a test. We can say,  ""we're not testing you, we're testing the system""  all we want. People still feel like they're taking a test, and they want to pass. They don't want to be defeated by a computer.
Because they want to be successful, users allocate their mental resources to what's happening on the screen, not what's happening in the room. Of course, this concentration can easily be broken, which is why it's a  cardinal rule of user testing to have  observers remain absolutely quiet  if they're in the room with users. Also, in fancy usability labs, I've seen users distracted by the noise of cameras moving along ceiling tracks, so that type of thing is best avoided. But, generally, as long as observers stay quiet and out of view (behind the user or behind the mirror), participants will remain engaged in their tasks.
One downside of users' tendency to engage strongly is that they sometimes work harder on tasks in a test session than they would at home. If the user says, ""I would stop here,"" you can bet that they'd probably stop a few screens earlier if they weren't being tested.
Suspension of Disbelief
In user testing, we pull people away from their offices or homes and ask them to pretend to perform business or personal tasks with our design. Obviously, an artificial scenario.
As part of the full usability life cycle, there are good reasons to conduct  field studies and observe users' behavior in their natural habitats. Unfortunately, field studies are much more expensive than lab studies, and it's typically difficult to get permission to conduct research inside other companies. During a design process, most usability sessions involve user testing; we're therefore lucky that users can typically overcome a lab's artificial nature and pretend to be at home.
The tendency to suspend disbelief is deeply rooted in the human condition, and may have developed to help prehistoric humans bond around the camp fire in support of storytelling and magic ceremonies. In the modern world, TV shows like  Star Trek  only work because of our propensity to suspend disbelief. Consider the number of untruths involved in watching  Star Trek:

You're not looking at people, you're looking at  pictures  of people, in the form of glowing dots on a video screen.
You're not looking at pictures of real people, you're looking at pictures of actors  pretending  to be characters, like Mr. Spock and Captain Picard, that don't exist.
You're not looking at pictures of actors using transporters, shooting phasers, and flying faster-than-light starships. All such activities are  simulated  with special effects.

You know all of this, and yet you engage in the story when watching the show.
Similarly, in usability studies, participants easily pretend that the scenario is real and that they're really using the design. For this to happen, you obviously need realistic test tasks and to have  recruited representative users who might actually perform such tasks in the real world. Assuming both, most usability participants will suspend disbelief and simply attempt the task at hand.
Suspension of disbelief goes so far that users engage strongly with  paper prototypes where the user interface is purely a piece of paper. As long as you can move through screens in pursuit of your goal, you will behave as if the system were real and not simulated.
In fact, sometimes users go too far in suspending disbelief and try to  role play  other  users' potential performance. You must put a stop to this immediately, as soon as you hear users speculate about what ""some people"" might do or might not know. Politely tell such users that you're testing many other people as well, but that you invited them to the test because their personal experiences are very important to the project. You can say to such users, ""be yourself"" and that ""you know what you know,"" then ask them to use their own job situation as the usage scenario's background legend.
When Engagement and Suspension of Disbelief Fail
User testing typically works, but there are exceptions. Occasionally, test participants are so lazy and difficult to engage that they never suspend disbelief and work on the tasks for real. For example, if you ask such users to select a product to solve a problem, they'll typically stop at the first remotely related product, even if it's basically unsuitable and wouldn't be bought by anybody who really had the target problem.
In rare cases, such nonrealistic usage is serious enough that you must simply excuse the participant and discard the session's data. If users haven't suspended disbelief and performed conscientiously, you can't trust that anything they've done represents real use.
More commonly, you can rescue the situation by employing a few facilitation tricks to encourage authentic behavior.
The easiest and most common approach is to  ask the user whether this is what he or she would do at the office  (or at home, for a consumer project). This small reminder is often enough to get users engaged. Variants of this technique include:

Ask users if they have enough information to make a decision (when they stop after finding the first nugget of information about the problem).
Ask users if they are sure that they've selected the best product (if they stop after finding the first likely purchase, without evaluating alternative options).

If users fail to take the tasks seriously in your first few test sessions, you can usually rescue the study by  modifying the test instructions  or task descriptions. For example, it's often enough to add an introductory remark such as, ""Pretend that you must not only find the best solution, but justify the choice to your boss.""
Including an absent boss in the test scenario encourages suspension of disbelief and usually works wonders. I've also found it effective to ask users to  ""write five bullet points for your boss, explaining the main pros and cons of this product.""  (You can also ask for three bullets of pros and three bullets of cons.)
In our  tests of the investor relations areas of corporate websites, we used a variant of this technique, simply asking the financial analysts and individual investors to decide whether they thought the company would do better or worse than the stock market and state the main reasons why.
Other times, you can induce constraints to better engage users with the task. For example, we once tested an e-commerce site that sold office supplies. In pilot testing, a test task for administrative assistants was to stock the office for a newly hired employee. Unfortunately, the pilot participant wanted to be really nice to this hypothetical new colleague, and bought all the most expensive pens, office chairs, and so on. Although in this case, we could have asked users to pretend that they had to answer to the boss, we chose instead to give them a specific budget. This small change was enough to make users consider each item carefully, which let us discover how they assessed product quality and value.
Despite the artificial nature of user testing, we typically see enough authentic behavior to easily identify design flaws that would diminish an interface's value for real customers. Getting to actually use something is so powerful an experience that most users provide good data. When they don't, you can typically prod users lightly with verbal reminders, or alter the task a bit to get them fully engaged."
123,2004-11-07,"The good news in user research is that we're building up a  massive body of knowledge about user behavior in online systems. The days are long gone when companies had to guess about website and intranet designs. The bad news is that the sheer amount of accumulated research findings can be overwhelming. Even worse? User research won't generate an additional penny of profit unless you understand it and act upon it.
Here's one way of quantifying the amount of current usability knowledge: We've published  3,326 pages of usability research reports. Those reports contain 1,217 general design guidelines and 1,961 screenshots, all of which have information about how specific designs helped or hindered real users. Other researchers publish reports as well, and you may also have an internal usability group or have commissioned consultants to study issues of interest to you.
In total, a huge mass of user research. How should you deal with all these findings?
First Steps
User research is a reality check. It tells you what really happens when people use computers. You can  speculate  on what customers want, or you can  find out. The latter is the more fruitful approach.
Research offers an understanding of how users behave online, and is a solid foundation on which to build a design. I still recommend that you user test your own design: any time you have a new idea, build a  paper prototype and test it so that you don't waste money implementing ideas that don't work. But, if you start with design ideas that are based on the actual behavior of real human beings, you'll have considerably fewer usability problems than if you target a design at a hypothetical or idealized user.
It can be overwhelming at first to see a long list of new research findings. Try to  process them in small bites. For example, look at your homepage or a typical content page in light of the new findings. Print out a copy and circle each design element that might violate a design guideline or cause users problems.
Make general issues concrete  by applying them to a familiar example. This is always a good way to build up understanding that can help you in future design projects.
You can also  use research findings as a checklist: go through your own design one guideline at a time and see whether you comply. Whenever you're in violation of an established usability finding, you can dig deeper into that finding's underlying user research and learn more about it. With your new knowledge, you might decide to fix your design to make it compliant with users' typical behavior. Or, of course, you might disagree with the research findings.
Handling Disagreements
There are two main reasons people disagree with a research study and its conclusions: their own research shows something different, or their personal opinions and preferences differ from the research recommendations.
If your research findings disagree with published results, you have two options. First, it's possible that your study had a methodology flaw, so it's worth reviewing the study in light of the new results. There are numerous  issues to consider to run a valid user test. Second, it could be that you're dealing with a special case, and your findings actually are different. Such cases do exist, although exceptions are more rare than people would like to think.
If your own intuition disagrees with published findings, view it as a learning opportunity that can improve your future insights.  Design is not religion. You don't have to defend the beliefs of your forefathers to the bitter end. Design is a business decision. You should follow the data and do what generates the biggest profits for your company, not what wins design awards.
If you disagree strongly, you can always run a study of your design to determine whether you are one of those rare exceptions. General usability guidelines typically hold true in about 90% of cases. There are many special circumstances that make the remaining 10% sufficiently atypical such that the best solution will be something other than the normal recommendation.
If you run an online business, you're in the user experience business: all the value flows through a user interface. It's essential to develop the expertise to interpret user research and an understanding of when to run usability studies. This is true even if you're not a usability specialist yourself and never want to personally run a study. You still have to know how to deal with the reports and make the research findings relevant to your business."
124,2004-07-18,"One of the biggest challenges in website and intranet design is creating the  information architecture: what goes where? A classic mistake is to structure the information space based on how  you  view the content — which often results in different subsites for each of your company's departments or information providers.
Rather than simply mirroring your org chart, you can better enhance usability by creating an information architecture that reflects how  users  view the content. In each of our  intranet studies, we've found that some of the biggest productivity gains occur when  companies restructure their intranet to reflect employees' workflow. And in  ecommerce, sales increase when products appear in the categories where users expect to find them.
All very good, but  how do you find out  the users' view of an information space and where they think each item should go? For  researching this type of mental model, the primary method is card sorting:

Write the name (and perhaps a short description) of each of the main items on an index card. Yes, good old paper cards. (Taking care not to use  terms that bias the users.)
Shuffle the cards and give the deck to a user. (The standard  recommendations for recruiting test participants apply: they must be representative users, etc.)
Ask each user to sort the cards into piles, placing items that belong together in the same pile. Users can make as many or as few piles as they want; some piles can be big, others small.
Optional extra steps include asking users to arrange the resulting piles into bigger groups, and to name the different groups and piles. The latter step can give you ideas for words and synonyms to use for navigation labels, links, headlines, and search engine optimization.

Because card sorting uses no technology, this  photo of a 1995 card sort looks the same as one conducted today.
Research Study
Fidelity Investments has one of the world's best usability teams, led by Dr. Thomas S. Tullis, senior VP of human interface design. Tullis and co-author Larry Wood recently reported the results of a study measuring the trade-off curve for testing various numbers of users in a card sorting exercise.
First, they tested 168 users, generating very solid results. They then simulated the outcome of running card sorting studies with smaller user groups by analyzing random subsets of the total dataset. For example, to see what a test of 20 users would generate, they selected 20 users randomly from the total set of 168 and analyzed only that subgroup's card sorting data. By selecting many such samples, it was possible to estimate the average findings from testing different numbers of users.
The main quantitative data from a card sorting study is a set of  similarity scores  that measures the similarity of user ratings for various item pairs. If all users sorted two cards into the same pile, then the two items represented by the cards would have 100% similarity. If half the users placed two cards together and half placed them in separate piles, those two items would have a 50% similarity score.
We can assess the outcome of a smaller card sorting study by asking how well its similarity scores correlate with the scores derived from testing a large user group. (A reminder: correlations run from -1 to +1. A correlation of 1 shows that the two datasets are perfectly aligned; 0 indicates no relationship; and negative correlations indicate datasets that are opposites of each other.)
How Many Users?
For most usability studies, I recommend  testing 5 users, since that's enough data to teach you most of what you'll ever learn in a test. For card sorting, however, there's only a 0.75 correlation between the results from 5 users and the ultimate results. That's not good enough.
You must test 15 users to reach a correlation of 0.90, which is a more comfortable place to stop. After 15 users, diminishing returns set in and correlations increase very little: testing 30 people gives a correlation of 0.95 — certainly better, but usually not worth twice the money. There are hardly any improvements from going beyond 30 users: you have to test 60 people to reach 0.98, and doing so is definitely wasteful.
Tullis and Wood recommend testing 20–30 users for card sorting. Based on their data,  my recommendation is to test 15 users.
Why do I recommend testing fewer users? I think that correlations of 0.90 (for 15 users) or maybe 0.93 (for 20) are good enough for most practical purposes. I can certainly see testing 30 people and reaching 0.95 if you have a big, well-funded project with a lot of money at stake (say, an intranet for 100,000 employees or an ecommerce site with half a billion dollars in revenues). But most projects have very limited resources for user research; the remaining 15 users are better ""spent"" on 3 qualitative usability tests of different design iterations.
Let Users Inform Your Design
I don't recommend designing an information architecture based purely on a card sort's numeric similarity scores. When deciding specifics of what goes where, you should rely just as much on the qualitative insights you gain in the testing sessions. Much of the value from card sorting comes from  listening to the users' comments  as they sort the cards: knowing  why  people place certain cards together gives deeper insight into their mental models than the pure fact that they sorted cards into the same pile.
Why More Users for Card Sorting?
We know that 5 users are enough for most usability studies, so why do we need three times as many participants to reach the same level of insight with card sorting? Because the methods differ in two key ways:

User testing is an  evaluation method: we already have a design, and we're trying to find out whether or not it's a good match with human nature and user needs. Although people differ substantially in their capabilities (domain knowledge, intelligence, and computer skills), if a certain design element causes difficulties, we'll see so after testing a few users. A low-end user might experience more severe difficulties than a high-end user, but the magnitude of the difficulties is not at issue unless you are running a measurement study (which requires more users). All you need to know is that the design element doesn't work for humans and should be changed.
Card sorting is a  generative method: we don't yet have a design, and our goal is to find out how people think about certain issues. There is great variability in different people's mental models and in the vocabulary they use to describe the same concepts. We must collect data from a fair number of users before we can achieve a stable picture of the users' preferred structure and determine how to accommodate differences among users.

If you have an existing website or intranet, testing a few users will tell you whether people have trouble with the information architecture. To generate a new structure from scratch, you must sample more people.
Luckily, you can  combine the two methods: First, use generative studies to set the direction for your design. Second, draft up a design, preferably using  paper prototyping, and run evaluation studies to refine the design. Because usability evaluations are fast and cheap, you can afford multiple rounds; they also provide quality assurance for your initial generative findings. This is why you shouldn't waste resources squeezing the last 0.02 points of correlation out of your card sorts. You'll catch any small mistakes in subsequent user testing, which will be much cheaper than doubling or tripling the size of your card sorting studies.
Study Weaknesses
The Fidelity study has two obvious weaknesses:

It's only one study. It's always better to have data from multiple companies.
The analysis was purely quantitative, focusing on a statistical analysis of similarity scores and ignoring user comments and other qualitative data.

These two weaknesses are not fatal. I view this as a pioneering study and a great contribution to our web usability knowledge. But, because of the study's weaknesses, it would be useful if somebody duplicated it with different information spaces, and also analyzed the qualitative data along with the numeric scores. Sounds like a good thesis project for a graduate student who's looking to research something with real-world impact (hint, hint).
Even though more data would be comforting, I have confidence in the Fidelity study's conclusions because they match my own observations from numerous card studies over many years. I've always said that it was necessary to test more users for card sorting than for traditional usability studies. And I've usually recommended about 15 users, though we've also had good results with as few as 12 when budgets were tight or users were particularly hard to recruit.
There are a myriad ways in which  quantitative studies can go wrong and mislead you. Thus, if you see a single quantitative study that contradicts all that's known from qualitative studies, it's prudent to disregard the new study and assume that it's likely to be bogus. But when a quantitative study confirms what's already known, it's likely to be correct, and you can use the new numbers as decent estimates, even if they're based on less data than you would ideally like.
Thus, the current recommendation is to  test 15 users for card sorting  in most projects and 30 users in big projects with lavish funding.
Reference
Tullis, Tom, and Wood, Larry. (2004) How Many Users Are Enough for a Card-Sorting Study?, Usability Professionals Association (UPA) 2004 Conference, Minneapolis, MN, June 7–11, 2004."
125,2004-02-01,"One goal beats all others when designing a customer survey for a website:  maximize the response rate. Low response rates can create actively misleading findings because they're based on a biased sample of your highly committed users as opposed to most users (who have better things to do than take your survey).
It doesn't matter what you ""learn"" from a survey; you can't trust the data if it doesn't represent your users.
How can you get average users to respond? The highest response rates come when surveys are  quick and painless. And the best way to reduce users' time and suffering is to reduce the number of questions.
Of course, you also must ensure that the survey is easy to operate and that users understand the questions and response options. If users misinterpret your writing, their answers will be misleading. Remember that an interactive questionnaire is a user interface: it should be designed on the basis of user testing and follow standard usability guidelines. (With  paper prototypes you can pilot three iterations of a survey in an afternoon.)
One of the ten biggest U.S. banks recently subjected business customers to a 32-screen survey. And they weren't easy screens, either. Many screens required users to evaluate four different business banking services on six parameters. My test user gave up after three screens, saying,  ""I'm a small business owner. I don't have time for this.""  (Later, I persisted in completing the full survey myself, but only so I could collect a horrifying set of screenshots for my next book.)
Survey bloat  is a natural consequence of having a diverse group of marketing managers, all of whom want customer feedback on their special issues. Please resist the temptation to collect all the information that anybody could ever want. You will end up with no information (or misleading information) instead.
Ask Fewer Questions — Maybe Only NPS
The simplest solution to survey bloat is to ask fewer questions. Ask questions that address only your core needs and skip the subtleties. Surveys are not great at gauging minor differences anyway — you need direct observation for that.
A recent article in  Harvard Business Review  by Frederick Reichheld on ""The One Number You Need to Grow"" documented that the vast majority of customer-satisfaction insight comes from answers to a single question:  ""How likely is it that you could recommend [X] to a friend or colleague?"" 
In 13 of 14 case studies, this one question was as strong a predictor of customer loyalty as any longer survey.
The percent of users who would ""very likely"" recommend minus the percentage who are ""unlikely"" to recommend is often called the  Net Promoter Score (NPS). The point being that people with no strong feelings either way don't count that much.
For discretionary intranet services, you could rephrase the question as  ""How likely is it that you could recommend [service X] to a colleague?""  For mandatory intranet services, you need a different question to assess satisfaction, because employees might ""recommend"" a problematic service simply because there are no other options.
Divide and Conquer
A second solution is to  ask different questions of different visitors. If you have ten questions that absolutely need answers, divide them into five different questionnaires with two questions each that  fit on a single screen. You can then randomly assign one of the five surveys to each visitor.
Your website is a computer: take advantage of its ability to run software and show different users different things. (Alternatively, you could run a different survey each weekday, assuming there are no significant differences between visitors on different days.)
The only time you must present multiple questions to the same user is when you want to run regression analyses or other multivariate statistics. But, in my experience, most Web people greet any mention of multivariate stats with blank stares, so it's probably rare that you'll actually use them.
Short surveys are better surveys. They certainly provide better data than bloated surveys, which few people will finish (if they start them at all).
Read More
Fred Reichheld and Rob Markey: The Ultimate Question 2.0: How Net Promoter Companies Thrive in a Customer-Driven World. (Or use link to Amazon.co.uk for readers in the UK/Europe.)"
126,2002-07-06,"The usability movement is sometimes criticized for being dull and for promoting boringly invariable designs. The chief reason for this is that some people equate design conventions with creative restrictions. However, this equation doesn't add up for two reasons.
Standards vs. Design Dictates
First, while it is true that usability is typically enhanced by consistency and adherence to design guidelines, this does not necessitate identical design. Rather, such conventions aim to create a vocabulary of building blocks that designers can combine in many vastly different, and often enjoyable, ways.
Consider natural language. Each word has an established meaning, and we typically combine words using a defined grammar. Literature that follows these conventions is easier to read and has a bigger audience than avant-garde, experimental literature. Still, such ""conventional"" novels are definitely not the same: Although they use fully standardized language, they can reach any desired extreme on a variety of emotional scales.
Usability = Engagement
The second reason usability is not opposed to fun is that the  greatest joy of using computers comes through user empowerment and engagement. It's very enjoyable to visit a website that works, where everything just clicks for you. In contrast, a user interface that doesn't do things the way you want feels sluggish, unpleasant, and possibly even hostile, despite the designer's no doubt sincere attempt to invoke positive emotions. A user's personal experience trumps anything the designer is trying to communicate. In talking about a design's ""look and feel,""  feel  wins every time.
As an example, Amazon.com uses associative links to create a fun and rewarding experience for users. Each book page offers associative links to five books frequently bought by other people who purchased the book youre interested in. Following these links can lead to  a powerful feeling of discovery. As a result, you can easily spend much more time shopping on Amazon than is dictated by the simple efficiency metric of buying the book you came for as quickly as possible.
Such  engagement requires usability. If users can't master the interface, they'll feel oppressed rather than empowered, and are unlikely to explore or use anything beyond the absolute minimum. On the Web, this ""minimum"" often turns out to be one or two page views, and then users are gone -- never to return.
There is certainly more to an enjoyable activity than the mere ability to complete it. At the same time, computers are currently difficult to use and much of the  Web feels like a vast wasteland. Given this, people can and do derive considerable pleasure in finding a well-crafted user experience that empowers and engages them.
Methods for Testing Satisfaction
Traditional user testing  is great at debugging user interface designs to find the elements that make the system difficult to use. Test methods are less evolved, however, when it comes to determining the enjoyable aspects of a design. In the past, this was not much of a problem because user interfaces were so difficult to use that all we could hope for was that they'd improve to the level where using them was not  actively unpleasant  . Websites in particular were designed in such great contrast to users' needs that simply exterminating bloated and useless designs has been the usability movement's great achievement over the last ten years.
Now, as we change from the negative endeavor of removing bad design to the positive pursuit of good design, we must modify the methodology to encompass more awareness of fulfilling, engaging, and fun design elements.
Most studies currently rely on classic and not completely satisfactory ways of  assessing user enjoyment:

A subjective satisfaction questionnaire  administered at the end of a study that provides a simple, overall system assessment.
Observations of the user's body language  for indications of satisfaction or displeasure (smiles or frowns), as well as for laughs, grunts, or explicit statements such as ""cool"" or ""boring.""

A skilled observer can gain much insight from the second approach, but it is a weak and possibly misleading source of data for less-skilled usability professionals, who constitute the vast majority of the world's test facilitators.
As for the first approach, subjective satisfaction questionnaires suffer the standard problem of being administered out of context: They typically rely on users' recollection of enjoyment, rather than the actual experience of use in the moment. You can alleviate (though not eliminate) this by administering several small questionnaires throughout the test session rather than saving all the questions for one larger questionnaire at the end.
Beyond Ease of Use
As always, you  cannot rely on simple, literal interpretations of users' statements. For example, in testing company websites, users almost always say that they don't want fun or entertaining content:  Just give me the answers as straight and as fast as possible.  And, in observing actual user behavior, we certainly do see negative reactions to frivolous content -- such as big photos of glamorous models or meaningless animations that bounce around the screen. But, at the same time, we also see users smile or exhibit other positive body language when they come across cleverly written content or moderately funny descriptions -- assuming they fall within the scope of users' expectations of professional writing in the website's genre. Thus, users seem to appreciate and enjoy a somewhat higher style to their content than they claim to prefer.
We need much better methods for testing enjoyable aspects of user interfaces. Such methods should be both robust and easy to apply, since people with relatively little expertise do the vast majority of user testing in the world.
That said,  ease of use  must remain our first priority. Technology is just too difficult for us to abandon this goal. But hopefully it will soon be time to emphasize   joy of use as well."
127,2002-01-19,"Field studies are one of the most valuable methods for setting a design project's direction and discovering unmet user needs. But studying and questioning users does no good if you tell them the answers--because then you won't truly learn anything new.
Last week,  The New York Times  ran a long  article about companies using anthropological techniques to study their customers. It's always great to see articles that promote field studies, but the information in the  Times  article perpetuated two common mistakes that not only produce bad data, but squander a company's research budget:

The reported studies emphasized interview questions, even though quietly observing users is more valuable and the real reason to go into the field.
All the talk about ""anthropology"" obscures the fact that all development teams should do field studies, and that teams can run studies on their own, without hiring a bunch of PhDs.

Don't Ask Leading Questions
The article contained the following snippet from a field study of barbecue-grill customers:
Q:  ""So you feel that grilling outdoors fosters family togetherness?""

A:  ""Sure.""
Q:  ""Is there anyone in your family who doesn't enjoy grilling?""
A:  ""My father.""
Q:  ""But you feel it's a bonding ritual all the same?""
A:  ""Yeah, kinda.""
Q:  ""How does grilling work in the text of your life? Would charcoal have interfered with the process of social bonding?""
A:  ""I'm not sure, really. We just prefer gas.""

Perhaps the researcher was simply hamming it up for the press. In any case, the above segment violates several basic interviewing principles:

Don't ask questions that can be answered with ""yes"" or ""no.""  You can elicit more information from the respondent using open-ended questions, which encourage them to talk and provide salient details.
Whether open or closed, definitely  don't ask leading questions. Once you state what the person supposedly feels, you bias any subsequent answers. People are reluctant to disagree with the interviewer's ""authority.""
Don't use jargon  (""text of your life"" and ""social bonding"" are obvious examples here). When talking to respondents, speak in their language; this draws them out and helps you understand how they truly feel.
Don't draw attention to specific issues  that you care about (in this case, the ""bonding ritual""). Doing so causes people to change their behavior and focus their answers on the issues you emphasize. This problem is particularly prevalent in interface design studies: The second you ask people about a specific design element, they notice it much more thereafter than they would have otherwise.

In addition to interviewing methodology errors, the story highlights an even worse problem by focusing exclusively on interviews instead of  observation  . Once you go through the hassle of setting up a field visit, the most important data you can collect is about  customer behavior. In other words, you  watch what people do and not what they say.  Did they bond? Did the Dad stay indoors the whole time? What was really going on? 
Run Your Own Field Studies
Most articles on field studies make it seem like they are terribly complicated and require a team of anthropologists. Of course, because the average design team doesn't have such specialists around, they naturally dismiss field studies and proceed on the basis of speculation (or focus groups, which are almost as bad as sitting around a table and making up the data).
In reality, basic field study techniques are fairly simple, and  everyone who works on a design team should go on customer visits  from time to time. Visiting a real customer site is an invaluable experience for designers, programmers, and marketers.
Intranet projects need field studies as well, and have an easier time scheduling the visits since they typically involve setting foot in another department or building.
Well-funded projects might rely on elaborate field methods that take months or years and require specialized staff. Such projects will probably learn more than projects that go for the fast methods, but they will not necessarily be more successful because the market opportunity may pass them by. Also, smaller studies permit more data collection at more project stages, and exposing team members to live data rather that digested reports is invaluable.
Intranet design teams  in particular desperately need to observe actual employee behavior in the field; doing so shows them the real opportunities for improved task support.
Collecting field data and visiting live customers are not the exclusive preserve of a closed guild of experts. It's the duty of all those who plan to inflict their designs on others.
To learn more about how to conduct field studies correctly, check out our full day course on User Research Methods."
128,2001-08-04,"In past years, the greatest usability barrier was the preponderance of cool design. Most projects were ruled by usability opponents who preferred complexity over simplicity. As a result, billions of dollars were wasted on flashy designs that were difficult to use.
One of the main advantages of the ""dot-bomb"" downturn is that cool design has suffered a severe setback. Companies are now focused on the bottom line:

Public websites, which formerly focused on building awareness, now aim at making it easy for customers to do business.
Intranets  are similarly refocused on  improving employee productivity. Many companies are attempting to create order, impose design standards, and enhance navigation on previously chaotic intranets.

Happily, glamour-based design has lost and usability advocates have won the first and hardest victory: Companies are now paying attention to usability needs.
Unfortunately, winning a battle with usability opponents doesn't win the war with complexity. It simply moves us to a new front line: The battle is now to get companies to do usability  right. 
Watch Users Work
Too frequently, I hear about companies basing their designs on user input obtained through misguided methods. A typical example? Create a few alternative designs, show them to a group of users, and ask which one they prefer.  Wrong.  If the users have not actually tried to use the designs, they'll base their comments on surface features. Such input often contrasts strongly with feedback based on real use.
For example: A spinning logo might look pretty cool if you don't need to accomplish anything on the page. Another example is the drop-down menu. Users always love the idea: finally a standard user interface widget that they understand and that stays the same on every page. However, while they offer users a sense of power over the design,  drop-down menus often have low usability and either confuse users or lead them to unintended parts of the site.
To discover which designs work best,  watch users as they attempt to perform tasks  with the user interface. This method is so simple that many people overlook it, assuming that there must be something more to usability testing. Of course, there are many ways to watch and  many tricks to running an optimal user test or field study. But ultimately, the way to get user data boils down to the  basic rules of usability:

Watch what people actually do.
Do not believe what people  say  they do.
Definitely don't believe what people predict they  may  do in the future.

Say, for example, that 50% of survey respondents claim they would buy more from ecommerce sites that offer 3-D product views. Does this mean you should rush to implement 3-D on your site? No. It means that ""3-D"" sounds cool. The world is littered with failed businesses that banked on people's attitude toward hypothetical products and services. In speculative surveys, people are simply guessing how they might act or which features they'll like; it doesn't meant they'll actually use or like them in real life.
When and How to Listen
When should you collect preference data from users? Only after they have used a design and have a real feeling for how well it supports them. Jonathan Levy and I analyzed data from 113 pairwise comparisons of user interfaces designed to support the same task and found a  0.44  correlation between users' measured performance and their stated preference.  The more a design supports users in easily and efficiently doing what they want to do, the more they like the design. Very understandable.
(Update: in newer research, I found a correlation of r=.53 between users' performance and preferences for websites. Higher than for the PC applications that had a .44 correlation, but still low because this shows that you can only predict about a quarter of how well a design works from knowing how much users say they like it.)
However, when collecting preference data, you must take human nature into account. When talking about past behavior, users'  self-reported data is typically 3 steps removed from the truth:

In answering questions (particularly in a focus group), people bend the truth to be closer to what they  think you want to hear  or what's socially acceptable.
In telling you what they do, people are really telling you what they  remember  doing. Human memory is very fallible, especially regarding the small details that are crucial for interface design. Users cannot remember some details at all, such as interface elements that they didn't see.
In reporting what they do remember, people  rationalize  their behavior. Countless times I have heard statements like ""I would have seen the button if it had been bigger."" Maybe. All we know is that the user didn't see the button.

So, do users know what they want? No, no, and no. Three times no.
Finally, you must consider how and when to solicit feedback. Although it might be tempting to simply post a survey online, you're unlikely to get reliable input (if you get any at all). Users who see the survey and fill it out before they've used the site will offer irrelevant answers. Users who see the survey after they've used the site will most likely leave without answering the questions. One question that does work well in a website survey is ""Why are you visiting our site today?"" This question goes to users' motivation and they can answer it as soon as they arrive.
Your best bet in soliciting reliable feedback is to have a captive audience: Conduct formal testing and ask users to fill out a survey at the end. With techniques like  paper prototyping, you can test designs and question users without implementing a thing. Following these basic usability rules and methods will help you ensure that your design is truly as cool as it looks.
Reference
Nielsen, J., and Levy, J. (1994). Measuring usability — preference vs. performance. Communications of the ACM 37, 4 (April), 66–75."
129,2001-01-20,"Usability can be measured, but it rarely is. The reason? Metrics are expensive and are a poor use of typically scarce usability resources.
Most companies still under-invest in usability. With a small budget, you're far better off passing on quantitative measures and reaching for the low-hanging fruit of qualitative methods, which provide a much better return on investment. Generally, to improve a design, insight is better than numbers.
However, the tide might be turning on usability funding. I've recently worked on several projects to establish formal usability metrics in different companies.  As organizations increase their usability investments, collecting actual measurements is a natural next step and does provide benefits. In general, usability metrics let you:

Track progress between releases.  You cannot fine-tune your methodology unless you know how well you're doing.
Assess your competitive position.  Are you better or worse than other companies?  Where  are you better or worse?
Make a Stop/Go decision before launch.  Is the design good enough to release to an unsuspecting world?
Create bonus plans for design managers and higher-level executives.  For example, you can determine bonus amounts for development project leaders based on how many customer-support calls or emails their products generated during the year.

How to Measure
It is easy to specify usability metrics, but hard to collect them. Typically, usability is measured relative to users' performance on a given set of test tasks. The most basic measures are based on the  definition of usability as a quality metric:

success rate (whether users can perform the task at all),
the time a task requires,
the error rate, and
users' subjective satisfaction.

It is also possible to collect more specific metrics, such as the percentage of time that users follow an optimal navigation path or the number of times they need to backtrack.
You can collect usability metrics for both  novice users and experienced users. Few websites have truly expert users, since people rarely spend enough time on any given site to learn it in great detail. Given this, most websites benefit most from studying novice users. Exceptions are sites like Yahoo and Amazon, which have highly committed and loyal users and can benefit from studying expert users.
Intranets, extranets, and weblications are similar to traditional software design and will hopefully have skilled users; studying experienced users is thus more important than working with the novice users who typically dominate public websites.
With qualitative user testing, it is  enough to test 3–5 users. After the fifth user tests, you have all the insight you are likely to get and your best bet is to go back to the drawing board and improve the design so that you can test it again. Testing more than five users wastes resources, reducing the number of design iterations and compromising the final design quality.
Unfortunately, when you're collecting usability metrics, you must test with more than five users. In order to get a reasonably tight confidence interval on the results, I usually recommend testing 20 users for each design. Thus, conducting quantitative usability studies is approximately four times as expensive as conducting qualitative ones. Considering that you can learn more from the simpler studies, I usually recommend against metrics unless the project is very well funded.
Comparing Two Designs
To illustrate quantitative results, we can look at those recently posted by Macromedia from its usability study of a Flash site, aimed at showing that Flash is not  necessarily  bad. Basically, Macromedia took a design, redesigned it according to a set of usability guidelines, and tested both versions with a group of users. Here are the results:



 
Original Design
Redesign


Task 1
12 sec.
6 sec.


Task 2
75 sec.
15 sec.


Task 3
9 sec.
8 sec.


Task 4
140 sec.
40 sec.


Satisfaction score*
44.75
74.50


*Measured on a scale ranging from
			12 (unsatisfactory on all counts) to 84 (excellent on all counts).



It is very rare for usability studies to employ tasks that are so simple that users can perform them in a few seconds. Usually, it is better to have the users  perform more goal-directed tasks that will take several minutes. In a project I'm working on now, the tasks often take more than half an hour (admittedly, it's a site that needs  much  improvement).
Given that the redesign scored better than the original design on all five measures, there is no doubt that the new design is better than the old one. The only sensible move is to go with the new design and launch it as quickly as possible. However, in many cases, results will not be so clear cut. In those cases, it's important to look in more detail at  how much  the design has improved.
Measuring Success
There are two ways of looking at the time-to-task measures in our example case:

Adding the time for all four tasks produces a single number that indicates ""how long it takes users to do stuff"" with each design. You can then easily compute the improvement. With the original design, the set of tasks took 236 seconds. With the new design, the set of tasks took 69 seconds. The improvement is thus  242%. This approach is reasonable if site visitors typically perform all four tasks in sequence. In other words, when the test tasks are really subtasks of a single, bigger task that is the unit of interest to users.
Even though it is simpler to add up the task times, doing so can be misleading if the tasks are not performed equally often. If, for example, users commonly perform Task 3 but rarely perform the other tasks, the new design would be only slightly better than the old one; task throughput would be nowhere near 242% higher. When tasks are unevenly performed, you should compute the improvement separately for each of the tasks:
	
Task 1: relative score 200% (improvement of 100%).
Task 2: relative score 500% (improvement of 400%).
Task 3: relative score 113% (improvement of 13%).
Task 4: relative score 350% (improvement of 250%).

	You can then take the geometric mean of these four scores, which leads to an overall improvement in task time of  150%.

Why do I recommend using the  geometric mean  rather than the more common arithmetic mean? Two reasons: First, you don't want a single big number to skew the result. Second, the geometric mean accounts fairly for cases in which some of the metrics are negative (i.e., the second design scores less than 100% of the first design).
Consider a simple example containing two metrics: one in which the new design doubles usability and one in which the new design has half the usability of the old. If you take the arithmetic average of the two scores (200% and 50%), you would conclude that the new design scored 125%. In other words, the new design would be 25% better than the old design. Obviously, this is not a reasonable conclusion.
The geometric mean provides a better answer. In general, the geometric mean of  N  numbers is the  N  'th root of the product of the numbers. In our sample case, you would multiply 2.0 with 0.5, take the square root, and arrive at 1.0 (or 100%), indicating that the new design has the same usability as the baseline.
Although it is possible to assign different weights to the different tasks when computing the geometric mean, absent any knowledge as to the relative frequency or importance of the tasks, I've assumed equal weights here.
Summarizing Results
Once you've gathered the metrics, you can use the numbers to formulate an overall conclusion about your design's usability. However, you should first examine the relative importance of performance versus satisfaction. In the Macromedia example, users' subjective satisfaction with the new design was 66% higher than the old design. For a business-oriented website or a website that is intended for frequent use (say, stock quotes), performance might be weighted higher than preference. For an entertainment site or a site that will only be used once, preference may get the higher weight. Before making a general conclusion, I would also prefer to have error rates and perhaps a few additional usability attributes, but, all else being equal, I typically give the same weight to all the usability metrics. Thus, in the Macromedia example, the geometric mean averages the set of scores as:  sqrt  (2.50*1.66)=2.04. In other words, the new design scores 204% compared with the baseline score of 100% for the control condition (the old design).
The new design thus has 104% higher usability than the old one. 
This result does not surprise me: It is  common for usability to double as a result of a redesign. In fact, whenever you redesign a website that was created without a systematic usability process, you can often improve measured usability even more. However, the first numbers you should focus on are those in your budget. Only when those figures are sufficiently large should you make metrics a part of your usability improvement strategy.
More Case Studies
See full report on the return-on-investment (ROI) from usability for many more examples of before–after usability metrics. For even more depth, see the full-day course on Measuring User Experience,"
130,1999-12-11,"The good news is that usability has been recognized as an important element of Internet success: the average speaker at industry conferences now promotes good user experience in preference to ""cool sites.""


 The bad news is that most sites employ horribly misguided methodologies that do not assess real usability. Sometimes the methods are simply worthless; other times they are directly misleading.


 Studying Opinion Instead of Use


 Traditional market research methods don't work for the Web. The basic problem is that one cannot ask users what they want and expect the answer to have any relation to their actual behavior when they go online.




   Focus groups
  

 can often be directly misleading. When people sit around a table and discuss what they might like to see on a site, they will often focus on superficial aspects and praise fancy features like animation and Flash effects. But if these same users were ever asked to actually use the site to accomplish a task, they would usually ignore the animations and would find that the Flash effects hurt them more than it helped them.


 Self-reported data is extremely weak and
 
  three levels removed from the truth:
 



  Users tell you what they think you want to hear or what they think is a socially preferred answer (especially when they are part of a group)
 

  Users tell you what they remember believing that they did (but memory is highly fallible, especially regarding the specifics of interaction behavior)
 

  Users can only report what they believe they did; not what they actually did, and people always rationalize their behavior when thinking about it after the fact; also they don't even notice many of the things they do
 



  Surveys are just opinion polls
 
 : a weak method even though survey results are reported too-frequently in the trade press. Just as with focus groups, you get results three levels removed from the truth. For example, an often-quoted survey by Zona Research found that 28% of respondents reported finding it somewhat or extremely difficult to locate products on the Web. I have even quoted this survey myself, even though I should know better than to quote the outcome of an opinion poll. The truth is that observations of actual, real-life user behavior show that people find the product they are looking for less than half of the time. Several usability studies have independently confirmed the same result:
 
  on average, users cannot find what they are looking for on today's Web.
 


 Why this paradox? More than
 
  50%
 
 of the time, people can't find what they are looking for, and yet only
 
  28%
 
 of respondents report problems. In all likelihood, close to 100% of the people who were polled had encountered a case where they could not find a product on a website that did sell it. But they may have assumed that the site didn't carry the product or they may have blamed themselves for not searching well enough or thoroughly enough. Or they may have found the product on another site (causing the first site to lose the sale), after which they thought of their Web experience as having been successful, even though they actually failed the first time they tried to find the product. But all they remember is that they did find it in the end.


 In a particularly misleading type of survey, a
 
  panel of users
 
 is asked to check out a website and fill in a questionnaire with their opinions of the site. The three
 
  methodological hazards
 
 in this method are:



  the users are members of a panel of people who have signed up to be professional opinion-givers in return for money; they are not representative of your customers who almost certainly would not have time to spend on such activities (unless your site is targeting students or the unemployed)
 

  being asked to check something out is completely different than having to use it to accomplish a real task; this is equally true whether the task is work-related (book an airline ticket for my boss to attend a meeting in London) or leisure-related (buy a cheap vacation in a city I like in Europe)
 

  self-reported behavior and opinions have very little relation to real behavior and real usability problems
 


 This said,
 
  short surveys are still good for simple questions
 
 like ""why are you visiting our site"" that relate to users' opinions instead of assessing the design.


 Automated Methods Cannot Work


 Another category of voodoo services sics a computer program on a site and produces an automated report that is claimed to measure the site's usability.


 Having a computer follow links and count the number of clicks is a
 
  very poor substitute
 
 for whether users can actually find what they are looking for. Real usability depends on
 
  which
 
 link you click on and how fast you discover the errors of your ways if you clicked on the wrong one. This cannot be assessed by computer. A program can count the time needed to follow the optimal path to the solution, but that's not how the average user behaves. One wrong word in a menu, and the user is lost for five minutes - or forever.


 Simple things like counting clicks to solutions are misleading. For example, I recently advised on an ecommerce site where people had to find certain products. The original design provided product pages in 3 clicks from the home page, and the revised design required one more click. Yet, shopping success was 7 times higher in the revised design because each of the new steps was completely intuitive. Even with one more click, the revised design was faster because users didn't have to spend as much time thinking about where to click. More importantly, it made people find the right product much more frequently, whereas the original design was very error prone. Despite this usability finding, automated assessment would have given a higher rating to the original design. Whether or not the choices make sense is the one thing a program can't check.


 Another measure typically computed by automated ""usability"" services is ""freshness"" as defined by the percentage of pages that are new. But you simply cannot tell whether a website is up-to-date by looking at the time stamps on the files. A site can be extremely fresh even if 90% of its content is more than a year old. That just means that it keeps good archives to supplement the current content. By now, there are probably less than 1% of the pages on nytimes.com that are ""current"" even though it is a daily newspaper and one of the freshest sites in the world. Conversely, a site can be stale even if most of the pages have been edited recently (if the changes are not the appropriate ones to bring the content up to date in ways that matter to users).


 How do you distinguish between two types of old files:



  good content that should be archived because it is still of value
 

  outdated content that should be removed or updated
 


 The answer is that you
 
  can't tell
 
 without understanding the content and the way it will be used. Even full natural language understanding would not be sufficient to allow a computer to make this judgment.


 Automated usability is downright dangerous because it will cause site managers to



  make the wrong choices since it often gives the wrong advice or causes them to pursue pseudo-important directions
 

  think that they are covered and don't need to spend resources on real usability activities
 


 What
 
  Can
 
 Be Automated?


 A few aspects of usability can be assessed automatically by a computer program:




   Response times
  
  : it is not necessary to see or understand a page in order to measure how long time it takes to download it. So a computer can provide a perfect estimate of response times. At the same time, most sites are so incredibly slow these days that it is not really necessary to track their download times to the millisecond. Instead of spending big bucks on a
  
   response time measurement service
  
  , simply ask the CEO to download the home page from his or her hotel room while logged in on a laptop on the next business trip. Anybody trying this simple exercise will know that the site is too slow and approximately how much the design needs to slim down.
 


   HTML validation
  
  : a computer can easily flag illegal HTML code and identify all deviations from the official Web Consortium standard. Unfortunately, many sites resort to illegal code in an attempt to code around bugs in the browsers, so it is still necessary to have a human decide whether a given instance of illegal HTML was included deliberately or whether it was a mistake.
 



    Linkrot
   

  can be measured to a first approximation: can the computer follow the link and get a page returned from the remote site? Unfortunately, the computer cannot measure whether the page that is returned is the one the author intended to link to. Some sites give articles a different URL when they move into archives and reuse the old URL for new articles.
  
   Big mistake
  
  since this makes it harder for other sites to link (and incoming links are the most powerful Web marketing method). If a site does change URLs around like this, then the linkrot program may report that the link does work, even if it now links to something completely irrelevant. Until we get natural language understanding (in 50 years?), there is no way that a computer can find out whether a destination page complies with the linking author's intentions.
 


   Accessibility
  
  for users with disabilities can only be partly measured. Sure, it's possible to have a computer check for things like use of ALT text for all images, but without natural language comprehension, the computer cannot determine whether the ALT text will be meaningful to a blind user and help him or her understand the site. Also, sometimes a page gets to be more usable by avoiding ALT text on certain images. Thus, automated measures of accessibility should only be used as a checklist and not as a final judgment.
 


 How to Gather Usability Data


 There is only one valid way to gather usability data: observe real users as they use your site to accomplish real tasks. This is actually the simplest of all the methods: just see what happens!



  Maybe it's because the method is so simple that it is not used more often. Anyway, it really is easy to get real usability insights. It's also
 

  very cheap
 

  since you only need to
 

  test a small number of users
 

  to find the main usability problems."
131,1999-01-09,"The collective brainpower of the Internet is an awesome beast that used to manifest itself on Usenet newsgroups. Most of these groups have degenerated into spam, flames, and newbie ignorance. The Web has not yet evolved good ways of utilizing this power, since most so-called ""community"" sites are equally degenerate.
I have always been impressed by the Web knowledge exhibited by the Alertbox readers, so as an experiment in collective brainpower, I decided to collect solutions to a specific Web usability problem from my readers. The  full problem statement is available as a sidebar, but briefly the question was  how to get feedback from users to determine usability and future design directions for a large Web archive of historical documents.
I received a large number of great suggestions and analyses, so even though a website has a larger ""news hole"" than a print publication, I have still had to be very selective. Here are the best solutions.
The Survey Form
Dave Koelle  from Raytheon Systems Company writes:
The primary goal of users who will access the Pepper Library will be to access historical documents; additional  indirect work, such as responding to a survey, may be glossed over or ignored  . This is one of the reasons why collaborative filtering technologies, such as Firefly, were not as successful as anticipated.

Therefore, I think the survey should be as simple as possible, while giving the user the option to add more information. Perhaps the most important question (maybe, ""How usable did you find this site?"") should be a multiple choice question, or a scale of 1 through 10. Asking for an actual typed response requires too much work on the user's behalf. There should, however, be an optional textbox for the user to volunteer feedback (and this may be your most useful feedback). I think this arrangement will appeal to users, thereby providing you with the most responses.

 
Jenna Burrell  from Cornell University writes:
I run a web site which provides links to educational resources for high school students. I've had a user survey which has gone through several rewrites since I set it up in 1995.
What I've found most useful based on the survey responses I've received is to  ask what topic the user was looking for  and whether or not they were able to find the desired information. Often users will suggest a topic I hadn't thought of including. Sometimes they will list a topic that I do have information on and then note that they were not able to find it. This helps me to rethink the organization of that specific subject, the keywords I used in the description of the relevant links, and where I placed the relevant links the user wasn't able to find.
I also think it's important to include a textbox that allows users to fill in whatever questions or comments they have. It allows users to describe any problems they encountered and oftentimes it has helped me to think of new questions I want to ask in the survey.
And, most importantly,  keep the survey short, or no one will want to answer it.

 
Jonathan Spencer  from  CyberArtisans writes:
Your survey must balance the desire to get the most information from each user with the tendency of most users not to fill out a form they don't have to fill out. To that end, I would suggest the following:

If you want a lot of information, offer your users something in return for filling out the survey form. Maybe a copy of the usability study, a printed copy of one of the documents or photographs, or something you feel comfortable distributing that you also think would be attractive to a user.
You might want to make the survey in several stages, each stage offering some ""goodie"" in return, or maybe require that the user fill out all 3 or 4 stages to get the goodie.
You might also create several sets of questions and provide them randomly, so that if someone happens to be sitting next to someone filling out the form, when they take the survey they will encounter different questions, thus forcing them to think rather than filling in what they remember the first person doing.
In any case, the survey should not be long, so the user can fill it out quickly and move on. You will get many more responses.
Arrange the site so the user fills out the survey immediately after using the site -- perhaps the user will encounter the survey after selecting the desired titles or abstracts and submitting the request.
If you don't want to offer goodies, then you should make the survey very short, just a very few questions that the user can deal with quickly. Again, it can be done in stages, especially if your users tend to come back repeatedly.


Very Small Surveys
John Arnold  from Lex Vehicle Leasing writes:
When it comes to gathering feedback I find that the times when I bother to respond to a question are the times when it least interrupts my information gathering.
I dislike the method Microsoft use on their support website - they have a question at the bottom of the page asking ""did this page give you the information you were looking for?"". The question is large and seems to be given as much importance as the page content. As an information seeker I'm far more interested in the content than in filling in a survey. Worse still, giving a response to their question takes you to another page - one without the original page content. Thus I lose the information I came for. Unacceptable.
On the other hand, the idea of  just asking a single question  is good. No intrusive, privacy bashing questions about name, address, sex, income etc. Just find out what you really want to know ""Did this site perform?"".
ZDNet and MSNBC among others often include a small survey question in a side bar on their pages. These really work for two reasons. Firstly, filling them in doesn't lose you the article information - the destination page includes the article again. Secondly, there is a reward for answering the question - you get to see the latest results. The second point is important. If there is a reward, however small, for answering the question then users will respond.
Most important of all, though, is that the user came for information. Take that off their screen at your peril!
Just as a quick technical solution - how about asking the question in a pop-up window? Give the results/reward in the same window. I am NOT proposing that each page load should automatically pop-up a small question window - that's REALLY annoying. There should be a button to open the question window and the results/reward page should have a clear CLOSE button to get rid of it again.

Don't Be Boring; Get Feedback on Live Alternatives
Michael ""Mac"" McCarthy, President of Web Publishing Inc. writes:
The biggest problem with reader surveys is that  readers don't know what they want, can't articulate what they want, and articulate things different from what they actually do want based on how they later act when you give them what they thought they wanted.

Give people  specific choices  to look at, especially live choices, and see how they act, is a good way. We made a dramatic change in how the home page of  JavaWorld   was organized, and tested it by offering the new version to visitors to the old version: ""Test our new home page and tell us what you think!"" They clicked through to the new designed, then filled out a form saying whether they liked it or not;  94% said they preferred it strongly  -- a forceful enough response to overwhelm our reservations.
Long surveys aren't the problem, boring surveys are. For various reasons we tend to ask generalized questions, in boring academic phrasings, with dry ""objective"" choices. The solution is not to hide the length of the survey with multi-page surveys (""Next...""), which just makes you feel like you're on a treadmill.
One solution is to at least sometimes  ask an interesting question  and give interesting choices. Be human, too.

""I thought this story was.... a) great! b) pretty good; c) whatever; d) yawn; e) a waste of electrons; f) n.a. - not my kind of story, so who am I to judge? g) other (____).""
Or give the background so I know why it matters: ""We're stuck on a redesign -- give us a hand, will you?...
""We're considering a redesign that would create three columns on the screen: The middle column would have the article, the left would contain navigation and house ads, the right column paid advertising. This would obviously squeeze the text into a narrower column than it's in now. Some of us think this great -- shorter scan lines supposedly make this stuff easier to read whether on the screen or printed out. Others think it makes our (already long) stories seem REALLY too long, especially when you print it out. Help us figure out which design works better by checking these two samples and then telling us how you like it.""

We've gotten  better response to folksy quizzes than formalistic ones  -- and more essays in reply to open-ended questions, and the answers seem more useful. The worst thing about most surveys is once you've digested the responses and your most common reaction is ""That's interesting - I wonder what they meant by  that  answer??""

Beyond Simple Surveys
Susan Druding, webmaster of the  Free Speech Movement Archives writes:
I'm not sure that [server log statistics are] the right approach to use. Until people access the information online you won't really know what their usage will be. Having people accessing the real material is the only way to see what they really focus on. I would suggest that you gear what you put on line and in what order on the basis of  what people most frequently request in person in your real archives at Florida State  . I think you would do better to study the access levels for the information that most seems to interest people and put that up first in descending order of requests at your Claude Pepper site.

I would strongly recommend that you set up some sort of ""C. Pepper Archives Newsletter"" on your site as soon as you open the site. Assure people that by signing up they will ONLY hear news of your archives and site and that you will not share the list with others. People DO sign up for these newsletters! You will be building a base of interested people to whom you can send site news and to whom you can send SHORT surveys or direct them to a SHORT online survey when you have one ready. We have a newsletter set up on the FSM-A site and receive a few a week. I have another Web site I do (on the art of  Quilting) and I've had an immensely good response setting up a newsletter. I have thousands who have signed up. I keep it short, no advertising and just focus on a few topics in each.

 
Daniel Schwabe  from PUC-Rio in Brazil writes:
First, select users that could be considered representative of you target audience, if at all possible.
Then, ask the users to describe, as briefly as possible, the  tasks they are trying to perform  while accessing the collection. Together with the tasks, they should describe the navigation steps and interface operations (when not trivial) they took to achieve their goals. Then ask them to evaluate this sequence (adequate, inadequate), including both aspects. For those who feel the path is not adequate in some sense, or the interface inappropriate, they should give a brief explanation why. If they want, they may suggest an alternative they feel would be better suited for their task.
The results of this survey would give you a feel for how well the intended set of tasks (as reported by the users) is supported by the site. There are several things that can be done with this result; we have developed a method whereby one can take these scenarios descriptions and actually synthesize a navigation design for the site. But I believe this would be already outside the scope of the original request.

 
Adam C. Engst  from  TidBITS writes:
An on-the-spot survey regarding the  usability of each section of the site  might be useful. In essence, provide a means for users to rank each section of the site  within that section  rather than requiring them to complete a survey that's external to the content of the site. That takes advantage of the information being fresh in the users' minds, plus lowers the barriers to starting yet another task (the external survey). Obviously, the questions would have to be few in number and short, but I could also imagine an option for users to continue on with a more complete survey if they so desired.
 
Ivan Handler  from Automated Concepts, Inc. writes:
I specialize in deploying ""Knowledge Management"" systems to industry. So far, these are primarily variations on document management which is really the service you are offering. In my experience with users both on and off the web, the most difficult issue to bridge is getting usable feedback from users. The primary reason for this is that when a user is accessing the document archive, it is because they have work to do. They get some kind of reward for their work (even if it is not getting screamed at , or not getting screamed at as much), what do they get by taking the time to fill out a ""usability"" questionnaire?
At least in a business, you will get negative feedback and plenty of it if the system is unusable, since then people will not be able to do their work. A public access web site will more often get ignored rather than getting actual feedback. While positive feedback is nice, the negative feedback is important, those are the users that will probably not come back unless something is done.
My suggestion is based on the following ideas:

make it very easy to provide feedback,
provide some kind of feedback to the user (beside the automated thank you notes currently in vogue)
demonstrate to the users that their input matters by showing how it actually provokes change
quantify this system by measuring the number of different users who provide feedback and the number of actual changes that are made to your site as a result.

A way to do this is to provide a simple suggestion box form that has a link on each page. Let the users tell you in simple or not so simple terms how you could make it easier for them to improve the site. Allow them to identify themselves with their email address. Then have an improvements page where you can document what has been changed recently, what is coming down the pike, what is being considered and what was rejected and why. I think that putting the suggestions related to a change is also a good idea (after you make sure that the user is open to the idea, or at least will allow the suggestion to be published anonymously). Also, when you actually consider a suggestion and decide to act on it or not, then send a note to the user.
This kind of mechanism will take some time to have a noticeable effect. On the other hand, people who really care about this site will be drawn in to the site through this mechanism. They will spread the word and you should not only get more hits, you should get more involvement.
Tracking the number of suggestions per time period over time should give you a nice increasing curve (unless the site is so spectacular that nobody has any suggestions, unlikely in my mind). While a nice academic study may allow the designer to publish a paper, I do not see it having much useful affect on your site.

A ""Friends of the Site"" User Group
Bob Fabian  from  Robert Fabian Associates writes:
An alternative [to server stats] would be to use selective voting by users of the collection. An on-line ""Friends of the Claude Pepper Library"" could be established. Members should be chosen to represent the communities the Library is dedicated to serving. These ""Friends"" would have special access to the Library, and be encouraged to vote for those portions of the collection that should be first digitized.
The ""Friends"" would answer the question: How can the limited budget for digitizing be used to deliver the maximum value for the communities served by the Library?
That's the question that the Library should be asking. And the on-line ""Friends of the Claude Pepper Library"" would provide compelling answers to that question. They would also be the natural reference group from which to get ""usability"" feedback. They would be an obvious source for suggested improvements. And they could be used to review alternate site designs.

User Registration or Profiles
Linda G. Marsh  from Compaq Computer Corporation writes:
The best way to gather qualitative feedback is to ask your users, and, in order to do so, you must know who they are. Once you know who they are, you can ask them a few simple questions, such as:

Why did you access our site?
Did you find what you needed?
What did you like about the site? (This is important. You don't want to fix something that's working.)
What did you dislike about the site?
How would you improve the site?

To satisfy my client's request to know who was using a Web-based course, we implemented a simple log in procedure. The procedure asked for a unique identifying number (e.g., employee badge, student ID or social security number) and the user's first and last name. Logging in was voluntary and users were allowed to bypass the login procedure if they wanted to. The log in procedure also explained why we were gathering data.
On subsequent log in attempts, users only entered the identifying number. We didn't do any user validation, but interestingly, most users entered their correct names and identifying numbers. Relatively few chose to bypass the log in procedure and few entered bogus ID's. Once we had the list of users, we contacted them individually to solicit input about the course.
You won't need to contact many users to learn what works about your site and what doesn't. After you've gathered enough data, you can discontinue the log in process.
The above paradigm is simple, can be implemented quickly, and will give you the data you need in a short time.
If you include a feedback option in your Web site, you probably won't get much information. Feedback pages are the equivalent of background noise on the Web.

Sidebars
In addition to these suggestions for the problem of collecting user feedback, I also received many insightful comments on related issues. The best are available as sidebars to avoid making the main article even longer:

advice on the underlying design problem of  putting large archives on the Web
report from a similar case study: collecting  feedback from users of a commercial hotel site"
132,1998-02-21,"Mark Bernstein recently sent me the following email:

	Driving to work, I realized that I'm not sure I know the answer to the following word problem. I suspect that most webmasters don't know it either, and that it would be useful to know. Possible Alertbox column?
	
		A word problem:

A. Beauregard Clump, boy webmaster, is reviewing his logs over the past weeks. The total number of hits that one of his pages received is as follows: 




					week

					hits



					1

					120



					2

					132



					3

					116



					4

					120



					5

					148





			Is the increased traffic in week 5 statistically significant?

			Is the growth in weekly traffic throughout this period statistically significant?


		 


	Let us start by looking at the second question.

	Exponential Growth

	When analyzing numbers related to the growth of a website, I normally recommend looking at them on a logarithmic scale. The reason is that  the Web and the Internet both experience exponential growth  . Therefore, Web statistics are better analyzed in terms of growth  rates  than in terms of linear growth.

	 

	Admittedly, the  Web's growth has slowed down  since the phenomenal pace we experienced in 1993 and 1994, but it still doubles in size every year. New sites go live, new users come online, and traffic keeps climbing.

	The diagram shows the reported traffic stats plotted with a logarithmic scale on the  y  -axis. With a logarithmic scale, an exponential growth curve shows as a straight line, and the best-fit growth curve has been added to the figure in red.



	A regression analysis gives R  2  =0.26 which means that 26% of the variance in the data seems to be due to an underlying growth in site traffic (the remaining 74% of the variance is random fluctuations). Unfortunately, the statistical significance of the regression is  p  =0.37 which means that we would have observed the same data 37% of the time even if there was  no  underlying growth at all but only random fluctuations in site traffic.

	Is a significance level of 37% low enough to conclude that the site is growing? Most scientists would say no, since they usually require a significance level of  p  <0.05 or even  p  <0.01 to conclude that a study supports a hypothesis. Since we are not looking for scientific truth, we can relax our requirements somewhat and conclude that the odds are in favor of slight growth.

	The main reason to study Web growth is to  plan server capacity and future business models  . To do so, we need to know not just the most likely growth of the site, but also the possible range of growth. The most likely growth of the site comes from the best-fit regression curve in the chart: an annualized growth rate of 442%, meaning that traffic in a year will be about 662 pageviews per week.

	When running a regression analysis, you can ask for a  confidence interval  , which is the range of possible values for the growth rate. I like working with 90% confidence intervals, meaning that the true value will fall outside the estimated range 10% of the time. For the sample site, the 90% confidence interval for the annualized growth rate is from -88% to 24,606%. This huge spread is a good indication that the data is too weak for any real conclusions. In other words, Beauregard Clump could easily be on a decline to 14 pages per week next year or his page could explode to 30,000 weekly pageviews. We will need more data to narrow the range and plan the site's future.

	Example With More Data: www.useit.com

	With data from more weeks it becomes possible to predict growth much more precisely. The following chart shows usage data from www.useit.com. Again, the data has been plotted on a logarithmic scale and fitted with a red regression line.

	 



	For this dataset, the exponential growth curve has a much better regression fit, with R  2  =0.96 and  p  <0.001. The best fit corresponds to an annualized growth rate of 505%, with a 90% confidence interval ranging from 433% to 588%. In other words, weekly traffic in February 1999 is most likely to be 253,000 pageviews but could be anywhere from 223,000 to 298,000.

	In calculating the statistics for useit.com I eliminated the traffic data from the last two weeks in December and the first week in January (shown as lighter dots in the diagram).  Traffic slows dramatically on many sites during the holiday season  , and it is therefore best to discard the data from this period when calculating any long-term trends. The exception from this rule would obviously be any sites that specifically aimed at selling Christmas presents or provide other holiday services.

	In the early years of the Web, it was also necessary to  analyze traffic data differently during the summer months  because the Web was dominated by university users who spent much of the summer away from their Internet accounts. Summer traffic is less exceptional now, both because of increased business use and because many students use other accounts when they are away from school. It may still be necessary to treat summer traffic differently when analyzing sites that are targeted at users in countries like Germany where people take long summer vacations and may not bring a laptop to the beach.

	A final point is that it is best to  analyze traffic data in weekly chunks  . Looking at daily traffic introduces too much noise from the varying transmission problems on the Internet. Also, many sites have very different traffic patterns on weekdays and on weekends. For a business-oriented site, it is very common for weekend traffic to drop to less than half of the normal rate. Fluctuations from Friday to Saturday and from Sunday to Monday obviously do not indicate any real change in site usage, so the statistics become much easier to analyze when aggregated over a full week.

	Individual Weekly Stats

	Now returning to question 1 in Mark Bernstein's word problem: Is the increase in traffic in Week 5 significant? The traditional way to answer this question would be to build a statistical model for the data from Weeks 1 through 4 and then calculate the probability that the observation from Week 5 would fit within the model's predictions. If the probability is low enough (typically, less than 5%), then you conclude that it is too unlikely to have happened if the site had behaved the same in Week 5 as in Weeks 1-4. In other words, something new must have happened to cause the increased traffic. If, on the other hand, the probability for fitting within the statistical model is high, then you conclude that nothing significant had changed and that the increase was just a random fluctuation.

	 

	Performing this statistical analysis requires the data to follow certain assumptions. For most statistical analyses, the data must follow a normal distribution (or at least be close to one). In this case, we have much too little data to conclude anything about the distribution of the observations, so it would be very difficult to conclude anything about a single week's traffic.

	I am being cautious because Web traffic has not yet been sufficiently studied for us to know its statistical properties. The two main things that are known are that long-term traffic tends to grow exponentially (the  average site grew 130% in 1997  ) and that  short-term traffic is extremely volatile  . It is very common for a website to have its  traffic double or be cut in half from one week to the next  . This high variability means that it is extremely difficult to judge traffic patterns based on short-term data. Only by looking at observations over several months does it become possible to distinguish between fluctuations and trends.

	Once the ""normal"" volatility for a given site is known it will be possible to calculate the probability that any given week's traffic is uncommonly large or small. There is insufficient data to provide an answer for the sample site, but my intuition says that the increase in Week 5 falls well within the random fluctuations expected on the Web.

	Because of the high variability in site traffic, it is usually best to  have enough spare server capacity to handle a doubled load without warning  . Furthermore, the long-term traffic trends should be tracked regularly, and upgrades planned  before  it is too late and irate users start leaving you for competing sites.

	 


	 

	Ironically, a few days after I wrote this essay, my own site experienced a capacity overload. www.useit.com has about five times the capacity needed to serve normal traffic, but this turned out to be insufficient to handle the load the day  Jesse Berst wrote about my work in AnchorDesk  . Traffic exploded and it was almost impossible to get through for about two hours - until the server was upgraded. At least I had a contingency plan, even though it took a little time to activate."
133,1997-01-01,"Focus groups are a somewhat informal technique that can help you assess user needs and feelings both before interface design and long after implementation. In a focus group, you bring together 6–9 users to discuss issues and concerns about the features of a user interface. The group typically lasts about 2 hours and is run by a moderator who maintains the group's focus.
Focus groups often bring out users' spontaneous reactions and ideas and let you observe some group dynamics and organizational issues. You can also ask people to discuss how they perform  activities that span many days or weeks: something that is expensive to observe directly. However, they can only assess what customers  say  they do and not the way customers actually operate the product. Since there are often major differences between what people say and what they do, direct observation of one user at a time always needs to be done to supplement focus groups.
Narrow View
Although focus groups can be a powerful tool in system development, you shouldn't use them as your only source of usability data. People with an advertising or marketing background often rely solely on focus groups to expose products to users. Thus, because advertising and marketing people frequently contribute to website development, focus groups are often used to evaluate web projects. Unfortunately, focus groups are a rather poor method for evaluating interface usability. It is thus dangerous to rely on them as your only method in a web design project. Traditional market research targets products for which usability is a minor concern. When judging, for example, which proposals a politician should support, how sweet a chocolate bar should be, or whether to show a new Mercedes braking in snow or in rain, you need expose a group of consumers only to different versions of the proposal, candy, or commercial, ask them which they prefer, and listen to their reasons as to why they prefer one or the other.
Software products, websites, and other interactive systems also need to be liked by customers, but no amount of subjective preference will make a product viable if users can't use it. To assess whether users can operate an interactive system, the only proper methodology is to watch one user at a time use the system. Because focus groups are groups, individuals rarely get the chance to explore the system on their own; instead, the moderator usually provides a product demo as the basis for discussion. Watching a demo is fundamentally different from actually using the product: There is never a question as to what to do next, and you don't have to ponder the meaning of numerous screen options.
Consider, for example, the problem of windowing versus scrolling as methods for changing the information visible on the screen. The windowing principle says that to see the information in the beginning of a file, the user moves the window to the top of the file. Scrolling, on the contrary, says that to see the beginning of the file, you scroll down the screen until the desired content becomes visible. In other words, the command to get to the top of the file should be called UP (or shown as an upward-pointing arrow) if windowing is preferred, whereas the same command should be called DOWN if scrolling is preferred.
When they actually carry out the task, most users perform better in the windowing model (which is therefore used in most current GUI standards). But if you give a demo of moving text files to people new to computers, many of them will say that the scrolling model characterizes what they are seeing (since they see the text move down to get to the beginning). If GUIs had been designed by focus groups, we would have ended up with a suboptimal command.
Benefits
In interactive systems development, the proper role of focus groups is not to assess interaction styles or design usability, but to discover what users want from the system. For example, in developing Sun's new online documentation system, we ran a focus group with system administrators to discover:

their thoughts and preferences on issues such as distributing and replicating huge documentation files across multiple servers
whether or not they needed faster access to local copies of the documentation on specific client machines

These questions would never emerge in a  usability test (although we did run usability studies to see if administrators could operate the system). We could have investigated the needs of system administrators in other ways — including field trips to customer locations — but it was more efficient to have a focus group discuss the problems in a single session.
Getting Focused
For participants, the focus-group session should feel free-flowing and relatively unstructured, but in reality, the moderator must follow a preplanned script of specific issues and set goals for the type of information to be gathered. During the group session, the moderator has the difficult job of keeping the discussion on track without inhibiting the flow of ideas and comments. The moderator also must ensure that all group members contribute to the discussion and must avoid letting one participant's opinions dominate. After the session, data analysis can be as simple as having the moderator write a short report summing up the prevailing mood in the group, illustrated with a few colorful quotes. You can also do more detailed analyses, but the unstructured nature of the groups make this difficult and time-consuming.
Focus groups require several representative users. Because you need a flowing discussion and various perspectives, the initial focus group should have at least 6 users. Typically, you should run more than one focus group, because the outcome of any single session may not be representative and discussions can get sidetracked.
Other Issues
As with any method based on   asking  users what they want — instead of measuring or observing how they  actually  use things — focus groups can produce inaccurate data because users may think they want one thing when they need another. You can minimize this problem by exposing users to the most concrete examples of the technology being discussed as possible.
For example, Irene Greif ran focus groups to assess a version management facility for Lotus 1-2-3. The new features were presented to the focus group as a way to let multiple users compare alternative views of a spreadsheet across computer networks. Initially, group members were skeptical about these ideas and expressed distrust in networks and nervousness about what other people would do to their spreadsheets. After seeing a prototype and scenarios of version management in use, participants moved from skepticism to enthusiasm.
A cheap way to approximate a focus group is to rely on email, websites, or online communities. For example, Yia Yang started a project on undo facilities by posting on the British academic network, asking users which undo facilities they used and how they liked them. Posting questions to a group with an interest in the issues can generate considerable discussion. A disadvantage is that online discussions are difficult (or impossible) to keep confidential unless they take place on an intranet, behind a firewall.
Another disadvantage to this approach is bias. Internet users tend to be people with above-average interest in computers, and participants in online discussion groups tend to have above-average involvement in the group's topic.
Although online discussions are unlikely to reflect the average user's concerns, they can be a good way of getting in touch with power users. These users have needs that will sometimes surface later for the average user. Thus, addressing the power users' needs may be a way of getting a head start on future usability work.
References
Focus Groups: A Practical Guide for Applied Research  (5th edition), by Richard A. Krueger and Mary Anne Casey.

USA, Canada, and most countries outside Europe:
Buy from Amazon.com 
Europe:
Buy from Amazon.co.uk 

See also:   Cost of running a focus group"
134,1997-01-01,"The introduction of the spreadsheet turned millions of people into programmers without the benefit of a computer science degree. Because of the resulting lack of knowledge about even the simplest debugging techniques, spreadsheet formulae and macros are riddled with bugs and million-dollar business decisions are sometimes based on calculation errors. It has been estimated that at least 40 percent of spreadsheets have bugs.
The introduction of the Web is causing a similar phenomenon in user interface design. My current estimate is that there will be about 10 billion Web pages on the Internet by the Year 2001. Intranets and extranets will probably hold at least 10 times that many pages. We already have two million pages on SunWeb (the intranet at Sun Microsystems).
Each Web page is a user interface design problem equivalent to that of a dialogue box: you must design a task flow that brings the most important items to users' attention and design alternative options for them to click on -- all the while keeping the meaning of these options clear for novice users. Considering that the world will design more than a 100 billion of these dialog-box equivalents in the next three or four years, extremely simple and inexpensive usability methods are crucial if we are to avoid a usability meltdown on the Web.
Amateur Designers
Most Web pages are designed by amateurs. Even at companies that believe in usable Web design, only the top few pages ever receive the attention of user interface professionals or the benefit of traditional user testing in the  laboratory  . The vast majority of pages are designed by marketing staff or others with little experience in interaction design or usability methods. Clearly, it is not an option to require that every Web page be designed by professionals or go through full-fledged traditional usability studies. Doing so would be equivalent to demanding that only people who can recite the collected works of Dijkstra in their sleep be allowed to build a spreadsheet. In the real world, such demands go nowhere: people want to develop their own spreadsheet analyses  now  , not wait months for the MIS pros to get around to helping them. Similarly, people want to build their own Web pages in their own departments without having to clear the results with the ""design police"" at corporate headquarters.
Inadequate use of usability engineering methods in software development projects have been estimated to cost the US economy about $30 billion per year in lost productivity (see Tom Landauer's excellent book  The Trouble with Computers  ). By my estimates,  bad intranet Web design will cost $50-100 billion per year in lost employee productivity  in 2001 ($50B is the conservative estimate; $100B is the median estimate; you don't want to hear the worst-case estimate!). Bad design on the open Internet will cost a few billion more, though much of this loss may not show up in gross national products, since it will happen during users' time away from the office.
A usability loss of $100 billion may sound like a lot, but considering that in 2001 there will probably be about 200 million people designing intranet pages, each designer's work will contribute only $500 of that usability loss -- not nearly enough to justify the costs of hiring professional designers or paying for advanced usability work.  Discount usability engineering   is our only hope. We must evangelize methods  simple  enough that departments can do their own usability work,  fast  enough that people will take the time, and  cheap  enough that it's still worth doing. The methods that can accomplish this are simplified user testing with one or two users per design and  heuristic evaluation  .
Teach Usability in Schools
User testing and heuristic evaluation should be taught as part of the standard elementary school curriculum The proper role of Internet technology in schools is not the completely naive ideal that politicians have proposed: kids sending e-mail to the world's best scientists asking for help with their homework. No one would stay a leading expert long if they spent their time answering a flood of messages: personally, I can't even keep up with unsolicited mail from PhD-level students. Much more productive is to have kids use the Internet to build their own hypertext information spaces as part of their course work. It is much more inspiring to write for others than for the teacher's red pencil. They should usability-test their pages with one or two other students and conduct critical usability inspections of different designs for each problem.
The only way we can hope to teach usability engineering from the third grade up is to teach discount usability methods. Advanced and sophisticated methods that require, say, an understanding of statistics are clearly impractical.
Some critics will no doubt say that it is unacceptable to teach kids less than perfect methodology or to test interface designs with only two users or to do any of the other things I recommend here. I agree that these methods are imperfect. But the only realistic alternative is to do nothing. Given the amount of usability work we'll need in the coming years, it is quite simply not possible to do it all with deluxe methodology.
Just Do It
The true choice is not between discount and deluxe usability engineering. If that were the choice, I would agree that the deluxe approach would bring better results. The true choice, however, is between doing something and doing nothing. Perfection is not an option. My choice is to do something!"
135,1995-06-27,"Originally presented as a keynote at the IFIP INTERACT'95 International Conference on Human-Computer Interaction, Lillehammer, Norway, June 27, 1995. 
The Need for More Usable Usability
User interface professionals ought to take their own medicine some more. How often have we heard UI folks complain that ""we get no respect"" (from development managers)? At the same time, we have nothing but scorn for any programmer who has the attitude that if users have problems with his or her program then it must be the users' fault.
If we consider usability engineering as a system, a design, or a set of interfaces with which development managers have to interact, then it obviously becomes the usability professionals' responsibility to design that system to maximize its communication with its users. My claim is that any problems in getting usability results used more in development are more due to lack of usability of the usability methods and results than they are caused by evil development managers who deliberately want to torment their users.
In order to get usability methods used more in real development projects, we must make the usability methods easier to use and more attractive. One way of doing so is to consider the way current usability methods are being used and what causes some methods to be used and others to remain ""a good idea which we might try on the next project."" As an example of such studies I will report on a study of what causes usability inspection methods to be used.
Usability Inspection Methods
Usability inspection (Nielsen and Mack, 1994) is the generic name for a set of methods based on having evaluators inspect or examine usability-related aspects of a user interface. Some evaluators can be usability specialists, but they can also be software development consultants with special expertise (e.g., knowledge of a particular interface style for graphical user interfaces), end users with content or task knowledge, or other types of professionals. The different inspection methods have slightly different goals, but normally usability inspection is intended as a way of evaluating user interface designs to find usability problems. In usability inspection, the evaluation of the user interface is based on the considered judgment of the inspector(s). The individual inspection methods vary as to how this judgment is derived and on what evaluative criteria inspectors are expected to base their judgments. In general, the defining characteristic of usability inspection is the reliance on judgment as a source of evaluative feedback on specific elements of a user interface. See the appendix for a short summary of the individual usability inspection methods discussed in this paper.
Usability inspection methods were first described in formal presentations in 1990 at the CHI'90 conference where papers were published on heuristic evaluation (Nielsen and Molich, 1990) and cognitive walkthroughs (Lewis et al., 1990). Now, only four to five years later, usability inspection methods have become some of the most widely used methods in the industry. As an example, in his closing plenary address at the Usability Professionals' Association's annual meeting in 1994 (UPA'94), Ken Dye, usability manager at Microsoft, listed the four major recent changes in Microsoft's approach to usability as:

Use of  heuristic evaluation 
Use of  ""discount"" user testing with small sample sizes
Contextual inquiry
Use of paper mock-ups as low-fidelity prototypes

Many other companies and usability consultants are also known to have embraced heuristic evaluation and other inspection methods in recent years. Here is an example of an email message I received from one consultant in August 1994:
""I am working [...] with an airline client. We have performed so far, 2 iterations of usability [...], the first being a heuristic evaluation. It provided us with tremendous information, and we were able to convince the client of its utility [...]. We saved them a lot of money, and are now ready to do a full lab usability test in 2 weeks. Once we're through that, we may still do more heuristic evaluation for some of the finer points.""
Work on the various usability inspection methods obviously started several years before the first formal conference presentations. Even so, current use of heuristic evaluation and other usability inspection methods is still a remarkable example of rapid technology transfer from research to practice over a period of very few years.
Technology Transfer
There are many characteristics of usability inspection methods that would seem to help them achieve rapid penetration in the ""marketplace of ideas"" in software development organizations:

Many companies have just recently realized the urgent need for increased usability activities to improve their user interfaces. Since usability inspection methods are cheap to use and do not require special equipment or lab facilities, they may be among the first methods tried.
The knowledge and experience of interface designers and usability specialists need to be broadly applied; inspections represent an efficient way to do this. Thus, inspections serve a similar function to style guides by spreading the expertise and knowledge of a few to a broader audience, meaning that they are well suited for use in the many companies that have a much smaller number of usability specialists than needed to provide full service to all projects.
Usability inspection methods present a fairly low hurdle to practitioners who want to use them. In general, it is possible to start using simple usability inspection after a few hours of training. Also, inspection methods can be used in many different stages of the system development lifecycle.
Usability inspection can be integrated easily into many established system development practices; it is not necessary to change the fundamental way projects are planned or managed in order to derive substantial benefits from usability inspection.
Usability inspection provides instant gratification to those who use it; lists of usability problems are available immediately after the inspection and thus provide concrete evidence of aspects of the interface that need to be improved.

To further study the uptake of new usability methods, I conducted a survey of the technology transfer of usability inspection methods.
Method
The data reported in the following was gathered by surveying the participants in a course on usability inspection taught in April 1993. A questionnaire was mailed to all 85 regular attendees in the tutorial taught by the author at the INTERCHI'93 conference in Amsterdam. Surveys were not sent to students under the assumption that they would often not be working on real projects and that they therefore could not provide representative replies to a technology transfer survey. Similarly, no questionnaires were sent to instructors from other INTERCHI'93 tutorials who were sitting in on the author's tutorial, since they were deemed to be less representative of the community at large.
Of the 85 mailed questionnaires, 4 were returned by the post office as undeliverable, meaning that 81 course attendees actually received the questionnaire. 42 completed questionnaires were received, representing a response rate of 52%.
The questionnaire was mailed in mid-November 1993 (6.5 months after the tutorial) with a reminder mailed in late December 1993 (8 months after the tutorial). 21 replies were received after the first mailing, and another 21 replies were received after the second mailing. The replies thus reflect the respondents' state approximately seven or eight months after the tutorial.
With a response rate of 49%, it is impossible to know for sure what the other half of the course participants would have replied if they had returned the questionnaire. However, data from the two response rounds allows us to speculate on possible differences based on the assumption that the non-respondents would be more like the second-round respondents than the first-round respondents. Table 1 compares these two groups on some relevant parameters. The first conclusion is that none of the differences between the groups are statistically different, meaning that it is likely that the respondents are fairly representative of the full population. Even so, there might be a slight tendency to having the respondents were associated with larger projects than the non-respondents and that the respondents were probably more experienced with respect to usability methods than the non-respondents. Thus, the true picture with respect to the full group of tutorial participants is might reflect slightly less usage of the usability inspection methods than reported here but probably not much less.
 

Table 1 
	Comparison of respondents from the first questionnaire round with the respondents from the second round. None of the differences between groups are statistically significant.


Question
First-round Respondents
Second-round Respondents
p 


Usability effort on project in staff-years
3.1
1.3
.2


Had used user testing before the course
89%
70%
.1


Had used heuristic evaluation after the course
65%
59%
.7


Number of different inspection methods used after course
2.2
1.8
.5



The median ratio between the usability effort of the respondents' latest project and the project's size in staff-year was 7%. Given the sample sizes, this is equivalent to the 6% of development budgets that was found to be devoted to usability in 31 projects with usability engineering efforts in a survey conducted in January 1993 (Nielsen, 1993). This result further adds to the speculation that our respondents are reasonably representative.
Questionnaire Results
Respondents were asked which of the inspection methods covered in the course they had used in the (approximately 7-8 month) period after the course. They were also asked whether they had conducted user testing after the course. The results from this question are shown in Table 2. Usage frequency in a specific period may be the best measure of the fit between the methods and project needs since it is independent of the methods' history. User testing and heuristic evaluation were clearly used much more than the other methods.
 

Table 2 
	Proportion of the respondents who had used each of the inspection methods and user testing in the 7-8 month period after the course, the number of times respondents had used the methods, and their mean rating of the usefulness of the methods on a 1-5 scale (5 best). Methods are sorted by frequency of use after the course.






Method
Respondents Using Method After INTERCHI
Times Respondents Had Used the Method (Whether Before or After the Course)
Mean Rating of Benefits from Using Method


User testing
55%
9.3
4.8


Heuristic evaluation
50%
9.1
4.5


Feature inspection
31%
3.8
4.3


Heuristic estimation
26%
8.3
4.4


Consistency inspection
26%
7.0
4.2


Standards inspection
26%
6.2
3.9


Pluralistic walkthrough
21%
3.9
4.0


Cognitive walkthrough
19%
6.1
4.1



Respondents were also asked how many times they had used the methods so far, whether before or after the course. Table 2 shows the mean number of times each method had been used by those respondents who had used it at all. This result is probably a less interesting indicator of method usefulness than is the proportion of respondents who had used the methods in the fixed time interval after the course, since it depends on the time at which the method was invented: older methods have had time to be used more than newer methods.
Finally, respondents were asked to judge the benefits of the various methods for their project(s), using the following 1-5 scale:
1 = completely useless
2 = mostly useless
3 = neutral
4 = somewhat useful
5 = very useful
The results from this question are also shown in Table 2. Respondents were only rated those methods with which they had experience, so not all methods were rated by the same number of people. The immediate conclusion from this question is that all the methods were judged useful, getting ratings of at least 3.9 on a scale where 3 was neutral.
 

Figure 1 
	Regression chart showing the relation between the rated usefulness of each method and the number of times those respondents who had tried a method had used it. Data was only given by respondents who had tried a method.






The statistics for proportion of respondents having used a method, their average usefulness rating of a method, and the average number of times they had used the method were all highly correlated. This is only to be expected, as people would presumably tend to use the most useful methods the most. Figure 1 shows the relation between usefulness and times a method was used (r = .71, p < .05) and Figure 2 shows the relation between usefulness and the proportion of respondents who had tried a method whether before or after the course (r = .85, p < .01). Two outliers were identified: Feature inspection had a usefulness rating of 4.3 which on the regression line would correspond to being used 6.7 times though in fact it had only been used 3.8 times on the average by those respondents who had used it. Also, heuristic estimation had a usefulness rating which on the regression line would correspond to having been tried by 56% even though it had in fact only been used by 38%. These two outliers can be explained by the fact that these two methods are the newest and least well documented of the inspection methods covered in the course.
 

Figure 2 
	Regression chart showing the relation between the rated usefulness of each method and the proportion of respondents who had tried the method. Usefulness ratings were only given by those respondents who had tried a method.






 
The figures are drawn to suggest that usage of methods follows from their usefulness to projects. One could in fact imagine that the respondents rated those methods the highest that they had personally used the most in order to avoid cognitive dissonance, meaning that causality worked in the opposite direction as that implicitly shown in the figures. However, the correlation between the individual respondents' ratings of the usefulness of a method and the number of times they had used the method themselves is very low (r=.05), indicating that the respondents judged the usefulness of the methods independently of how much they had used them personally. There is only a high correlation in the aggregate between the mean values for each method. Thus, we conclude that the reason for this high correlation is likely to be that usability methods are used more if they are judged to be of benefit to the project. This is not a surprising conclusion but it does imply that inventors of new usability methods will need to convince usability specialists that their methods will be of benefit to concrete development projects.
 



Table 3 
	Proportion of respondents who used the methods the way they were taught. For each method, the proportion is computed relative to those respondents who had used the method at least once.






Method
Respondents using the method
			as it was taught


Pluralistic walkthrough
27%


Heuristic estimation
25%


Heuristic evaluation
24%


Standards inspection
22%


Cognitive walkthrough
15%


Feature inspection
12%


Consistency inspection
0%



The survey showed that only 18% of respondents used the methods the way they were taught. 68% used the methods with minor modifications, and 15% used the methods with major modifications (numbers averaged across methods). In general, as shown in Table 3, the simpler methods seemed to have the largest proportion of respondents using them as they were taught. Of course, it is perfectly acceptable for people to modify the methods according to their specific project needs and the circumstances in their organization. The high degree of method modification does raise one issue with respect to research on usability methodology, in that one cannot be sure that different projects use the ""same"" methods the same way, meaning that one will have to be careful when comparing reported results.
The normal recommendation for heuristic evaluation is to use 3-5 evaluators. Only 35% of the respondents who used heuristic evaluation did so, however. 38% used two evaluators and 15% only used a single evaluator. The histogram in Figure 3 shows the distribution of number of evaluators used for heuristic evaluation.
With respect to user testing, even though 35% did use 3-6 test participants (which would normally be referred to as discount usability testing), fully 50% of the respondents used 10 participants or more. Thus, ""deluxe usability testing"" is still being used to a great extent. The histogram in Figure 4 shows the distribution of number of test participants used for a test.
 




 



Figure 3 
			Histogram of the number of evaluators normally used by the respondents for heuristic evaluations.
 
Figure 4 
			Histogram of the number of test users normally used by the respondents for user testing.



As one might have expected, the participants' motivation for taking the course had major impact on the degree to which they actually used the inspection methods taught in the course. People who expected to need the methods for their current project indeed did use the methods more than people who expected to need them for their next project, who again used more methods than people who did not anticipate any immediate need for the methods. Table 4 shows the number of different inspection methods used in the (7-8 month) period after the course for participants with different motivation. The table also shows the number of inspection methods planned for use during the next six months. Here, the participants with pure academic or intellectual interests have the most ambitions plans, but we still see that people who had the most immediate needs when they originally took the course plan to use more methods than people who had less immediate needs.
 

Table 4 
	Relation between the main reason people took the course and the number of different methods they have used.






Motivation for taking the course
Proportion of the respondents
Number of different inspection methods used since the course
Number of different inspection methods planned for use during the next six months


Specific need to know for current project
31%
3.0
2.2


Expect to need to know for next project
21%
1.4
1.7


Expect the topic to be important in future, but don't anticipate any immediate need
14%
1.2
1.3


Pure academic or intellectual interest
12%
2.0
3.4



In addition to the reasons listed in Table 4, 22% of the respondents indicated other reasons for taking the course. 5% of the respondents wanted to see how the instructor presented the materials in order to get material for use in their own classes and 5% wanted to validate their own experience with usability inspection and/or were developing new inspection methods. The remaining 12% of the respondents were distributed over a variety of other reasons for taking the course, each of which was only given by a single respondent.
Free-Form Comments
At the end of the questionnaire, respondents were asked to state their reasons for using or not using the various methods. A total of 186 comments were collected, comprising 119 reasons why methods were used and 67 reasons why methods were not used.




 
Cognitive walkthrough
Consistency inspection
Feature inspection
Heuristic evaluation
Heuristic estimation
Pluralistic walkthrough
Standards inspection
User testing
Proportion of all comments




Method generates good/bad information
9 / 1
5 / 0
5 / 0
3 / 1
4 / 2
5 / 0
6 / 0
20 / 0
33%


Resource and/or time requirements
1 / 3
1 / 3
4 / 1
8 / 1
1 / 2
0 / 11
1 / 0
0 / 2
21%


Expertise and/or skills required
1 / 8
1 / 3
0 / 4
5 / 1
0 / 3
 
1 / 4
 
17%


Specific characteristics of individual project
2 / 0
2 / 4
1 / 2
 
2 / 1
 
0 / 6
1 / 0
11%


Communication, team-building, propaganda
 
2 / 0
1 / 0
 
3 / 0
5 / 0
 
4 / 0
8%


Method mandated by management
 
1 / 0
1 / 0
1 / 0
1 / 0
 
1 / 0
2 / 0
4%


Interaction between multiple methods
 
 
 
3 / 0
1 / 0
1 / 0
0 / 1
 
3%


Other reasons
0 / 2
 
 
2 / 0
 
 
 
 
2%


Proportion of comments that were positive
48%
55%
63%
88%
60%
50%
45%
93%




 
Table 5 
Classification of the 186 free-form comments made by respondents when asked to explain why they used (or did not use) a method. In each cell, the first number indicates reasons given for using a method and the second number (after the slash) indicates reasons given for not using a method (empty cells indicate that nobody made a comment about a method in that category). Scroll the table to the right to see more data.
 
Table 5 summarizes the free-form comments according to the following categories:

Method generates good/bad information: reasons referring to the extent to which the results of using a method are generally useful.
Resource and/or time requirements: reasons related to the expense and time needed to use a method.
Expertise and/or skills required: reasons based on how easy or difficult it is to use a method. Mostly, positive comments praise methods for being easy and approachable and negative comments criticize methods for being too difficult to learn. One exception was a comment that listed it as a reason to use heuristic evaluation that it allowed usability specialists to apply their expertise.
Specific characteristics of individual project: reasons referring to why individual circumstances made a method attractive or problematic for a specific project. For example, one comment mentioned that there was no need for consistency inspection in a project because it was the first GUI in the company and thus did not have to be consistent with anything.
Communication, team-building, propaganda: reasons referring to the ways in which use of a method helps evangelize usability, generate buy-in, or simply placate various interest groups.
Method mandated by management: reasons mentioning that something was done because it was a requirement in that organization.
Interaction between multiple methods: reasons referring to the way the specific method interacts with or supplements other usability methods.

It can be seen from Table 5 that the most important attribute of a usability method is the quality of the data it generates and that user testing is seen as superior in that respect. In other words, for a new usability method to be successful, it should first of all be able to generate useful information.
The two following criteria in the table are both related to the ease of using the methods: resources and time as well as expertise and skill needed. The respondents view heuristic evaluation as superior in this regard and express reservations with respect to cognitive walkthroughs and pluralistic walkthroughs. Remember that the survey respondents came from projects that had already decided to use usability engineering and that had invested in sending staff to an international conference. The situation in many other organizations is likely to make the cost and expertise issues even more important elsewhere.
Conclusions
In planning for technology transfer of new usability methods, we have seen that the first requirement is to make sure that the method provides information that is useful in making user interfaces better. Equally important, however, is to make the method cheap and fast to use and to make it easy to learn. Actually, method proponents should make sure to cultivate the impression that their method is easy to learn since decisions as to what methods to use are frequently made based on the method's reputation, and not by assessing actual experience from pilot usage. It is likely that cognitive walkthrough suffers from an image problem due to the early, complicated, version of the method (Lewis et al., 1990), even though recent work has made it easier to use (Wharton et al., 1994). The need for methods to be cheap is likely to be even stronger in the average development projects than in those represented in this survey, given that they were found to have above-average usability budgets.
Furthermore, methods should be flexible and able to adapt to changing circumstances and the specific needs of individual projects. The free-form comments analyzed in Table 5 show project needs as accounting for 11% of the reasons listed for use or non-use of a method, but a stronger indication of the need for adaptability is the statistic that only 18% of respondents used the methods the way they were taught, whereas 68% required minor modifications and 15% required major modifications.
A good example of flexibility is the way heuristic evaluation can be used with varying numbers of evaluators. The way the method is usually taught (Nielsen, 1994a) requires the use of 3-5 evaluators who should preferably be usability specialists. Yet, as shown in Figure 3, many projects were able to use heuristic evaluation with a smaller number of evaluators. Of course, the results will not be quite as good, but the method exhibits ""graceful degradation"" in the sense that small deviations from the recommended practice only results in slightly reduced benefits.
The survey very clearly showed that the way to get people to use usability methods is to get to them at the time when they have specific needs for the methods on their current project (Table 4). This finding again makes it easier to transfer methods that have wide applicability across a variety of stages of the usability lifecycle. Heuristic evaluation is a good example of such a method since it can be applied to early paper mock-ups or written specifications as well as later prototypes, ready-to-ship software, and even the clean-up of legacy mainframe screens that need to be used for a few more years without available funding for major redesign.
A final issue in technology transfer is the need for aggressive advocacy. Figure 1 shows that heuristic evaluation is used somewhat more than its rated utility would justify and that feature inspection is used much less that it should be. The most likely reason for this difference is that heuristic evaluation has been the topic of many talks, panels, seminars, books, and even satellite TV shows (Shneiderman, 1993) over the last few years, whereas feature inspection has had no vocal champions in the user interface community.
Acknowledgments
I thank Michael Muller for help in developing the survey and the many anonymous respondents for taking the time to reply. I thank Robin Jeffries and Michael Muller for helpful comments on an earlier version of this manuscript.
References

Bell, B. (1992). Using programming walkthroughs to design a visual language. Technical Report CU-CS-581-92 (Ph.D. Thesis), University of Colorado, Boulder, CO.
Bias, R. G. (1994). The pluralistic usability walkthrough: Coordinated empathies. In Nielsen, J., and Mack, R. L. (Eds.), Usability Inspection Methods, John Wiley & Sons, New York, 65-78.
Kahn, M. J., and Prail, A. (1994). Formal usability inspections. In Nielsen, J., and Mack, R.L. (Eds.), Usability Inspection Methods, John Wiley & Sons, New York, 141-172.
Lewis, C., Polson, P., Wharton, C., and Rieman, J. (1990). Testing a walkthrough methodology for theory-based design of walk-up-and-use interfaces. Proceedings ACM CHI'90 Conference (Seattle, WA, April 1-5), 235-242.
Nielsen, J. (1993).  Usability Engineering (revised paperback edition 1994). Academic Press, Boston.
Nielsen, J. (1994a). Heuristic evaluation. In Nielsen, J., and Mack, R. L. (Eds.), Usability Inspection Methods. John Wiley & Sons, New York. 25-62.
Nielsen, J. (1994b). Enhancing the explanatory power of usability heuristics. Proceedings ACM CHI'94 Conference (Boston, MA, April 24-28), 152-158.
Nielsen, J., and Mack, R. L. (Eds.) (1994).  Usability Inspection Methods. John Wiley & Sons, New York.
Nielsen, J., and Molich, R. (1990). Heuristic evaluation of user interfaces. Proc. ACM CHI'90 (Seattle, WA, April 1-5), 249-256.
Nielsen, J., and Phillips, V. L. (1993). Estimating the relative usability of two interfaces: Heuristic, formal, and empirical methods compared. Proceedings ACM/IFIP INTERCHI'93 Conference (Amsterdam, The Netherlands, April 24-29), 214-221.
Shneiderman, B. (Host) (1993). User Interface Strategies '94. Satellite TV show and subsequent videotapes produced by the University of Maryland's Instructional Television System, College Park, MD.
Wharton, C., Rieman, J., Lewis, C., and Polson, P. (1994). The cognitive walkthrough method: A practitioner's guide. In Nielsen, J., and Mack, R. L. (Eds.), Usability Inspection Methods, John Wiley & Sons, New York, 105-140.
Wixon, D., Jones, S., Tse, L., and Casaday, G. (1994). Inspections and design reviews: Framework, history, and reflection. In Nielsen, J., and Mack, R.L. (Eds.), Usability Inspection Methods, John Wiley & Sons, New York, 79-104."
136,1995-05-25,"Usability lab setup for paper prototyping, card sorting, and traditional usability testing. 
Paper Prototyping
We used several rounds of usability testing to improve the user interface for Sun's new WWW pages. The initial tests focused on the home page and were done using paper prototyping. We also conducted competitive usability tests where users were observed while browsing WWW pages from other companies.

Usability testing with paper prototypes involves printing out screen designs and having users perform tasks using the printed screens.  
For the Initial usability studies done using paper prototyping, we printed out early homepage designs on a color printer. The printouts were magnified to compensate for the poorer quality of the color pixels in the printout and to make it easier for the observers to see what the user was pointing at.
The test was conducted by showing the page to users and asking them to first comment on their general impression of the page and then to point to any element on the page that they thought they could click on and tell us what they expected would happen. This simple method provided us with early feedback indicating the importance of a prominent placement of the month name (since we wanted users to know that the page would change monthly) and the need to make the ""What's Happening"" bar look very clickable.
The photo shows  version B of the homepage ""on the operating table,"" but we ran paper prototype tests of many more versions. It's the beauty of this method that it allows for very rapid iteration and tests of many alternative user interface designs.
A nice trick for paper prototyping (which Meghan Ede suggested to us) is to tape up an area of the desk with masking tape. Marking up an area usually ensures that the user keeps the printed page within the area without a need for the experimenter to try to stop the user from moving the printout around too much. We preferred having the printed page stay within a set area to facilitate video taping as well as observation by other team members in the control room.
Card Sorting to Discover the Users' Model of the Information Space
The category grouping used for the main icons on the home page was derived based on a card sorting study: several users were given a series of index cards with various concepts from the server written on them. The users were then asked to sort cards into piles, placing cards that seemed similar to them into the same pile. The users were also allowed to group piles to indicate looser levels of similarity, and we finally asked the users to name their piles. These names provided us with additional insights into the users' mental model of the information space and served as inspiration for the names we finally chose. Concepts that were placed in the same pile by many users were deemed sufficiently similar that we should place them in the same category in our new design. 
See my article about how we  designed Sun's intranet, SunWeb, in 1994 for another example of card sorting and a photo of a variant of the method, where users are asked to sort cards onto existing categories. (Often called  closed card sorting  to differentiate it from the more common open sorting where users build their own categories without any constraints.)  

For card sorting, different topics are written on cards to be grouped by participants.  
Traditional Usability Test of Running System
Of course, we also used traditional user testing, where users were asked to use a running prototype of the new pages. This is how most usability tests are set up: the user interface that is being tested is running on a computer in the usability lab and the user sits down at the computer and starts working. Normally, we give users a series of set tasks (e.g., find information about the Spring distributed operating system project) and we certainly did so for the web studies. Sometimes, rather primitive HTML mock-ups were used, with many dangling links or links that pointed to pages on the old server that used an obsolete user interface. Even so, we were able to learn a lot about how people use WWW pages. For this particular type of user interfaces we also found it important to let the user spend some time exploring the information space freely: we wanted to see what interface elements the users naturally found interesting without any prompting. We did keep some set tasks since we also wanted to know how usable the design was for people who needed to find specific information.
Most tests were conducted in the usability lab in Mountain View, CA as shown in the photo, though we also conducted three tests in Sun offices in Europe and Asia to assess international usability. The usability lab has a large one-way mirror that allows team members in the control room to observe the test and discuss its outcome without interrupting the test user. Some times the experimenter sits in the control room but my preference is to sit with the user in the lab itself. We have two cameras in the ceiling (not shown) and one camera on a tripod. These cameras are used to record the user's reactions to the interface as well as to film the screen. We do have a scan converter that is used to record a complete screen image, but we often find that there is some extra value from having a camera pointed at the screen because it can record cases where the user points to something on the screen.
It is hard to see on this photo, but to the right of the computer is the lavaliere microphone which we ask users to wear during the test. We have found that the only way to get good sound is to use these clip-on models since table mikes capture too much noise to make it clear for people in the observation room what the user is saying.
A small trick: we have a small table clock to the left of the computer. This clock makes it easy for the experimenter to time the test and to manage the available session time much more unobtrusively than when having to look at a wristwatch.

Traditional usability testing consists of users interacting with the live Web site. 
Usability Test of the Old Web Site
In addition to studying our various new design ideas, we also conducted a usability study of Sun's old WWW design. Of course, we were well aware that it contained many usability problems (e.g., inconsistent headerbars and several very strange imagemaps) but we still wanted to learn what worked well and what worked less well in the old design. One interesting finding was that the top button-bar did not look enough as buttons: the design did not have a clickability affordance but was seen as mere decoration by most users. We redesigned this specific aspect of the old design immediately without waiting for the full redesign to come online. The following figure shows before and after versions of the top row of buttons (""What's New"", etc.) in the old design:


Changing the buttons to make them look more clickable as illustrated above resulted in  416% increased use  over a two-month period (January-March 1995). Considering that the use of the server in general increased by ""only"" 48% in the same period, there can be no doubt that usability engineering worked and resulted in a significantly improved button design."
137,1995-05-25,"Nine iterations of the  home page design
Several iterations of some of the  icon designs
Usability engineering methods used in the design process
Usability issues in the redesign of the same website two years later
Advanced homepage designs that didn't work in 1997 usability testing
Paper on how Darrell Sano and I designed the user interface for  SunWeb (Sun's internal web)

Fundamental Design Concepts
We don't believe that you can succeed on the WWW just by putting some cool stuff out there. Doing so might have been enough when the Web was young: two years before this project, in 1993, I remember visiting the ""What's New with NCSA Mosaic"" page daily to see what new sites were available and getting very excited about the Australian bird songs. But by 1995, who needed another Web site? People are suffocating from information overload, so WWW designers have to become much more user-oriented and provide value-added information to attract traffic to their server.
For the May 1995 design, we decided to provide value-added information in the form of a monthly magazine cover and to be highly selective in choosing a small number of cover stories. Some people don't understand the value of  less is more, but if  everything  is highlighted, then  nothing  has prominence. I estimate that it costs the world economy about half a million dollars in lost user productivity every time we add one more design element to Sun's home page. It is the responsibility of the Web editor to prioritize the information space for the user and to point out a very small number of recommended information objects. The beauty of hypertext is that the user can then browse the information space further and dive deeper into the specific information of interest to that individual user.
Three major findings from our extensive usability studies were:

People have very  little patience  for poorly designed WWW sites. As one user put it: ""the more well-organized a page is, the more faith I will have in the info."" Many other users told us that they would be out of a server, never to return, if they got too many server errors or ""under construction"" signs (a user: ""either the information is there or it is not; don't waste my time with stuff you are not going to give me"").
Users don't want to  scroll: information that is not on the top screen when a page comes up is only read by very interested users. In our tests, users stated that a design that made everything fit on a single page was an indication that the designers had taken care to do a good job, whereas other pages that contained several screen's worth of lists or unstructured material were an indication of sloppy work that made them question the quality of the information contained on those sites.  (Note added December 1997:  this conclusion has changed somewhat due to studies in 1997.) 
Users don't want to  read: reading speeds are more than 25% slower from computer screens than from paper, but that does not mean that you should write 25% less than you would in a paper document. You should write 50% less! Users recklessly skip over any text that they deem to be fluff (e.g., welcome messages or introductory paragraphs) and scan for highlighted terms (e.g., hypertext links).

Acknowledgments
Many people worked on the user interface part of the 1995 redesign of Sun's WWW pages. The members of the UI team were:

Marsh Chamberlain
Debra Coelho
Steve Gibson
Terre Layton
Rick Levine
Maria Marguet
Jakob Nielsen
John Tang

The engineering team, the editorial team, several overseas Sun offices, and the many content providers also contributed significantly to the user interface design. And as always we are grateful for the assistance from our highly capable usability lab staff."
138,1995-01-01,"Heuristic evaluation is a good method for finding both major and minor problems in a user interface. As one might have expected, major problems are slightly easier to find than minor problems, with the probability for finding a given  major  usability problem at 42 percent on the average for single evaluators in six case studies (Nielsen 1992). The corresponding probability for finding a given  minor  problem was only 32 percent.
Even though major problems are easier to find, this does not mean that the evaluators concentrate exclusively on the major problems. In case studies of six user interfaces (Nielsen 1992), heuristic evaluation identified a total of 59 major usability problems and 152 minor usability problems. Thus, it is apparent that the lists of usability problems found by heuristic evaluation will tend to be dominated by minor problems, which is one reason  severity ratings form a useful supplement to the method. Even though major usability problems are by definition the most important ones to find and to fix, minor usability problems are still relevant. Many such minor problems seem to be easier to find by heuristic evaluation than by other methods. One example of such a minor problem found by heuristic evaluation was the use of inconsistent typography in two parts of a user interface. The same information would sometimes be shown in a serif font  (like this one)  and sometimes in a sans serif font  (like this one)  , thus slowing users down a little bit as they have to expend additional effort on matching the two pieces of information. This type of minor usability problem could not be observed in a user test unless an extremely careful analysis were performed on the basis of a large number of videotaped or logged interactions, since the slowdown is very small and would not stop users from completing their tasks.
Usability problems can be located in a dialogue in four different ways: at a single location in the interface, at two or more locations that have to be compared to find the problem, as a problem with the overall structure of the interface, and finally as something that ought to be included in the interface but is currently missing. An analysis of 211 usability problems (Nielsen 1992) found that the difference between the four location categories was small and not statistically significant. In other words, evaluators were approximately equally good at finding all four kinds of usability problems. However, the interaction effect between location category and interface implementation was significant and had a very large effect. Problems in the category ""something missing"" were slightly easier to find than other problems in running systems, but much harder to find than other problems in paper prototypes. This finding corresponds to an earlier, qualitative, analysis of the usability problems that were harder to find in a paper implementation than in a running system (Nielsen 1990). Because of this difference, one should look harder for missing dialogue elements when evaluating paper mock-ups.
A likely explanation of this phenomenon is that evaluators using a running system may tend to get stuck when needing a missing interface element (and thus notice it), whereas evaluators of a paper ""implementation"" just turn to the next page and focus on the interface elements found there.
Alternating Heuristic Evaluation and User Testing
Even though heuristic evaluation finds many usability problems that are not found by user testing, it is also the case that it may miss some problems that can be found by user testing. Evaluators are probably especially likely to overlook usability problems if the system is highly domain-dependent and they have little domain expertise. In some case studies from internal telephone company systems, some problems were so domain-specific that they would have been virtually impossible to find without user testing.
Since heuristic evaluation and user testing each finds usability problems overlooked by the other method, it is recommended that both methods be used. Because there is no reason to spend resources on evaluating an interface with many known usability problems only to have many of them come up again, it is normally best to use iterative design between uses of the two evaluation methods. Typically, one would first perform a heuristic evaluation to clean up the interface and remove as many ""obvious"" usability problems as possible. After a redesign of the interface, it would be subjected to user testing both to check the outcome of the iterative design step and to find remaining usability problems that were not picked up by the heuristic evaluation.
There are two major reasons for alternating between heuristic evaluation and user testing as suggested here. First, a heuristic evaluation pass can eliminate a number of usability problems without the need to ""waste users,"" who sometimes can be difficult to find and schedule in large numbers. Second, these two categories of usability assessment methods have been shown to find fairly distinct sets of usability problems; therefore, they supplement each other rather than lead to repetitive findings (Desurvire et al. 1992; Jeffries et al. 1991; Karat et al. 1992).
As another example, consider a video telephone system for interconnecting offices (Cool et al. 1992). Such a system has the potential for changing the way people work and interact, but these changes will become clear only after an extended usage period. Also, as with many computer-supported cooperative work applications, video telephones require a critical mass of users for the test to be realistic: If most of the people you want to call do not have a video connection, you will not rely on the system. Thus, on the one hand field testing is necessary to learn about changes in the users' long-term behavior, but on the other hand such studies will be very expensive. Therefore, one will want to supplement them with heuristic evaluation and laboratory-based user testing so that the larger field population does not have to suffer from glaring usability problems that could have been found much more cheaply. Iterative design of such a system will be a combination of a few, longer-lasting ""outer iterations"" with field testing and a larger number of more rapid ""inner iterations"" that are used to polish the interface before it is released to the field users.
References

Cool, C., Fish, R. S., Kraut, R. E., and Lowery, C. M. 1992. Iterative design of video communication systems.  Proc. ACM CSCW'92 Conf. Computer-Supported Cooperative Work  (Toronto, Canada, November 1-4): 25-32.
Desurvire, H. W., Kondziela, J. M., and Atwood, M. E. 1992. What is gained and lost when using evaluation methods other than empirical testing. In  People and Computers VII  , edited by Monk, A., Diaper, D., and Harrison, M. D., 89-102. Cambridge: Cambridge University Press. A shorter version of this paper is available in the  Digest of Short Talks presented at CHI'92  (Monterey, CA, May 7): 125-126.
Jeffries, R., Miller, J. R., Wharton, C., and Uyeda, K. M. 1991. User interface evaluation in the real world: A comparison of four techniques.  Proceedings ACM CHI'91 Conference  (New Orleans, LA, April 28-May 2): 119-124.
Karat, C., Campbell, R. L., and Fiegel, T. 1992. Comparison of empirical testing and walkthrough methods in user interface evaluation.  Proceedings ACM CHI'92 Conference  (Monterey, CA, May 3-7): 397-404.
Nielsen, J. 1990. Paper versus computer implementations as mockup scenarios for heuristic evaluation.  Proc. IFIP INTERACT'90 Third Intl. Conf. Human-Computer Interaction  (Cambridge, U.K., August 27-31): 315-320.
Nielsen, J. 1992. Finding usability problems through heuristic evaluation.  Proceedings ACM CHI'92 Conference  (Monterey, CA, May 3-7): 373-380."
139,1994-11-01,"For more information, see the chapters on each of the methods in the book   Usability Inspection Methods (John Wiley & Sons).

Heuristic evaluation is the most informal method and involves having usability specialists judge whether each dialogue element follows established usability principles (the ""heuristics"").
Heuristic estimation  is a variant in which the inspectors are asked to estimate the relative usability of two (or more) designs in quantitative terms (typically expected user performance).
Cognitive walkthrough  uses a more explicitly detailed procedure to simulate a user's problem-solving process at each step through the dialogue, checking if the simulated user's goals and memory content can be assumed to lead to the next correct action.
Pluralistic walkthrough  uses group meetings where users, developers, and human factors people step through a scenario, discussing each dialogue element.
Feature inspection  lists sequences of features used to accomplish typical tasks, checks for long sequences, cumbersome steps, steps that would not be natural for users to try, and steps that require extensive knowledge/experience in order to assess a proposed feature set.
Consistency inspection  has designers who represent multiple other projects inspect an interface to see whether it does things in the same way as their own designs.
Standards inspection  has an expert on an interface standard inspect the interface for compliance.
Formal usability inspection  combines individual and group inspections in a six-step procedure with strictly defined roles to with elements of both heuristic evaluation and a simplified form of cognitive walkthroughs.

Heuristic evaluation, heuristic estimation, cognitive walkthrough, feature inspection, and standards inspection normally have the interface inspected by a single evaluator at a time (though heuristic evaluation is based on combining inspection reports from a set of independent evaluators to form the list of usability problems and heuristic estimation involves computing the mean of the individual estimates). In contrast, pluralistic walkthrough and consistency inspection are group inspection methods. Many usability inspection methods are so easy to apply that it is possible to have regular developers serve as evaluators, though better results are normally achieved when using usability specialists"
140,1995-05-25,"Nine iterations of the  home page design
Several iterations of some of the  icon designs
Usability engineering methods used in the design process
Usability issues in the redesign of the same website two years later
Advanced homepage designs that didn't work in 1997 usability testing
Paper on how Darrell Sano and I designed the user interface for  SunWeb (Sun's internal web)

Fundamental Design Concepts
We don't believe that you can succeed on the WWW just by putting some cool stuff out there. Doing so might have been enough when the Web was young: two years before this project, in 1993, I remember visiting the ""What's New with NCSA Mosaic"" page daily to see what new sites were available and getting very excited about the Australian bird songs. But by 1995, who needed another Web site? People are suffocating from information overload, so WWW designers have to become much more user-oriented and provide value-added information to attract traffic to their server.
For the May 1995 design, we decided to provide value-added information in the form of a monthly magazine cover and to be highly selective in choosing a small number of cover stories. Some people don't understand the value of  less is more, but if  everything  is highlighted, then  nothing  has prominence. I estimate that it costs the world economy about half a million dollars in lost user productivity every time we add one more design element to Sun's home page. It is the responsibility of the Web editor to prioritize the information space for the user and to point out a very small number of recommended information objects. The beauty of hypertext is that the user can then browse the information space further and dive deeper into the specific information of interest to that individual user.
Three major findings from our extensive usability studies were:

People have very  little patience  for poorly designed WWW sites. As one user put it: ""the more well-organized a page is, the more faith I will have in the info."" Many other users told us that they would be out of a server, never to return, if they got too many server errors or ""under construction"" signs (a user: ""either the information is there or it is not; don't waste my time with stuff you are not going to give me"").
Users don't want to  scroll: information that is not on the top screen when a page comes up is only read by very interested users. In our tests, users stated that a design that made everything fit on a single page was an indication that the designers had taken care to do a good job, whereas other pages that contained several screen's worth of lists or unstructured material were an indication of sloppy work that made them question the quality of the information contained on those sites.  (Note added December 1997:  this conclusion has changed somewhat due to studies in 1997.) 
Users don't want to  read: reading speeds are more than 25% slower from computer screens than from paper, but that does not mean that you should write 25% less than you would in a paper document. You should write 50% less! Users recklessly skip over any text that they deem to be fluff (e.g., welcome messages or introductory paragraphs) and scan for highlighted terms (e.g., hypertext links).

Acknowledgments
Many people worked on the user interface part of the 1995 redesign of Sun's WWW pages. The members of the UI team were:

Marsh Chamberlain
Debra Coelho
Steve Gibson
Terre Layton
Rick Levine
Maria Marguet
Jakob Nielsen
John Tang

The engineering team, the editorial team, several overseas Sun offices, and the many content providers also contributed significantly to the user interface design. And as always we are grateful for the assistance from our highly capable usability lab staff."
141,1995-01-01,"Heuristic evaluation is a good method for finding both major and minor problems in a user interface. As one might have expected, major problems are slightly easier to find than minor problems, with the probability for finding a given  major  usability problem at 42 percent on the average for single evaluators in six case studies (Nielsen 1992). The corresponding probability for finding a given  minor  problem was only 32 percent.
Even though major problems are easier to find, this does not mean that the evaluators concentrate exclusively on the major problems. In case studies of six user interfaces (Nielsen 1992), heuristic evaluation identified a total of 59 major usability problems and 152 minor usability problems. Thus, it is apparent that the lists of usability problems found by heuristic evaluation will tend to be dominated by minor problems, which is one reason  severity ratings form a useful supplement to the method. Even though major usability problems are by definition the most important ones to find and to fix, minor usability problems are still relevant. Many such minor problems seem to be easier to find by heuristic evaluation than by other methods. One example of such a minor problem found by heuristic evaluation was the use of inconsistent typography in two parts of a user interface. The same information would sometimes be shown in a serif font  (like this one)  and sometimes in a sans serif font  (like this one)  , thus slowing users down a little bit as they have to expend additional effort on matching the two pieces of information. This type of minor usability problem could not be observed in a user test unless an extremely careful analysis were performed on the basis of a large number of videotaped or logged interactions, since the slowdown is very small and would not stop users from completing their tasks.
Usability problems can be located in a dialogue in four different ways: at a single location in the interface, at two or more locations that have to be compared to find the problem, as a problem with the overall structure of the interface, and finally as something that ought to be included in the interface but is currently missing. An analysis of 211 usability problems (Nielsen 1992) found that the difference between the four location categories was small and not statistically significant. In other words, evaluators were approximately equally good at finding all four kinds of usability problems. However, the interaction effect between location category and interface implementation was significant and had a very large effect. Problems in the category ""something missing"" were slightly easier to find than other problems in running systems, but much harder to find than other problems in paper prototypes. This finding corresponds to an earlier, qualitative, analysis of the usability problems that were harder to find in a paper implementation than in a running system (Nielsen 1990). Because of this difference, one should look harder for missing dialogue elements when evaluating paper mock-ups.
A likely explanation of this phenomenon is that evaluators using a running system may tend to get stuck when needing a missing interface element (and thus notice it), whereas evaluators of a paper ""implementation"" just turn to the next page and focus on the interface elements found there.
Alternating Heuristic Evaluation and User Testing
Even though heuristic evaluation finds many usability problems that are not found by user testing, it is also the case that it may miss some problems that can be found by user testing. Evaluators are probably especially likely to overlook usability problems if the system is highly domain-dependent and they have little domain expertise. In some case studies from internal telephone company systems, some problems were so domain-specific that they would have been virtually impossible to find without user testing.
Since heuristic evaluation and user testing each finds usability problems overlooked by the other method, it is recommended that both methods be used. Because there is no reason to spend resources on evaluating an interface with many known usability problems only to have many of them come up again, it is normally best to use iterative design between uses of the two evaluation methods. Typically, one would first perform a heuristic evaluation to clean up the interface and remove as many ""obvious"" usability problems as possible. After a redesign of the interface, it would be subjected to user testing both to check the outcome of the iterative design step and to find remaining usability problems that were not picked up by the heuristic evaluation.
There are two major reasons for alternating between heuristic evaluation and user testing as suggested here. First, a heuristic evaluation pass can eliminate a number of usability problems without the need to ""waste users,"" who sometimes can be difficult to find and schedule in large numbers. Second, these two categories of usability assessment methods have been shown to find fairly distinct sets of usability problems; therefore, they supplement each other rather than lead to repetitive findings (Desurvire et al. 1992; Jeffries et al. 1991; Karat et al. 1992).
As another example, consider a video telephone system for interconnecting offices (Cool et al. 1992). Such a system has the potential for changing the way people work and interact, but these changes will become clear only after an extended usage period. Also, as with many computer-supported cooperative work applications, video telephones require a critical mass of users for the test to be realistic: If most of the people you want to call do not have a video connection, you will not rely on the system. Thus, on the one hand field testing is necessary to learn about changes in the users' long-term behavior, but on the other hand such studies will be very expensive. Therefore, one will want to supplement them with heuristic evaluation and laboratory-based user testing so that the larger field population does not have to suffer from glaring usability problems that could have been found much more cheaply. Iterative design of such a system will be a combination of a few, longer-lasting ""outer iterations"" with field testing and a larger number of more rapid ""inner iterations"" that are used to polish the interface before it is released to the field users.
References

Cool, C., Fish, R. S., Kraut, R. E., and Lowery, C. M. 1992. Iterative design of video communication systems.  Proc. ACM CSCW'92 Conf. Computer-Supported Cooperative Work  (Toronto, Canada, November 1-4): 25-32.
Desurvire, H. W., Kondziela, J. M., and Atwood, M. E. 1992. What is gained and lost when using evaluation methods other than empirical testing. In  People and Computers VII  , edited by Monk, A., Diaper, D., and Harrison, M. D., 89-102. Cambridge: Cambridge University Press. A shorter version of this paper is available in the  Digest of Short Talks presented at CHI'92  (Monterey, CA, May 7): 125-126.
Jeffries, R., Miller, J. R., Wharton, C., and Uyeda, K. M. 1991. User interface evaluation in the real world: A comparison of four techniques.  Proceedings ACM CHI'91 Conference  (New Orleans, LA, April 28-May 2): 119-124.
Karat, C., Campbell, R. L., and Fiegel, T. 1992. Comparison of empirical testing and walkthrough methods in user interface evaluation.  Proceedings ACM CHI'92 Conference  (Monterey, CA, May 3-7): 397-404.
Nielsen, J. 1990. Paper versus computer implementations as mockup scenarios for heuristic evaluation.  Proc. IFIP INTERACT'90 Third Intl. Conf. Human-Computer Interaction  (Cambridge, U.K., August 27-31): 315-320.
Nielsen, J. 1992. Finding usability problems through heuristic evaluation.  Proceedings ACM CHI'92 Conference  (Monterey, CA, May 3-7): 373-380."
142,1994-11-01,"For more information, see the chapters on each of the methods in the book   Usability Inspection Methods (John Wiley & Sons).

Heuristic evaluation is the most informal method and involves having usability specialists judge whether each dialogue element follows established usability principles (the ""heuristics"").
Heuristic estimation  is a variant in which the inspectors are asked to estimate the relative usability of two (or more) designs in quantitative terms (typically expected user performance).
Cognitive walkthrough  uses a more explicitly detailed procedure to simulate a user's problem-solving process at each step through the dialogue, checking if the simulated user's goals and memory content can be assumed to lead to the next correct action.
Pluralistic walkthrough  uses group meetings where users, developers, and human factors people step through a scenario, discussing each dialogue element.
Feature inspection  lists sequences of features used to accomplish typical tasks, checks for long sequences, cumbersome steps, steps that would not be natural for users to try, and steps that require extensive knowledge/experience in order to assess a proposed feature set.
Consistency inspection  has designers who represent multiple other projects inspect an interface to see whether it does things in the same way as their own designs.
Standards inspection  has an expert on an interface standard inspect the interface for compliance.
Formal usability inspection  combines individual and group inspections in a six-step procedure with strictly defined roles to with elements of both heuristic evaluation and a simplified form of cognitive walkthroughs.

Heuristic evaluation, heuristic estimation, cognitive walkthrough, feature inspection, and standards inspection normally have the interface inspected by a single evaluator at a time (though heuristic evaluation is based on combining inspection reports from a set of independent evaluators to form the list of usability problems and heuristic estimation involves computing the mean of the individual estimates). In contrast, pluralistic walkthrough and consistency inspection are group inspection methods. Many usability inspection methods are so easy to apply that it is possible to have regular developers serve as evaluators, though better results are normally achieved when using usability specialists"
143,1994-11-01,"Severity ratings can be used to allocate the most resources to fix the most serious problems and can also provide a rough estimate of the need for additional usability efforts. If the severity ratings indicate that several disastrous usability problems remain in an interface, it will probably be unadvisable to release it. But one might decide to go ahead with the release of a system with several usability problems if they are all judged as being cosmetic in nature.
The severity of a usability problem is a combination of three factors:

The  frequency  with which the problem occurs: Is it common or rare?
The  impact  of the problem if it occurs: Will it be easy or difficult for the users to overcome?
The  persistence  of the problem: Is it a one-time problem that users can overcome once they know about it or will users repeatedly be bothered by the problem?

Finally, of course, one needs to assess the  market impact  of the problem since certain usability problems can have a devastating effect on the popularity of a product, even if they are ""objectively"" quite easy to overcome. Even though severity has several components, it is common to combine all aspects of severity in a single severity rating as an overall assessment of each usability problem in order to facilitate prioritizing and decision-making.
The following 0 to 4 rating scale can be used to rate the severity of usability problems:
0  = I don't agree that this is a usability problem at all
1  = Cosmetic problem only: need not be fixed unless extra time is available on project
2  = Minor usability problem: fixing this should be given low priority
3  = Major usability problem: important to fix, so should be given high priority
4  = Usability catastrophe: imperative to fix this before product can be released
Severity Ratings in Heuristic Evaluation
It is difficult to get good severity estimates from the evaluators during a  heuristic evaluation  session when they are more focused on finding new usability problems. Also, each evaluator will only find a small number of the usability problems, so a set of severity ratings of only the problems found by that evaluator will be incomplete. Instead, severity ratings can be collected by sending a questionnaire to the evaluators after the actual evaluation sessions, listing the complete set of usability problems that have been discovered, and asking them to rate the severity of each problem. Since each evaluator has only identified a subset of the problems included in the list, the problems need to be described in reasonable depth, possibly using screendumps as illustrations. The descriptions can be synthesized by the evaluation observer from the aggregate of comments made by those evaluators who had found each problem (or, if written evaluation reports are used, the descriptions can be synthesized from the descriptions in the reports). These descriptions allow the evaluators to assess the various problems fairly easily even if they have not found them in their own evaluation session. Typically, evaluators need only spend about 30 minutes to provide their severity ratings. It is important to note that each evaluator should provide individual severity ratings independently of the other evaluators.
Often, the evaluators will not have access to the actual system while they are considering the severity of the various usability problems. It is possible that the evaluators can gain additional insights by revisiting parts of the running interface rather than relying on their memory and the written problem descriptions. At the same time, there is no doubt that the evaluators will be slower at arriving at the severity ratings if they are given the option of interacting further with the system. Also, scheduling problems will sometimes make it difficult to provide everybody with computer access at convenient times if special computer resources are needed to run a prototype system or if software distribution is limited due to confidentiality considerations.
My experience indicates that severity ratings from a single evaluator are too unreliable to be trusted. As more evaluators are asked to judge the severity of usability problems, the quality of the mean severity rating increases rapidly, and using the  mean of a set of ratings from three evaluators  is satisfactory for many practical purposes."
144,1994-01-31,"Orignially published as: Nielsen, J. (1994). Usability laboratories. Behaviour & Information Technology 13, 1&2, 3–8.
Comment added December 1996:  This article was written in 1994 to summarize a special issue on usability laboratories I edited for the journal Behaviour & Information Technology. The special issue itself is well worth reading if you can get hold of it (many large technical libraries subscribe to the journal and should have the issue). The special issue on usability labs was published as a double issue: Behaviour & Information Technology, vol.  13, nos. 1-2, January-April 1994.
Usability is playing a steadily more important role in software development. This can be seen in many ways, including the growing budgets for usability engineering. In 1971 Shackel estimated that a reasonable share for usability budgets for non-military systems was about 3% (Shackel 1971). Later, in 1989, a study by Wasserman (1989) of several corporations found that ""many leading companies"" allocated about 4-6% of their research and development staff to interface design and usability work. Finally, in January 1993, I surveyed 31 development projects and found that the median share of their budgets allocated to usability engineering was 6% (Nielsen 1993). Thus, usability budgets have been steadily growing. Other indications of the added emphasis on usability is the increasing number of personal computer trade press magazines that include usability measures in their reviews and the overwhelming response to the call for papers for this special issue of  Behaviour & Information Technology  on usability laboratories
Assuming that a company has decided to improve the usability of its products, what should it do? Even though this is a special issue on usability laboratories, I am not sure that the answer is to build a usability lab right away. Even though usability laboratories are great resources for usability engineering groups, it is possible to get started with simpler usability methods that can be used immediately on current projects without having to wait for the lab to be built. See Nielsen (1993) for an overview of these methods, which are often referred to as ""discount usability engineering."" Almost always, the result of applying simple usability methods to current projects is a blinding insight that usability methods improve products substantially, making it hard to believe how anybody could ever develop user interfaces without usability engineering. Unfortunately, many people still do just that, and we need cheap and simple methods to enable them to overcome the initial hurdle of starting to use usability engineering.
Once a company has recognized the benefits of the systematic use of usability methods to improve its products, management often decides to make usability a permanent part of the development process and to establish resources to facilitate the use of usability methods by the various project teams. Two such typical usability resources are the usability group and the usability laboratory.
The staffing, management, and organizational placement of usability groups are important issues that must unfortunately be left unresolved here since virtually no research is available to resolve them. Suffice it to say that usability laboratories as a physical resource can be used by the usability groups almost no matter how they are organized. For example, one of the main schisms in the organizational placement of usability specialists is whether to centralize them in a single human factors department or to distribute them as specialized team members of the individual development projects. Some of the arguments in favor of centralized usability departments are that they are better at attracting talented usability staff because of their higher visibility; that they can nurture the special skills of usability specialists by providing an environment focused on usability issues where new techniques are discussed and developed; that they provide a clear management chain (with the ensuing career paths) for usability staff; that they can maintain corporate interface standards and serve as ""interface police"" to ensure consistency; and that they can take an objective view of the user interfaces they are evaluating since the interfaces come from outside departments. Some of the arguments in favor of distributed usability staff is that it is more satisfying for usability specialists to work for an extended time on a single product than to consult briefly on multiple products; that usability specialists on a product team are more likely to contribute to the design efforts throughout the lifecycle rather than just clean up the GUI; that some domains are so complicated that one needs to ""live"" with the product team for an extended period of time to be able to contribute; that usability specialists will only be taken seriously by developers if both groups are part of the same team; and that the communication channels will be shorter (leading to greater productivity) the less organizational distance there is between the produ cers and the consumers of usability knowledge. In spite of the clear distinction between the two types of organizations and the great need to know more about their relative advantages and disadvantages, no research results are currently available to assess what circumstances should lead one to prefer one organization of usability groups over the other.
Interestingly, even though a centralized usability group is the prime candidate to manage a usability lab, the existence of a corporate usability lab is not necessarily an argument in favor of a centralized usability department. It is possible to have a lab supported by a small number of dedicated support staff even while it is being used by usability specialists from a large number of distributed groups. As noted by Dayton, Tudor, and Root in their paper on Bellcore's user-centered-design support center in this issue, a shared usability lab can even serve as a gathering point to provide some of the cross-fertilization and educational benefits to distributed usability specialists that others get from a centralized department.
Usability laboratories are typically used for user testing as discussed in most papers and summarized in the next paper by Salzman and Rivers, ""Smoke and mirrors: Setting the stage for a successful usability test."" There is no doubt that user testing is the main justification for most usability laboratories and that user testing is one of the foundations of usability engineering. Once a usability lab is in place, however, it becomes a convenient resource for many other kinds of usability activities such as focus groups or task analysis. Palmiter, Lynch, Lewis, and Stempski discuss several such non-test forms of data collection in their paper on ""Breaking away from the conventional usability lab,"" and Zirkler and Ballman describe ways of combining focus groups and traditional testing in their paper on ""Usability testing in a competitive market."" Usability laboratories are sometimes also used to record design sessions, though this is mostly done as part of research projects and not as part of practical development projects. One exception is the use of participatory design sessions using methods like PICTIVE (Muller, Wildman, and White 1993) where several users and designers sketch out interface designs using bits of colored paper that is moved around on a desk. With this design technique, much of the design information is never written down, so capturing the dynamics of the design session on video can provide a valuable record for later reference.
Usability laboratories can also be used for certain variants of heuristic evaluation (Nielsen 1994). Heuristic evaluation is based on having usability specialists or other evaluators inspect an interface to find usability problems that violate a list of established usability principles (the ""heuristics"") and does not normally require a special laboratory since the evaluators can work anywhere and are normally supposed to do so individually. Sometimes, however, one wants to have an observer present to log the usability problems discovered by the evaluators so that they do not need to spend time and effort on writing up a report. It may be valuable for this observer to have access to a video record of the evaluation session, and it may also sometimes be advantageous to have developers or other project representatives observe a heuristic evaluation session from behind the one-way mirror in a usability lab. In general, though, only a small minority of heuristic evaluation sessions take place in usability laboratories. 




Company Name
Main Product
Other Labs in Company?
Date of First Usability Lab in Company
Floor Space of Typical Subject Room in Sq. Meters
Floor Space of Total Lab Area in Sq. Meters
Number of Rooms (Subject Rooms, Control Rooms, etc.)
Number of Cameras in Typical Subject Room
Scan Converter Used to Directly Tape Screen Image?
One-Way Mirror?
Usability Staff Supporting vs. Utilizing Lab


Ameritech
Communications service
No
1989
12.5
237
7
2
Yes
Yes
1 / 10


Bellcore
Telco software
Yes
1985
12.3
121
7
2
No
Yes
0.3 / 30


BT (British Telecom)
Telephone service
No
1988
40
96
3
3
Yes
Yes
0.5 / 70


IBM
Computer systems
Yes
1981
11.7
165
14
2
No
Yes
0.1 / 4


MAYA Design Group
Design consultants
No
1990
8.8
42.9
3
2
No
Yes
3 / 12


Microsoft Corp.
PC software
No
1989
10.8
181.3
19
2
Yes
Yes
4 / 22


NCR
Computer systems
Yes
1966
13.4
31.2
3
2
Yes
Yes
2 / 15


Philips, Corp. Design
Consumer electronics
Yes
1990
30
40
2
3
No
Yes
0.25 / 10


Philips, IPO
Consumer electronics
Yes
1990
9
35
3
2
No
No
1 / 25


SAP
Enterprise business apps
No
1992
37
63
2
1
No
Yes
2 / 12


SunSoft
Workstation software
No
1988
25.1
202.3
8
3
Yes
Yes
3.5 / 8


Symantec
PC software
No
1992
23.8
47.6
2
2
Yes
Yes
1 / 1


Taligent
PC operating systems
No
1992
13.4
26.8
2
2
No
Yes
1 / 2


Mean 
38%
1987
19.1
99.2
5.8
2.2
46%
92%
1.5 / 17.0


Median 
No
1989
13.4
63.8
3
2
No
Yes
1 / 12





Table 1. 
Overview of the usability laboratories discussed in this special issue. For each company, the usability laboratory described in this table is the one discussed in the paper in this special issue by authors from that company. For IBM, the lab is the one described by Fath et al., and for SunSoft, the lab is the one described by Rohn.
Notes: The first number in the entry for ""usability staff supporting vs. using the lab indicates"" the headcount of the staff that is dedicated to keeping the lab up and running. The second number in this entry indicates the number of usability specialists who share the lab for their work, even though they may not all be using it full-time. One square meter is 10.76 square feet. Scroll the table to the right to see more data.

The papers in this special issue describe thirteen usability laboratories that are summarized in Table 1. As noted in the table, 38% of the companies have other usability laboratories that are not represented in the table, so it should only be seen as representing a survey of usability labs and not as a complete listing of labs. It can be seen from the table that the defining characteristics of a usability laboratory seem to be video cameras (used in all the labs) and the one-way mirror (installed in 92% of the labs). The table also shows that usability laboratories are a fairly recent phenomenon, with the median year of the first usability lab in these companies being 1989. Of course, some companies have had usability laboratories for a long time. Also, we should note that companies in industries like aerospace and control room design have employed usability laboratories for many years even though they are not represented in the table. The papers in this issue are mostly from the computer and telecommunications industries, though de Vries, van Gelderen, and Brigham write about usability labs for consumer products at Philips. In general, the lessons described in the papers in this issue may apply most directly to the usability of information technology, but there is no reason to believe that most of the same methods could not be used for other types of products and services.
The table shows that the median usability laboratory has three rooms. Normally, the room distribution is a room for the test subject(s), a control room for the experimenter and other usability specialists involved in running the experiment and operating the recording and logging tools, and an ""executive observation lounge"" where additional staff can observe the test without interfering with either the subject or the experimenters. Sometimes additional rooms are used for waiting areas for subjects, and the larger labs also often have special video editing rooms to avoid occupying an entire test suite by using the control room facilities for editing rather than testing. The observers in the executive observation lounge may sometimes in fact be executives, but more commonly they are members of the development team who take the opportunity of the user test to get exposure to the users. Even though the video tape equipment in the labs are often used to produce very communicative highlight tapes of the most notable usability problems and colorful user utterances, there is still no substitute for observing a test live.
It can be seen from the table that some usability labs have a very large number of rooms. Often, this only means that multiple tests can be conducted in parallel, but sometimes larger facilities are used for experiments in computer-supported cooperative work, video conferencing, and other cases where multiple users have to be combined for a single test. Lund's paper on Ameritech's usability laboratory discusses this issue in further depth.
At this time of writing, only slightly less than half of the laboratories surveyed in the table used scan converters to make it possible to tape the computer screen image directly without having to point a camera at it. Scan converters have been somewhat expensive, but since they are dropping in price, their use can be expected to be more widespread in the future.
One of the major advantages of having a usability laboratory is that the incremental hurdle for user testing of a new product becomes fairly small since all the equipment is already in place in a dedicated room that is available for testing. This effect is important because of the compressed development schedules that often leave little time for delay. Thus, if usability testing can be done now and with minimal overhead, it will get done. Similarly, usability may get left out of a project if there is too much delay or effort involved before results become available. Because of this phenomenon, the support staff form a very important part of a usability laboratory in terms of keeping it up and running, stocked with supplies, and taking care of the practical details of recruiting and scheduling test users. In my opinion, the ratio of one support person to twelve usability specialists running tests that is shown as the median in Table 1 is actually too small. I believe that a higher number of support staff are well worth their cost in terms of more efficient usability work (which again leads to a larger amount of usability work being done).
The building of a usability laboratory involves a myriad of decisions and trade-offs. Most of the papers in this special issue touch upon several such issues, and the papers by Sazagari, Rohn, Uyeda, and Neugebauer and Spielmann are particularly detailed. The papers by Lucas and Fisher; Dayton, Tudor, and Root; Lund; and Blatt, Jacobson, and Miller take the detailed needs analysis one step further and describe how they redesigned their second-generation usability laboratories based on experience with older labs and using trusted user-centered design principles to find out what usability specialists really need in their lab.
A usability laboratory does not necessarily need to be a fixed facility in a given set of rooms constructed for the purpose. Szczur describes how she used a regular conference room at NASA as a low-budget usability lab by using part of the room for the subjects and part of the room for the observers. Several papers describe ""portable usability labs"" with video kits and other data logging equipment that can be brought to the field to allow testing, and Zirkler and Ballman emphasize the need to visit customer sites when assessing the usability of systems like their specialized databases for users in the legal and financial communities. Smilowitz, Darnell, and Benson compare standard usability testing in the lab with Beta testing done by having customers test the software at their own sites and report usability problems on their own without supervision by a usability specialist. Even though the Beta tests did have some limitations (notably, that they were restricted to the very last stages of the development lifecycle), they did provide a cheap source of additional usability data that should be considered as a supplement to the lab-based sources.
In addition to the building and equipment of the usability laboratory, an important issue is obviously the actual methods used in running experiments in the lab. Many papers discuss methodology issues, and Fath, Mann, and Holzman provide detailed coverage of five main phases used in evaluation sessions in IBM's Atlanta usability laboratory: designing the evaluation, preparing for it, conducting it, analyzing the data, and reporting the results. One of their conclusions is that as much of the data reduction process as possible should be automated. Usability testing generates huge masses of raw data, and any meaningful conclusions have to be based on analyses that summarize the test events in a comprehensible manner. Hoiem and Sullivan provide detailed information about the integrated set of usability data collection and analysis tools built for Microsoft's lab. In general, it is characteristic for the state of the art that almost all CAUSE tools are homemade for the individual labs. There are probably two reasons for this: First, we still do not know enough about what computerized tools usability professionals actually need, and we know even less about how to make such tools sufficiently usable and efficient. Second, the market is still fairly small, though it is constantly growing and may one day support a wider variety of commercial offerings.
A classic form of CAUSE tool used in many usability labs is the event logger, which is used by an observer to record occurrences of events of interest as the user progresses through the experiment. Typically, events are automatically timestamped (and often linked to a videotape record of the event), and the logger also records the type of event as classified by the human observer in real time -- possibly with a supplementary natural language annotation. In addition to human-coded event logs, usability labs often produce keystroke logs of all user input and activity logs of higher-level dialogue actions (like menu selections, error messages, etc.). These logs can be subjected to many different kinds of analysis, including the sequential data analysis techniques discussed in the paper by Coumo.
Much user testing is simply aimed at generating qualitative insights that are communicated through lists of usability problems and highlights videos showing striking cases of user frustration. Such insights are sufficient for most practical usability engineering applications where the goal is the improvement of a user interface through iterative design. Often, there is no real reason to expend resources of gathering statistically significant data on a user interface that is known to contain major usability problems and has to be changed anyway. By using a probabilistic model of the finding of usability problems and an economic model of project management, Nielsen and Landauer (1993) found that one often gets the optimal cost-benefit ratio by  using between three and five test users for each round of user testing.
User testing with the goal of learning about the design to improve its next iteration falls in the category of formative evaluation. Sometimes, one wants to do summative evaluation that is quantitative in nature and results in numbers that can be compared across products. One case where summative evaluation is desired is the comparative product reviews produced by personal computer magazines like the ones discussed in Bawa's paper. Another application of competitive testing is the selection of recommended (or prescribed) software for a big company where the benefits in terms of reduced training and support and increased productivity are sufficiently large to warrant the investment of considerable resources on choosing the most usable product from the available offerings on the market. To my knowledge, not many user organizations currently perform such competitive usability testing before major software purchases, though there are a few that do. Also, it is becoming fairly common for software vendors to commission third-party usability consultants to perform comparative studies and then advertise the results if their own product wins. Finally, summative evaluation is sometimes used for regular software development projects when one wants to investigate whether a revised product has achieved a sufficient improvement in usability over the previous version. The paper by Bevan and Macleod describes a comprehensive set of measurement tools and principles developed as part of the ESPRIT MUSiC project on usability metrics.
Even though usability metrics can be very elaborate (for example, Bevan and Macleod describe the measurement of heart rate variability as a way of assessing the user's mental effort over time), it is also possible to have a ""discount usability engineering"" approach to usability metrics. As an example, Molich's paper presents a technique for keeping track of the number of ""user interface disasters"" (certain kinds of serious usability problems experienced by at least two users) as a simple, yet quantifiable measure of interface quality.
As shown by the contrast between the full-blown usability metrics and the simplistic count of interface disasters, there are many different possible approaches to usability engineering (Nielsen 1993). It is important to realize that it is possible to achieve major improvements in usability even if one does not utilize all the most advanced techniques and even if one has a fairly primitive usability lab (or sets up a temporary lab in a conference room as described by Szczur). The single-most important decision in usability engineering is simply to do it! The best intentions of some day building the perfect lab will result in exactly zero improvement in current products, and if the choice is between perfection or doing nothing, nothing will win every time. Luckily, it is possible to start small and then grow a usability effort over time as management discovers the huge benefits one normally gets (Ehrlich and Rohn 1994). It is in the nature of things that this special issue mostly has papers on leading usability laboratories from companies that have a better-than-average approach to usability engineering. This does not mean that people in less fortunate companies should abandon usability, it only means that they have something to strive for as they start small and gradually expand their usability groups and usability labs.
Acknowledgments
The main review burden in selecting the papers was shouldered by the following reviewers who had been especially selected for this special issue: Tomas Berns (Nomos Management AB), Linda I. Borghesani (The MITRE Corporation), Joseph S. Dumas (American Institutes for Research), Tom G. Gough (University of Leeds), Lovie Ann Melkus (IBM Consulting Group), James R. Miller (Apple Computer, Inc.), Michele Morris (BNR Europe Ltd.), Kenneth Nemire (Interface Technologies), Markku I. Nurminen (University of Turku), Diane J. Schiano (Interval Research Corporation), Sylvia Sheppard (NASA Goddard Space Flight Center), Nancy Storch (Lawrence Livermore National Laboratory), Paulus-Hubert Vossen (Fraunhofer-Institut IAO), and David Ziedman (Philips Interactive Media of America). Additional ad-hoc reviews were provided by: Rita M. Bush (Bellcore), Tom Dayton (Bellcore), Arye R. Ephrath (Bellcore), Richard D. Herring (Bellcore), Arnold M. Lund (Ameritech), Michael J. Muller (U S WEST Advanced Technologies), and Estela M. Tice (Bellcore).
References

Bias, R. G. and Mayhew, D. J. (Eds.) 1994,  Cost-Justifying Usability (Academic Press, Boston, MA).
Ehrlich, K. and Rohn, J. 1994, Cost-justification of usability engineering: A vendor's perspective, in Bias, R.G., and Mayhew, D.J. (Eds.), Cost-Justifying Usability (Academic Press, Boston, MA).
Muller, M. J., Wildman, D. M., and White, E. A. 1993, ""Equal opportunity"" PD using PICTIVE.  Communications of the ACM   36, 4, 64-66.
Nielsen, J. 1993, Usability Engineering (Academic Press, Boston, MA).
Nielsen, J. 1994, Heuristic evaluation, in Nielsen, J., and Mack, R. L. (Eds.),   Usability Inspection Methods. (John Wiley & Sons, New York, NY), 25-64.
Nielsen, J. and Landauer, T. K. 1993, A mathematical model of the finding of usability problems,  Proceedings of the ACM INTERCHI'93 Conference  (Amsterdam, the Netherlands, April 24-29), 206-213.
Shackel, B. 1971, Human factors in the P.L.A. meat handling automation scheme. A case study and some conclusions.  International Journal of Production Research   9, 1, 95-121.
Wasserman, A. S. 1989, Redesigning Xerox: A design strategy based on operability. In Klemmer, E. T. (Ed.),  Ergonomics: Harness the Power of Human Factors in Your Business, Ablex, Norwood, NJ. 7-44."
145,1994-01-01,"One of the basic questions during the development of a computer system and its user interface is  what the users will want to do with the system. Unfortunately, a task analysis of users' current activities is not sufficient to predict what they will do in the future. It is well known that people's use of computers change over time and that new and unexpected uses are found for most new systems. A famous historical example is the introduction of the ARPANET computer network which led to a surprising widespread use of electronic mail even though it had been primarily intended for remote logins and the sharing of computers and databases. Another example of unexpected use of a computer system is the way some users build up fairly large databases in spreadsheet programs because they find them easier to use than regular database programs.

	One way to handle this problem would be to implement the basic features for which a user need has been established and then release updated versions as additional user needs become apparent. Since software maintenance is expensive, it is better to include as large as possible a proportion of the eventual feature set in the initial software design. From an interface design perspective, a comprehensive initial feature list will result in a cleaner and more integrated design than a system that has been extensively retrofitted. Unfortunately, it is impossible to predict every feature users will ever need or want in the future with complete accuracy. Even so, it is still useful to have a partial solution that would suggest a number of features that users would likely want, given that it has already been determined that they need certain features.

	This essay describes a technique for extending a task analysis based on the goal composition principle suggested by  Clayton Lewis on page 134 of his article ""A research agenda for the nineties in human-computer interaction"" in  Human-Computer Interaction  vol. 5 no. 2. Basically, goal composition starts by considering each primary goal that the user may have when using the system. A list of possible additional features is then generated by combining each of these goals with a set of general meta-goals that extend the primary goals. These general meta-goals are called goal composition mechanisms. As a non-computer example, a goal composition mechanism for cooking would be to find a way to serve the food. Thus, the primary goal of preparing coffee can be extended to the further goal of getting a coffee cup.

	For the following discussion of goal composition, electronic mail will be used as an example to illustrate how each mechanism can be used to generate further potential user goals. The basic feature of email is to send a message to another user in the form of a text file. An initial task analysis might have shown that email can be useful in many cases such as when the other user is located far away or is hard to reach and that the basic user goals are sending and receiving messages.

	The goal composition mechanisms described here are fairly general and will apply to most of the features of most computer systems. Four of these mechanisms were described by Clayton Lewis and an additional nine mechanisms were discovered through further analysis. It is an open research question to what degree the list is complete in the sense that no more goal composition mechanisms should be added. Additional goal composition mechanisms might be appropriate for special application domains or interaction styles, but the focus of this column is on generally applicable mechanisms. The three main categories of goal composition are generalization, integration, and user control.

	Generalization Mechanisms

	Generalization allows users to increase the scope of a feature by applying it to more objects. The three generalization mechanisms are multiplexing, reuse, and supergoaling.

	Multiplexing

	Multiplexing refers to the ability to have many instances of a goal be achieved as a single goal. For example, when sending email, one often wants to send to many users in a single operation, so multiplexing would at least imply the ability to specify many recipients for a given message. Extended support for email multiplexing would involve features such as distribution lists to allow the user to refer to many recipients by a single name.

	Reuse

	Reuse enables the user to utilize part of the work towards one goal when wanting to achieve a similar goal. For example, a spreadsheet user who has generated a spreadsheet formula to sum one row might have the further goal of copying this formula to sum other rows. A simplistic copy and paste implementation which allowed the user to move the formula would not achieve this reuse goal since the copies of the formula would still sum the original row. Most spreadsheets therefore provide some operation for copying a formula while updating it to refer to a new set of cells.

	In the email example, users should be allowed to resend an old message, while possibly changing the list of recipients and editing the text. Note that this reuse of old messages would be significantly more useful if the user had an easy way to find out what previous messages were archived and had an easy way to retrieve a specific message according to a variety of criteria. Reuse may thus often take place in combination with a recording and retrieving mechanism as further discussed below.

	Supergoaling

	Supergoaling involves the use of any particular goal as a subgoal for some larger activity. Even without any particular supergoaling support, most goals form part of a goal hierarchy since they are achieved for some higher purpose. Supergoaling may be supported by programmable interfaces that allows users to combine features from multiple application to a single task, or they may be supported by linking and scripting mechanisms that will allow third-party applications to access the features of the program currently under development.

	Email is not just a stand-alone application but also a support mechanism for many other user goals. Email-enabled applications are currently all the rage, indicating that application vendors have discovered that email can be a subgoal for many other activities: when you have created a spreadsheet, you may want to share it with others. Similarly, a World-Wide Web browser might include a way for the user to send a hypertext link to another user with a recommendation to look it up. As another example, if we were designing a spelling checker for word processing, it might be reasonable to supergoal it for use in other contexts such as email authoring.

	Integration Mechanisms

	Integration mechanisms aim at allowing each individual feature to be used in combination with other system facilities. Some of these goal composition mechanisms may be supported automatically by multiprocessing window systems, but even so, a system designer may have to consider how to support them better than the basic level provided by the system. The four integration mechanisms are interleaving, suspension and postponement, result passing, and automated use.

	Interleaving

	Interleaving goals allows the user to accomplish other goals at the same time as the particular goal that is being analyzed. When composing email users may have the goal of checking their online calendar and this may be accomplished by allowing multiple windows to be open. Users may furthermore want to add an appointment received over email to the calendar, and this goal might be achieved by drag-and-drop if the calendar can parse the text of the appointment message. Even better, a fully object-oriented email system may know that an appointment object would need to be scheduled and would allow users to do so without calling up the calendar: upon receiving the incoming email message, the system would recognize an appointment object and would check the user's calendar to see whether the user was available or busy and would inform the user of the result when the user opened the message to read it.

	Suspension and Postponement

	Users should be allowed to suspend work towards goal in order to resume the activity later. When writing an email message, the user might need to find additional information or the user might just want to procrastinate. In any case, the user should be allowed to save whatever work has already been done on the email message in order to return to it later. Most email systems at least allow users to go to other windows before completing the message (that is, the user can interleave other tasks), but the email system should also allow the user to close message windows and have several unfinished messages stored for later completion.

	Even if the user finishes work on the task, the user may not want the resulting action to happen until some additional condition has been met. In that case, the user should be allowed to postpone completion of the goal until the conditions are met. For example, when writing an email message reminding people to attend a meeting, the user should be able to tell the system to send the mail an hour before the meeting is scheduled to start. Suspension and postponement features imply the additional need for methods of learning what unfinished goals are present in the system. For suspended goals, the user will need to either resume or delete the unfinished work, and for postponed goals, changing conditions may cause the user to revise their request for future action. For example, if the meeting was cancelled, the user would not want to have the reminder sent out.

	Result Passing

	Result passing involves using the result of one goal to help achieve some other goal. In email, the goal of reading a message often gives way to the goal of sending a reply message to the originator of the first message, and many email programs therefore include a special reply function that passes the address of the originator from the reader interface to the sender interface. Reply functions often also offer the ability to include parts of the original message in the reply message for easy reference during discussions. Even though simple copy-and-paste commands can be often be used for result passing, the inclusion of a quoted message in email normally involves some slight additions to the text to indicate its status as a quotation.

	A practical design guideline derived from the result passing principle is an ability for programs to save their state in standard formats as far as possible so that the result of using them can be transferred to as many other programs as possible. For example, a word processor might have the ability to save documents as plain ASCII files in addition to various formats with more fancy formatting.

	Automated Use

	Automated use requires the ability to have other programs activate the new facility and use it to achieve the goal. By making this possible, many new types of integration and an additional level of flexibility become possible, since these other programs might do anything that can be programmed. If users want to achieve a given goal interactively, then this goal might presumably also be relevant for other programs used by those users. Note that automated use is closely related to the issue of supergoaling discussed under generalization mechanisms. The difference is that supergoaling involves having the users construct larger goals for their dynamically changing purposes (possibly through a macro scripting language) whereas automated use is performed by the computer without explicit human intervention.

	The above example of supergoaling a spelling checker to allow it to be accessed from the email program might be extended to an example of automated use where the computer would check the spelling of all email messages without being asked for it.

	Automated use of email would allow a third party program to generate a complete email message, complete with a header specifying the recipient's complete address. A more accommodating email facility would enable the other programs to send email using aliases, mailing lists, and any other simplifying features normally made available to an interactive user. As an example, if the license cost of a software system was dependent on how often it was used, the system could automatically email its vendor each month with information on its use. A less benevolent example is the infamous IBM Christmas card chain letter that was automatically forwarded to all users in the recipient's alias directory, and this example obviously indicates the need for some safeguards on automated use of features like email.

	User Control Mechanisms

	User control mechanisms allow users to inspect and change the way the computer carries out their instructions. The six user control mechanisms are monitoring, result investigation, recording and retrieving, alternative enumeration, reverting, and modification and editing.

	Monitoring

	After the user has asked the computer to perform a task, the user may want to determine whether the goal has been achieved or what progress is being made toward the goal. For the goal of sending email to a recipient, obvious monitoring goals would be to check whether the message has made it to the other end and whether the recipient has seen it yet.

	Result Investigation

	Users sometimes have difficulty in predicting exactly what a given computer command will do, and result investigation features may therefore be needed to explain the outcome of user actions to enable users to evaluate whether they actually did what they wanted to do. Of course, providing feedback is one of the more basic user interface design guidelines, but result investigation goes one step further than regular feedback and is only activated upon explicit user request. Also, result investigation might be initiated in a predictive mode before the user has committed to performing the action. For email, result investigation might allow the user to expand aliases and mailing lists to get a full list of all the recipients before sending a message.

	Recording and Retrieving

	In addition to doing things, users often also want to keep a record of what was accomplished. For traditional command-line interfaces, this record is clearly visible by scrolling back the window, but for most GUIs, actions are only visible while they are being carried out, so an additional feature is needed to access the interaction history. In our email example, users often want to know what mail has been sent to a given recipient or exactly who received a given message, so they need to be able to access a log of their outgoing mail. Of course, recording user actions implies the further need for mechanisms to  retrieve  the recorded information according to a variety of search criteria (one of which will normally be time).

	Alternative Enumeration

	Often, several possible user goals are very closely related and it would be difficult for the user to remember the exact difference between the desired outcomes. In these cases, the user should be able to express the goal in general terms and have the computer enumerate the available alternatives. If the user wants to send email to Joe Something, the computer should allow the user to enter ""Joe"" and then show a list of the seven Joes in the user's alias list.

	Reverting

	Users may sometimes realize that their actions resulted in undesired consequences, and they may therefore want to revert to the system state that was in effect before the goal was accomplished. This mechanism is often realized by an undo command. For email, it may be impossible to retract statements that have already been read by the recipient, but it should certainly be possible to cancel a previously sent message if it has not been read yet.

	Modification and Editing

	In addition to the ability to cancel an operation and start over, users often need the ability to modify the outcomes of prior operations. If an email message has bounced due to an address error, the user should be able to resend it and edit the address without having to retype the address all over (with the possibility of new typos). Notice how a ""resend"" feature might be implemented by a combination of the standard features for ""retrieve"" (message from the record of outgoing mail) and ""reuse"" (this message as the basis for a new one). Even so, users might still like to have an explicit ""resend"" feature that would eliminate the need to search for the original message.

	Another example that highlights the need for editing is mail forwarding: Users may want to add their own comments or remove irrelevant parts of the message before sending it on.

	Conclusions

	Here is a list of the goal composition mechanisms discussed in this essay:


		Generalization Mechanisms
		

				Multiplexing

				Reuse

				Supergoaling



		Integration Mechanisms
		

				Interleaving

				Suspension and Postponement

				Result Passing

				Automated Use



		User Control Mechanisms
		

				Monitoring

				Result Investigation

				Recording and Retrieving

				Alternative Enumeration

				Reverting

				Modification and Editing




	The list can be used as a checklist when designing new computer features to make sure that one has considered these possible extensions of the new features. Not all composition mechanisms apply to all the goals one might want to support on a computer. Also, one should be careful not to overload an interface with too many features. Even so, the experience of designers who have used my checklist indicates that it can be useful in rounding out ideas for new features to make sure that relevant extensions have not been overlooked.

	As will be apparent from the examples, the features suggested by goal composition can often be designed in quite different ways. The goal composition technique can only suggest the areas to which a designer should pay attention. It will then be the responsibility of the designer to determine the best way to extend the basic user goal with additional features suggested by goal composition. Furthermore, the design of real applications needs to consider the extent to which particular features will actually be used since it is often best to drop unimportant features. The goal composition technique cannot offer predictions of the relative importance and frequency of use of system features it suggests."